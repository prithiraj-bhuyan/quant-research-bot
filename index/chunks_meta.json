[
  {
    "text": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of\nGeneral-Purpose Large Language Models\nBoyu Zhang2\u2217, Hongyang (Bruce) Yang1\u2217, Xiao-Yang Liu1\u2020\n1Columbia University; 2Swinburne University of Technology\n{HY2500, XL2427}@columbia.edu; boyu.zhang68@gmail.com\nAbstract\nSentiment analysis is a vital tool for uncovering in-\nsights from financial articles, news, and social me-\ndia, shaping our understanding of market move-\nments. Despite the impressive capabilities of large\nlanguage models (LLMs) in financial natural lan-\nguage processing (NLP), they still struggle with ac-\ncurately interpreting numerical values and grasp-\ning financial context, limiting their effectiveness in\npredicting financial sentiment. In this paper, we\nintroduce a simple yet effective instruction tuning\napproach to address these issues. By transform-\ning a small portion of supervised financial senti-\nment analysis data into instruction data and fine-\ntuning a general-purpose LLM with this method,\nwe achieve remarkable advancements in financial\nsentiment analysis.\nIn the experiment, our ap-\nproach outperforms state-of-the-art supervised sen-\ntiment analysis models, as well as widely used\nLLMs like ChatGPT and LLaMAs, particularly in\nscenarios where numerical understanding and con-\ntextual comprehension are vital.\n1\nIntroduction\nFinancial sentiment analysis, the task of discerning investor\nsentiment from financial articles, news, and social media, is\nan essential instrument for comprehending and forecasting\nmarket movements. Conventional models often struggle with\nseveral difficulties, including insensitivity to numeric values,\ndifficulties interpreting sentiment without explicit context,\nand the challenges associated with financial jargon, multi-\nlingual data, temporal dependency, insufficient labeled data,\nand the inherent noise in social media data.\nLarge Language Models (LLMs) have been pivotal in mit-\nigating some of these challenges, demonstrating a significant\ncontribution to the field of financial natural language pro-\ncessing (NLP). One distinguishing feature is LLMs\u2019 inherent\ngeneral knowledge garnered during pre-training on vast and\ndiverse corpora, including financial texts. However, LLMs\ndo not have enough financial context. Their performance in\n\u2217Equal contribution.\n\u2020Corresponding author.\ninterpreting numerical values is often inadequate and may\nstruggle to accurately determine sentiment when the context\nis absent or ambiguous. These challenges underline the need\nfor improved models that can adeptly understand the intricate\nnuances of financial sentiment analysis.\nIn response to these challenges, our study explores the po-\ntential of instruction tuning of general-purpose LLMs for sen-\ntiment analysis in the finance sector. In this study, we investi-\ngate two primary research questions: 1) How to enable LLMs\nto address the issue of numerical sensitivity in financial sen-\ntiment analysis? and 2) What is the role of contextual un-\nderstanding in improving financial sentiment analysis?\nWe propose Instruct-FinGPT by instruction tuning [Wei et\nal., 2022] a pre-trained LLM (namely LLaMA [Touvron et\nal., 2023]). Through this approach, we transform the classifi-\ncation based sentiment analysis dataset into a generation task,\nthereby allowing LLMs to apply their extensive training and\nsuperior analytical capabilities more effectively. The ultimate\ngoal of Instruct-FinGPT is to enhance the performance in fi-\nnancial sentiment analysis by minimizing the requirement of\nfine-tuning data and maximizing the contextual understand-\ning and numerical sensitivity inherent to LLMs.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3623,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "al., 2022] a pre-trained LLM (namely LLaMA [Touvron et\nal., 2023]). Through this approach, we transform the classifi-\ncation based sentiment analysis dataset into a generation task,\nthereby allowing LLMs to apply their extensive training and\nsuperior analytical capabilities more effectively. The ultimate\ngoal of Instruct-FinGPT is to enhance the performance in fi-\nnancial sentiment analysis by minimizing the requirement of\nfine-tuning data and maximizing the contextual understand-\ning and numerical sensitivity inherent to LLMs. By introduc-\ning this novel method, we aspire to push the boundaries of\ncurrent methodologies, opening up promising avenues for fu-\nture exploration in the realm of financial sentiment analysis.\nThe primary contributions of this paper are as follows:\n\u2022 We design an instruction-tuned FinGPT model for fi-\nnancial sentiment analysis. This model surpasses both\ngeneral-purpose LLMs and state-of-the-art supervised\nmodels in benchmark performance, despite utilizing\nonly a small amount of instruction data and training re-\nsources.\n\u2022 We address the critical issue of numerical sensitivity\nin financial sentiment analysis, a component often ne-\nglected by existing models, enhancing the model\u2019s abil-\nity to accurately interpret sentiment from financial news.\n\u2022 We underscore the importance of contextual understand-\ning in financial sentiment analysis, leveraging the inher-\nent general knowledge of LLMs for improved perfor-\nmance in sentiment analysis, especially when the con-\ntext is missing or vague.\nOur study provides new insights into the application of\narXiv:2306.12659v1 [cs.CL] 22 Jun 2023\n\nLLMs for financial sentiment analysis, offering potential so-\nlutions to some of the enduring challenges in the field.\n2\nRelated Work\nThe task of sentiment analysis, particularly in the financial\ndomain, has been a significant area of research in the field\nof Natural Language Processing (NLP). There are several\nworks in literature [Xing et al., 2018; Loughran and Mc-\nDonald, 2011; Tai and Kao, 2013; Hamilton et al., 2016;\nDay and Lee, 2016; Chan and Chong, 2017; Sohangir et al.,\n2018; Araci, 2019; Mishev et al., 2020] that utilize different\nmethodologies for performing financial sentiment analysis,\nranging from lexicon-based techniques to machine learning\nand deep learning approaches.\nOne noteworthy work is by Araci [Atkins et al., 2018],\nwhich presents an ensemble of traditional machine learning\nalgorithms for predicting the direction of stock market move-\nment based on financial news articles. While this work made\nstrides in using machine learning for financial sentiment anal-\nysis, it does not extensively address the challenges related to\nnumerical sensitivity or contextual understanding.\nIn terms of deep learning approaches, the transformer-\nbased model BERT [Kenton and Toutanova, 2019] has been\nwidely used for sentiment analysis tasks due to its power-\nful context understanding capability. However, BERT and its\nderivatives typically require substantial amounts of labeled\ndata for fine-tuning, which might be challenging to obtain in\nthe financial domain.\nMore recently, FinBERT [Araci, 2019], a variant of BERT\ndesigned explicitly for the financial domain, was developed\nto address these issues. FinBERT has been fine-tuned on the\nfinancial text and has shown promising results in financial\nsentiment analysis.",
    "chunk_index": 1,
    "start_char": 3090,
    "end_char": 6461,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "been\nwidely used for sentiment analysis tasks due to its power-\nful context understanding capability. However, BERT and its\nderivatives typically require substantial amounts of labeled\ndata for fine-tuning, which might be challenging to obtain in\nthe financial domain.\nMore recently, FinBERT [Araci, 2019], a variant of BERT\ndesigned explicitly for the financial domain, was developed\nto address these issues. FinBERT has been fine-tuned on the\nfinancial text and has shown promising results in financial\nsentiment analysis. Nonetheless, it suffers from limitations\nsuch as insensitivity to numerical values and struggles with\nthe context where the necessary information may be missing.\nFLANG [Shah et al., 2022] additionally presents financial as-\nsessment benchmarks across five distinct NLP tasks within\nthe financial sector, along with the incorporation of conven-\ntional benchmarks prevalent in prior research.\nWhile BloombergGPT [Wu et al., 2023] demonstrates im-\npressive performance in sentiment analysis tasks, there are\ninherent challenges to its accessibility and applicability for\nbroader usage. The model, proprietary to Bloomberg, was\ntrained on a vast corpus of specialized financial data, which\nmay not be readily available to others.\nMoreover, the re-\nsources required to train such a model are substantial (1.3M\nGPU hours, a cost of around $5M). This is in contrast to\nour approach, which demonstrates substantial effectiveness\nwith a significantly smaller corpus and less computational re-\nsources (estimated around less than $300 per training), mak-\ning it more feasible for wider deployment.\nOur work stands distinct in its focus on leveraging the\npower of LLMs, their inherent general knowledge, and rea-\nsoning capabilities to perform sentiment analysis in the finan-\ncial domain. We explore a novel instruction tuning approach\nand demonstrate its effectiveness in our experiments.\n3\nOur Method\nDespite the pre-trained LLMs such as GPT-3 and LLaMA\ncan acquire the general abilities for solving various tasks, in-\ncreasing studies have shown that LLM\u2019s abilities can be fur-\nther adapted according to specific goals. Our approach uses\ninstruction tuning to adapt the general-purpose LLMs to fi-\nnancial sentiment analysis, enhancing their understanding of\nnumerical values and context in this specific task. The pro-\ncess involves transforming the sentiment analysis task from a\nclassification task to a text generation task, which aligns bet-\nter with the capabilities of LLMs. Further, we use the trans-\nformed dataset to instruction finetune the LLMs in a super-\nvised learning way. Last, we map the generated outputs into\nsentiment labels during inference.\n3.1\nInstruction Tuning\nWe adopt the instruction tuning method of an LLM on finan-\ncial sentiment analysis datasets. This process is divided into\nthree main steps:\nFormatting Financial Sentiment Analysis Dataset into\nInstruction Tuning Dataset\nThe existing financial sentiment analysis datasets are format-\nted as text classification task where the inputs are the finan-\ncial news or headlines and the outputs are integer-type la-\nbels representing positive, negative and neutral sentiments.\nOur first step is to formulate these classification datasets into\ninstruction-formatted dataset.",
    "chunk_index": 2,
    "start_char": 5937,
    "end_char": 9207,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "This process is divided into\nthree main steps:\nFormatting Financial Sentiment Analysis Dataset into\nInstruction Tuning Dataset\nThe existing financial sentiment analysis datasets are format-\nted as text classification task where the inputs are the finan-\ncial news or headlines and the outputs are integer-type la-\nbels representing positive, negative and neutral sentiments.\nOur first step is to formulate these classification datasets into\ninstruction-formatted dataset.\nFollowing [Zhao et al., 2023], we create 10 human-written\ninstructions describing the task of financial sentiment anal-\nysis, and formulate each sample from the original dataset by\ncombining one randomly selected instruction with the input\nand output in the format of \u201dHuman: [instruction] + [input],\nAssistant: [output]\u201d. This process is shown in Fig 1.\nInstruction Tuning LLaMA-7B\nWhile pretrained LLMs possess capabilities such as reason-\ning, understanding numbers, world knowledge, and multilin-\ngualism, they struggle to effectively apply these abilities to\nspecific tasks. This limitation hinders their ability to achieve\nstate-of-the-art (SOTA) performance on specific tasks, thus\nrestricting their application potential. For instance, [Wei et\nal., 2022] found that the zero-shot performance of LLMs is\nsignificantly lower compared to their few-shot performance.\nIn our scenario, we leverage instruction data, which typically\nincludes numeric values, financial context, and financial jar-\ngon, to provide supervised signals. Through instruction tun-\ning, we align the LLM\u2019s capabilities with the sentiment anal-\nysis labels, achieving a more precise and nuanced understand-\ning of sentiments expressed in financial texts which enables it\nto outperform both pretrained LLMs and supervised models\nspecifically designed for financial sentiment analysis.\nWe illustrate our approach using instruction tuning with\nthe LLM model called LLaMA-7B as an example to validate\nour ideas. Instruction tuning involves fine-tuning pre-trained\nLLMs by leveraging a collection of formatted instances in\nnatural language [Wei et al., 2022]. It is a method closely\naligned with supervised fine-tuning. During the training pro-\ncess, we specifically employ the formatted instances to fine-\n\n \nConsumer credit $18.9BN, Exp. \n$16BN, Last $9.6BN.\nEstee Lauder Q2 adj. EPS \n$2.11; FactSet consensus $1.90.\nThe situation of coated \nmagazine printing paper \nwill continue to be weak.\nSentence\n2\n1\n0\nLabel\n \nHuman: Determine the sentiment of the financial news a\ns negative, neutral or positive: Consumer credit \n$18.9BN, Exp. $16BN, Last $9.6BN.\nPrompt\nPositive\nNeutral\nNegative\nResponse\nHuman: Classify the tone of the financial news as \npositive, neutral, or negative: Estee Lauder Q2 adj. EPS \n$2.11; FactSet consensus $1.90.\nHuman: Analyze the sentiment of the financial news as \nneutral, positive, or negative: The situation of coated \nmagazine printing paper will continue to be weak.\nPrompt \nTemplates\nFormatting\nFigure 1: Formatting sentiment analysis dataset into instruction tuning dataset.\ntune the LLaMA-7B LLM using a supervised learning ap-\nproach, i.e., training with a sequence-to-sequence loss. This\nchoice allows us to showcase the effectiveness and applicabil-\nity of instruction tuning in enhancing the financial sentiment\nanalysis performance of LLMs like LLaMA-7B.\nMapping the Generated Outputs into Sentiment Labels\nSince the instruction finetuned LLaMA-7B is an autoregres-\nsive generative model, even though we train it",
    "chunk_index": 3,
    "start_char": 8736,
    "end_char": 12233,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "to be weak.\nPrompt \nTemplates\nFormatting\nFigure 1: Formatting sentiment analysis dataset into instruction tuning dataset.\ntune the LLaMA-7B LLM using a supervised learning ap-\nproach, i.e., training with a sequence-to-sequence loss. This\nchoice allows us to showcase the effectiveness and applicabil-\nity of instruction tuning in enhancing the financial sentiment\nanalysis performance of LLMs like LLaMA-7B.\nMapping the Generated Outputs into Sentiment Labels\nSince the instruction finetuned LLaMA-7B is an autoregres-\nsive generative model, even though we train it using instruc-\ntion templates to guide its output towards the desired senti-\nment judgments, it still has the possibility of generating free-\nstyle text. Therefore, we need to map the model\u2019s output back\nto the specified three emotions for proper evaluation. Our ap-\nproach is as follows: if the model\u2019s output contains \u201dpositive,\u201d\n\u201dnegative,\u201d or \u201dneutral\u201d terms, we map it to the corresponding\nlabel; otherwise, we consider it as \u201dneutral\u201d sentiment.\n3.2\nComparison Between LLMs and FinBERT for\nSentiment Analysis\nOur approach employs LLMs and compares their efficacy in\nsentiment analysis with the well-established FinBERT model.\nThe comparison is based on three pivotal aspects:\n\u2022 Contextual understanding: LLMs have an advantage\ndue to their large-scale pretraining on diverse data.\nThis provides them with a more comprehensive gen-\neral knowledge, enabling a superior understanding of the\ncontext compared to FinBERT. The diversity and rich-\nness of the training datasets of LLMs are unmatched,\nproviding them with a well-rounded knowledge that out-\nshines FinBERT\u2019s capability.\n\u2022 Numerical sensitivity: Financial texts often incorporate\nsignificant numerical data, which plays a crucial role\nin conveying the sentiment. LLMs, with their inherent\nnumerical sensitivity, exhibit an enhanced capacity for\ninterpreting the sentiment implied by numerical fluctu-\nations. Refer to certain scholarly reports for in-depth\nstudies on this characteristic of LLMs.\n\u2022 Decoder-only vs encoder-only models: FinBERT is an\nencoder-only model which encodes the input sequence\ninto a representation and relies on a separate classifier to\nmake predictions based on the encoded representation.\nOn the other hand, the employed LLM is a decoder-only\nmodel which can generate the entire output sequence,\nincluding the class label, directly from a latent represen-\ntation or fixed-length vector. This character allows the\nLLMs easily adapt to various tasks without modifying\nthe model structure while the encoder-only models re-\nquire the development of task-specific classifiers, which\ncan be more labor-intensive.\n4\nPerformance Evaluation\nIn this section, we evaluate the effectiveness of our proposed\nmethod from three perspectives: general sentiment analy-\nsis, numerical understanding, and general knowledge supple-\nmenting. To validate our method\u2019s performance, we compare\nit against state-of-the-art sentiment analysis model, FinBERT,\nand the general-purpose LLM, ChatGPT.\nOur experimental results validate the effectiveness of our\napproach. With only a small amount of fine-tuning data, our\nmodel consistently achieves superior performance in senti-\nment analysis compared to FinBERT and ChatGPT.\n4.1\nDatasets\nOur training data is an amalgamation of the Twitter Finan-\ncial News dataset [Magic, 2022] and FiQA dataset [Maia et\nal., 2018], resulting in a comprehensive collection of 10, 501\nsamples.",
    "chunk_index": 4,
    "start_char": 11668,
    "end_char": 15122,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "performance, we compare\nit against state-of-the-art sentiment analysis model, FinBERT,\nand the general-purpose LLM, ChatGPT.\nOur experimental results validate the effectiveness of our\napproach. With only a small amount of fine-tuning data, our\nmodel consistently achieves superior performance in senti-\nment analysis compared to FinBERT and ChatGPT.\n4.1\nDatasets\nOur training data is an amalgamation of the Twitter Finan-\ncial News dataset [Magic, 2022] and FiQA dataset [Maia et\nal., 2018], resulting in a comprehensive collection of 10, 501\nsamples.\nTraining Datasets\n\u2022 Twitter financial news sentiment training:\nThis\ndataset is a corpus of news tweets that pertain to the fi-\nnancial sector and is exclusively in English. Its primary\npurpose is the classification of financial sentiment within\nthe context of Twitter discussions.\nThe dataset com-\nprises 9,540 samples for training, each annotated with\none of three labels: Bearish, Bullish, or Neutral.\n\u2022 FiQA dataset: This dataset, which is readily accessible\nvia HuggingFace, includes 961 samples. Each sample\nhas been annotated with one of three labels: positive,\nneutral, or negative, denoting the sentiment conveyed in\nthe corresponding text.\nTesting Datasets\n\u2022 Twitter financial news sentiment validation (Twit-\nter Val):\nThis dataset, accessible through Hugging-\n\nDatasets\nModels\nName\nSize\nMetrics\nFinBERT\nLLaMA-7B\nInstruct-FinGPT-7B\nTwitter Val\n2388\nAcc\n0.725\n0.54\n0.880\nF1\n0.668\n0.36\n0.841\nTesting Time\n18 seconds (1 GPU)\n498 seconds (8 GPUs)\n498 seconds (8 GPUs)\nNumerical\n117\nAcc\n0.633\n0.60\n0.837\nF1\n0.630\n0.42\n0.795\nContextual\n20\nAcc\n0.50\n0.55\n0.80\nF1\n0.22\n0.34\n0.63\nTable 1: Experimental results on the Twitter financial news sentiment validation, numerical, and contextual datasets\nFace, contains 2,390 samples annotated with three la-\nbels: Bearish, Bullish, or Neutral.\n\u2022 Numerical sensitivity dataset (numerical):\nThis\ndataset, which we automatically filtered from Twitter\nVal, includes 117 samples. These samples contain at\nleast two numerical values related to financial indicators\nwithout strong indication words such as \u2019raise\u2019, \u2019fall\u2019,\n\u2019increase\u2019, \u2019decrease\u2019.\n\u2022 Contextual understanding dataset (contextual): This\ndataset, which we randomly selected from Twitter Val,\nincludes 20 samples. These samples lack the essential\ncontexts to make a sentiment prediciton.\n\u2022 Financial PhraseBank (FPB) dataset: This dataset\n[Malo et al., 2014] comprises 4,840 samples randomly\nextracted from financial news articles available on the\nLexisNexis database. The samples were carefully anno-\ntated by a team of 16 annotators with backgrounds in\nfinance and business, ensuring high quality annotations.\n4.2\nModel Training\nThe training parameters are given in Table 2. For our Instruct-\nFinGPT-7B model, we initialize it with LLaMA-7B model\nand perform instruction tuning over 10 epochs. The training\nprocess utilizes the AdamW optimizer [Loshchilov and Hut-\nter, 2017], with a batch size of 32, an initial learning rate of\n1e\u22125, and a weight decay of 0.1. To maintain efficiency, we\nset a maximum input text length of 512 tokens. We utilize\nDeepSpeed [Rasley et al., 2020] for the fine-tuning process\non 8 A100 (40GB) GPUs, resulting in a total training time of\n58 minutes.",
    "chunk_index": 5,
    "start_char": 14571,
    "end_char": 17803,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "process utilizes the AdamW optimizer [Loshchilov and Hut-\nter, 2017], with a batch size of 32, an initial learning rate of\n1e\u22125, and a weight decay of 0.1. To maintain efficiency, we\nset a maximum input text length of 512 tokens. We utilize\nDeepSpeed [Rasley et al., 2020] for the fine-tuning process\non 8 A100 (40GB) GPUs, resulting in a total training time of\n58 minutes.\nParameter\nValue\nLearning rate\n1e-5\nWeight Decay\n0.1\nBatch size\n32\nTraining epochs\n10\nLR Scheduler\nCosineAnnealing\nNum warmup Steps\n0\nMax Token Length\n512\nGPUs\n8 A100 (40GB)\nTable 2: Training parameters.\n4.3\nBaseline Models\nLLaMA-7B\n[Touvron et al.,\n2023] We obtained the\nLLaMA-7B1 model from Meta and use it for inference, keep-\ning the same inference setting as our Instruct-FinGPT-7B.\nFinBERT\nWe obtained the FinBERT model from the Hug-\nging Face Model Hub. The FinBERT model is used for sen-\ntiment analysis after pre-processing raw data, which includes\ntokenizing the text and padding or truncating it to fit the\nmodel\u2019s max input length. Once pre-processed, the data is run\nthrough FinBERT for inference, providing sentiment analysis\nresults (positive, negative, or neutral) for each text input.\nChatGPT\nThe utilization of OpenAI\u2019s ChatGPT API for\nsentiment analysis comprises a streamlined four-step process:\n1. API setup: This involves setting up the OpenAI Python\nclient, which serves as an interface to interact with the\nChatGPT API.\n2. Data preparation: The Instruction Tuning dataset as\nshown in Figs. 1 is employed for the inference with the\nChatGPT model.\n3. API call: Due to existing limitations, the GPT-3.5 API\nis used for requests. The GPT-4.0 version is currently\nunavailable for programmatic access and can only be in-\nteracted with via a web interface.\n4. Response interpretation: The response from the API\nincludes the sentiment of the text directly. This direct\nsentiment output simplifies the task of sentiment analy-\nsis.\n4.4\nEvaluation and Analysis\nTo evaluate the performance of our model, we test it on a\nbenchmark financial sentiment analysis dataset and contrast\nthe results with those of FinBERT. The key evaluation met-\nrics center around the model\u2019s capability to manage numeri-\ncal values and comprehend sentiment within various contexts.\nPerformance Metrics\nThe primary performance metrics\nfor our model include accuracy, and F1-score. Accuracy mea-\nsures the proportion of correct predictions, and the F1-score\nrepresents the harmonic mean of precision and recall.\n1We use LLaMA-7B for research and education purposes.\n\nNews\nTrue Value\nFinBERT\nChatGPT 3.5\nChatGPT 4.0\nInstruct-FinGPT\nPre-tax loss totaled euro 0.3 million,\ncompared to a loss of euro 2.2 mil-\nlion in the first quarter of 2005.\nPositive\nNegative\nNegative\nPositive\nPositive\nMadison Square Garden Q2 EPS\n$3.93 vs. $3.42.\nPositive\nNegative\nPositive\nPositive\nPositive\nConsumer credit $18.9BN, Exp.\n$16BN, Last $9.6BN.\nPositive\nNeutral\nPositive\nPositive\nPositive\nEstee Lauder Q2 adj.\nEPS $2.11;\nFactSet consensus $1.90.\nNeutral\nNeutral\nPositive\nPositive\nNeutral\nTable 3: Examples and results on the numerical sensitivity dataset.\nNews\nTrue Value\nFinBERT\nChatGPT-3.5\nChatGPT-4.0\nInstruct-FinGPT\nThe situation of coated magazine\nprinting paper will continue to be\nweak.",
    "chunk_index": 6,
    "start_char": 17430,
    "end_char": 20667,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "of euro 2.2 mil-\nlion in the first quarter of 2005.\nPositive\nNegative\nNegative\nPositive\nPositive\nMadison Square Garden Q2 EPS\n$3.93 vs. $3.42.\nPositive\nNegative\nPositive\nPositive\nPositive\nConsumer credit $18.9BN, Exp.\n$16BN, Last $9.6BN.\nPositive\nNeutral\nPositive\nPositive\nPositive\nEstee Lauder Q2 adj.\nEPS $2.11;\nFactSet consensus $1.90.\nNeutral\nNeutral\nPositive\nPositive\nNeutral\nTable 3: Examples and results on the numerical sensitivity dataset.\nNews\nTrue Value\nFinBERT\nChatGPT-3.5\nChatGPT-4.0\nInstruct-FinGPT\nThe situation of coated magazine\nprinting paper will continue to be\nweak.\nNegative\nNeutral\nNegative\nNegative\nNegative\nBoeing announces additional order\nfor 737 MAX planes.\nNeutral\nPositive\nPositive\nPositive\nPositive\nBoeing:\nDeliveries\n24\nJets\nin\nNovember.\nPositive\nNeutral\nPositive\nNeutral\nPositive\nPPD\u2019s stock indicated in early going\nto open at $30, or 11% above $27\nIPO price.\nNeutral\nPositive\nPositive\nPositive\nNeutral\nTable 4: Examples and results on the contextual understanding dataset.\nOverall Performance\nBased on the evaluation results in\nTable 1, our instruction tuned LLaMA-7B (Instruct-FinGPT-\n7B) consistently outperforms both FinBERT and LLaMA-7B\nacross all three datasets in terms of accuracy and F1 score.\nEspecially, comparing our Instruct-FinGPT-7B with the orig-\ninal LLaMA-7B model (without instruction tuning), it is evi-\ndent that the instruction tuning method significantly improves\nthe model\u2019s performance on financial sentiment analysis.\nAnalysis of Numerical Sensitivity\nNumerical data plays a\ncrucial role in financial sentiment analysis, as it often reflects\nimportant financial indicators. In Table 3, we assess the mod-\nels\u2019 ability to comprehend and interpret sentiment associated\nwith numbers.\n\u2022 Example 1: This is an example from FinBERT, where\nFinBERT failed in this case. However, ChatGPT 4.0 and\nInstruct-FinGPT correctly recognize the substantial de-\ncrease in the loss from 2.2 million to 0.3 million, indi-\ncating a positive sentiment.\n\u2022 Example 2: The increase in EPS is correctly identified\nas a positive sentiment by all models except FinBERT.\n\u2022 Example 3: The exceeding of consumer credit expecta-\ntions and the previous value is recognized as a positive\nsentiment by all models except FinBERT.\n\u2022 Example 4: The statement about Estee Lauder and Fact-\nSet consensus is neutral, as it merely states the facts\nwithout indicating a positive or negative sentiment.\nOur model demonstrates varying levels of effectiveness in\nunderstanding and interpreting sentiment associated with nu-\nmerical data.\nAnalysis of Contextual Understanding\nThe ability of our\nmodel to interpret sentiment in different contexts is an im-\nportant aspect of its performance evaluation. Financial news\ncan be nuanced, and a statement that may appear negative in\none context could be neutral or even positive in another. We\nassess the models\u2019 performance in contextual understanding\nbased on the examples provided in Table 4.\n\u2022 Example 1: This is an example from FinBERT, where\nFinBERT failed in this case. But ChatGPT and Instruct-\nFinGPT recognized that the situation of coated magazine\nprinting paper is expected to remain weak, indicating a\nnegative outlook for the industry. LLMs\u2019 language un-\nderstanding capabilities and knowledge of financial con-\ntexts enable them to accurately interpret such statements\nand predict the sentiment.",
    "chunk_index": 7,
    "start_char": 20081,
    "end_char": 23446,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "another. We\nassess the models\u2019 performance in contextual understanding\nbased on the examples provided in Table 4.\n\u2022 Example 1: This is an example from FinBERT, where\nFinBERT failed in this case. But ChatGPT and Instruct-\nFinGPT recognized that the situation of coated magazine\nprinting paper is expected to remain weak, indicating a\nnegative outlook for the industry. LLMs\u2019 language un-\nderstanding capabilities and knowledge of financial con-\ntexts enable them to accurately interpret such statements\nand predict the sentiment.\n\u2022 Example 2: In this specific case, indicating that Boe-\ning has received more orders for their aircraft. It looks\nlike positive news. However, without further context, it\u2019s\nchallenging to determine the sentiment accurately. The\nlack of specific details about the order, the customer, or\nany potential implications can make it difficult to assess\nthe sentiment correctly.\n\u2022 Example 3: The sentiment of the financial news is pos-\nitive. The statement highlights that Boeing delivered 24\njets in November, indicating a successful and productive\nmonth for the company.\n\u2022 Example 4: All of the models failed on this one. The\nopening price of a stock is higher than the IPO price\ndoesn\u2019t necessarily indicate the stock is rising from its\ncurrent market price.\n\nPerformance\nChatGPT 3.5\nLLaMA-7B\nOurs-7B\nFPB (ACC)\n0.64\n0.60\n0.76\nFPB (F1)\n0.51\n0.40\n0.74\nTable 5:\nZero-shot evaluation between ChatGPT and Instruct-\nFinGPT on the entire dataset of financial phaseBank.\nOverall, our model demonstrates a better understanding of\nthe contextual sentiment in these examples compared to Fin-\nBERT and ChatGPT. It successfully recognizes the negative\nsentiment in Example 1 and accurately identifies the neutral\nsentiment in Example 2 and the positive sentiment in Exam-\nple 3. These results highlight the importance of contextual\nunderstanding in financial sentiment analysis and the varia-\ntions in performance across different models.\nZero-Shot Generalization to Other Financial Datasets\nFinally, we evaluate the zero-shot ability of our model, which\nrefers to how well the model can generalize to other unseen\nfinancial datasets. A model with strong zero-shot capabilities\ncan provide more robust and versatile results in real-world\napplications. We compare our Instruct-FinGPT with Chat-\nGPT3.5 and LLaMA-7B on the full FPB dataset. Here we\ndo not compare with FinBERT because it uses FPB as the\ntraining set.\nThe evaluation results are shown in Table 5.\nBased on\nthese results, it can be concluded that the instruction tuned\nLLaMA-7B model performs the best among the three, achiev-\ning the highest accuracy and F1 score. The fine-tuning pro-\ncess with sentiment instruction data seems to have improved\nthe model\u2019s ability to capture sentiment in financial phrases,\nresulting in better zero-shot performance compared to both\nChatGPT and the original LLaMA-7B model.\n5\nConclusion and Future Work\nIn this paper, we have presented an innovative approach for\nfinancial sentiment analysis by harnessing the general knowl-\nedge and reasoning capabilities of LLMs. Our method repre-\nsents a substantial contribution to the field of sentiment analy-\nsis, demonstrating that instruction tuning of an LLM can yield\nsuperior performance with a small amount of task-specific\ndata.",
    "chunk_index": 8,
    "start_char": 22918,
    "end_char": 26205,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "to have improved\nthe model\u2019s ability to capture sentiment in financial phrases,\nresulting in better zero-shot performance compared to both\nChatGPT and the original LLaMA-7B model.\n5\nConclusion and Future Work\nIn this paper, we have presented an innovative approach for\nfinancial sentiment analysis by harnessing the general knowl-\nedge and reasoning capabilities of LLMs. Our method repre-\nsents a substantial contribution to the field of sentiment analy-\nsis, demonstrating that instruction tuning of an LLM can yield\nsuperior performance with a small amount of task-specific\ndata. Our findings pave the way for future research into the\npotential of LLMs for a broad range of financial tasks.\nDisclaimer: We are sharing codes for academic pur-\nposes under the MIT education license. Nothing herein is\nfinancial advice, and NOT a recommendation to trade real\nmoney. Please use common sense and always first consult\na professional before trading or investing.\nReferences\n[Araci, 2019] Dogu Araci.\nFinBERT: Financial sentiment\nanalysis with pre-trained language models.\nIn arXiv\npreprint arXiv:1908.10063, 2019.\n[Atkins et al., 2018] Adam Atkins, Mahesan Niranjan, and\nEnrico Gerding.\nFinancial news predicts stock market\nvolatility better than close price. The Journal of Finance\nand Data Science, 4(2):120\u2013137, 2018.\n[Chan and Chong, 2017] Samuel WK Chan and Mickey WC\nChong. Sentiment analysis in financial texts. Decision\nSupport Systems, 94:53\u201364, 2017.\n[Day and Lee, 2016] Min-Yuh Day and Chia-Chou Lee.\nDeep learning for financial sentiment analysis on finance\nnews providers. In IEEE/ACM International Conference\non Advances in Social Networks Analysis and Mining\n(ASONAM), pages 1127\u20131134. IEEE, 2016.\n[Hamilton et al., 2016] William L Hamilton, Kevin Clark,\nJure Leskovec, and Dan Jurafsky.\nInducing domain-\nspecific sentiment lexicons from unlabeled corpora.\nIn\nProceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP), volume 2016,\npage 595. NIH Public Access, 2016.\n[Kenton and Toutanova, 2019] Jacob\nDevlin\nMing-\nWei Chang Kenton and Lee Kristina Toutanova. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding.\nIn Proceedings of NAACL-HLT,\npages 4171\u20134186, 2019.\n[Loshchilov and Hutter, 2017] Ilya Loshchilov and Frank\nHutter. Fixing weight decay regularization in adam. arXiv\npreprint arXiv:1711.05101, 2017.\n[Loughran and McDonald, 2011] Tim Loughran and Bill\nMcDonald. When is a liability not a liability? Textual\nanalysis, dictionaries, and 10-Ks. The Journal of Finance,\n66(1):35\u201365, 2011.\n[Magic, 2022] Neural Magic. Twitter financial news senti-\nment. http://precog.iiitd.edu.in/people/anupama, 2022.\n[Maia et al., 2018] Macedo Maia, Siegfried Handschuh, An-\ndre Freitas,\nBrian Davis,\nRoss McDermott,\nManel\nZarrouk, and Alexandra. Balahur. Www \u201918: Companion\nproceedings of the the web conference 2018. In Interna-\ntional World Wide Web Conferences Steering Committee,\nRepublic and Canton of Geneva, CHE, 2018.\n[Malo et al., 2014] Pekka Malo, Ankur Sinha, Pekka Korho-\nnen, Jyrki Wallenius, and Pyry Takala. Good debt or bad\ndebt: Detecting semantic orientations in economic texts.\nJournal of the Association for Information Science and\nTechnology, 65(4):782\u2013796, 2014.\n[Mishev et al., 2020] Kostadin Mishev,\nAna Gjorgjevikj,\nIrena Vodenska, Lubomir T Chitkushev, and Dimitar Tra-\njanov. Evaluation of sentiment analysis in finance: from\nlexicons to transformers. IEEE Access, 8:131662\u2013131682,\n2020.",
    "chunk_index": 9,
    "start_char": 25623,
    "end_char": 29105,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "Conferences Steering Committee,\nRepublic and Canton of Geneva, CHE, 2018.\n[Malo et al., 2014] Pekka Malo, Ankur Sinha, Pekka Korho-\nnen, Jyrki Wallenius, and Pyry Takala. Good debt or bad\ndebt: Detecting semantic orientations in economic texts.\nJournal of the Association for Information Science and\nTechnology, 65(4):782\u2013796, 2014.\n[Mishev et al., 2020] Kostadin Mishev,\nAna Gjorgjevikj,\nIrena Vodenska, Lubomir T Chitkushev, and Dimitar Tra-\njanov. Evaluation of sentiment analysis in finance: from\nlexicons to transformers. IEEE Access, 8:131662\u2013131682,\n2020.\n[Rasley et al., 2020] Jeff\nRasley,\nSamyam\nRajbhandari,\nOlatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with\nover 100 billion parameters. In Association for Computing\nMachinery, KDD \u201920, page 3505\u20133506, New York, NY,\nUSA, 2020.\n[Shah et al., 2022] Raj Sanjay Shah, Kunal Chawla, Dheeraj\nEidnani, Agam Shah, Wendi Du, Sudheer Chava, Na-\ntraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang.\nWhen flue meets flang: Benchmarks and large pretrained\nlanguage model for financial domain. In Proceedings of\n\nthe Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP). Association for Computa-\ntional Linguistics, 2022.\n[Sohangir et al., 2018] Sahar Sohangir,\nDingding Wang,\nAnna Pomeranets, and Taghi M Khoshgoftaar. Big data:\nDeep learning for financial sentiment analysis. Journal of\nBig Data, 5(1):1\u201325, 2018.\n[Tai and Kao, 2013] Yen-Jen Tai and Hung-Yu Kao. Auto-\nmatic domain-specific sentiment lexicon generation with\nlabel propagation. In Proceedings of International Con-\nference on Information Integration and Web-based Appli-\ncations & Services, pages 53\u201362, 2013.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric\nHambro, Faisal Azhar, et al.\nLLaMA: Open and ef-\nficient foundation language models.\narXiv preprint\narXiv:2302.13971, 2023.\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao,\nKelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. Finetuned language mod-\nels are zero-shot learners. In International Conference on\nLearning Representations, 2022.\n[Wu et al., 2023] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim\nDabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\nhanjan Kambadur, David Rosenberg, and Gideon Mann.\nBloombergGPT: A large language model for finance.\narXiv preprint arXiv:2303.17564, 2023.\n[Xing et al., 2018] Frank Z Xing, Erik Cambria, and Roy E\nWelsch. Natural language based financial forecasting: a\nsurvey. Artificial Intelligence Review, 50(1):49\u201373, 2018.\n[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li,\nTianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu\nLiu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large\nlanguage models, 2023.",
    "chunk_index": 10,
    "start_char": 28543,
    "end_char": 31554,
    "paper_title": "Instruct-FinGPT Financial Sentiment Analysis by In",
    "paper_category": "cs.CL",
    "paper_filename": "Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.CL/Instruct-FinGPT_Financial_Sentiment_Analysis_by_In.pdf"
  },
  {
    "text": "Pragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n1 \n Pragmatic Information Rates, Generalizations of the \nKelly Criterion, and Financial Market Efficiency \nEdward D Weinberger, Ph.D. \n \nWeinberger Post-Quantitative, Inc. \n370 Central Park West, #110 \nNew York, NY 100251 \nedw@wpq-inc.com \n \nDepartment of Finance and Risk Engineering \nPolytechnic Institute of New York University \n6 Metrotech Center \nBrooklyn, NY 12201 \n \nGraduate School of Management \n127 Carlson Hall \nClark University \nWorcester, MA 01610 \n \nAbstract \n \nThis paper is part of an ongoing investigation of \u201cpragmatic information,\u201d defined in \nWeinberger (2002) as \u201cthe amount of information actually used in making a decision.\u201d \nBecause a study of information rates led to the Noiseless and Noisy Coding Theorems, \ntwo of the most important results of Shannon\u2019s theory, we begin the paper by defining a \npragmatic information rate, showing that all of the relevant limits make sense, and \ninterpreting them as the improvement in compression obtained from using the correct \ndistribution of transmitted symbols. \n \nThe first of two applications of the theory extends the information theoretic analysis of \nthe Kelly Criterion, and its generalization, the \u201chorse race\u201d, to a series of races where the \nstochastic process of winning horses and strategies depend on some stationary process, \nincluding, but not limited to the history of previous races. If the bettor is receiving \nmessages (side information) about the probability distribution of winners, the increase in \nthe doubling rate of the bettor's winnings is the pragmatic information of the messages. \nThe results presented here are well known in the literature, but they are included as an \nimportant illustration of the relevance of pragmatic information to finance. \n \nA second application is to the question of market efficiency. An efficient market is, by \ndefinition, a market in which the pragmatic information of the \u201ctradable past\u201d with \n \n1 Address to which correspondence should be sent. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n2 \nrespect to current prices is zero. Under this definition, we show that markets whose \nreturns are characterized by a GARCH(1,1) process cannot be efficient. \n \nFinally, a pragmatic informational analogue to Shannon\u2019s Noisy Coding Theorem \nsuggests that a cause of market inefficiency is that the underlying fundamentals are \nchanging so fast that the price discovery mechanism simply cannot keep up. This may \nhappen most readily in the run-up to a financial bubble, where investors\u2019 willful \nignorance degrades the information processing capabilities of the market. \n \nKey Words and Phrases \n \nShannon-McMillan-Breiman Theorem; mutual information; pragmatic information; \nexpected log return; log-optimal portfolio; ergodic stock market; asymptotic optimality \nprinciple; efficient market hypothesis. \n \nMSC-class \n \n94A17, 91G80 \n\\body \n \nIntroduction \n \nWeinberger (2002) published a definition of \u201cpragmatic information\u201d as \u201cthe amount of \ninformation in a message that is actually used to make a decision\u201d, or as a measure of the \namount of meaningful information in a message. This definition distinguishes pragmatic \ninformation from the usual (Shannon) measure of information, which refers merely to the \nreduction in uncertainty resulting from the receipt of a message, and not to the meaning \nthat the uncertainty reduction has to the receiver.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3495,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "94A17, 91G80 \n\\body \n \nIntroduction \n \nWeinberger (2002) published a definition of \u201cpragmatic information\u201d as \u201cthe amount of \ninformation in a message that is actually used to make a decision\u201d, or as a measure of the \namount of meaningful information in a message. This definition distinguishes pragmatic \ninformation from the usual (Shannon) measure of information, which refers merely to the \nreduction in uncertainty resulting from the receipt of a message, and not to the meaning \nthat the uncertainty reduction has to the receiver. The most obvious, if not the only \nobjective meaning that a message can have to a receiver is in observed changes in \nsubsequent behavior; indeed, Weaver made precisely this point in his extended \nintroduction to the Shannon paper that began information theory (Shannon and Weaver, \n1962). Although Shannon\u2019s paper began with a demonstration that his celebrated entropy \nmeasure was, given a few reasonable conditions, unique, Shannon wrote that the ultimate \nimportance of this quantity was not that it obeyed the uniqueness theorem, but that it \nrepresented the minimum compressed length of a message, a result now familiar as the \nNoiseless Coding Theorem. Therefore it seems reasonable to ask whether there is an \nequivalent \u201cNoiseless Coding Theorem for Pragmatic Information\u201d and what that might \nmean. \n \nAnswering the above question requires some background. First, the Noiseless Coding \nTheorem was only the first of a series of successively more general results; in that \nShannon\u2019s result only applied to \n\uf0b7 symbol sequences generated from a finite alphabet, as opposed to the infinite set of \nreal numbers, \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n3 \n\uf0b7 measurements of the expected length of the encoding, thus leaving open the \npossibility that any individual sequence could have a markedly different compressed \nlength. \n\uf0b7 sequences generated by a so-called discrete Markov source, i.e. one in which the \n(stationary) conditional probability of observing each symbol, given the entire past \nhistory of the sequence, is identical to the conditional probability of observing that \nsymbol, given only the most recent N < \u221e symbols. \nSubsequent attempts to generalize the Noiseless Coding Theorem beyond the above \nrestrictions culminated in the Shannon-McMillan-Breiman Theorem (for finite alphabet \nsequences) and its generalization to potentially real valued sequences by Barron (1985). \nThese theorems guarantee that, in the limit of long sequences, individual sequences must \nall2 have the compressed length per input symbol predicted by the Noiseless Coding \nTheorem. Furthermore, the class of sequences to which the sequence applies is \nconsiderably broader than those produced by discrete Markov sources. \n \nAn obvious application of the present work is in the prediction, or, per the efficient \nmarket hypothesis (Brigham and Ehrhardt, 2005), difficulty of prediction of financial time \nseries. Indeed, \n\uf0b7 the weak form of market efficiency is simply the statement that the pragmatic \ninformation of past returns in predicting future returns is zero; \n\uf0b7 the semi-strong form of market efficiency is simply the statement that the pragmatic \ninformation of all public information for predicting future returns is zero;",
    "chunk_index": 1,
    "start_char": 2959,
    "end_char": 6271,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "the present work is in the prediction, or, per the efficient \nmarket hypothesis (Brigham and Ehrhardt, 2005), difficulty of prediction of financial time \nseries. Indeed, \n\uf0b7 the weak form of market efficiency is simply the statement that the pragmatic \ninformation of past returns in predicting future returns is zero; \n\uf0b7 the semi-strong form of market efficiency is simply the statement that the pragmatic \ninformation of all public information for predicting future returns is zero; \n\uf0b7 the strong form of market efficiency is simply the statement that the pragmatic \ninformation of all information, including insider information, for predicting future \nreturns is zero. \n \nIn addition to the Noiseless Coding Theorem, the other landmark result in Shannon\u2019s \npaper was his so-called \u201cNoisy Coding Theorem\u201d, a fundamental limitation on the rate at \nwhich information can be transmitted through a noisy communications channel. This \ntheorem states that every such channel has a well defined \u201cchannel capacity\u201d, a rate \nbelow which the receiver can detect and correct any transmission errors, so that \ntransmission can be made effectively noiseless at any rate below the channel capacity. \nThe theorem also states that the receiver cannot, in general, do the same for transmissions \nabove the channel capacity. Again, is there a pragmatic informational analogue to such a \ntheorem, and, if so, what might it mean? \n \nThis paper is therefore organized as follows: The section after this introduction provides \nsome relevant background, including the formal statements of the above mentioned \ninformation theoretic results and the ideas surrounding them. As part of that review, we \nconsider a class of processes that are provably not discrete Markov sources and we argue \nthat they are, in fact, better models for financial price series than random walks. Thus \nmotivated, a third section discusses the theoretical underpinnings of measuring pragmatic \ninformation rates, followed by a section that relates pragmatic information to the \n \n2 Actually \u201calmost all,\u201d in the sense that the set of all such sequences has probability 1. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n4 \ngeneralized Kelly Criterion and to efficient market theory. The final two sections of the \npaper are considerably less concerned with specific results; the first of these sections \nbriefly considers the implications of Shannon\u2019s Noisy Coding Theorem for both the \ntheory of pragmatic information in general and its specific application to finance, the \nsecond presents a summary and some conclusions to be drawn from this work. \n \nWe adopt the information theoretic convention that all logarithms are base 2 logarithms, \nunless otherwise noted. Thus, the information theoretic expressions in this paper have \nunits of \u201cbits\u201d (as opposed to the units of \u201cnats\u201d if natural logs were used). \n \nBackground \n \nAn Introduction to Pragmatic Information \n \nIt is tempting to define the pragmatic information of a message, m, as the relative \nentropy, D (P || Q), of the probabilities of the decision maker\u2019s actions before and after \nreceiving m. This quantity is defined over a finite symbol string A, consisting of \nconcatenations of the symbols A1, A2, A3, \u2026, A|A |.",
    "chunk_index": 2,
    "start_char": 5788,
    "end_char": 9061,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "expressions in this paper have \nunits of \u201cbits\u201d (as opposed to the units of \u201cnats\u201d if natural logs were used). \n \nBackground \n \nAn Introduction to Pragmatic Information \n \nIt is tempting to define the pragmatic information of a message, m, as the relative \nentropy, D (P || Q), of the probabilities of the decision maker\u2019s actions before and after \nreceiving m. This quantity is defined over a finite symbol string A, consisting of \nconcatenations of the symbols A1, A2, A3, \u2026, A|A |. drawn from the finite alphabet A, as3 \n \nD (P || Q) = \u03a3 A P (A) log [P(A) \u2215 Q(A)], (1) \n \nwhere P(A) is the probability of A subsequent to the receipt of m and Q(A) is the \nprobability of Q beforehand. The use of the character A to represent this symbol string \nserves as a reminder that these symbols actually represent actions, albeit actions broadly \nconstrued. \n \nA theorem similar to the Noiseless Coding Theorem, albeit less well known, states that a \nmessage erroneously compressed using the wrong probabilities, Q (A), when the actual \nsymbol probabilities, P(A), will require an additional compressed length equal in \nexpected value to the relative entropy between the wrong distribution and the right one. \nSurely, some kind of learning is clearly taking place if the act of compressing the \nsequence can be improved, and the additional amount of compression measures, in some \nsense, the amount that has been learned. The formal statement of the theorem would \ntherefore seem to provide a useful quantification of this learning, as in the \n \n\u201cWrong Code\u201d Theorem (Cover and Thomas, 1991). If an encoding that \nwould be optimal assuming the wrong probability measure Q(A) is used in place \nof the optimal encoding for the finite symbol string A using the correct probability, \nP(A), then the expected length of the resulting encoding, EP [l(\u03b1)] = \u03a3A P(A) l(A), \nsatisfies the bounds \n \n \n3 Here, and in the sequel, all logarithms are to the base | A |, where | A | is the size of A, unless otherwise \nindicated. We also adopt the convention that \u03b1 denotes a randomly selected symbol, as opposed to A, \nwhich would indicate that the symbol is, for example, an index in a sum, as above. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n5 \nH (P) + D (P || Q) \u2264 EP [l(\u03b1)] \u2264 H (P) + D (P || Q) + 1, \n \nwhere H (P) is the entropy of \u03b1,D (P || Q) is defined as in (1), and l(\u03b1) is the \nlength, in symbols, of the encoding of \u03b1 using Q. \n \nHowever, as Weinberger (2002) makes clear, pragmatic information is more sensibly \ndefined over the ensemble, M, of possible messages. There, it is assumed that the actual \nmessage sent, which we will denote as \u03bc, is a random variable with known distribution \nPr{ \u03bc = m} = \u03c6 (m). In particular, it is shown there that the definition of pragmatic \ninformation as the mutual information between M and the ensemble of outcomes, A, i.e.",
    "chunk_index": 3,
    "start_char": 8577,
    "end_char": 11469,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "the \nlength, in symbols, of the encoding of \u03b1 using Q. \n \nHowever, as Weinberger (2002) makes clear, pragmatic information is more sensibly \ndefined over the ensemble, M, of possible messages. There, it is assumed that the actual \nmessage sent, which we will denote as \u03bc, is a random variable with known distribution \nPr{ \u03bc = m} = \u03c6 (m). In particular, it is shown there that the definition of pragmatic \ninformation as the mutual information between M and the ensemble of outcomes, A, i.e. \n \nI (A ; M) = \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\uf029\n\uf028\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf0e5\n\uf0e5\n\uf0e5\uf0e5\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf03d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\nm\nm\nA\nm\nA\nm\nA\nm\nA\nm\nm\nA\nm\nA\nPr\n|\nPr\nlog\n|\nPr\nPr\nPr\nPr\n,\nPr\nlog\n,\nPr\nA\nA\n \n(2) \n \nhas the all-important property that the joint pragmatic information from independent \nmessage ensembles is the sum of the pragmatic information values from each ensemble. \nIt is also the relative information of the probability distribution of outputs subsequent to \nthe receipt of each message with respect to the prior distribution, averaged over all \npossible messages. \n \nAnother problem with using (1) as a definition of pragmatic information emerges in the \nlimit of long sequences. Recall that the Noiseless Coding Theorem states that \n \nH (P) \u2264 E [lP(\u03b1)] \u2264 H (P) + 1, \n \nwhere lP(\u03b1) is the length, in symbols, of the optimal (shortest) encoding of \u03b1. Dividing \nthrough by N, the length of \u03b1, would give the exact result that the information rate, h, is \nexactly equal to the expected encoded length per symbol in the limit N \u2192 \u221e, if the limit \nexists. In the case of Shannon entropy, it does exist for all stationary sequences (Cover \nand Thomas, 1991). In contrast, Shields (1992) shows that the corresponding \u201crelative \ninformation rate\u201d, limN\u2192\u221e N-1D (P || Q) , need not exist. \n \nAlso, Shannon\u2019s result applies only to the expected length of the encoding, thus providing \nno assurance that any individual sequence will not have a markedly different compressed \nlength. The most general statement of the Noiseless Coding Theorem that could be \nhoped for is that effectively every sufficiently long symbol sequence would have the right \nencoded length, provided that it is generated from an ergodic source4. That this is \nactually true is the content of the \n \n4 An ergodic source is, for our purposes, a stationary stochastic process in which no set, S, of symbol \nsequences is mapped via time translation onto itself, except for sets of zero probability and sets of \nprobability one. Note that stationarity requires that the probability of S also remains invariant under the \ntime translation. Per Birkhoff\u2019s ergodic theorem (Billingsly, 1978), these conditions guarantee that \naverages taken over a sufficiently long individual sequence will correspond to averages taken over the \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n6 \n \nTheorem (Shannon-McMillan-Breiman).",
    "chunk_index": 4,
    "start_char": 10979,
    "end_char": 13837,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "via time translation onto itself, except for sets of zero probability and sets of \nprobability one. Note that stationarity requires that the probability of S also remains invariant under the \ntime translation. Per Birkhoff\u2019s ergodic theorem (Billingsly, 1978), these conditions guarantee that \naverages taken over a sufficiently long individual sequence will correspond to averages taken over the \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n6 \n \nTheorem (Shannon-McMillan-Breiman). \n \n\uf07b\n\uf07d\n\uf05b\n\uf05d\n\uf028\uf029\n\uf05b\n\uf05d\n\u03b1\nlog\n1\n,\n,\n,\nlog\n1\nlim\nlim\n1\n1\n0\nP\nE\nN\n\u03b1\n\u03b1\n\u03b1\nP\nN\nP\nn\nN\nN\n\uf02d\n\uf03d\n\uf02d\n\uf0a5\n\uf0ae\n\uf02d\n\uf0a5\n\uf0ae\n\uf04b\n \n \nwith probability 1 (Billingsley, 1977) for any ergodic source, \u03b1, generated from a \nfinite alphabet. \n \nNote that log[P{ \u03b10, \u03b11, \u03b12, \u2026 , \u03b1N-1}]/N is a random variable because \u03b10, \u03b11, \u03b12, \u2026 , \u03b1N-1 \nis a particular realization of a stochastic process. Yet the Shannon-McMillan-Breiman \n(SMB) Theorem states that, in the limit as N \u2192 \u221e, the length of the optimally encoded \nversion of the substring is the (non-random) log of the probability of the observed \nsubstring, almost surely under P. This is precisely the result needed to establish the \nstrong form of the Noiseless Coding Theorem. Indeed, per Cover and Thomas, (1991), \nthe length, l(\u03b10 \u2026 \u03b1N-1), of the optimal encoding of \u03b10 thru \u03b1N-1, satisfies \n \n- log P{\u03b10 \u2026 \u03b1N-1} \u2264 l(\u03b10 \u2026 \u03b1N-1) \u2264 - log P{\u03b10 \u2026 \u03b1N-1} + 1, \n \nso \n \n\uf07b\n\uf07d\n\uf05b\n\uf05d\n\uf028\n\uf029,\n,\n,\n,\n1\n,\n,\n,\nlog\n1\n1\n1\n0\n1\n1\n0\nlim\nlim\n\uf02d\n\uf0a5\n\uf0ae\n\uf02d\n\uf0a5\n\uf0ae\n\uf03d\n\uf02d\nN\nN\nN\nN\n\u03b1\n\u03b1\n\u03b1\nN\n\u03b1\n\u03b1\n\u03b1\nP\nN\n\uf04b\n\uf06c\n\uf04b\n \n \nas required. This result argues powerfully in favor of using Shannon\u2019s definition as the \nper-symbol measure of information in a long message, especially since, per Barron \n(1985), the theorem remains true if \u03b1 is a sequence of real random variables. Clearly, a \nsimilar argument could be made for any definition of pragmatic information for which a \nsimilar result is true, and, as will be shown shortly, this is the case for the per-symbol \nversion of (2). \n \nFrom Random Walks to Random Grammars \n \nThe simplest kind of random symbol sequence is a sequence of independently chosen \nsymbols, or, more generally, random real variables. The only \u201cinformation\u201d in the \nsequence is which values were chosen for which position in the sequence, and the only \nparameters needed to characterize the process are the probabilities of observing these \nvalues. Evidently, the pragmatic information that an observer would use in predicting \nsubsequent values in the sequence, given its history, is zero, corresponding to the fact that \na price series with independent changes is, by definition, weakly efficient. Arguments \nover whether markets really are efficient have been raging for decades in such works as \nA Random Walk Down Wall Street (Malkiel, 1991) and A Non-Random Walk Down Wall \n \nensemble of all such sequences.",
    "chunk_index": 5,
    "start_char": 13318,
    "end_char": 16133,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "the process are the probabilities of observing these \nvalues. Evidently, the pragmatic information that an observer would use in predicting \nsubsequent values in the sequence, given its history, is zero, corresponding to the fact that \na price series with independent changes is, by definition, weakly efficient. Arguments \nover whether markets really are efficient have been raging for decades in such works as \nA Random Walk Down Wall Street (Malkiel, 1991) and A Non-Random Walk Down Wall \n \nensemble of all such sequences. Otherwise, it is possible that even stationary processes could get trapped \nin some subset of the ensemble. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n7 \nStreet (Lo and MacKinlay, 1999). Previous attempts to resolve such arguments generally \nassume certain kinds of intertemporal dependence, such as serial correlation. \nNevertheless, true market efficiency rules out any kind of intertemporal dependence, \nincluding that implied by the \u201ctechnical analysis\u201d of complex price patterns (Murphy, \n1999). \n \nA step up in complexity is the class of symbol sequences generated by a Markov chain, in \nwhich successive symbols are chosen as a function of a finite number, n, of previous \nsymbols. Again, the class of such random processes can be generalized somewhat by \nallowing each symbol to be a real valued random variable. Textbook treatments of \nMarkov processes almost always consider so-called first order Markov processes, in \nwhich the probability that a value appears in the sequence can only depend on the value \nimmediately preceding it. While first order Markov processes can be generalized to so-\ncalled nth order Markov processes, in which the probability that a value appears in the \nsequence depends on the n values immediately preceding it, nth order Markov processes \nare isomorphic to vector valued first order processes. Therefore, k step transition \nprobabilities for nth order Markov processes also decay exponentially with k. \n \nTrading systems are often built on rules such as \u201cif a price exceeds a given threshold, \nthen sell,\u201d rules which are effectively the transition rules of a finite state automaton. \nAlso, most popular models of price volatility are Markov processes. Typical of these is \nthe familiar univariate GARCH(1,1) process, effectively a random walk with variable \nstep size, given at step n by \n \nRn = R0 + \u03c3 n-1 B n \nPn = (1 + R n-1) P n-1 \n\u03c32\nn = \u03b1 \u03c32\n n-1 + \u03b2 \u03c32\n0 + \u03b3 R2\nn, \n \nwhere R0, \u03b1, \u03b2, and \u03b3 are constants and Pn, Rn, and \u03c3n are, respectively, the price, per-\nperiod return, and per period volatility. Bn is a normal random variable with mean zero \nand unit variance in the time units of the problem. Again, all of these processes are \ncharacterized by exponential decay of transients. \n \nHowever, financial markets consist of literally millions of coupled trading systems, both \nexplicitly implemented in computer software and implicit in human traders\u2019 minds. It \ntherefore seems likely that a more complex stochastic process would emerge.",
    "chunk_index": 6,
    "start_char": 15607,
    "end_char": 18662,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "Bn is a normal random variable with mean zero \nand unit variance in the time units of the problem. Again, all of these processes are \ncharacterized by exponential decay of transients. \n \nHowever, financial markets consist of literally millions of coupled trading systems, both \nexplicitly implemented in computer software and implicit in human traders\u2019 minds. It \ntherefore seems likely that a more complex stochastic process would emerge. There is, in \nfact, a large literature suggesting fractal and/or multi-fractal scaling in financial markets, \nincluding an analysis of over a million increments of the Standard and Poor\u2019s 500 Index \nfrom the years 1984-1989 (Mantegna & Stanley, 1995). This literature includes a series \nof models in Calvet and Fisher (2008), in which transients decay as an inverse power of \nthe elapsed time, rather than exponentially. \n \nIt is precisely because financial markets could exhibit these more complex kinds of \ndependencies that a study of the pragmatic information of historical price series for \npredicting future prices is relevant. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n8 \n \nLimit Theorems for Pragmatic Information \n \nIn this section, we establish first, a version of the \u201cwrong code theorem\u201d for pragmatic \ninformation, proving along the way that a pragmatic information rate for stationary \nsequences and stationary side messages must exist. For ergodic sequences, we need only \nquote the known result that, just as with the Shannon-McMillan-Breiman Theorem, the \npragmatic information rate is, in the limit of long sequences and side messages, the \nobservable increase in the entropy rate of the sequence. \n \nFirst, the formal statement of the \n \n\u201cWrong Code\u201d Theorem for pragmatic information. Suppose an encoding of \na finite symbol string \u03b1 that would be optimal assuming the wrong probability \nmeasure Q(\u03b1) is used in place of the optimal encoding. Suppose further that, \ngiven the (random) side message \u03bc, the actual probability of observing \u03b1 could \nbe determined to be P(\u03b1 = A | \u03bc = m). Then E\u03bc[l(\u03b1)] , the expected length of the \nresulting encoding, averaged over all possible side messages \u03bc, satisfies the \nbounds \n \nH (\u03b1 | \u03bc) + I (\u03b1; \u03bc) \u2264 E\u03bc[l(\u03b1)] \u2264 H (\u03b1 | \u03bc) + I (\u03b1; \u03bc) + 1, \n \nwhere H (\u03b1 | \u03bc) is the conditional entropy of \u03b1, given side message \u03bc, I (\u03b1; \u03bc) is \ndefined as in (2), and l(\u03b1) is the length, in symbols, of the encoding of \u03b1 using Q. \n \nProof: Let \u03bc be one of the possible side messages, and write P\u03bc and E\u03bc in place of the \nmore cumbersome P(\u03b1 | \u03bc) and EP(\u03b1 | \u03bc) , respectively. As guaranteed by the Wrong Code \nTheorem of Cover and Thomas (1991), \n \nH (P\u03bc) + D (P\u03bc || Q) \u2264 E\u03bc [l(\u03b1)] \u2264 H (P\u03bc) + D (P\u03bc || Q) + 1. \n \nRecalling that \u03bc is, in fact, one possible value of the random variable \u03bc and taking \nexpectations over this random variable, the resulting inequalities are \n \n\u03a3\u03bc \u03c6\u03bcH (P\u03bc) + \u03a3\u03bc \u03c6\u03bc D (P\u03bc || Q) \u2264 \u03a3 \u03bc \u03c6\u03bcE\u03bc [l(\u03b1)] \u2264 \u03a3\u03bc \u03c6\u03bcH (P\u03bc) + \u03a3\u03bc \u03c6\u03bc D (P\u03bc || Q) + 1.",
    "chunk_index": 7,
    "start_char": 18223,
    "end_char": 21202,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "\u03bc) and EP(\u03b1 | \u03bc) , respectively. As guaranteed by the Wrong Code \nTheorem of Cover and Thomas (1991), \n \nH (P\u03bc) + D (P\u03bc || Q) \u2264 E\u03bc [l(\u03b1)] \u2264 H (P\u03bc) + D (P\u03bc || Q) + 1. \n \nRecalling that \u03bc is, in fact, one possible value of the random variable \u03bc and taking \nexpectations over this random variable, the resulting inequalities are \n \n\u03a3\u03bc \u03c6\u03bcH (P\u03bc) + \u03a3\u03bc \u03c6\u03bc D (P\u03bc || Q) \u2264 \u03a3 \u03bc \u03c6\u03bcE\u03bc [l(\u03b1)] \u2264 \u03a3\u03bc \u03c6\u03bcH (P\u03bc) + \u03a3\u03bc \u03c6\u03bc D (P\u03bc || Q) + 1. \n \nThe first two sums in the leftmost and rightmost inequalities are, respectively, H (\u03b1 | \u03bc) \nand I (\u03b1; \u03bc) , and the expectation in the middle sum is indeed E\u03bc [l(\u03b1)]. \u25a0 \n \nThe next step is to consider what happens to pragmatic information in the limit of long \nsequences. However, we must also consider what happens with the side messages, as \ndifferent applications seem to demand different versions of the limit. For example, a \nreceiver/decision maker may have already received an entire side message \u03bc before any \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n9 \noutput information is generated, perhaps because the set of possible side messages is \nfinite. We are then interested in \nN\nN\n1\nlim\n\uf0a5\n\uf0ae\n I (\u03b11 \u2026 \u03b1N; \u03bc). Another possibility is that the \ndecision maker is adding characters to the \u03b1 sequence information at the same time it is \nreceiving information via characters in \u03bc. In this case, we are interested in \nN\nN\n1\nlim\n\uf0a5\n\uf0ae\n I (\u03b11 \n\u2026 \u03b1N; m1 \u2026 m N). In contrast to relative entropy, these limits do exist, per the \n \nExistence Theorem for Pragmatic Information Rates for Stationary \nProcesses. If n is the length of \u03b1, the limits \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\nI (\u03b11 \u2026 \u03b1n; \u03bc), \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\nI (\u03b1; \u03bc1 \u2026 \u03bc n), \n\uf0a5\n\uf0ae\nnlimI (\u03b1n | \u03b11 \u2026 \u03b1n-1; \u03bc), \n\uf0a5\n\uf0ae\nnlimI (\u03b11 \u2026 \u03b1n; \u03bc n | \u03bc \n1 \u2026 \u03bc n-1) , and \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n I (\u03b11 \u2026 \u03b1n; \u03bc1 \u2026 \u03bc n) all exist, thus justifying the use of the \nnotation i(\u03b1; \u03bc) to represent the common limit. \n \nProof: The proof of the existence and equality of the first and third limits parallels the \nproof of the existence and equality of the two versions of the entropy rate, \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n H (\u03b11 \u2026 \n\u03b1n) and \n\uf0a5\n\uf0ae\nnlim H (\u03b1n | \u03b11 \u2026 \u03b1n-1). Additional details can be found in Cover and Thomas \n(1991). \n \nAs with the entropy rate, the terms in the sequence comprising I (\u03b1n | \u03b11 \u2026 \u03b1n-1; \u03bc), for a \nfixed random variable \u03bc , are monotonically decreasing towards zero with increasing n, \nsince we must have, for stationary \u03b1, \n \n0 \u2264 I (\u03b1n | \u03b11 \u2026 \u03b1n-1; \u03bc) \u2264 I (\u03b1n | \u03b12 \u2026 \u03b1n-1; \u03bc) = I (\u03b1 n-1 | \u03b11 \u2026 \u03b1n-2;",
    "chunk_index": 8,
    "start_char": 20785,
    "end_char": 23246,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "H (\u03b1n | \u03b11 \u2026 \u03b1n-1). Additional details can be found in Cover and Thomas \n(1991). \n \nAs with the entropy rate, the terms in the sequence comprising I (\u03b1n | \u03b11 \u2026 \u03b1n-1; \u03bc), for a \nfixed random variable \u03bc , are monotonically decreasing towards zero with increasing n, \nsince we must have, for stationary \u03b1, \n \n0 \u2264 I (\u03b1n | \u03b11 \u2026 \u03b1n-1; \u03bc) \u2264 I (\u03b1n | \u03b12 \u2026 \u03b1n-1; \u03bc) = I (\u03b1 n-1 | \u03b11 \u2026 \u03b1n-2; \u03bc). \n \nA basic theorem in real analysis guarantees that such sequences converge to a definite \nlimit. Another such theorem also guarantees that Cesaro sums of convergent sequences \nconverge to the limit of the summands. Since the chain rule for mutual information, \n \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n I (\u03b11 \u2026 \u03b1n; \u03bc) = \n\uf0e5\n\uf03d\n\uf0a5\n\uf0ae\nN\ni\nn\nn\n1\n1\nlim\nI (\u03b1i | \u03b11 \u2026 \u03b1i -1; \u03bc), \n \nis such a sum, the sequence of such left sides must also converge to the same quantity, \nwhich we denote as i(\u03b1; \u03bc). \n \nThe convergence of the second and fourth limits follows from the complete symmetry \nbetween side messages and what we have called \u201cactions\u201d. The convergence of the fifth \nfollows from the fact that \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n10 \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n I (\u03b11 \u2026 \u03b1n; \u03bc 1 \u2026 \u03bc n) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[H (\u03b11 \u2026 \u03b1n) + H (\u03bc 1 \u2026 \u03bc n) \u2013 H (\u03b11 \u2026 \u03b1n; \u03bc 1 \u2026 \u03bc n)] \n \nand that the individual limits of the three terms on the right exist and sum to the quantity \ndefined as i(\u03b1; \u03bc) above. \u25a0 \n \nWe can then use the above results to conclude that \n \ni(\u03b1; \u03bc) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n \nEP [lQ(\u03b1) - lP(\u03b1; \u03bc)], \n \nwhere lP(\u03b1; \u03bc) is the length, in symbols, of the optimal (shortest) encoding of \u03b1 because it \nuses the information in \u03bc to determine the actual probability distribution of \u03b1. In \ncontrast, lQ (\u03b1) is the length, in symbols, of an encoding of \u03b1 using Q(\u03b1), the probability \ndistribution that would be used for encoding without benefit of \u03bc. This result is sufficient \nto conclude that, for all stationary \u03b1 and \u03bc, the pragmatic information rate is precisely the \nexpected improvement in the length per encoded symbol as the result of receiving \u03bc. \n \nThe equivalent to the Shannon-McMillan-Breiman Theorem for mutual information does \nnot seem to be named after anyone. We therefore state it as the \n \nErgodic Theorem for Mutual Information Rates (Barron, 1985). Let P\u03b1 be the \nprobability measure of the ergodic random sequence \u03b1, let P\u03bc be the \nprobability measure of the ergodic random sequence \u03bc, and let P\u03b1, \u03bc be the \njoint probability measure of the ergodic random sequences \u03b1 and \u03bc.",
    "chunk_index": 9,
    "start_char": 22867,
    "end_char": 25336,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "We therefore state it as the \n \nErgodic Theorem for Mutual Information Rates (Barron, 1985). Let P\u03b1 be the \nprobability measure of the ergodic random sequence \u03b1, let P\u03bc be the \nprobability measure of the ergodic random sequence \u03bc, and let P\u03b1, \u03bc be the \njoint probability measure of the ergodic random sequences \u03b1 and \u03bc. Then, \nwith P\u03b1, \u03bc probability one, \n \n\uf028\n\uf029\n\uf028\uf029\uf028\uf029\n\uf028\n\uf029\n\uf06d\n\uf061\n\uf061\n\uf06d\n\uf06d\n\uf061\n\uf061\n\uf06d\n\uf061\n\uf06d\n,\n,\nlog\n1\nlim\n,\ni\nP\nP\nP\nn\nn\n\uf03d\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0a5\n\uf0ae\n \n \n \nIn particular, with probability one in the measure P\u03b1, \u03bc, \n \ni(\u03b1; \u03bc) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[lQ(\u03b1) - lP(\u03b1; \u03bc)]. \n \n \nNote that \u03b1 and \u03bc can be either finite alphabet or real valued stationary processes. \n \nA point that was insufficiently emphasized in previous work about pragmatic information \nis the tie-in with the theory of noisy communications channels. Central to the latter is a \ncareful formalization of an information source and a communications channel. The \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n11 \nsource is characterized by the probability triple, (Y, Y, \u03c8), consisting of a probability \nspace Y, the collection, Y, of measurable subsets of Y, and the relevant measure, \u03c8. Gray \n(2009) defines the formal channel as the triple [Y, \u03bdy, Z] , where Y and Z are probability \nspaces and, for each y \u03b5 Y, \u03bdy is a probability measure on Z, the collection of measurable \nsubsets of Z. The subscript attached to \u03bdy captures the intuition that there will be different \nprobabilities of receiving z \u03b5 Z, depending on which y was sent. All of this formalism \nmakes possible the definition of what Billingsley (1978) calls the \u201cinput-output measure,\u201d \n\u03a8 as \n \n\uf028\n\uf029\n\uf028\uf029\uf028\n\uf029\ndy\nC\nC\nB\nB\ny\n\uf06d\n\uf06e\uf0f2\n\uf03d\n\uf059\n,\n \n \nfor each B in Y and C in Z. If the receiver could know the channel input, y0 , y1, \u2026 yn-1, \nthe amount of information transmitted through the channel would be H (y0 , y1, \u2026 yn-1), \ncomputed via the measure \u03bc. However, the receiver only knows the channel output, z0 , \nz1, \u2026 zn-1, so the actual amount of information transmitted is \n \nI (y0 , y1, \u2026 yn-1; z0 , z1, \u2026 zn-1) = H (y0 , y1, \u2026 yn-1) \u2013 H (y0 , y1, \u2026 yn-1| z0 , z1, \u2026 zn-1), \n \ni.e. the reduction in the uncertainty of the original message, less the remaining \nuncertainty, given the output of the channel, computed using \u03a8. In other words, the \namount of information transmitted is precisely the pragmatic information of the input and \noutput of the channel. \n \nAn Application to Portfolio Management \n \nFollowing Cover and Thomas (1991), we apply the above results to the ongoing \nperformance of a portfolio of M assets that we are allowed to trade at the beginning of \neach period.",
    "chunk_index": 10,
    "start_char": 25017,
    "end_char": 27643,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "the remaining \nuncertainty, given the output of the channel, computed using \u03a8. In other words, the \namount of information transmitted is precisely the pragmatic information of the input and \noutput of the channel. \n \nAn Application to Portfolio Management \n \nFollowing Cover and Thomas (1991), we apply the above results to the ongoing \nperformance of a portfolio of M assets that we are allowed to trade at the beginning of \neach period. Let the fraction of the portfolio allocated to the mth asset at the nth period be \n0 \u2264 bn\nm \u2264 1, with \u03a3m bn\nm = 1 and suppose that no short sales are allowed. Denote the so-\ncalled \u201cwealth relatives\u201d of the assets for the nth period as the (random) vector Xn, where \nthe wealth relative of each asset at each period is the ratio of its price at the beginning of \nthe period to its price at the end. It follows that each component of Xn and bn are non-\nnegative real numbers and that the value of the portfolio at the beginning of the Nth period \n(for N > 0) is \n \n\uf0d5\n\uf03d\n\uf03d\nN\nn\nn\nT\nn\nN\nS\n0\n.\nX\nb\n \n \nCover and Thomas define the doubling rate \n \n,\nlog2\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf03d\nN\nS\nW\nN\nN E\n \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n12 \nand consider the limit of WN as N \u2192 \u221e, after a suitable warning that the limit may or may \nnot exist. One sufficient condition for the limit to exist is if Xn is ergodic and exactly one \nof the components of Xn has a non-zero value for each n (the so-called \u201chorse race\u201d, a \ngeneralization of the more familiar Kelly criterion). Then5, the fractional change in the \nportfolio after the nth race is \u2211 i bi Xi (n), where the random variable Xi(n) is defined as \n \n\uf028\uf029\n\uf0ee\n\uf0ed\n\uf0ec\n\uf03d\notherwise\nrace\nthe\nwins\nhorse\nif\n0\nth\ni\ni\nn\ni\nR\nn\nX\n \n \nwith probability pi(n). In general, pi(n) and bi can depend on previous races, as well as \nother history-specific conditions, though we assume that Ri cannot. In any case, for a \nsingle race, \n \n\uf028\uf029\n\uf028\uf029\n\uf028\n\uf029\n\uf0e5\n\uf0e5\n\uf03d\n\uf03d\n\uf03d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\nM\ni\ni\ni\ni\nM\ni\ni\ni\nR\nb\nn\np\nn\nX\nb\nE\n1\n2\n1\n2\nlog\nlog\n \n \nBy setting qi = 1/(RiT), with \u03a3k (1/Ri) = T, we ensure that 0 \u2264 qi \u2264 1, for all i, and that \u03a3k \nqk = 1. We can therefore interpret the q\u2019s as \u201ctrack probabilities,\u201d the odds that the \nbookies are prepared to offer. The appropriate substitutions give \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n,\nlog\n \n \nlog\nlog\n2\n1\n2\n1\n2\nT\nTq\np\np\nb\np\nR\nb\np\nM\ni\ni\ni\ni\ni\ni\nM\ni\ni\ni\ni\n\uf02d\n\uf02d\n\uf03d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf03d\uf0e5",
    "chunk_index": 11,
    "start_char": 27205,
    "end_char": 29571,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "qi \u2264 1, for all i, and that \u03a3k \nqk = 1. We can therefore interpret the q\u2019s as \u201ctrack probabilities,\u201d the odds that the \nbookies are prepared to offer. The appropriate substitutions give \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n,\nlog\n \n \nlog\nlog\n2\n1\n2\n1\n2\nT\nTq\np\np\nb\np\nR\nb\np\nM\ni\ni\ni\ni\ni\ni\nM\ni\ni\ni\ni\n\uf02d\n\uf02d\n\uf03d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf03d\uf0e5\n\uf0e5\n\uf03d\n\uf03d\nb\np\nq\np\nD\nD\n \n \nwhere we have introduced the notation p(n) for the vector of the pi(n)\u2019s, q for the vector \nof the qi\u2019s, and b for the vector of the bi\u2019s. The case where T = 1 corresponds to the \nsituation where the race is fair with respect to the track probabilities (and is the case \nconsidered by Cover and Thomas). Indeed, for T = 1 and the bet allocations bi = qi, S(n + \n1) = S(n) for all n and for all values of the X\u2019s. When T > 1, the race is rigged against the \nbettor, who can only make money (via a positive doubling rate) if his betting allocation is \na better estimate of p(n) than the track probabilities q. This is the situation of a \u201cprice \ntaker\u201d who must buy at an \u201coffered price\u201d that is higher than the market maker\u2019s selling \nor \u201cbid price\u201d and who must sell at the market maker\u2019s \u201cbid price\u201d (Figure 1 graphically \nillustrates the relevant conventions.). On the other hand, when T < 1, the race is rigged in \nfavor of the bettor, who can make money with estimates of p(n) no better than, and \nperhaps even somewhat worse than q. This is the situation of the market maker, who \nbuys at the bid and sells at the offer. However, regardless of the value of T, the best \npolicy for the bettor is always to choose b = p(n) because \n \n5 For M = 2, this model corresponds to the situation where a trader has decided to commit a portion b < 1 of \nhis wealth in the hope of realizing a gain of bR, with R > 1, the usual Kelly criterion situation It is the \nexperience of the author that professional traders actually think this way. Sometimes, a trader might decide \nto close out a trade at one or more intermediate points, i.e. before either b is lost or bR is won, a situation \nthat can be incorporated into the present model by assuming that M > 2. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n13 \n \n\uf028\uf029\n\uf028\n\uf029\n,0\n \n\uf0b3\nb\np n\nD\n \n \nwith equality if and only if b = p(n). The optimal doubling rate after N races is therefore \n \n\uf05b\n\uf05d\n\uf028\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\uf029\nT\nq\nn\np\nn\np\nT\nn\nS\nE\nW\nn\ni\ni\ni\ni\nN\nn\nN\nN\nN\nN\n2\n2\n1\n2\n1\n2",
    "chunk_index": 12,
    "start_char": 29277,
    "end_char": 31625,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "model by assuming that M > 2. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n13 \n \n\uf028\uf029\n\uf028\n\uf029\n,0\n \n\uf0b3\nb\np n\nD\n \n \nwith equality if and only if b = p(n). The optimal doubling rate after N races is therefore \n \n\uf05b\n\uf05d\n\uf028\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\uf029\nT\nq\nn\np\nn\np\nT\nn\nS\nE\nW\nn\ni\ni\ni\ni\nN\nn\nN\nN\nN\nN\n2\n2\n1\n2\n1\n2\n1\nlog\nlog\nlog\n \nlog\n*\n\uf02d\n\uf03d\n\uf02d\n\uf03d\n\uf03d\n\uf0e5\uf0e5\n\uf0e5\nq\np\nD\n \n \nWe note that (log2\n SN)/N \u2192 WN* almost surely as N \u2192 \u221e if Xn, the sequence of race \noutcomes, is ergodic. Note also that, in the special case where qi = 1/M for all i (\u201ceven \nodds\u201d for each horse), the above expression for the optimal doubling rate approaches log2 \nM \u2013 h, where h is the per-symbol entropy of the sequence of race outcomes, shown to \nhave a definite value by the Shannon-McMillan-Breiman Theorem. We recognize the \nquantity log2 M \u2013 h as the channel capacity associated with the probability vectors p(n). \n \nPragmatic information enters the picture by supposing that the factors that influence p(n) \nat the nth race are made available to the bettor via a \u201cside message\u201d, \u03bc(n), which could \ninclude such things as the previous history of the process. We reflect the knowledge of \nthese factors by re-writing p(n) as p(\u03bc), the probability vector given the side message \u03bc \n(with its argument n suppressed) with components (p1|\u03bc, p2|\u03bc, \u2026, pM|\u03bc). At the nth race, we \nnow have \n\uf028\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\n\uf029\n\uf028\uf029\n\uf028\n\uf029\nT\nR\nb\np\nn\nX\nb\nE\nM\ni\ni\ni\ni\nM\ni\ni\ni\n2\n1\n2\n|\n1\n2\nlog\n \n \nlog\nlog\n\uf02d\n\uf02d\n\uf03d\n\uf03d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf0e5\n\uf0e5\n\uf03d\n\uf03d\nb\np\nq\np\n\uf06d\n\uf06d\n\uf06d\nD\nD\n \n \nIt follows that at the nth race, the bettor should choose b = p(\u03bc) for all n. If \u03c6\u00b5 is the \nprobability of side message \u00b5, the optimal expected doubling rate, now averaged over all \nsuch side messages, is given by \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n14 \n\uf05b\n\uf05d\n\uf028\uf029\n\uf028\n\uf029\nT\nq\np\np\np\np\np\nT\np\nq\np\np\np\nT\nq\np\np\nE\nT\nE\nS\nE\nW\nn\ni\ni\ni\ni\nN\nn\ni\ni\ni\ni\nN\nn\ni\ni\ni\ni\ni\ni\nN\nn\ni\ni\ni\ni\nN\nn\nN\nN\nN\nN\n2\n2\n1\n,\n2\n,\n1\n2\n,\n2\n,\n1\n2\n|\n2\n|\n1\n2\n1\n2\n1\nlog\nlog\nlog\nlog\nlog\nlog\nlog\nlog\n \nlog\n*\n\uf02d\n\uf02b\n\uf03d\n\uf02d\n\uf03d\n\uf02d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf03d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf02d\n\uf03d\n\uf03d\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d",
    "chunk_index": 13,
    "start_char": 31314,
    "end_char": 33372,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "i\ni\ni\nN\nn\ni\ni\ni\ni\nN\nn\nN\nN\nN\nN\n2\n2\n1\n,\n2\n,\n1\n2\n,\n2\n,\n1\n2\n|\n2\n|\n1\n2\n1\n2\n1\nlog\nlog\nlog\nlog\nlog\nlog\nlog\nlog\n \nlog\n*\n\uf02d\n\uf02b\n\uf03d\n\uf02d\n\uf03d\n\uf02d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf03d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf02d\n\uf03d\n\uf03d\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\uf0e5\n\uf0e5\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06d\n\uf06a\n\uf06a\nq\np\nD\n \n \nComparing the above expression for WN* with the equivalent expression without side \ninformation, we see that the additional term in the former is precisely the pragmatic \ninformation of the side messages. Furthermore, if the side messages are an ergodic \nmessage stream, then, once more, the convergence, as N \u2192 \u221e, of (log2\n SN)/N to its \nexpected value is almost sure. \n \nIn the more general case where more than one of the Xi\u2019s can be positive, and X is merely \nstationary, Cover and Thomas\u2019s only guidance as to finding the optimal allocation is that, \nif cn(X) is optimal and an(X) is another allocation, then E[an(X) T X n / cn(X) T X n] \u2264 1. \nNevertheless, if bn(X) is optimal, given side information \u03bcn, and cn is optimal without it, \nthe mutual information between \u03bc and X still bounds the difference in the optimal \ndoubling rates, Wb \u2013 Wc = \u0394 W, provided the expectation is taken over both \u03bc and X. In \ndemonstrating this result, let f(vn | Xn, \u03bcn) be the conditional density of Xn, given Xn and \n\u03bcn, g(vn | Xn) for the conditional density of Xn, given only Xn, and let \u03c6(\u03bcn) be the \nunconditional density of \u03bcn. It is also convenient to suppress the X dependence of b and \nc. We have, if we use subscripts for variables over which expectations are taken, \n \n\uf028\n\uf029\n\uf05b\n\uf05d\n\uf028\n\uf029\n\uf05b\n\uf05d\n\uf028\n\uf029\n\uf05b\n\uf05d\n\uf028\n\uf029\n\uf05b\n\uf05d\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0eb\n\uf0e9\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf02d\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf02d\n\uf03d\n\uf044\n\uf02d\n\uf03d\n\uf0a5\n\uf0ae\n\uf02d\n\uf03d\n\uf02d\n\uf03d\n\uf0a5\n\uf0ae\n\uf02d\n\uf03d\n\uf02d\n\uf03d\n\uf0a5\n\uf0ae\n\uf0e5\n\uf0e5\n\uf0e5\n\uf0e5\n\uf0e5\n1\n2\n1\n1\n2\n1\n1\n2\n1\n1\n2\n1\n1\n2\n1\nlog\n1\nlim\nlog\nlog\n1\nlim\nlog\nlog\n1\nlim\nn\nn\nT\nn\nT\nN\nn\nX\nN\nn\nn\nT\nN\nn\nX\nn\nn\nT\nN\nn\nX\nN\nn\nn\nT\nN\nn\nX\nn\nn\nT\nN\nn\nX\nN\nX\nX\nE\nN\nX\nE\nX\nE\nN\nX\nE\nX\nE\nN\nW\nN\nn\nN\nN\nn\nN\nn\nN\nn\nn\nN\nN\nE\nE\nE\nX\nc\nb\nX\nc\nX\nb\nX\nc\nX\nb\nX\nX\nX\n\u03bc\n\u03bc\n\u03bc\n\u03bc\n \n \nThe first line of the above set of equations represents a re-writing of the expectation of \nthe definition as an iterated expectation.",
    "chunk_index": 14,
    "start_char": 33185,
    "end_char": 35172,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "T\nN\nn\nX\nN\nn\nn\nT\nN\nn\nX\nn\nn\nT\nN\nn\nX\nN\nX\nX\nE\nN\nX\nE\nX\nE\nN\nX\nE\nX\nE\nN\nW\nN\nn\nN\nN\nn\nN\nn\nN\nn\nn\nN\nN\nE\nE\nE\nX\nc\nb\nX\nc\nX\nb\nX\nc\nX\nb\nX\nX\nX\n\u03bc\n\u03bc\n\u03bc\n\u03bc\n \n \nThe first line of the above set of equations represents a re-writing of the expectation of \nthe definition as an iterated expectation. The second line uses the fact that we don\u2019t \nchange the second term on the right side if we take expectations over a variable upon \nwhich nothing in that second term depends. Upon explicitly writing out the inner \nexpectation in the third equation, we have \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n15 \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf03d\n\uf044\n\uf0e5\uf0f2\n\uf0f2\n\uf02d\n\uf03d\n\uf0a5\n\uf0a5\n\uf02d\n\uf0a5\n\uf0a5\n\uf02d\n\uf0a5\n\uf0ae\n1\n0\n2\nlog\n,\n|\n1\nlim\nN\nn\nn\nn\nT\nn\nn\nT\nn\nn\nn\nn\nn\nn\nN\nN\nd\nf\nd\nN\nW\nE\nv\nv\nc\nv\nb\nm\nX\nv\nm\nm\nX\n\uf06a\n \nWe can re-write the argument of the logarithm above as \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf03d\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nT\nn\nn\nT\nn\nn\nT\nn\nn\nT\nn\ng\nf\nf\ng\nX\nv\nm\nX\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nv\nc\nv\nb\n|\n,\n|\n,\n|\n|\n2\nlog\nlog2\n, \n \nfrom which it follows that the integrals in the above sum can be re-written as \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf02b\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf03d\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf0f2\uf0a5\n\uf0a5\n\uf02d\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\n\uf0fa\uf0fb\n\uf0f9\n\uf0ea\uf0eb\n\uf0e9\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nT\nn\nn\nT\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nT\nn\nn\nT\nn\nn\nn\nn\nn\nn\nd\ng\nf\nf\nd\nd\nf\ng\nf\nd\nd\ng\nf\nf\ng\nf\nd\nv\nX\nv\nm\nX\nv\nm\nX\nv\nm\nm\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nm\nX\nv\nm\nm\nv\nX\nv\nm\nX\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nm\nX\nv\nm\nm\n|\n,\n|\n2\nlog\n,\n|\n,\n|\n|",
    "chunk_index": 15,
    "start_char": 34902,
    "end_char": 36355,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "T\nn\nn\nn\nn\nn\nn\nd\ng\nf\nf\nd\nd\nf\ng\nf\nd\nd\ng\nf\nf\ng\nf\nd\nv\nX\nv\nm\nX\nv\nm\nX\nv\nm\nm\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nm\nX\nv\nm\nm\nv\nX\nv\nm\nX\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nm\nX\nv\nm\nm\n|\n,\n|\n2\nlog\n,\n|\n,\n|\n|\n2\nlog\n,\n|\n|\n,\n|\n,\n|\n|\n2\nlog\n,\n|\n\uf06a\n\uf06a\n\uf06a\n \n \n \nThe second term in the right of the last equation is the conditional pragmatic information \nof Xn and \u03bcn, given Xn. For the first term in the above equation, Jensen\u2019s inequality \nallows us to write \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf03d\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf0a3\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf0f2\n\uf0f2\n\uf0f2\n\uf0a5\n\uf0a5\n\uf02d\n\uf0a5\n\uf0a5\n\uf02d\n\uf0a5\n\uf0a5\n\uf02d\nn\nn\nT\nn\nn\nT\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nT\nn\nn\nT\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nT\nn\nT\nn\nn\nd\ng\nd\nf\ng\nf\nd\nf\ng\nf\nn\nn\nv\nv\nc\nv\nb\nX\nv\nv\nm\nX\nv\nX\nv\nv\nc\nv\nb\nm\nX\nv\nv\nm\nX\nv\nX\nv\nx\nc\nx\nb\nm\nv\n|\nlog\n,\n|\n|\n,\n|\nlog\n,\n|\n|\nlog\n|\n2\n2\n2\n \n \nRecall that cn is the optimum allocation for the nth period if the side information is \nunavailable and that g(vn) is the conditional density of Xn, regardless of side information, \na situation in which bn is not necessarily optimal. As a result, \n \n\uf028\n\uf029\n1\n|\n\uf0a3\n\uf0f2\n\uf0a5\n\uf0a5\n\uf02d\nn\nn\nT\nn\nn\nT\nn\nn\nn\nd\ng\nv\nv\nc\nv\nb\nX\nv\n \nand \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n16 \n\uf028\n\uf029\n0\n|\nlog2\n\uf0a3\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf0f2\n\uf0a5\n\uf0a5\n\uf02d\nn\nn\nT\nn\nn\nT\nn\nn\nn\nd\ng\nv\nv\nc\nv\nb\nX\nv\n \n \nWe conclude that \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029,\n;\n;\nlim\n|\n;\n1\nlim\n1\n1\n\uf06d\n\uf06d\n\uf06d\nX\nX\nX\nX\nX\ni\nN\nX\nN\nW\nN\nN\nN\nN\nN\nn\nn\nn\nn\nN\nN\nE\nE\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf0a3\n\uf044\n\uf0a5\n\uf0ae\n\uf03d\n\uf02d\n\uf0a5\n\uf0ae\n\uf0e5\nI\nI\n \n \nas claimed. \n \nAn Application to Market Efficiency \n \nAs suggested previously, pragmatic information can be a useful tool in the study of \nfinancial market efficiency. A physical system in equilibrium is characterized by being \nin a state of maximum entropy.",
    "chunk_index": 16,
    "start_char": 36194,
    "end_char": 37795,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "X\nX\nX\ni\nN\nX\nN\nW\nN\nN\nN\nN\nN\nn\nn\nn\nn\nN\nN\nE\nE\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf03d\n\uf0fe\uf0fd\uf0fc\n\uf0ee\uf0ed\uf0ec\n\uf0a3\n\uf044\n\uf0a5\n\uf0ae\n\uf03d\n\uf02d\n\uf0a5\n\uf0ae\n\uf0e5\nI\nI\n \n \nas claimed. \n \nAn Application to Market Efficiency \n \nAs suggested previously, pragmatic information can be a useful tool in the study of \nfinancial market efficiency. A physical system in equilibrium is characterized by being \nin a state of maximum entropy. Similarly, markets are in equilibrium when there is no \npossibility of arbitrage, since the pragmatic information of the future, given the past is \nzero. As Figure 1 makes clear, a well defined equilibrium price, Pi (t), exists for a security \nonly at those moments when the best bid is at least as large as the best offer, and, even \nthen, that price exists only for the exact position size that was traded. The rest of the \ntime, the best that can be said is that the existence of a bid/offer spread implies that \nprices, and thus equilibria, are \u201cnoisy\u201d, in a sense we now make precise. \n \nThe previous discussion shows that the per-symbol rate \n \nh (P) \u2013 h (P | \u03bc) = i(P ; \u03bc) \n \nexists almost everywhere under the input-output measure, and that it represents the rate at \nwhich a market converts economically relevant information, \u03bc, into prices, P. We are \ntherefore justified in identifying market efficiency with the statement that \n \nh (P) \u2013 h (P | \u03bc-) = i(P ; \u03bc-) = 0. \n \nHere, \u03bc- represents economically relevant information known in the \u201ctradable past\u201d, i.e. \nthe past separated from the present by the (ever decreasing) lag between the time we \nacquire economically relevant information and the time we can utilize this information in \na trade. If i(P ; \u03bc-) = 0, then h (P) = h (P | \u03bc-), implying that a knowledge of the \u201ctradable \npast\u201d tells us nothing worth knowing about present prices. In fact, this is demonstrably \nnot true, as we now show. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n17 \n \nIn comparing their multifractal model to GARCH(1,1) price dynamics, Calvet and Fisher \n(2008) state that \u201cGARCH(1,1) is often viewed as a standard benchmark that is very \ndifficult to outperform in forecasting exercises.\u201d Another, even more widely used \nbenchmark for financial modeling is the normal distribution of returns. Accordingly, we \nchoose a GARCH(1,1) process with normally distributed returns for a study of i(P ; \u03bc-). \n \nCover and Thomas (1991) give the formula h (P) = \u00bd log2 (2\u03c0 e \u03c30\n2) for the entropy (in \nbits) of the unconstrained distribution of returns, where e has its usual meaning as the \nbase of natural logarithms and \u03c30 is the unconstrained volatility of returns. The \nappearance of \u03c30 in this formula should not be surprising, because it is a reasonable proxy \nfor uncertainty in returns on an absolute scale6. Similarly, h (P | \u03bc-), the entropy of \nreturns, conditional on the tradable past, is \u00bd log2 (2\u03c0 e \u03c3n\n2), given that \n \n\u03c3n\n2\n = \u03b1 \u03c3n-1\n2 + \u03b2 \u03c30\n2 + \u03b3 Rn\n2\n .",
    "chunk_index": 17,
    "start_char": 37449,
    "end_char": 40339,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "\u00bd log2 (2\u03c0 e \u03c30\n2) for the entropy (in \nbits) of the unconstrained distribution of returns, where e has its usual meaning as the \nbase of natural logarithms and \u03c30 is the unconstrained volatility of returns. The \nappearance of \u03c30 in this formula should not be surprising, because it is a reasonable proxy \nfor uncertainty in returns on an absolute scale6. Similarly, h (P | \u03bc-), the entropy of \nreturns, conditional on the tradable past, is \u00bd log2 (2\u03c0 e \u03c3n\n2), given that \n \n\u03c3n\n2\n = \u03b1 \u03c3n-1\n2 + \u03b2 \u03c30\n2 + \u03b3 Rn\n2\n . \n \nSince we must have \u03b1 + \u03b2 + \u03b3 = 1 to ensure that \u03c32\n n remains positive and finite, \n \n\uf028\n\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf03d\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf02b\n\uf02d\n\uf02d\n\uf02b\n\uf03d\n\uf02d\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n1\n1\n1\nlog\n1\nlog\n2\nlog\n2\nlog\n2\n0\n1\n2\n0\n1\n2\n2\n1\n2\n1\n2\n2\n2\n2\n2\n1\n2\n2\n2\n1\n2\n2\n2\n1\n0\n1\n0\n0\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\n\uf067\n\uf073\n\uf067\n\uf061\n\uf061\uf073\n\uf073\n\uf073\n\uf070\n\uf073\n\uf070\nn\nn\nn\nR\nR\ne\ne\ni\nn\nn\n \n \nIf Rn is a stochastic process, then both \u03c3n and the above expression must also be \nstochastic. It therefore makes sense to take expectations. We thus obtain \n \n \n6 as opposed to the logarithmic scale used by entropy. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n18 \n\uf05b\uf05d\n\uf07b\uf07d\n,0\n1\nlog\n1\n1\n1\nlog\n1\n1\n1\nlog\n1\n1\n1\nlog\n2\n2\n1\n2\n0\n1\n2\n0\n1\n2\n2\n1\n2\n0\n1\n2\n0\n1\n2\n2\n1\n2\n0\n1\n2\n0\n1\n2\n2\n1\n\uf03d\n\uf02d\n\uf03d\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf03d\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf0b3\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\nn\nn\nn\nn\nn\nn\nR\nE\nE\nR\nE\nR\nE\ni",
    "chunk_index": 18,
    "start_char": 39827,
    "end_char": 41455,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf0b3\n\uf0ef\uf0fe\n\uf0ef\uf0fd\n\uf0fc\n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf0fa\n\uf0fa\n\uf0fb\n\uf0f9\n\uf0ea\n\uf0ea\n\uf0eb\n\uf0e9\n\uf02d\n\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\n\uf02b\n\uf02d\n\uf03d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf02d\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\n\uf073\n\uf067\n\uf073\n\uf073\n\uf061\nn\nn\nn\nn\nn\nn\nR\nE\nE\nR\nE\nR\nE\ni\nE\n \n \nwhere the inequality in the second line follows from Jensen\u2019s inequality, and the \nexpectations are zero by the assumption of stationarity. Because h (P) \u2265 h (P | \u03bc-), we \nmust have E[i] \u2265 0 in any case. However, Jensen\u2019s inequality is strict except for the \nunlikely scenario in which \u03c3n = Rn = \u03c30 almost surely. In all other scenarios, we \nconclude that E[i] is strictly positive always, and thus i itself must be strictly positive, at \nleast some of the time. \n \nBut the real advantage of the present formulation of market efficiency is the possibility of \nactually quantifying the amount of inefficiency and investigating where significant \nsources of inefficiency arise. For example, in most estimates of the GARCH(1,1) model, \n\u03b1 is significantly larger than \u03b3, which corresponds to the intuition that the predictability of \nvolatility plays a greater role in any market inefficiency than the predictability of price \nbecause volatility is the more abstract quantity. More comprehensive studies of the \nobserved pragmatic information in various markets, possibly gleaned from the \ncompressibility of various price series, might provide additional details about how real \nmarkets approximate the ideal of efficiency. \n \nThe Noisy Coding Theorem as a Phase Transition and What that Means for Finance \n \nAs indicated in the Introduction, the primary goal of this paper is to identify the \nappropriate pragmatic information theoretic parallels to the Noiseless Coding Theorem of \nstandard information theory. In this section, however, we briefly consider the most \nsignificant result in information theory, namely, Shannon\u2019s Noisy Coding Theorem \n(Cover and Thomas, 1991; Gray, 2009). According to this theorem, there exists a precise \n\u201cchannel capacity\u201d, C, such that information can be transmitted at any rate R < C with \narbitrarily small probability of error. In contrast, if R > C, the probability of error is \nbounded away from zero. In general, C will depend on both the channel and \u03c8, the \nprobability measure of the input signals. The \u03c8 dependence is eliminated by defining the \n\u201cchannel capacity\u201d, C, as \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n19 \nC = sup \u03c8(Y) [h (Y) \u2013 h (Y | Z)], \n \nwhere the supremum is taken over all probability measures, \u03c8 (Y) , of input signals, Y, \nand the entropy rates h (Y) and h (Y | Z) are given by \n \nh (Y) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[H (Y1 \u2026 Yn)] \nand \n \nh (Y | Z) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[H (Y1 \u2026 Yn | Z1 \u2026 Zn)], \n \nrespectively.",
    "chunk_index": 19,
    "start_char": 41288,
    "end_char": 43962,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "Criterion, and \u2026 \n \n \n19 \nC = sup \u03c8(Y) [h (Y) \u2013 h (Y | Z)], \n \nwhere the supremum is taken over all probability measures, \u03c8 (Y) , of input signals, Y, \nand the entropy rates h (Y) and h (Y | Z) are given by \n \nh (Y) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[H (Y1 \u2026 Yn)] \nand \n \nh (Y | Z) = \nn\nn\n1\nlim\n\uf0a5\n\uf0ae\n[H (Y1 \u2026 Yn | Z1 \u2026 Zn)], \n \nrespectively. \n \nThe proof of the Noisy Coding Theorem usually assumes that the channel is memoryless, \nin that the output of the channel depends on neither previous inputs nor previous outputs. \nIf the supremum in the definition of the channel capacity is over all ergodic or stationary \nsources, this assumption is justified, because it can be shown that feedback from previous \ninputs does not increase the channel capacity. However, if only normally distributed \nsignals are input, feedback does increase channel capacity. Perhaps feedback or a \nchannel with memory would increase channel capacity for sources restricted either to \nGARCH processes or multi-fractal processes. It is certainly true that a receiver can alter \nthe statistical characteristics of an input signal. For example, the output of a finite state \nmachine (Hopcroft, et. al., 2006) that receives a sequence of independent input random \nvariables is a Markov Chain, as the probability of a specific output will, in general, \ndepend on the previous state of the receiver. \n \nThe significance of the Noisy Coding Theorem in its original context is that the channel \ncapacity can be deduced from physical properties of the channel, such as its signal to \nnoise ratio, etc. Physical and psychological considerations might well determine a \n\u201cchannel capacity\u201d in the financial context, as well, in that there is surely a limit to the \nrate at which investors can respond to changing market conditions. What seems to be \nunique to the financial context is that investors are constantly re-evaluating what \nconstitutes \u201csignal\u201d and what constitutes \u201cnoise\u201d. Perhaps more importantly, investors \nsometimes willfully ignore the realities of the market, as we saw so tragically in the run-\nup to the current financial crisis. We can treat such willful ignorance as an effective \nlowering of the channel capacity. \n \nIn physics, phase transitions (sudden, qualitative changes in a physical system with small \nchanges in temperature, pressure, or other state variable) are characterized by a \ndiscontinuity in the physical entropy of the system or its derivative with respect to that \nstate variable (Thompson, 1979). The Noisy Coding Theorem is effectively the statement \nthat the mutual information has a discontinuous derivative with respect to the input \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n20 \nentropy rate at the channel capacity. Since the input entropy rate to a communications \nchannel is the analogue of the physical entropy, the Noisy Coding Theorem is therefore \nalso the statement that there is a phase transition of the mutual information of the \nchannel.",
    "chunk_index": 20,
    "start_char": 43639,
    "end_char": 46625,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "statement \nthat the mutual information has a discontinuous derivative with respect to the input \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n20 \nentropy rate at the channel capacity. Since the input entropy rate to a communications \nchannel is the analogue of the physical entropy, the Noisy Coding Theorem is therefore \nalso the statement that there is a phase transition of the mutual information of the \nchannel. In fact, papers such as (Kabashima and Saad, 1999) study a class of error \ncorrecting codes using the detailed formalism developed to describe physical phase \ntransitions. These kinds of results are of interest because wide classes of systems seem to \nexhibit qualitatively and sometimes even quantitatively similar behaviors, irrespective of \ntheir detailed composition. For example, the so-called quasispecies model of evolution \n(Eigen, 1971, Eigen and Schuster, 1979, Eigen, Schuster, and McCaskill, 1988) has an \n\u201cerror threshold\u201d, a mutation rate above which selective advantages cannot be passed on \nto future generations. This error threshold, which is effectively a phase transition \n(Campos and Fontanari, 1999), can also be viewed as an attempt by nature to transmit \ngenetic information from one generation to the next at a rate that is faster than a genetic \n\u201cchannel capacity\u201d. \n \nSummary and Conclusion \n \nAs indicated in the Introduction, this paper is part of an ongoing investigation of the \nnotion of \u201cpragmatic information.\u201d One goal of this investigation, and one of the first \ngoals of this paper was to establish that the mutual information between the input to a \ndecision making process and the decisions that are output is a sensible definition of \npragmatic information. What seems novel about this view is that the actions made as the \nresult of a decision are also part of the communications channel that transmitted the \ninformation to the decision maker. This paper has shown that this kind of analysis can be \nextended to systems involving information rates. \n \nWhile the mathematics underlying mutual information is well known, the pragmatic \ninformation paradigm seems capable of providing new insight to apparently diverse \nphenomena. In a previous paper (Weinberger, 2002), it was shown that a pragmatic \ninformation could be associated with a biological population and that the rate of its \nincrease could be identified with the rate at which the population was evolving. In this \npaper, we associated pragmatic information with optimal doubling rates of financial \nmarket returns and with a measure of market inefficiency. We also saw that the run-up to \na financial bubble can be characterized as a lowering of a pragmatic informational \nchannel capacity. Whether these ideas themselves contain any pragmatic information \nwill only become clear as the result of further research. \n \nREFERENCES \n \nAlogoet, P. and Cover, T. (1988). \u201cAsymptotic Optimality and Asymptotic Equipartition \nProperties of Log-Optimum Investment,\u201d The Annals of Probability, 16 (2), 876-898. \nBarron, A. (1985). \u201cThe Strong Ergodic Theorem for Densities: Generalized Shannon-\nMcMillan-Breiman Theorem,\u201d The Annals of Probability, 13, No. 4, 1292-1303. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n21 \nBillingsley, P. (1978).",
    "chunk_index": 21,
    "start_char": 46173,
    "end_char": 49501,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "capacity. Whether these ideas themselves contain any pragmatic information \nwill only become clear as the result of further research. \n \nREFERENCES \n \nAlogoet, P. and Cover, T. (1988). \u201cAsymptotic Optimality and Asymptotic Equipartition \nProperties of Log-Optimum Investment,\u201d The Annals of Probability, 16 (2), 876-898. \nBarron, A. (1985). \u201cThe Strong Ergodic Theorem for Densities: Generalized Shannon-\nMcMillan-Breiman Theorem,\u201d The Annals of Probability, 13, No. 4, 1292-1303. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n21 \nBillingsley, P. (1978). Ergodic Theory and Information Theory, Robert E. Krieger Publishing \nCompany, Huntington, NY. \nBrigham, E. and Ehrhardt, M. (2005). Financial Management, Theory and Practice, 11th Ed., \nThompson Southwestern, \nCalvet, L. and Fisher, A. (2008). Multifractal Volatility: Theory, Forecasting, and Pricing, \nAcademic Press, Oxford, England. ISBN 978-0-1-12-150013-9. \nCampos, P. and Fontanari, J. (1999). \u201cFinite-size scaling of the error threshold transition in finite \npopulations,\u201d Journal of Physics A: Mathematical and General 32 L1 doi: 10.1088/0305-\n4470/32/1/001 \nCover, T. and Thomas, J. (1991). Elements of Information Theory, John Wiley & Sons, New \nYork. \nEigen, M. (1971).\"Self-Organization of Matter and the Evolution of Biological Macromolecules,\" \nDie Naturwissenschaften, 58: 465-532. \nEigen, M., and Schuster, P. (1979). The Hypercycle, Springer, Berlin. \nEigen, M., McCaskill, J., and Schuster, P. (1988). \"Molecular Quasi-Species\", The Journal of \nPhysical Chemistry 92, 6881. \nFarmer, J. D., Patelli, P., and Zovko, I. (2004). \u201cThe predictive power of zero intelligence in \nfinancial markets,\u201d Proceedings of the National Academy of Sciences, 102 (6), Feb. 8, 2005, \n2254-2259. Full text available on the web at \nhttp://www.pnas.org/content/102/6/2254.full.pdf+html \nFeller, W. (1970). An Introduction to Probability Theory, John Wiley & Sons, New York. \nGray, R. (2009). Entropy and Information Theory, Springer-Verlag, New York. Full text \navailable on the web at www-ee.stanford.edu/~gray/it.html. \nHopcroft, J., Motwani, R., and Ullman, J. (2006). Introduction to Automata Theory, Languages , \nand Computation, Third Ed., Addison-Wesley, ISBN 978-0321462251. \nKabashima, A. and Saad, D. (1999). \u201cStatistical mechanics of error correcting codes,\u201d \nEurophysics Letters, 45, No. 4, 97-103. \nLo, A. and MacKinlay, A. C. (1999). A Non-Random Walk Down Wall Street, Princeton \nUniversity Press, Princeton, N. J. \nLo, A., Mamaysky, H. and Wang, J. (2000). \u201cFoundations of Technical Analysis: Computational \nAlgorithms, Statistical Inference, and Empirical Implementation,\u201d Journal of Finance, 55, 1705-\n1765. \nMalkiel, B. (1991). A Random Walk Down Wall Street, W.W. Norton, ISBN 978-0393315295. \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n22 \nMontegna, R. and Stanley, H.E. (1995). \u201cScaling behavior in the dynamics of an economic \nindex,\u201d Nature, 376, 46-49, July 6, 1995. \nMurphy, J. (1999). Technical Analysis of the Financial Markets: A Comprehensive Guide to \nTrading Methods and Applications, New York Institute of Finance, New York, NY. \nShannon, C. and Weaver, W. (1962). The Mathematical Theory of Communication. University \nof Illinois Press, Champaign-Urbana. \nSchroeder, Manfred (1991). Fractals, Chaos, Power Laws, W.H. Freeman & Co., New York. \nShields, P. (1991a). \u201cCutting and Stacking: A Method for Constructing Stationary Processes,\u201d \nIEEE Transactions on Information Theory, 37, #6, 1605-1617, November 1991. \nShields, P. (1991b). \u201cSome Divergence-rate Counterexamples,\u201d Journal of Theoretical \nProbability. \nTaleb, N. (2010).",
    "chunk_index": 22,
    "start_char": 48911,
    "end_char": 52600,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "Trading Methods and Applications, New York Institute of Finance, New York, NY. \nShannon, C. and Weaver, W. (1962). The Mathematical Theory of Communication. University \nof Illinois Press, Champaign-Urbana. \nSchroeder, Manfred (1991). Fractals, Chaos, Power Laws, W.H. Freeman & Co., New York. \nShields, P. (1991a). \u201cCutting and Stacking: A Method for Constructing Stationary Processes,\u201d \nIEEE Transactions on Information Theory, 37, #6, 1605-1617, November 1991. \nShields, P. (1991b). \u201cSome Divergence-rate Counterexamples,\u201d Journal of Theoretical \nProbability. \nTaleb, N. (2010). The Black Swan, 2nd Ed., Random House Trade- Paperback. ISBN 978-\n0812973815 \nThompson, C. (1979). Mathematical Statistical Mechanics, Princeton University Press, \nPrinceton, New Jersey. \n \nWeinberger, E. (2002). \u201cA Theory of Pragmatic Information and Its Application to the \nQuasispecies Model of Biological Evolution,\u201d BioSystems, 66, No. 3, 105-119. Electronic \nversion: http://arxiv.org/abs/nlin.AO/0105030. \nWeinberger, E. (2006). \u201cPragmatic Information and Gaian Development,\u201d Mind and Matter, 4, \n#2, 219-234. Electronic version: http://arxiv.org/abs/nlin.AO/0606012. \n \n \n\nPragmatic Information Rates, Generalizations of the Kelly Criterion, and \u2026 \n \n \n23 \n \nIncreasing \navailability \nSupply \n(offers) \nDemand \n(bids) \n\u201cMessages\u201d to price \ndiscovery process, \nincluding previous \nprice history, \neconomic \nfundamentals, etc. \nINVESTORS\u2019 \nCOLLECTIVE \nTRADING \nDECISIONS \nInternal \nNoise \nBest \nbid \nBest \noffer \nBid/offer spread \nIncreasing price \nPRICE DISCOVERY \nMECHANISM \nExternal \nNoise \nPRICES \nFIGURE 1. From market conditions to prices via countless individual trading \ndecisions, resulting in the complex ecosystem of bid/s and offers that constitutes \nthe price discovery process. Note that both the \u201cinternal noise\u201d of market \nrumors, bad judgment, etc. and the \u201cexternal noise\u201d of errors in economic data \nmeasurements, structural limitations in markets, etc. can both limit the precision \nof price discovery",
    "chunk_index": 23,
    "start_char": 52020,
    "end_char": 54028,
    "paper_title": "Pragmatic Information Rates Generalizations of the",
    "paper_category": "cs.IT",
    "paper_filename": "Pragmatic_Information_Rates_Generalizations_of_the.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/cs.IT/Pragmatic_Information_Rates_Generalizations_of_the.pdf"
  },
  {
    "text": "1 \nMarket-Based \u201cActual\u201d Returns of Investors \n \nVictor Olkhov \nIndependent, Moscow, Russia \nvictor.olkhov@gmail.com \nORCID: 0000-0003-0944-5113 \nVers. 20 Feb., 2024 \n \nABSTRACT \n \nWe describe how the market-based average and volatility of the \u201cactual\u201d return, which the \ninvestors gain within their market sales, depend on the statistical moments, volatilities, and \ncorrelations of the current and past market trade values. We describe three successive \napproximations. First, we derive the dependence of the market-based average and volatility \nof a single sale return on market trade statistical moments determined by multiple purchases \nin the past. Then, we describe the dependence of average and volatility of return that a single \ninvestor gains during the \u201ctrading day.\u201d Finally, we derive the market-based average and \nvolatility of return of different investors during the \u201ctrading day\" as a function of volatilities \nand correlations of market trade values. That highlights the distribution of the \u201cactual\u201d return \nof market trade and can serve as a benchmark for \u201cpurchasing\u201d investors. \n \nKeywords\uf020: market-based volatility, stock returns, trade value correlations \nJEL: C1, E4, F3, G1, G12 \n \n \n\uf020This research received no support, specific grants, or financial assistance from funding agencies in the public, \ncommercial, or nonprofit sectors. We welcome valuable offers of grants, support, and positions. \n\n \n2 \n1. Introduction \nThe literature describes the two kinds of stock returns. For convenience, we note such returns \nas \u201canticipated\u201d and \u201cactual\u201d returns. The most studied \u201canticipated\u201d return r(t,\u03c4) is \ndetermined by the ratio r(t,\u03c4)=p(t)/p(t-\u03c4) of the stock price p(t) traded \u201ctoday\u201d at time t and \nthe price p(t-\u03c4) traded at t-\u03c4 in the past. The \u201canticipated\u201d stock returns r(t,\u03c4) describe the \nexpected, anticipated gains or losses that investors could get if they bought stocks at time t-\u03c4 \nin the past and then sold them at time t \u201ctoday.\u201d Modeling and predictions of the \n\u201canticipated\u201d stock return at horizon T define the core issues of financial economics (Fisher \nand Lorie, 1964; Mandelbrot, Fisher, and Calvet, 1997; Campbell, 1985; Brown, 1989; Fama, \n1990; Fama and French, 1992; Lettau and Ludvigson, 2003; Greenwood and Shleifer, 2013; \nvan Binsbergen and Koijen, 2015; Martin and Wagner, 2019). The description of the \nprobabilistic properties of the \u201canticipated\u201d stock return during any specific averaging time \ninterval \u0394 \u201ctoday,\u201d or, as we note, a \u201ctrading day,\u201d and predictions of the probability of return \nat horizon T \u201cnext day,\u201d deliver the most desired results for investors and traders. The \nfrequency-based analysis of the return time series assesses the probability distributions of the \n\u201canticipated\u201d return during a \u201ctrading day\u201d (Amaral et al., 2000; Andersen et al., 2001; \nKnight and Satchell, 2001; Tsay, 2005; Andersen and Benzoni, 2009). \nHowever, the \u201canticipated\u201d stock return r(t,\u03c4)=p(t)/p(t-\u03c4) with the time shift \u03c4 describes the \ngains or losses that may or may not match the real, \u201cactual\u201d return of investors. As \u201cactual,\u201d \nwe consider the return that a particular investor gains from selling the stocks at the time t, \nwhich he has purchased in 10 minutes, a day, a week, or any time in the past. Obviously, not \nall stocks that investors sell at time t \u201ctoday\u201d were purchased at time t-\u03c4 in the past.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3372,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "2001; Tsay, 2005; Andersen and Benzoni, 2009). \nHowever, the \u201canticipated\u201d stock return r(t,\u03c4)=p(t)/p(t-\u03c4) with the time shift \u03c4 describes the \ngains or losses that may or may not match the real, \u201cactual\u201d return of investors. As \u201cactual,\u201d \nwe consider the return that a particular investor gains from selling the stocks at the time t, \nwhich he has purchased in 10 minutes, a day, a week, or any time in the past. Obviously, not \nall stocks that investors sell at time t \u201ctoday\u201d were purchased at time t-\u03c4 in the past. Some \nstocks were purchased earlier or later than t-\u03c4 and at a price that differs from p(t-\u03c4). Thus, \ninvestors who sell stocks at time t gain returns that are different from \u201canticipated\u201d returns, \nr(t,\u03c4)=p(t)/p(t-\u03c4). So, \u201canticipated\u201d returns r(t,\u03c4) describe an option investors may gain, and \n\u201cactual\u201d returns describe the benefits investors realize. During a \u201ctrading day,\u201d traders and \ninvestors sell stocks that were initially purchased at different times in the past. Investors sell \nstocks during the \u201ctrading day\u201d and gain returns on stocks they purchased in 10 minutes, a \nweek, or any time in the past. To assess the average returns, or statistical moments of \u201cactual\u201d \nreturns, that investors gain, one should take into account returns with different time shifts. \nThat differs \u201cactual\u201d return from the description of the statistical properties of \u201canticipated\u201d \nreturn. Investigation of the \u201cactual\u201d returns of institutional, professional, or individual \ninvestors forms a separate problem. Different aspects of \u201cactual\u201d returns were studied by \n\n \n3 \n(Schlarbaum, Lewellen and Lease, 1978; Stanley, Lewellen and Schlarbaum, 1980; Baker \nand Wurgler, 2004; Ivkovi\u0107, Sialm and Weisbenner, 2004; Gabaix, et al 2005; Daniel and \nHirshleifer, 2016; Koijen, Richmond and Yogo, 2020; Hardouvelis, Karalas and Vayanos, \n2021) and others. \nHowever, the statistical properties of \u201canticipated\u201d and \u201cactual\u201d returns are mostly studied in \nthe same way. To assess the probability P(r) of \u201canticipated\u201d or \u201cactual\u201d returns during the \ntime interval \u0394, one studies the time series of returns and assesses the frequency mr/N: \n\ud835\udc43(\ud835\udc5f) ~ \n\ud835\udc5a\ud835\udc5f\n\ud835\udc41 \n \n \n \n \n(1.1) \nIn (1.1), mr denotes the number of returns that equal r, and N is the total number of terms of \nthe time series during \u0394. That is the conventional assessment of the probability of any event, \nand analyzing its time series during \u0394 follows the basis of probability theory (Shephard, \n1991; Shiryaev, 1999; Shreve, 2004). For convenience, we further refer to such assessments \n(1.1) as the frequency-based probabilities of stock returns. It is regular and completely correct \nto assess the probability of return if the time series of return r(ti,\u03c4) during the averaging \ninterval \u0394 are considered independent, self-reliant variables. However, return at time ti is \ndetermined by stock price p(ti) at time ti and price p(ti-\u03c4) at time ti-\u03c4. Moreover, market trade \nvalues C(ti), volumes U(ti), and price p(ti) at time ti obey the primitive trade pricing equation \n(1.2): \n\ud835\udc36(\ud835\udc61\ud835\udc56) = \ud835\udc5d(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56) \n \n \n \n \n(1.2) \nThe equation (1.2) states that the statistical properties of trade value C(ti) and volume U(ti) \nshould determine the properties of price as a random variable during \u0394.",
    "chunk_index": 1,
    "start_char": 2854,
    "end_char": 6100,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "time series of return r(ti,\u03c4) during the averaging \ninterval \u0394 are considered independent, self-reliant variables. However, return at time ti is \ndetermined by stock price p(ti) at time ti and price p(ti-\u03c4) at time ti-\u03c4. Moreover, market trade \nvalues C(ti), volumes U(ti), and price p(ti) at time ti obey the primitive trade pricing equation \n(1.2): \n\ud835\udc36(\ud835\udc61\ud835\udc56) = \ud835\udc5d(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56) \n \n \n \n \n(1.2) \nThe equation (1.2) states that the statistical properties of trade value C(ti) and volume U(ti) \nshould determine the properties of price as a random variable during \u0394. For convenience, in \nthis paper, all prices are adjusted to the present time t. We consider market trade values, \nvolumes, market prices, and returns as random variables during the selected time-averaging \ninterval \u0394. We believe that the consideration of market prices and stock returns as financial \nand economic matters should take into account the impact of the size of the trade values C(ti) \nand volumes U(ti) (1.2) on the average price, return, volatility, and statistics of returns. The \nwell-known volume weighted average price (VWAP) (Berkowitz et al., 1988; Duffie and \nDworczak, 2018), which differs from the frequency-based assessment of the mean price, \ndemonstrates the impact of the size of trade volumes U(ti) on the average price. It is \nreasonable that the statistical properties of the market trade values C(ti) and volumes U(ti) \ndetermine the statistics of the market price, and in turn, the price statistics determine the \nstatistics of stock return. The randomness of market trade determines the statistical moments \nof price and return, and that differs from the frequency-based probability assessments of the \n\n \n4 \nprice and return time series. We denote as market-based, the dependence of the statistical \nmoments of return on the statistical moments and correlations of market trade values C(ti) \nand volumes U(ti) during the interval \u0394. A description of the statistical moments of market \nprices and \u201canticipated\u201d stock returns that depend on the statistical moments and correlations \nof market trade values C(ti) and volumes U(ti) is given in Olkhov (2021-2023). We use these \nreferences to describe the market-based statistical moments of \u201cactual\u201d return. \nOur paper describes the market-based statistical moments of the \u201cactual\u201d returns of investors, \nwhich they gain during the averaging interval \u0394 that we denote a \u201ctrading day\u201d. We call all \nstocks that are sold by all investors during the averaging interval \u0394 a trading day portfolio. \nWe derive the dependence of the statistical moments of the \u201cactual\u201d return of investors on the \nstatistical moments and correlations of market trade values C(ti) and volumes U(ti). In turn, \nthe statistical moments of market trade values and volumes are assessed by the regular \nfrequency-based (1.1) probability (Shephard, 1991; Shiryaev, 1999; Shreve, 2004). We \nconsider statistical moments of return, which all investors gain from the sale of the trading \nday portfolio, as benchmarks for investors who purchase stocks during the same \u201ctrading \nday\u201d. \nIn this paper we reduce our description by the first two market-based statistical moments of \n\u201cactual\u201d return and derive the dependence of average and volatility of return on statistical \nmoments and correlations of market trade values and volumes for different cases.",
    "chunk_index": 2,
    "start_char": 5545,
    "end_char": 8898,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "statistical moments of market trade values and volumes are assessed by the regular \nfrequency-based (1.1) probability (Shephard, 1991; Shiryaev, 1999; Shreve, 2004). We \nconsider statistical moments of return, which all investors gain from the sale of the trading \nday portfolio, as benchmarks for investors who purchase stocks during the same \u201ctrading \nday\u201d. \nIn this paper we reduce our description by the first two market-based statistical moments of \n\u201cactual\u201d return and derive the dependence of average and volatility of return on statistical \nmoments and correlations of market trade values and volumes for different cases. \nIn Section 2, we describe the dependence of the market-based averages and the volatilities of \nprice and the \u201canticipated\u201d stock return on statistical moments and correlations of the current \nand past trade values and volumes. In Section 3, we consider the dependence of market-based \naverage and volatility of the \u201cactual\u201d return of a single deal as a result of numerous purchases \nin the past. Section 4 presents the average and volatility of the \u201cactual\u201d return, which an \ninvestor gains if he makes a lot of sales during the \u201ctrading day.\u201d In Section 5, we consider \nthe market-based average and volatility of the return of different investors during the \u201ctrading \nday.\u201d Section 6, Conclusion. We assume that readers know the basics of probability theory, \nstatistical moments, etc. \n2. Market-based averages and volatilities of price and \u201canticipated\u201d return \nAs \u201canticipated,\u201d we consider the stock return r(ti,\u03c4)=p(ti)/p(ti-\u03c4) determined as the ratio of \nmarket trade price p(ti) at time ti with respect to the price p(ti-\u03c4) at time ti-\u03c4. We consider the \ntrade of identical stocks and adjust all prices to the present. Let us consider the time series of \n\n \n5 \nthe trade values C(ti) and volumes U(ti) at time ti and assume that the trades are made with a \nconstant time shift \u03b5: \n\ud835\udc61\ud835\udc56+1 \u2212\ud835\udc61\ud835\udc56= \ud835\udf00 ; \ud835\udf00\u2212\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60\ud835\udc61 \nMarket trade time series present irregular and highly variable data. To forecast the stock \nreturns, one should choose the averaging interval \u0394>>\u03b5 and estimate the average variables. \nWe consider the present time t=t0 as \u201ctoday,\u201d and the time tk=t-k\u0394, k=1,2,.. describes k\u0394 \nintervals in the past: \n\u2206\ud835\udc58= [\ud835\udc61\ud835\udc58\u2212\n\u2206\n2 ; \ud835\udc61\ud835\udc58+\n\u2206\n2] ; \ud835\udc61\ud835\udc58= \ud835\udc61\u2212\u2206\u2219\ud835\udc58 ; \ud835\udc58= 0, 1, 2, \u2026 \n(2.1) \nWe assume that each interval \u0394k, k=0,1,.., \u03940= \u0394, contains the same number N of terms ti of \nthe time series: \n\ud835\udc61\ud835\udc56\u2208\u2206 ; \ud835\udc56= 1, 2, . . \ud835\udc41 \n \n \n \n(2.2) \nWe denote the averaging interval \u0394 (2.2) at present time t as the \u201ctrading day.\u201d We consider \nthe trade values C(ti), volumes U(ti), and prices p(ti) during each interval \u0394k (2.1) as random \nvariables. To describe a random variable, one can equally use the probability measure, the \ncharacteristic function, or the set of statistical moments of the random variable (Shephard, \n1991; Shiryaev, 1999; Shreve, 2004). Finite number of statistical moments describes \napproximations of the characteristic function and probability measure. We derive finite \nnumber of the statistical moments of price and return that describe approximations of their \nprobability measures.",
    "chunk_index": 3,
    "start_char": 8269,
    "end_char": 11383,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "trade values C(ti), volumes U(ti), and prices p(ti) during each interval \u0394k (2.1) as random \nvariables. To describe a random variable, one can equally use the probability measure, the \ncharacteristic function, or the set of statistical moments of the random variable (Shephard, \n1991; Shiryaev, 1999; Shreve, 2004). Finite number of statistical moments describes \napproximations of the characteristic function and probability measure. We derive finite \nnumber of the statistical moments of price and return that describe approximations of their \nprobability measures. We assess the statistical moments of the trade value C(t;n) and volume \nU(t;n) averaged during the \u201ctrading day\u201d \u0394 (2.2) by finite number N of trade time series using \nfrequency-based probability (1.1) as: \n\ud835\udc36(\ud835\udc61; \ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56)]~\n1\n\ud835\udc41\u2211\n\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n \n \n \n(2.3) \n \ud835\udc48(\ud835\udc61; \ud835\udc5b) = \ud835\udc38[\ud835\udc48\ud835\udc5b(\ud835\udc61\ud835\udc56)]~\n1\n\ud835\udc41\u2211\n\ud835\udc48\ud835\udc5b(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n ; \ud835\udc5b= 1,2,. \n \n(2.4) \nWe denote E[\u2026] as the frequency-based mathematical expectation (2.3; 2.4) during \u0394 (2.2) \nand the symbol \u201c~\u201d to underline that the finite number N of trades determines the \nassessments, the estimators of the statistical moments of trade value C(t;n,) and volume \nU(t;n) at time t=t0 \u201ctoday.\u201d The finite number n of the statistical moments (2.3; 2.4) assesses \nonly approximations of the probability and the characteristic functions of the random \nvariables C(ti) and U(ti). \nWe denote Em[..] as market-based mathematical expectation to differ it from the \nconventional frequency-based one (2.3; 2.4) and describe first two market-based statistical \nmoments of price a(t;n)=Em[pn(ti)] and return h(t,\u03c4;n)=Em[rn(ti,\u03c4)] for n=1,2. We denote the \n\n \n6 \n\u201canticipated\u201d return r(ti,\u03c4) (2.5) with time shift \u03c4 as the ratio of prices p(ti) at times ti to price \np(ti-\u03c4) in the past: \n \ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udf0f) =\n\ud835\udc5d(\ud835\udc61\ud835\udc56)\n\ud835\udc5d(\ud835\udc61\ud835\udc56\u2212\ud835\udf0f) \n \n \n \n \n(2.5) \nLet us transform the trade price equation (1.2): \n\ud835\udc36(\ud835\udc61\ud835\udc56) = \ud835\udc5d(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56) =\n\ud835\udc5d(\ud835\udc61\ud835\udc56)\n\ud835\udc5d(\ud835\udc61\ud835\udc56\u2212\ud835\udf0f) \ud835\udc5d(\ud835\udc61\ud835\udc56\u2212\ud835\udf0f)\ud835\udc48(\ud835\udc61\ud835\udc56) = \ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udf0f) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f) \n \n \nEquations (2.6) link up the sale value C(ti), return r(ti,\u03c4), and the original value Co(ti,\u03c4) of the \nsame trade volume U(ti) at price p(ti-\u03c4) in the past: \n\ud835\udc36(\ud835\udc61\ud835\udc56) = \ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udf0f) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f) ; \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f) = \ud835\udc5d(\ud835\udc61\ud835\udc56\u2212\ud835\udf0f)\ud835\udc48(\ud835\udc61\ud835\udc56) \n \n(2.6) \nEquation (2.6) is similar to the trade price equation (1.2), but the price p(ti-\u03c4) defines the \noriginal value Co(ti,\u03c4) of the trade volume U(ti) in the past at time ti-\u03c4. Similar to (2.3), we \nassess the frequency-based statistical moments Co(t,\u03c4;n) of the original value Co(ti,\u03c4): \n\ud835\udc36\ud835\udc5c(\ud835\udc61, \ud835\udf0f; \ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udf0f)]~\n1\n\ud835\udc41\u2211\n\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udf0f)\n\ud835\udc41\n\ud835\udc56=1\n \n \n(2.7) \nThe m-th degree of (1.2; 2.6) for m=1,2,.., give equations (2.8; 2.9): \n\ud835\udc36\ud835\udc5a(\ud835\udc61\ud835\udc56) = \ud835\udc5d\ud835\udc5a(\ud835\udc61\ud835\udc56)\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56) \n \n \n \n(2.8) \n\ud835\udc36\ud835\udc5c\n\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udf0f) = \ud835\udc5d\ud835\udc5a(\ud835\udc61\ud835\udc56\u2212\ud835\udf0f)\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56) ; \ud835\udc36\ud835\udc5a(\ud835\udc61\ud835\udc56) = \ud835\udc5f\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udf0f) \ud835\udc36\ud835\udc5c\n\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udf0f) \n(2.9) \nThe equation (2.8) generates the set of weight functions w(ti;m) (2.10) \n\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a) =\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\u2211\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n ; \u2211\n\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n= 1 \n \n \n(2.10) \nThe weight functions w(ti;m) (2.10) define the average p(t;n,m) (2.11) of the n-th degree of \nprice pn(ti): \n\ud835\udc5d(\ud835\udc61; \ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc5d\ud835\udc5b(\ud835\udc61\ud835\udc56)\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n=\n1\n\u2211\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n \u2211\n\ud835\udc5d\ud835\udc5b(\ud835\udc61\ud835\udc56)\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n \n \n(2.11) \nRelations (2.11) present generalization of the well-known volume weighted average price \n(VWAP) (Berkowitz et al., 1988;",
    "chunk_index": 4,
    "start_char": 10816,
    "end_char": 13954,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udf0f) \n(2.9) \nThe equation (2.8) generates the set of weight functions w(ti;m) (2.10) \n\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a) =\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\u2211\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n ; \u2211\n\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n= 1 \n \n \n(2.10) \nThe weight functions w(ti;m) (2.10) define the average p(t;n,m) (2.11) of the n-th degree of \nprice pn(ti): \n\ud835\udc5d(\ud835\udc61; \ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc5d\ud835\udc5b(\ud835\udc61\ud835\udc56)\ud835\udc64(\ud835\udc61\ud835\udc56; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n=\n1\n\u2211\n\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n \u2211\n\ud835\udc5d\ud835\udc5b(\ud835\udc61\ud835\udc56)\ud835\udc48\ud835\udc5a(\ud835\udc61\ud835\udc56)\n\ud835\udc41\n\ud835\udc56=1\n \n \n(2.11) \nRelations (2.11) present generalization of the well-known volume weighted average price \n(VWAP) (Berkowitz et al., 1988; Duffie and Dworczak, 2018) p(t;1,1) for n=1,2,.., and \nm=1,2,.. . We consider VWAP p(t;1,1) as market-based average price a(t;1) and take: \n\ud835\udc4e(\ud835\udc61; 1) = \ud835\udc38\ud835\udc5a[\ud835\udc5d(\ud835\udc61\ud835\udc56)] = \ud835\udc5d(\ud835\udc61; \ud835\udc5b, \ud835\udc5a) \n \n \n(2.12) \nThe choice (2.12) of market-based average price a(t;1) determines the dependence of the first \nfour market-based statistical moments a(t;n), n=2,3,4 on statistical moments and correlations \nof trade value and volume (Olkhov, 2022). The dependence of the 2-d price statistical \nmoment a(t;2) (2.13) and price volatility \u03c3p\n2(t) (2.14) on the statistical moments and \ncorrelations of trade values and volumes take the form: \n\ud835\udc4e(\ud835\udc61; 2) = \ud835\udc38\ud835\udc5a[\ud835\udc5d2(\ud835\udc61\ud835\udc56)] =\n\ud835\udc36(\ud835\udc61;2)+2\ud835\udc4e2(\ud835\udc61;1)\u03a9\ud835\udc48\n2 (\ud835\udc61)\u22122\ud835\udc4e(\ud835\udc61;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56)}\n\ud835\udc48(\ud835\udc61;2)\n \n \n(2.13) \n\ud835\udf0e2(\ud835\udc61) = \ud835\udc38\ud835\udc5a[(\ud835\udc5d(\ud835\udc61\ud835\udc56) \u2212\ud835\udc4e(\ud835\udc61; 1))2] =\n\u03a9\ud835\udc36\n2(\ud835\udc61)+\ud835\udc4e2(\ud835\udc61;1)\u03a9\ud835\udc48\n2 (\ud835\udc61)\u22122\ud835\udc4e(\ud835\udc61;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56)}\n\ud835\udc48(\ud835\udc61;2)\n \n \n(2.14) \n\n \n7 \nIn (2.13; 2.14) \u03a9C\n2(t) (2.15) and \u03a9U\n2(t) (2.16) denote volatilities of trade value and volume: \n\u03a9\ud835\udc36\n2(\ud835\udc61) = \ud835\udc38[(\ud835\udc36(\ud835\udc61\ud835\udc56) \u2212\ud835\udc36(\ud835\udc61; 1))2] = \ud835\udc36(\ud835\udc61; 2) \u2212\ud835\udc362(\ud835\udc61; 1) \n \n(2.15) \n \u03a9\ud835\udc48\n2 (\ud835\udc61) = \ud835\udc38[(\ud835\udc48(\ud835\udc61\ud835\udc56) \u2212\ud835\udc48(\ud835\udc61; 1))2] = \ud835\udc48(\ud835\udc61; 2) \u2212\ud835\udc482(\ud835\udc61; 1) \n(2.16) \nThe correlation corr{C(ti)U(ti)} (2.17) between the trade value C(ti) and volume U(ti) is \ndetermined by the joint mathematical expectation E[C(ti)U(ti)] of the trade value and volume: \n\ud835\udc36\ud835\udc48(\ud835\udc61; 1,1) = \ud835\udc38[\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56)] = \ud835\udc36(\ud835\udc61; 1)\ud835\udc48(\ud835\udc61; 1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc56)} \n(2.17) \nWe refer to Olkhov (2021; 2022) for further details. The similar method allows the use of the \nequations (2.7; 2.9) to derive the dependence of market-based statistical moments of the \n\u201canticipated\u201d returns on statistical moments and correlations of the current and past trade \nvalues and trade volumes (Olkhov, 2023). Similar to (2.11) the weight functions (2.18): \n\ud835\udc67(\ud835\udc61, \ud835\udf0f; \ud835\udc5a) =\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc56,\ud835\udf0f)\n\u2211\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc56,\ud835\udf0f)\n\ud835\udc41\n\ud835\udc56=1\n ; \u2211\n\ud835\udc67(\ud835\udc61\ud835\udc56, \ud835\udf0f; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n= 1 \n \n(2.18) \ndefine the average r(t,\u03c4;n,m) (2.19) of the n-th degree of return rn(ti,\u03c4): \n\ud835\udc5f(\ud835\udc61, \ud835\udf0f; \ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc5f\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udf0f)\ud835\udc67(\ud835\udc61, \ud835\udf0f; \ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n=\n1\n\u2211\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc56,\ud835\udf0f)\n\ud835\udc41\n\ud835\udc56=1\n \u2211\n\ud835\udc5f\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udf0f)\ud835\udc36\ud835\udc5c\n\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udf0f)\n\ud835\udc41\n\ud835\udc56=1\n \n(2.19) \nThe definition of the average return r(t,\u03c4;1,1) (2.19) coincides with Markowitz\u2019s (1952) \ndefinition of the portfolio return as an average return weighted by the \u201crelative amount Xi \ninvested in security i.\u201d To approve r(t,\u03c4;1,1) (2.19), one can consider the shares sold during \nthe \u201ctrading day\u201d as a \u201cportfolio\u201d and use Markowitz\u2019s definition of portfolio return that was \npresented more than 35 years ahead of the definition of VWAP (Berkowitz et al., 1988). The \nreplacement of \u201crelative amount invested in security\u201d by the relative volume of trade \ndetermined by the weight function w(ti;1) (2.10) determines the VWAP. We take \nMarkowitz\u2019s definition of portfolio return in the form r(t,\u03c4;1,1) (2.20) as market-based \naverage return h(t,\u03c4;1): \n\u210e(\ud835\udc61, \ud835\udf0f;",
    "chunk_index": 5,
    "start_char": 13476,
    "end_char": 16624,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "security i.\u201d To approve r(t,\u03c4;1,1) (2.19), one can consider the shares sold during \nthe \u201ctrading day\u201d as a \u201cportfolio\u201d and use Markowitz\u2019s definition of portfolio return that was \npresented more than 35 years ahead of the definition of VWAP (Berkowitz et al., 1988). The \nreplacement of \u201crelative amount invested in security\u201d by the relative volume of trade \ndetermined by the weight function w(ti;1) (2.10) determines the VWAP. We take \nMarkowitz\u2019s definition of portfolio return in the form r(t,\u03c4;1,1) (2.20) as market-based \naverage return h(t,\u03c4;1): \n\u210e(\ud835\udc61, \ud835\udf0f; 1) = \ud835\udc38\ud835\udc5a[\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udf0f)] = \ud835\udc5f(\ud835\udc61, \ud835\udf0f; 1,1) \n \n \n(2.20) \nIn (Olkhov, 2023) we derive the dependence of the 2-d market-based statistical moment \nh(t,\u03c4;2) (2.21) and return volatility \u03c32(t,\u03c4) (2.22) on statistical moments and correlations of the \ncurrent and past trade values and volumes: \n\u210e(\ud835\udc61, \ud835\udf0f; 2) = \ud835\udc38\ud835\udc5a[\ud835\udc5f2(\ud835\udc61\ud835\udc56, \ud835\udf0f)] =\n\ud835\udc36(\ud835\udc61;2)+2\u210e2(\ud835\udc61,\ud835\udf0f;1)\ud835\udef72(\ud835\udc61,\ud835\udf0f)\u22122\u210e(\ud835\udc61,\ud835\udf0f;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56,\ud835\udf0f)}\n\ud835\udc36\ud835\udc5c(\ud835\udc61,\ud835\udf0f;2)\n \n \n(2.21) \n\ud835\udf0e2(\ud835\udc61, \ud835\udf0f) = \ud835\udc38\ud835\udc5a[(\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udf0f) \u2212\u210e(\ud835\udc61, \ud835\udf0f; 1))2] =\n\u03a9\ud835\udc36\n2(\ud835\udc61)+\u210e2(\ud835\udc61,\ud835\udf0f;1)\ud835\udef72(\ud835\udc61,\ud835\udf0f)\u22122\u210e(\ud835\udc61,\ud835\udf0f;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56,\ud835\udf0f)}\n\ud835\udc36\ud835\udc5c(\ud835\udc61,\ud835\udf0f;2)\n (2.22) \nThe 2-d statistical moment h(t,\u03c4;2) (2.21) and return volatility \u03c32(t,\u03c4) (2.22) depend on the \nvolatility \u03a9C\n2(t) (2.15) of the current value and on volatility \u03a62(t,\u03c4) (2.23) of the past value: \n\ud835\udef72(\ud835\udc61, \ud835\udf0f) = \ud835\udc38[(\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f) \u2212\ud835\udc36\ud835\udc5c(\ud835\udc61, \ud835\udf0f; 1))2] = \ud835\udc36\ud835\udc5c(\ud835\udc61, \ud835\udf0f; 2) \u2212\ud835\udc36\ud835\udc5c\n2(\ud835\udc61, \ud835\udf0f; 1) \n(2.23) \n\n \n8 \nThe joint mathematical expectations (2.24) determines the correlation corr{C(ti)Co(ti,\u03c4)} of \ncurrent and past trade values: \n\ud835\udc38[\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f)] =\n1\n\ud835\udc41\u2211\n\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f) = \ud835\udc36(\ud835\udc61; 1)\n\ud835\udc41\n\ud835\udc56=1\n\ud835\udc36\ud835\udc5c(\ud835\udc61, \ud835\udf0f; 1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56, \ud835\udf0f)} (2.24) \nThe relations (2.12-2.14; 2.20-2.22) describe the market-based average, the 2-d statistical \nmoments, and volatilities of price and \u201canticipated\u201d stock return, and we refer to Olkhov \n(2021-2023) for further details. \nNow we use the above results to describe the market-based average and volatility of \u201cactual\u201d \nreturns for three cases. In Section 3, for the stock return of a single sale, we assess the \nmarket-based average and volatility that are generated by numerous purchases of stocks in \nthe past. In Section 4, we describe the market-based average and volatility of the return of a \nsingle investor, which he gains due to multiple sales during the \u201ctrading day.\u201d In Section 5, \nwe describe the market-based volatility of returns that different investors gain during the \n\u201ctrading day.\u201d \n3. Market-based \u201cactual\u201d return of a single sale \nIn this section, we consider market-based statistical moments of \u201cactual\u201d return that an \ninvestor gains within a single sale of the volume U(ti) of stocks. We propose that the investor, \nat time ti \u201ctoday,\u201d sells U(ti) stocks at a price p(ti). We assume that the investor purchased \nthis amount of stocks U(ti) by small stakes of shares U(tj(i)) at different times tj(i), j=1,2, \n..M(i) in the past at a price p(tj(i)). We consider all prices p(tj(i)) adjusted to the present at \ntime t. The investor at time tj(i) purchases the original value Co(tj(i)) of the volume U(tj(i)) of \nstocks at price p(tj(i)): \n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n \n \n \n(3.1) \nFor each volume U(tj(i)) of stocks purchased in the past at a price p(tj(i)) we use equation \n(3.2) similar to (2.6): \n\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5d(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56)) =\n\ud835\udc5d(\ud835\udc61\ud835\udc56)",
    "chunk_index": 6,
    "start_char": 16063,
    "end_char": 19280,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "stakes of shares U(tj(i)) at different times tj(i), j=1,2, \n..M(i) in the past at a price p(tj(i)). We consider all prices p(tj(i)) adjusted to the present at \ntime t. The investor at time tj(i) purchases the original value Co(tj(i)) of the volume U(tj(i)) of \nstocks at price p(tj(i)): \n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n \n \n \n(3.1) \nFor each volume U(tj(i)) of stocks purchased in the past at a price p(tj(i)) we use equation \n(3.2) similar to (2.6): \n\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5d(\ud835\udc61\ud835\udc56)\ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56)) =\n\ud835\udc5d(\ud835\udc61\ud835\udc56)\n\ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5f(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n \n\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \u2261\n\ud835\udc5d(\ud835\udc61\ud835\udc56)\n\ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n \n \n \n \n \n \ud835\udc36(\ud835\udc61\ud835\udc56) = \u2211\n\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n ; \ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n(3.2) \nThe equation (3.2) introduces the current C(ti,tj(i)) value of the small stakes of shares U(tj(i)) \nthat were originally purchased at price p(tj(i)) in the past. At time ti, the investor gains the \n\u201cactual\u201d return r(ti,tj(i)) by selling the volume U(tj(i)) at price p(ti). We denote Co(tj(i)) (3.2) \n\n \n9 \nas the original value in the past at a price p(tj(i)) adjusted to the present. The total sale volume \nU(ti) (3.3) was purchased in the past by M(i) small stakes of shares U(tj(i)) at prices p(tj(i)). \nThe original value Co(ti) (3.3) of the volume U(ti) (3.4) takes the form: \n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56) = \u2211\n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n= \u2211\n \ud835\udc5d(\ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n(3.3) \n\ud835\udc48(\ud835\udc61\ud835\udc56) = \u2211\n\ud835\udc48(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n \n \n(3.4) \nObviously, the purchases of stocks in the past at different prices p(tj(i)) result in a different \n\u201cactual\u201d return r(ti,tj(i)). If the total number M(i) of the purchases is sufficiently large, then \n(3.2) allows derive the statistical moments of the \u201cactual\u201d returns r(ti,tj(i)) of a single market \nsale at a time ti. One should follow section 2 and, similar to (2.9), take the m-th power of \n(3.2): \n\ud835\udc36\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) = \ud835\udc5f\ud835\udc5a(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c\n\ud835\udc5a(\ud835\udc61\ud835\udc57(\ud835\udc56)) \n \n \n(3.5) \nWe introduce the n-th statistical moments C(ti;n) (3.6) of the sale value C(ti,tj(i)) at ti and the \nn-th statistical moments Co(ti;n) (3.8) of the original value Cp(tj(i)) as: \n\ud835\udc36(\ud835\udc61\ud835\udc56; \ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))]~\n1\n\ud835\udc40(\ud835\udc56) \u2211\n\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n (3.6) \nThe average current value C(ti;1) (3.6) for n=1 multiplied by M(i) equals the current value \nC(ti) (3.2) of the sale at time ti: \n\ud835\udc36(\ud835\udc61\ud835\udc56) = \u2211\n\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n= \ud835\udc40(\ud835\udc56)\ud835\udc36(\ud835\udc61\ud835\udc56; 1) \n \n \n \n(3.7) \nThe statistical moments Co(ti;n) (3.8) of the original value Co(tj(i)) take the form: \n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; \ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc57(\ud835\udc56))] ~\n1\n\ud835\udc40(\ud835\udc56) \u2211\n\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n \n(3.8) \nSimilar to (2.18; 2.19), for the equations (3.5), obtain the weight functions s(ti,tj(i));m) (3.9): \n\ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); \ud835\udc5a) =\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\u2211\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n ; \u2211\n\ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); \ud835\udc5a)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n= 1 \n \n(3.9) \nThe weight functions s(ti,tj(i));m) (3.9) determine the average r(ti;n,m) (3.10) of the n-th \ndegree of return rn(ti,tj(i)) of a single deal: \n\ud835\udc5f(\ud835\udc61\ud835\udc56; \ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc5f\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); \ud835\udc5a)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n \n(3.10) \nWe follow (2.20) and define the dependence of the market-based average g(ti;1) (3.11) \n\u201cactual\u201d return of a single sale of stocks at time ti on the statistical moments of current and \npast trade values (3.6; 3.8; 3.10): \n\ud835\udc54(\ud835\udc61\ud835\udc56; 1) = \ud835\udc38\ud835\udc5a[\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))] = \ud835\udc5f(\ud835\udc61; 1,1) \n \n \n(3.11)",
    "chunk_index": 7,
    "start_char": 18793,
    "end_char": 21889,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n ; \u2211\n\ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); \ud835\udc5a)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n= 1 \n \n(3.9) \nThe weight functions s(ti,tj(i));m) (3.9) determine the average r(ti;n,m) (3.10) of the n-th \ndegree of return rn(ti,tj(i)) of a single deal: \n\ud835\udc5f(\ud835\udc61\ud835\udc56; \ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc5f\ud835\udc5b(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); \ud835\udc5a)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n \n(3.10) \nWe follow (2.20) and define the dependence of the market-based average g(ti;1) (3.11) \n\u201cactual\u201d return of a single sale of stocks at time ti on the statistical moments of current and \npast trade values (3.6; 3.8; 3.10): \n\ud835\udc54(\ud835\udc61\ud835\udc56; 1) = \ud835\udc38\ud835\udc5a[\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))] = \ud835\udc5f(\ud835\udc61; 1,1) \n \n \n(3.11) \n\n \n10 \nWe use (3.6; 3.8) and for the 2-d market-based statistical moment g(ti;2) (3.13) and volatility \n\u03c3g\n2(ti) (3.14) of \u201cactual\u201d return of a single sale of stocks at time ti obtain relations (3.12-3.17) \nthat are similar to (2.15; 2.21- 2.24): \n\ud835\udc54(\ud835\udc61\ud835\udc56; 2) = \ud835\udc38\ud835\udc5a[\ud835\udc5f2 (\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56))] \n \n \n \n \n\ud835\udf0e\ud835\udc54\n2(\ud835\udc61\ud835\udc56) = \ud835\udc38\ud835\udc5a[(\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \u2212\ud835\udc54(\ud835\udc61\ud835\udc56; 1))2] \nWe define market-based volatility \u03c3g\n2(ti) (3.13) as: \n\ud835\udf0e\ud835\udc54\n2(\ud835\udc61\ud835\udc56) = \u2211\n(\ud835\udc5f(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \u2212\ud835\udc54(\ud835\udc61\ud835\udc56; 1))2\ud835\udc60(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56); 2)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n(3.12) \nEquation (3.12) provide the consistency of the 1-st g(ti;1) (3.11) and 2-d g(ti;2) (3.13) \nmarket-based statistical moments of \u201cactual\u201d return of a single sale and guarantee non-\nnegativity of volatility \u03c3g\n2(ti) (3.14) of \u201cactual\u201d return: \n\ud835\udc54(\ud835\udc61\ud835\udc56; 2) =\n\ud835\udc36(\ud835\udc61\ud835\udc56;2)+2\ud835\udc542(\ud835\udc61\ud835\udc56;1)\ud835\udef72(\ud835\udc61\ud835\udc56)\u22122\ud835\udc54(\ud835\udc61\ud835\udc56;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56,\ud835\udc61\ud835\udc57(\ud835\udc56))\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))}\n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56;2)\n \n(3.13) \n\ud835\udf0e\ud835\udc54\n2(\ud835\udc61\ud835\udc56) =\n\u03a9\ud835\udc36\n2(\ud835\udc61\ud835\udc56)+\ud835\udc542(\ud835\udc61\ud835\udc56;1)\ud835\udef72(\ud835\udc61\ud835\udc56)\u22122\ud835\udc54(\ud835\udc61\ud835\udc56;1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56,\ud835\udc61\ud835\udc57(\ud835\udc56))\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))}\n\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56;2)\n \n \n(3.14) \nIn (3.13; 3.14) we denote the volatilities of current \u03a9C\n2(ti) (3.15) and past \u03a62(ti) (3.16) trade \nvalues of a single sale at time ti : \n\u03a9\ud835\udc36\n2(\ud835\udc61\ud835\udc56) = \ud835\udc38[(\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \u2212\ud835\udc36(\ud835\udc61\ud835\udc56; 1))2] = \ud835\udc36(\ud835\udc61\ud835\udc56; 2) \u2212\ud835\udc362(\ud835\udc61\ud835\udc56; 1) \n \n(3.15) \n\ud835\udef72(\ud835\udc61\ud835\udc56) = \ud835\udc38[(\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56)) \u2212\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1))2] = \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 2) \u2212\ud835\udc36\ud835\udc5c\n2(\ud835\udc61\ud835\udc56; 1) \n \n(3.16) \nThe relations (3.17; 3.18) determine the correlation corr{C(ti,tj(i))Co(tj(i))} between the current \nC(ti,tj(i)) trade value at ti of stocks that were originally purchased in the past at tj(i) and their \noriginal value Co(tj(i)): \n\ud835\udc36\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56) = \ud835\udc38[\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))] =\n1\n\ud835\udc40(\ud835\udc56) \u2211\n\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n(3.17) \n\ud835\udc36\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56) = \ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56, \ud835\udc61\ud835\udc57(\ud835\udc56)) \ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc57(\ud835\udc56))} \n(3.18) \n4. Market-based \u201cactual\u201d return of a single investor \nIn this section, we consider the market-based average and volatility of the \u201cactual\u201d return of a \nsingle investor during the \u201ctrading day\u201d (2.2). \nActually, different deals at times ti, i=1,2,..N during the \u201ctrading day\u201d (2.2) result in different \nmarket-based average returns g(ti;n) (3.11). One can consider different returns g(ti;n) (3.11) \nthat a single investor gains during the \u201ctrading day\u201d as a random variable. To describe the \nmarket-based random properties of average returns g(ti;n) (3.11) we consider equation (4.1) \n\n \n11 \nthat describe the dependence of on market-based average return g(ti;1) of a single deal during \nthe \u201ctrading day\u201d on average current C(ti;1) (3.6) and past Co(ti;1) (3.8) trade values: \n\ud835\udc36(\ud835\udc61\ud835\udc56; 1) = \ud835\udc54(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1) \n \n \n \n(4.1) \nIn the equation (4.1) we consider the current C(ti;1) (3.6), past Co(ti;1) (3.8) trade values, and \naverage return g(ti;1) at time ti as random variables during the \u201ctrading day\u201d (2.2). One can \nconsider (4.1) similar to the trade return equations (3.2; 3.5) and reproduce the same \ncalculations but with respect to the market-based average return g(ti;1) (3.11) of a single deal.",
    "chunk_index": 8,
    "start_char": 21352,
    "end_char": 24642,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "g(ti;1) of a single deal during \nthe \u201ctrading day\u201d on average current C(ti;1) (3.6) and past Co(ti;1) (3.8) trade values: \n\ud835\udc36(\ud835\udc61\ud835\udc56; 1) = \ud835\udc54(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1) \n \n \n \n(4.1) \nIn the equation (4.1) we consider the current C(ti;1) (3.6), past Co(ti;1) (3.8) trade values, and \naverage return g(ti;1) at time ti as random variables during the \u201ctrading day\u201d (2.2). One can \nconsider (4.1) similar to the trade return equations (3.2; 3.5) and reproduce the same \ncalculations but with respect to the market-based average return g(ti;1) (3.11) of a single deal. \nLet us take the n-th degree of (4.1): \n\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56; 1) = \ud835\udc54\ud835\udc5b(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc56; 1) \n \n \n \n(4.2) \nSimilar to (3.6-3.10) we define the frequency-based n-th statistical moments (4.3; 4.4) of \ncurrent and past trade values: \n\ud835\udc36(\ud835\udc61; 1|\ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56; 1)]~\n1\n\ud835\udc41\u2211\n\ud835\udc36\ud835\udc5b(\ud835\udc61\ud835\udc56; 1)\n\ud835\udc41\n\ud835\udc56=1\n \n \n \n(4.3) \n\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|\ud835\udc5b) = \ud835\udc38[\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc56; 1)]~\n1\n\ud835\udc41\u2211\n\ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61\ud835\udc56; 1)\n\ud835\udc41\n\ud835\udc56=1\n \n \n \n(4.4) \nSimilar to (3.9), we define the weight functions \u03b3(ti;1|m) (4.5): \n\ud835\udefe(\ud835\udc61\ud835\udc56; 1|\ud835\udc5a) =\n\ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61\ud835\udc56;1)\n\u2211\n\ud835\udc36\ud835\udc5c\ud835\udc5a((\ud835\udc61\ud835\udc56;1))\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n ; \u2211\n\ud835\udefe(\ud835\udc61\ud835\udc56; 1|\ud835\udc5a)\n\ud835\udc41\n\ud835\udc56=1\n= 1 \n(4.5) \nThe weight functions \u03b3(ti;1|m) (4.5) and the equation (4.2) define the average \n\u03f1(t;1|n,m)=E[gn(ti;1)] (4.6) of the n-th degree gn(ti;1): \n\ud835\udf1a(\ud835\udc61; 1|\ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc54\ud835\udc5b(\ud835\udc61\ud835\udc56; 1) \ud835\udefe(\ud835\udc61\ud835\udc56; 1|\ud835\udc5a)\n\ud835\udc40(\ud835\udc56)\n\ud835\udc57=1\n \n \n \n(4.6) \nSimilar to (3.11) we choose the market-based average of return G(t|1) (4.7) that is determined \nby numerous deals of a single investor during the \u201ctrading day\u201d to be equal the average \n\u03f1(t;1|1,1) (4.6): \n\ud835\udc3a(\ud835\udc61|1) = \ud835\udc38\ud835\udc5a[\ud835\udc54(\ud835\udc61\ud835\udc56; 1)] = \ud835\udf1a(\ud835\udc61; 1|1,1) \n \n \n(4.7) \nSimilar to (3.12-3.17) the choice of G(t|1) (4.7) determines the market-based 2-d statistical \nmoment G(t|2) (4.8) and the volatility \u03c3G\n2(t) (4.9) of the return of a sing investor: \n\ud835\udc3a(\ud835\udc61|2) = \ud835\udc38\ud835\udc5a[\ud835\udc542(\ud835\udc61\ud835\udc56; 1)] \n \n \n \n \n \n\ud835\udc3a(\ud835\udc61; 1|2) =\n\ud835\udc36(\ud835\udc61;1|2)+2\ud835\udc3a2(\ud835\udc61|1)\ud835\udef72(\ud835\udc61)\u22122\ud835\udc3a(\ud835\udc61|1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56;1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56;1)}\n\ud835\udc36\ud835\udc5c(\ud835\udc61;1|2)\n \n \n \n(4.8) \n\ud835\udf0e\ud835\udc3a\n2(\ud835\udc61\ud835\udc56) = \ud835\udc38\ud835\udc5a[(\ud835\udc54(\ud835\udc61\ud835\udc56; 1) \u2212\ud835\udc3a(\ud835\udc61; 1|1))2] \n \n \n \n \n\ud835\udf0e\ud835\udc3a\n2(\ud835\udc61) =\n\u03a9\ud835\udc36\n2(\ud835\udc61)+\ud835\udc3a2(\ud835\udc61|1)\ud835\udef72(\ud835\udc61)\u22122\ud835\udc3a(\ud835\udc61|1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56;1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56;1)}\n\ud835\udc36\ud835\udc5c(\ud835\udc61;1|2)\n \n \n(4.9) \nThe market-based volatility \u03c3G\n2(t) (4.9) of return of a single investor depends on volatility \n\u03a9C\n2(t) (4.10) of his average current sales C(ti;1) (3.6) and on volatility \u03a62(t) (4.11) of the \naverage original values C0(ti;1) (3.8) at times ti during the \u201ctrading day\u201d: \n\n \n12 \n\u03a9\ud835\udc36\n2(\ud835\udc61) = \ud835\udc38[(\ud835\udc36(\ud835\udc61\ud835\udc56; 1) \u2212\ud835\udc36(\ud835\udc61; 1|1))2] = \ud835\udc36(\ud835\udc61; 1|2) \u2212\ud835\udc362(\ud835\udc61; 1|1) \n \n(4.10) \n\ud835\udef72(\ud835\udc61) = \ud835\udc38[(\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1) \u2212\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1))2] = \ud835\udc36\ud835\udc5c(\ud835\udc61; 1|2) \u2212\ud835\udc36\ud835\udc5c\n2(\ud835\udc61; 1|1) \n(4.11) \nAs well, the market-based volatility \u03c3G\n2(t) (4.9) depends on correlation corr{C(ti;1)Co(ti;1)} \n(4.13) between the average current sales C(ti;1) (3.6) and original values C0(ti;1) (3.8) at \ntimes ti during the \u201ctrading day\u201d. From (4.3; 4.4) obtain: \n\ud835\udc38[\ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)] =\n1\n\ud835\udc41\u2211\n\ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)\n\ud835\udc41\n\ud835\udc56=1\n \n \n \n(4.12) \n\ud835\udc38[\ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)] = \ud835\udc36(\ud835\udc61; 1|1)\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)} \n(4.13) \n5. Market based volatility of return of different investors \nIn this section, we assess the market-based average and volatility of \u201cactual\u201d return that many \ndifferent investors gain during the \u201ctrading day.\u201d Indeed, the average return that each of \nnumerous investors gain during the \u201ctrading day\u201d varies a lot. One can consider the average \nreturns of many different investors as a random variable.",
    "chunk_index": 9,
    "start_char": 24096,
    "end_char": 27225,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)] = \ud835\udc36(\ud835\udc61; 1|1)\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36(\ud835\udc61\ud835\udc56; 1)\ud835\udc36\ud835\udc5c(\ud835\udc61\ud835\udc56; 1)} \n(4.13) \n5. Market based volatility of return of different investors \nIn this section, we assess the market-based average and volatility of \u201cactual\u201d return that many \ndifferent investors gain during the \u201ctrading day.\u201d Indeed, the average return that each of \nnumerous investors gain during the \u201ctrading day\u201d varies a lot. One can consider the average \nreturns of many different investors as a random variable. The market-based volatility of \naverage returns describes the uncertainty of trade outcome of investors during the \u201ctrading \nday.\u201d \nLet us assume that during the \u201ctrading day\u201d the investor q, q=1,..Q gain average return \nG(t;1|q) (4.7) and consider the trade return equation (5.1) on average current value C(t;1|1,q) \n(4.3), original value C0(t;1|1,q), and average return G(t;1|q) (4.7) of investor q: \n\ud835\udc36(\ud835\udc61; 1|1, \ud835\udc5e) = \ud835\udc3a(\ud835\udc61; 1|\ud835\udc5e) \ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1, \ud835\udc5e) ; \ud835\udc5e= 1, \u2026 \ud835\udc44 \n \n(5.1) \nIn (5.1) we add variable q in the current value C(t;1|1,q) (4.3), original value C0(t;1|1,q), and \naverage return G(t;1|q) (4.7) to highlight their dependence on trade outcomes of investor q \nduring the \u201ctrading day.\u201d We consider the equation (5.1) similar to (4.1) and take the n-th \ndegree of (5.1): \n\ud835\udc36\ud835\udc5b(\ud835\udc61; 1|1, \ud835\udc5e) = \ud835\udc3a\ud835\udc5b(\ud835\udc61; 1|\ud835\udc5e) \ud835\udc36\ud835\udc5c\n\ud835\udc5b(\ud835\udc61; 1|1, \ud835\udc5e) \n \n \n(5.2) \n Similar to (4.3; 4.4) we define the frequency-based m-th statistical moments of current \nC(t|m) (5.3) and past Co(t|m) (5.4) trade values that are determined by of different investors \nduring the \u201ctrading day\u201d: \n\ud835\udc36(\ud835\udc61|\ud835\udc5a) =\n1\n\ud835\udc44\u2211\n\ud835\udc36\ud835\udc5a(\ud835\udc61; 1|1, \ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n \n \n \n \n(5.3) \n\ud835\udc36\ud835\udc5c(\ud835\udc61|\ud835\udc5a) = \u2211\n \ud835\udc36\ud835\udc5c\n\ud835\udc5a(\ud835\udc61; 1|1, \ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n \n \n \n \n(5.4) \nSimilar to (4.5), equation (5.2) defines the weight functions \u03c6(t|m,q) (5.5): \n\ud835\udf11(\ud835\udc61|\ud835\udc5a, \ud835\udc5e) =\n \ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61;1|1,\ud835\udc5e)\n\u2211\n \ud835\udc36\ud835\udc5c\ud835\udc5a(\ud835\udc61;1|1,\ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n ; \u2211\n\ud835\udf11(\ud835\udc61|\ud835\udc5a, \ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n= 1 \n \n(5.5) \n\n \n13 \nThe weight functions \u03c6(t|m,q) (5.5) define the frequency-based averages D(t|n,m) (5.6) of the \nn-th degree of return Gn(t;1|q) (4.7) over the set of numerous investors q=1,2,..Q: \n\ud835\udc37(\ud835\udc61|\ud835\udc5b, \ud835\udc5a) = \u2211\n\ud835\udc3a\ud835\udc5b(\ud835\udc61; 1|\ud835\udc5e) \ud835\udf11(\ud835\udc61|\ud835\udc5a, \ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n \n \n \n(5.6) \nWe define the market-based average return R(t|1) (5.7) as: \n\ud835\udc45(\ud835\udc61|1) = \ud835\udc38\ud835\udc5a[\ud835\udc3a(\ud835\udc61\ud835\udc56; 1|\ud835\udc5e)] = \ud835\udc37(\ud835\udc61|1,1) \n \n \n(5.7) \nSimilar to (4.9) we define market-based volatility \u03c3R\n2(t) (5.8) of return determined by \nnumerous different investors during the \u201ctrading day\u201d: \n\ud835\udf0e\ud835\udc45\n2(\ud835\udc61\ud835\udc56) = \ud835\udc38\ud835\udc5a[(\ud835\udc3a(\ud835\udc61\ud835\udc56; 1|\ud835\udc5e) \u2212\ud835\udc45(\ud835\udc61|1))2] \n\ud835\udf0e\ud835\udc45\n2(\ud835\udc61\ud835\udc56) = \u2211\n(\ud835\udc3a(\ud835\udc61\ud835\udc56; 1|\ud835\udc5e) \u2212\ud835\udc45(\ud835\udc61|1))2 \ud835\udf11(\ud835\udc61|\ud835\udc5a, \ud835\udc5e)\n\ud835\udc44\n\ud835\udc5e=1\n \n\ud835\udf0e\ud835\udc45\n2(\ud835\udc61) =\n\u03a9\ud835\udc45\n2(\ud835\udc61)+\ud835\udc452(\ud835\udc61|1)\ud835\udef7\ud835\udc452(\ud835\udc61)\u22122\ud835\udc45(\ud835\udc61|1)\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36((\ud835\udc61;1|1,\ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61;1|1,\ud835\udc5e))}\n\ud835\udc36\ud835\udc5c(\ud835\udc61|2)\n \n(5.8) \nThe market-based volatility \u03c3R\n2(t) (5.8) depends upon the volatilities of current \u03a9R\n2(t) (5.9) \nand past \u03a6R\n2(t) (5.10) trade values during the \u201ctrading day\u201d: \n\u03a9\ud835\udc45\n2(\ud835\udc61) = \ud835\udc38[(\ud835\udc36(\ud835\udc61; 1|1, \ud835\udc5e) \u2212\ud835\udc36(\ud835\udc61|1))2] = \ud835\udc36(\ud835\udc61|2) \u2212\ud835\udc362(\ud835\udc61|1) \n(5.9) \n\ud835\udef7\ud835\udc45\n2(\ud835\udc61) = \ud835\udc38[(\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1, \ud835\udc5e) \u2212\ud835\udc36\ud835\udc5c(\ud835\udc61|1))2] = \ud835\udc36\ud835\udc5c(\ud835\udc61|2) \u2212\ud835\udc36\ud835\udc5c\n2(\ud835\udc61|1) \n(5.10) \nThe \nmarket-based \nvolatility \n\u03c3R\n2(t) \n(5.8) \nalso \ndepends \non \ncorrelation \ncorr{C(t;1|1,q)Co(t;1|1,q)} (5.12) between the current C(t;1|1,q)) (5.3) and original values \nC0(t;1|1,q) (5.4) of investors q=1,2,..Q during the \u201ctrading day\u201d: \n\ud835\udc38[\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))] =\n1\n\ud835\udc44\u2211\n\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))\n\ud835\udc44\n\ud835\udc5e=1\n \n \n(5.11) \n\ud835\udc38[\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))] = \ud835\udc36(\ud835\udc61|1)\ud835\udc36\ud835\udc5c(\ud835\udc61|1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))} (5.12) \n6.",
    "chunk_index": 10,
    "start_char": 26755,
    "end_char": 29915,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "\ud835\udc36(\ud835\udc61|2) \u2212\ud835\udc362(\ud835\udc61|1) \n(5.9) \n\ud835\udef7\ud835\udc45\n2(\ud835\udc61) = \ud835\udc38[(\ud835\udc36\ud835\udc5c(\ud835\udc61; 1|1, \ud835\udc5e) \u2212\ud835\udc36\ud835\udc5c(\ud835\udc61|1))2] = \ud835\udc36\ud835\udc5c(\ud835\udc61|2) \u2212\ud835\udc36\ud835\udc5c\n2(\ud835\udc61|1) \n(5.10) \nThe \nmarket-based \nvolatility \n\u03c3R\n2(t) \n(5.8) \nalso \ndepends \non \ncorrelation \ncorr{C(t;1|1,q)Co(t;1|1,q)} (5.12) between the current C(t;1|1,q)) (5.3) and original values \nC0(t;1|1,q) (5.4) of investors q=1,2,..Q during the \u201ctrading day\u201d: \n\ud835\udc38[\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))] =\n1\n\ud835\udc44\u2211\n\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))\n\ud835\udc44\n\ud835\udc5e=1\n \n \n(5.11) \n\ud835\udc38[\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))] = \ud835\udc36(\ud835\udc61|1)\ud835\udc36\ud835\udc5c(\ud835\udc61|1) + \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f{\ud835\udc36((\ud835\udc61; 1|1, \ud835\udc5e))\ud835\udc36\ud835\udc5c((\ud835\udc61; 1|1, \ud835\udc5e))} (5.12) \n6. Conclusion \nThis paper describes three successive approximations of the market-based averages \nand volatilities of the \u201cactual\u201d return that the investors gain during the \u201ctrading day.\u201d We \ndescribe the approximations of return generated by a single trade sale, by a single investor, \nand by all investors during the \u201ctrading day.\u201d We derive the dependence of the market-based \naverages and volatilities on statistical moments, volatilities, and correlations of the current \nand past trade values. Let us highlight some problems that seem to be important for the \ndescription of financial markets. \nThe market-based average and volatility of return that all investors \u201cactually\u201d gain as \na result of their sales during the \u201ctrading day\u201d can serve as benchmarks and impact the \ndecisions of \u201cpurchasing\u201d investors in financial markets. \u201cPurchasing\u201d investors can assess \ntheir forecast of the expected returns at horizon T in comparison with the \u201cactual\u201d returns that \n\n \n14 \n\u201cselling\u201d investors already gain. Analysis of relations between the statistical moments of \nreturn that \u201cselling\u201d investors already gain and predictions of the statistical moments of return \nof \u201cpurchasing\u201d investors can help develop further asset pricing models and portfolio theory. \nThe volatility of return (5.8) describes the distribution of the \u201cactual\u201d returns over numerous \ninvestors in the stock market. \nProbably, it is difficult to collect and study the market data that permit the \nassessments of market-based averages and volatilities of the \u201cactual\u201d return of a single \ninvestor and of all investors during the \u201ctrading day.\u201d The market data that define the market-\nbased averages and volatilities of \u201canticipated\u201d returns (Section 2) are much more available. \nIt is important to study relations between statistical moments of \u201canticipated\u201d and \u201cactual\u201d \nreturn and highlight possible dependence between these factors. \nThe fluctuations of the \u201canticipated\u201d returns due to the variations of the time shift \u03c4 \ncan impact the duration of stock holding by the investors. That, in turn, can change the scales \nand fluctuations of \u201canticipated\u201d returns determined by the time shift \u03c4. Investigation of the \nhidden mutual dependence between the market-based statistics of the \u201cactual\u201d return of \ninvestors and the statistics of the \u201canticipated\u201d return can help increase the efficiency of \nportfolio performance and asset pricing models. \n \n\n \n15 \nReferences \nAmaral, L., Plerou, V., Gopikrishnan, P., Meyer, M. and E. Stanley, (2000). The Distribution \nof Returns of Stock Prices, Int.J.Theoretical and Applied Finance, 3(3), 365-369 \nAndersen, T., Bollerslev, T., Diebold, F, and H. Ebens. (2001). The Distribution of Realized \nStock Return Volatility, Journal of Financial Economics, 61, 43-76 \nAndersen, T. and L. Benzoni, (2009). Realized Volatility, 555-570, in Andersen, T., Davis, \nR., Krei\u00df, J-P. and T.",
    "chunk_index": 11,
    "start_char": 29396,
    "end_char": 32811,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "return can help increase the efficiency of \nportfolio performance and asset pricing models. \n \n\n \n15 \nReferences \nAmaral, L., Plerou, V., Gopikrishnan, P., Meyer, M. and E. Stanley, (2000). The Distribution \nof Returns of Stock Prices, Int.J.Theoretical and Applied Finance, 3(3), 365-369 \nAndersen, T., Bollerslev, T., Diebold, F, and H. Ebens. (2001). The Distribution of Realized \nStock Return Volatility, Journal of Financial Economics, 61, 43-76 \nAndersen, T. and L. Benzoni, (2009). Realized Volatility, 555-570, in Andersen, T., Davis, \nR., Krei\u00df, J-P. and T. Mikosch, Handbook of Financial Time Series, Springer-Verlag Berlin \nHeidelberg, 1-1031. \nBaker, M. and J. Wurgler, (2004). Investor Sentiment And The Cross-Section Of Stock \nReturns, NBER, Cambridge, WP 10449, 1-47 \nBerkowitz, S.A., Dennis, E., Logue, D.E., Noser, E.A. Jr. (1988). The Total Cost of \nTransactions on the NYSE, The Journal of Finance, 43, (1), 97-112 \nvan Binsbergen, J. and R. Koijen, (2015). The Term Structure Of Returns: Facts And Theory, \nNBER WP 21234, Cambridge, 1-38 \nBrown, S.J. (1989). The Number of Factors in Security Returns, J. Finance, 44(5), 1247-1262 \nCampbell, J. (1985). Stock Returns And The Term Structure, NBER WP1626, 1-53 \nDaniel, K. and D. Hirshleifer, (2016). Overconfident investors, Predictable Returns, And \nExcessive Trading, NBER, Cambridge, WP 21945, 1-36 \nDuffie, D. and P. Dworczak, (2018). Robust Benchmark Design, NBER WP 20540, 1-56 \nGreenwood, R. and A. Shleifer, (2013). Expectations of Returns and Expected Returns, \nWP18686, NBER, Cambridge, 1-52 \nFama, E.F. (1990). Stock Returns, Expected Returns, and Real Activity, J. Finance, 45(4), \n1089-1108 \nFama, E.F. and K. R. French, (1992). The Cross-Section of Expected Stock Returns, \nJ.Finance, 47 (2), 427-465 \nFisher, L. and J. Lorie, (1964). Rates Of Return On Investments In Common Stocks, J. \nBusiness, 37(1), 1-21 \nGabaix, X., Gopikrishnan, P., Plerou, V. and E. Stanley, (2005). Institutional investors And \nStock Market Volatility, NBER, Cambridge, WP 11722, 1-50 \nGreenwood, R. and A. Shleifer, (2013). Expectations Of Returns And Expected Returns, \nNBER, Cambridge, WP 18686, 1-51 \nHardouvelis, G., Karalas, G. and D. Vayanos, (2021). The Distribution of investor Beliefs, \nStock Ownership and Stock Returns, NBER, Cambridge, WP 28697, 1-47 \n\n \n16 \nIvkovi\u0107, Z., Sialm, C. and S. Weisbenner, (2004). Portfolio Concentration and The \nPerformance of Individual investors, NBER, Cambridge, WP 10675, 1-52 \nKnight, J. and S. Satchell, (Ed). (2001). Return Distributions In Finance, Butterworth-\nHeinemann, Oxford, 1-328 \nKoijen, R.S., Richmond, R.J. and M. Yogo (2020). Which investors Matter For Equity \nValuations And Expected Returns?, NBER, Cambridge, WP 27402, 1-53 \nLettau, M. and S. C. Ludvigson, (2003). Expected Returns And Expected Dividend Growth, \nWP 9605, NBER, Cambridge, 1-48 \nMandelbrot, B., Fisher, A. and L. Calvet, (1997). A Multifractal Model of Asset Returns, \nYale University, Cowles Foundation Discussion WP1164, 1-39 \nMarkowitz, H. (1952). Portfolio Selection, J. Finance, 7(1), 77-91 \nMartin, I. and C. Wagner (2019). What Is the Expected Return on a Stock?, J. Finance, 74(4), \n1887-1929 \nOlkhov, V. (2021). Three Remarks On Asset Pricing, SSRN WP3852261, 1-21 \nOlkhov, V. (2022). Market-Based Asset Price Probability, MPRA WP115382, 1-18 \nOlkhov, V. (2023). Market-Based Probability of Stock Returns, SSRN, WP 4350975, 1- 17 \nSchlarbaum, G.G., Lewellen, G.",
    "chunk_index": 12,
    "start_char": 32245,
    "end_char": 35706,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "L. Calvet, (1997). A Multifractal Model of Asset Returns, \nYale University, Cowles Foundation Discussion WP1164, 1-39 \nMarkowitz, H. (1952). Portfolio Selection, J. Finance, 7(1), 77-91 \nMartin, I. and C. Wagner (2019). What Is the Expected Return on a Stock?, J. Finance, 74(4), \n1887-1929 \nOlkhov, V. (2021). Three Remarks On Asset Pricing, SSRN WP3852261, 1-21 \nOlkhov, V. (2022). Market-Based Asset Price Probability, MPRA WP115382, 1-18 \nOlkhov, V. (2023). Market-Based Probability of Stock Returns, SSRN, WP 4350975, 1- 17 \nSchlarbaum, G.G., Lewellen, G. and R. C. Lease, (1978). Realized Returns on Common \nStock Investments: The Experience of Individual investors, J. of Business, 51(2) 299-325 \nShephard, N.G. (1991). From Characteristic Function to Distribution Function: A Simple \nFramework for the Theory. Econometric Theory, 7 (4), 519-529 \nShiryaev, A.N. (1999). Essentials Of Stochastic Finance: Facts, Models, Theory. World Sc. \nPub., Singapore. 1-852 \nShreve, S. E. (2004). Stochastic calculus for finance, Springer finance series, NY, USA \nStanley, K.L., Lewellen, G. and G.G. Schlarbaum, (1980). Further Evidence of on the Value \nof Professional Investment Research, NBER, Cambridge, WP 536, 1-19 \nTsay, R.S. (2005). Analysis of Financial Time Series, J.Wiley&Sons, Inc., New Jersey, 1-638",
    "chunk_index": 13,
    "start_char": 35146,
    "end_char": 36454,
    "paper_title": "Market-Based Actual Returns of Investors",
    "paper_category": "econ.GN",
    "paper_filename": "Market-Based_Actual_Returns_of_Investors.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/econ.GN/Market-Based_Actual_Returns_of_Investors.pdf"
  },
  {
    "text": "Anticipating cryptocurrency prices\nusing machine learning\nLaura Alessandrettia, Abeer ElBahrawyb, Luca Maria Aielloc, and\nAndrea Baronchellib,d,*\naTechnical University of Denmark, DK-2800 Kgs. Lyngby, Denmark\nbCity, University of London, Department of Mathematics, London EC1V 0HB, UK\ncNokia Bell Labs, Cambridge CB3 0FA, UK\ndUCL Centre for Blockchain Technologies, University College London, UK\n*Corresponding author: Andrea.Baronchelli.1@city.ac.uk\nMarch 6, 2019\nAbstract\nMachine learning and AI-assisted trading have attracted growing interest for the past few years.\nHere, we use this approach to test the hypothesis that the ine\ufb03ciency of the cryptocurrency market\ncan be exploited to generate abnormal pro\ufb01ts. We analyse daily data for 1, 681 cryptocurrencies\nfor the period between Nov. 2015 and Apr. 2018. We show that simple trading strategies assisted\nby state-of-the-art machine learning algorithms outperform standard benchmarks. Our results show\nthat non-trivial, but ultimately simple, algorithmic mechanisms can help anticipate the short-term\nevolution of the cryptocurrency market.\nIntroduction\nThe popularity of cryptocurrencies has skyrocketed in 2017 due to several consecutive months of super-\nexponential growth of their market capitalisation [1], which peaked at more than $800 billions in Jan.\n2018. Today, there are more than 1, 500 actively traded cryptocurrencies. Between 2.9 and 5.8 millions\nof private as well as institutional investors are in the di\ufb00erent transaction networks, according to a\nrecent survey [2], and access to the market has become easier over time. Major cryptocurrencies can be\nbought using \ufb01at currency in a number of online exchanges (e.g., Binance [3], Upbit [4], Kraken [5], etc)\nand then be used in their turn to buy less popular cryptocurrencies. The volume of daily exchanges is\ncurrently superior to $15 billions. Since 2017, over 170 hedge funds specialised in cryptocurrencies have\nemerged and bitcoin futures have been launched to address institutional demand for trading and hedging\nBitcoin [6].\nThe market is diverse and provides investors with many di\ufb00erent products. Just to mention a few,\nBitcoin was expressly designed as a medium of exchange [7, 8]; Dash o\ufb00ers improved services on top\nof Bitcoin\u2019s feature set, including instantaneous and private transactions [9]; Ethereum is a public,\nblockchain-based distributed computing platform featuring smart contract (scripting) functionality, and\n1\narXiv:1805.08550v4 [physics.soc-ph] 9 Nov 2018\n\nEther is a cryptocurrency whose blockchain is generated by the Ethereum platform [10]; Ripple is a\nreal-time gross settlement system (RTGS), currency exchange and remittance network Ripple [11], and\nIOTA is focused on providing secure communications and payments between agents on the Internet of\nThings [12].\nThe emergence of a self-organised market of virtual currencies and/or assets whose value is generated\nprimarily by social consensus [13] has naturally attracted interest from the scienti\ufb01c community [8,14\u201330].\nRecent results have shown that the long-term properties of the cryptocurrency marked have remained\nstable between 2013 and 2017 and are compatible with a scenario in which investors simply sample the\nmarket and allocate their money according to the cryptocurrency\u2019s market shares [1].\nWhile this is\ntrue on average, various studies have focused on the analysis and forecasting of price \ufb02uctuations, using\nmostly traditional approaches for \ufb01nancial markets analysis and prediction [31\u201335].",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3516,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "primarily by social consensus [13] has naturally attracted interest from the scienti\ufb01c community [8,14\u201330].\nRecent results have shown that the long-term properties of the cryptocurrency marked have remained\nstable between 2013 and 2017 and are compatible with a scenario in which investors simply sample the\nmarket and allocate their money according to the cryptocurrency\u2019s market shares [1].\nWhile this is\ntrue on average, various studies have focused on the analysis and forecasting of price \ufb02uctuations, using\nmostly traditional approaches for \ufb01nancial markets analysis and prediction [31\u201335].\nThe success of machine learning techniques for stock markets prediction [36\u201342], suggests that these\nmethods could be e\ufb00ective also in predicting cryptocurrencies prices. However, the application of machine\nlearning algorithms to the cryptocurrency market has been limited so far to the analysis of Bitcoin prices,\nusing random forests [43], Bayesian neural network [44], long short-term memory neural network [45]\nand other algorithms [32, 46].\nThese studies were able to anticipate, to di\ufb00erent degrees, the price\n\ufb02uctuations of Bitcoin, and revealed that best results were achieved by neural network based algorithms.\nDeep reinforcement learning was showed to beat the uniform buy and hold strategy [47] in predicting\nthe prices of 12 cryptocurrencies over one year period [48].\nOther attempts to use machine learning to predict the prices of cryptocurrencies other than Bit-\ncoin come from non-academic sources [49\u201354]. Most of these analyses focused on a limited number of\ncurrencies and did not provide benchmark comparisons for their results.\nHere, we test the performance of three models in predicting daily cryptocurrency price for 1,681\ncurrencies. Two of the models are based on gradient boosting decision trees [55] and one is based on long\nshort-term memory (LSTM) recurrent neural networks [56]. In all cases, we build investment portfolios\nbased on the predictions and we compare their performance in terms of return on investment. We \ufb01nd\nthat all of the three models perform better than a baseline \u2018simple moving average\u2019 model [57\u201360] where\na currency\u2019s price is predicted as the average price across the preceding days, and that the method\nbased on long short-term memory recurrent neural networks systematically yields the best return on\ninvestment.\nThe article is structured as follows: In section Materials and Methods we describe the data (see Data\ndescription and pre-processing), the metrics characterizing cryptocurrencies that are used along the paper\n(see Metrics), the forecasting algorithms (see Forecasting algorithms), and the evaluation metrics (see\nEvaluation). In section Results, we present and compare the results obtained with the three forecasting\nalgorithms and the baseline method. In section Conclusion, we conclude and discuss results.\nMaterials and Methods\nData description and pre-processing\nCryptocurrency data was extracted from the website Coin Market Cap [61], collecting daily data from\n300 exchange markets platforms starting in the period between November 11, 2015 and April 24, 2018.\nThe dataset contains the daily price in U.S. dollars, the market capitalisation and the trading volume\nof 1, 681 cryptocurrencies, where the market capitalization is the product between price and circulating\nsupply, and the volume is the number of coins exchanged in a day. The daily price is computed as the\n2",
    "chunk_index": 1,
    "start_char": 2920,
    "end_char": 6363,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "In section Conclusion, we conclude and discuss results.\nMaterials and Methods\nData description and pre-processing\nCryptocurrency data was extracted from the website Coin Market Cap [61], collecting daily data from\n300 exchange markets platforms starting in the period between November 11, 2015 and April 24, 2018.\nThe dataset contains the daily price in U.S. dollars, the market capitalisation and the trading volume\nof 1, 681 cryptocurrencies, where the market capitalization is the product between price and circulating\nsupply, and the volume is the number of coins exchanged in a day. The daily price is computed as the\n2\n\nvolume weighted average of all prices reported at each market. Fig. 1 shows the number of currencies\nwith trading volume larger than Vmin over time, for di\ufb00erent values of Vmin. In the following sections,\nwe consider that only currencies with daily trading volume higher than 105 USD can be traded at any\ngiven day.\nThe website lists cryptocurrencies traded on public exchange markets that have existed for more than\n30 days and for which an API as well as a public URL showing the total mined supply are available.\nInformation on the market capitalization of cryptocurrencies that are not traded in the 6 hours preceding\nthe weekly release of data is not included on the website. Cryptocurrencies inactive for 7 days are not\nincluded in the list released. These measures imply that some cryptocurrencies can disappear from the\nlist to reappear later on. In this case, we consider the price to be the same as before disappearing.\nHowever, this choice does not a\ufb00ect results since only in 28 cases the currency has volume higher than\n105 USD right before disappearing (note that there are 124,328 entries in the dataset with volume larger\nthan 105 USD).\nJan 16\nMay 16\nSep 16\nJan 17\nMay 17\nSep 17\nJan 18\n10\n0\n10\n1\n10\n2\n10\n3\ncryptocurrencies\nVmin = $0\nVmin = $1000\nVmin = $10000\nVmin = $100000\nFigure 1: Number of cryptocurrencies. The cryptocurrencies with volume higher than Vmin as a\nfunction of time, for di\ufb00erent values of Vmin. For visualization purposes, curves are averaged over a\nrolling window of 10 days.\nMetrics\nCryptocurrencies are characterised over time by several metrics, namely\n\u2022 Price, The exchange rate, determined by supply and demand dynamics.\n\u2022 Market capitalization, The product of the circulating supply and the price.\n\u2022 Market share, The market capitalization of a currency normalized by the total market capitaliza-\ntion.\n\u2022 Rank, The rank of currency based on its market capitalization.\n\u2022 Volume, Coins traded in the last 24 hours.\n\u2022 Age, Lifetime of the currency in days.\nThe pro\ufb01tability of a currency c over time can be quanti\ufb01ed through the return on investment (ROI),\nmeasuring the return of an investment made at day ti relative to the cost [62]. The index i rolls across\ndays and it is included between 0 and 844, with t0 = January 1, 2016, and t844 = April 24, 2018. Since\n3",
    "chunk_index": 2,
    "start_char": 5739,
    "end_char": 8671,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "its market capitalization.\n\u2022 Volume, Coins traded in the last 24 hours.\n\u2022 Age, Lifetime of the currency in days.\nThe pro\ufb01tability of a currency c over time can be quanti\ufb01ed through the return on investment (ROI),\nmeasuring the return of an investment made at day ti relative to the cost [62]. The index i rolls across\ndays and it is included between 0 and 844, with t0 = January 1, 2016, and t844 = April 24, 2018. Since\n3\n\nwe are interested in the short-term performance, we consider the return on investment after 1 day de\ufb01ned\nas\nROI(c, ti) = price(c, ti) \u2212price(c, ti \u22121)\nprice(c, ti \u22121)\n.\n(1)\nIn Fig. 2, we show the evolution of the ROI over time for Bitcoin (orange line) and on average for\ncurrencies whose volume is larger than Vmin = 105 USD at ti \u22121 (blue line). In both case, the average\nreturn on investment over the period considered is larger than 0, re\ufb02ecting the overall growth of the\nmarket.\nJan 16\nMay 16\nSep 16\nJan 17\nMay 17\nSep 17\nJan 18\nday\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nroi\ncryptos with volume>10\n5\nmean=0.011\nBitcoin\nmean=0.005\nFigure 2: Return on investment over time. The daily return on investment for Bitcoin (orange line)\nand the average for currencies with volume larger than Vmin = 105 USD (blue line). Their average value\nacross time (dashed lines) is larger than 0. For visualization purposes, curves are averaged over a rolling\nwindow of 10 days.\nForecasting algorithms\nWe test and compare three supervised methods for short-term price forecasting. The \ufb01rst two methods\nrely on XGboost [63], an open-source scalable machine learning system for tree boosting used in a\nnumber of winning Kaggle solutions (17/29 in 2015) [64]. The third method is based on the long short-\nterm memory (LSTM) algorithm for recurrent neural networks [56] that have demonstrated to achieve\nstate-of-the-art results in time-series forecasting [65].\nMethod 1: The \ufb01rst method considers one single regression model to describe the change in price of\nall currencies (see Fig. 3). The model is an ensemble of regression trees built by the XGboost algorithm.\nThe features of the model are characteristics of a currency between time tj \u2212w and tj \u22121 and the target\nis the ROI of the currency at time tj, where w is a parameter to be determined. The characteristics\nconsidered for each currency are: price, market capitalization, market share, rank, volume and ROI (see\nequation Eq. (1)). The features for the regression are built across the window between tj \u2212w and tj \u22121\nincluded (see Fig. 3). Speci\ufb01cally, we consider the average, the standard deviation, the median, the last\nvalue and the trend (e.g. the di\ufb00erence between last and \ufb01rst value) of the properties listed above. In the\ntraining phase, we include all currencies with volume larger than 105 USD, and tj between ti \u2212Wtraining\nand ti. In general, larger training windows do not necessarily lead to better results (see results section),\nbecause the market evolves across time.",
    "chunk_index": 3,
    "start_char": 8249,
    "end_char": 11189,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "regression are built across the window between tj \u2212w and tj \u22121\nincluded (see Fig. 3). Speci\ufb01cally, we consider the average, the standard deviation, the median, the last\nvalue and the trend (e.g. the di\ufb00erence between last and \ufb01rst value) of the properties listed above. In the\ntraining phase, we include all currencies with volume larger than 105 USD, and tj between ti \u2212Wtraining\nand ti. In general, larger training windows do not necessarily lead to better results (see results section),\nbecause the market evolves across time. In the prediction phase, we test on the set of existing currencies\nat day ti. This procedure is repeated for values of ti included between January 1, 2016 and April 24,\n2018.\nMethod 2: Also the second method relies on XGboost, but now the algorithm is used to build a\ndi\ufb00erent regression model for each currency ci (see Fig. 4). The features of the model for currency ci are\n4\n\nti\nWtraining\nti\nWtraining + 1\ntj\nti\n1\nti\nC1\nCj\nCN\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nw\nFeatures\nT\nw\nFeatures\nT\nw\nFeatures\nT\nw\nTraining\nTest\nFigure 3: Schematic description of Method 1. The training set is composed of features and tar-\nget (T) pairs, where features are various characteristics of a currency ci, computed across the w days\npreceding time tj and the target T is the price of ci at tj. The features-target pairs are computed for\nall currencies ci and all values of tj included between ti \u2212Wtraining and ti \u22121. The test set includes\nfeatures-target pairs for all currencies with trading volume larger than 105 USD at ti, where the target\nis the price at time ti and features are computed in the w days preceding ti.\nti\nWtraining\nti\nWtraining + 1\ntj\nti\n1\nti\nC1\nCj\nCN\nFeatures\nFeatures\nFeatures\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nT\nFeatures\nFeatures\nw\nFeatures\nw\nFeatures\nw\nFeatures\nw\nTraining\nTest\nFigure 4: Schematic description of Method 2. The training set is composed of features and tar-\nget (T) pairs, where features are various characteristics of all currencies, computed across the w days\npreceding time tj and the target T is the price of ci at tj. The features-target pairs include a single\ncurrency ci, for all values of tj included between ti \u2212Wtraining and ti \u22121. The test set contains a single\nfeatures-target pair: the characteristics of all currencies, computed across the w days preceding time ti\nand the price of ci at ti.\nthe characteristics of all the currencies in the dataset between tj \u2212w and tj \u22121 included and the target\nis the ROI of ci at day tj(i.e., now the algorithm learns to predict the price of the currency i based on\nthe features of all the currencies in the system between tj \u2212w and tj \u22121). The features of the model are\nthe same used in Method 1 (e.g.",
    "chunk_index": 4,
    "start_char": 10660,
    "end_char": 13438,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "across the w days preceding time ti\nand the price of ci at ti.\nthe characteristics of all the currencies in the dataset between tj \u2212w and tj \u22121 included and the target\nis the ROI of ci at day tj(i.e., now the algorithm learns to predict the price of the currency i based on\nthe features of all the currencies in the system between tj \u2212w and tj \u22121). The features of the model are\nthe same used in Method 1 (e.g. the average, standard, deviation, median, last value, di\ufb00erence between\nlast and \ufb01rst value of the following quantities: price, market capitalisation, market share, rank, volume\nand ROI) across a window of length w. The model for currency ci is trained with pairs features target\nbetween times ti \u2212Wtraining and ti \u22121. The prediction set include only one pair: the features (computed\nbetween ti \u2212w and ti \u22121) and the target (computed at ti) of currency ci.\nMethod 3: The third method is based on Long Short Term Memory networks, a special kind of\nRecurrent Neural Networks, capable of learning long-term dependencies. As for Method 2, we build a\ndi\ufb00erent model for each currency. Each model predicts the ROI of a given currency at day ti based on\nthe values of the ROI of the same currency between days ti \u2212w and ti \u22121 included.\n5\n\nBaseline method: As baseline method, we adopt the simple moving average strategy (SMA) widely\ntested and used as a null model in stock market prediction [57\u201360]. It estimates the price of a currency\nat day ti as the average price of the same currency between ti \u2212w and ti \u22121 included.\nEvaluation\nWe compare the performance of various investment portfolios built based on the algorithms predictions.\nThe investment portfolio is built at time ti \u22121 by equally splitting an initial capital among the top n\ncurrencies predicted with positive return. Hence, the total return at time ti is:\nR(ti) = 1\nn\nn\nX\nc=1\nROI(c, ti).\nThe portfolios performance is evaluated by computing the Sharpe ratio and the geometric mean return.\nThe Sharpe ratio is de\ufb01ned as:\nS(ti) = R\nsR\n,\nwhere R is the average return on investment obtained between times 0 and ti, and sR, the corresponding\nstandard deviation.\nThe geometric mean return is de\ufb01ned as:\nG(ti) =\nT\nv\nu\nu\nt\nti\nY\ntj=1\n1 + R(tj),\nwhere ti corresponds to the total number of days considered. The cumulative return obtained at ti after\ninvesting and selling on the following day for the whole period is de\ufb01ned as G(ti)2.\nThe number of currencies n to include in a portfolio is chosen at ti by optimising either the geometric\nmean G(ti \u22121) (geometric mean optimisation) or the Sharpe ratio S(ti \u22121) (Sharpe ratio optimisation)\nover the possible choices of n. The same approach is used to choose the parameters of Method 1 (w and\nWtraining), Method 2 (w and Wtraining), and the baseline method (w).",
    "chunk_index": 5,
    "start_char": 13028,
    "end_char": 15801,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "on the following day for the whole period is de\ufb01ned as G(ti)2.\nThe number of currencies n to include in a portfolio is chosen at ti by optimising either the geometric\nmean G(ti \u22121) (geometric mean optimisation) or the Sharpe ratio S(ti \u22121) (Sharpe ratio optimisation)\nover the possible choices of n. The same approach is used to choose the parameters of Method 1 (w and\nWtraining), Method 2 (w and Wtraining), and the baseline method (w).\nResults\nWe predict the price of the currencies at day ti, for all ti included between Jan, 1st 2016 and Apr 24th,\n2018. The analysis considers all currencies whose age is larger than 50 days since their \ufb01rst appearance\nand whose volume is larger than $100000. To discount for the e\ufb00ect of the overall market movement\n(i.e., market growth, for most of the considered period), we consider cryptocurrencies prices expressed\nin Bitcoin. This implies that Bitcoin is excluded from our analysis.\nParameter setting\nFirst, we choose the parameters for each method. Parameters include the number of currencies n to\ninclude the portfolio as well as the parameters speci\ufb01c to each method. In most cases, at each day ti we\nchoose the parameters that maximise either the geometric mean G(ti \u22121) (geometric mean optimisation)\nor the Sharpe ratio S(ti \u22121) (Sharpe ratio optimisation) computed between times 0 and ti.\n6\n\nBaseline strategy: We test the performance of the baseline strategy for choices of window w \u22652\n(the minimal requirement for the ROI to be di\ufb00erent from 0) and w < 30. We \ufb01nd that the value of w\nmazimising the geometric mean return (see Appendix Fig. A1-A) and the Sharpe Ratio (see Appendix\nFig. A1-D) \ufb02uctuates especially before November 2016 and has median value 4 in both cases.\nThe\nnumber of currencies included in the portfolio oscillates between 1 and 11 with median at 3, both for\nthe Sharpe Ratio (see Fig. A1-B) and the geometric mean return (see Fig. A1-E) optimisation.\nMethod 1: We explore values of the window w in {3, 5, 7, 10} days and the training period Wtraining\nin {5, 10, 20} days (see Appendix Fig. A2). We \ufb01nd that the median value of the selected window w\nacross time is 7 for both the Sharpe ratio and the geometric mean optimisation. The median value of\nWtraining is 5 under geometric mean optimisation and 10 under Sharpe ratio optimisation. The number\nof currencies included in the portfolio oscillates between 1 and 43 with median at 15 for the Sharpe Ratio\n(see Appendix Fig. A2-A) and 9 for the geometric mean return (see Appendix Fig. A2-C) optimisations.\nMethod 2: We explore values of the window w in {3, 5, 7, 10} days and the training period Wtraining\nin {5, 10, 20} days (see Appendix Fig. A3). The median value of the selected window w across time is\n3 for both the Sharpe ratio and the geometric mean optimisation. The median value of Wtraining is 10\nunder geometric mean and Sharpe ratio optimisation.",
    "chunk_index": 6,
    "start_char": 15363,
    "end_char": 18247,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "(see Appendix Fig. A2-A) and 9 for the geometric mean return (see Appendix Fig. A2-C) optimisations.\nMethod 2: We explore values of the window w in {3, 5, 7, 10} days and the training period Wtraining\nin {5, 10, 20} days (see Appendix Fig. A3). The median value of the selected window w across time is\n3 for both the Sharpe ratio and the geometric mean optimisation. The median value of Wtraining is 10\nunder geometric mean and Sharpe ratio optimisation. The number of currencies included has median at\n17 for the Sharpe Ratio and 7 for the geometric mean optimisation (see Appendix Fig. A3-A and C).\nMethod 3: The LSTM has three parameters: The number of epochs, or complete passes through\nthe dataset during the training phase; the number of neurons in the neural network, and the length\nof the window w. These parameters are chosen by optimising the price prediction of three currencies\n(Bitcoin, Ripple, and Ethereum) that have on average the largest market share across time (excluding\nBitcoin Cash that is a fork of Bitcoin). Results (see SI, Fig. A4) reveal that, in the range of parameters\nexplored, the best results are achieved for w = 50. Results are not particularly a\ufb00ected by the choice\nof the number of neurones nor the number of epochs. We choose 1 neuron and 1000 epochs since the\nlarger these two parameters, the larger the computational time. The number of currencies to include in\nthe portfolio is optimised over time by mazimising the geometric mean return (see Appendix Fig. A5-A)\nand the Sharpe ratio (see Appendix Fig. A5-B). In both cases the median number of currencies included\nis 1.\nCumulative return\nIn Fig. 5, we show the cumulative return obtained using the 4 methods. The cumulative returns achieved\non April,24 under the Sharpe Ratio optimisation are \u223c65 BTC (Baseline), \u223c1.1 \u00b7 103 BTC (Method 1),\n\u223c95 BTC (Method 2), \u223c1.2\u00b7109 BTC (Method 3). Under geometric mean optimisation we obtain \u223c25\nBTC (Baseline), \u223c19\u00b7103 BTC (Method 1), \u223c1.25 BTC (Method 2), \u223c3.6\u00b7108 BTC (Method 3). The\ncumulative returns obtained in USD are higher (see Fig. A9). This is expected, since the Bitcoin price\nhas increased during the period considered. While some of these \ufb01gures appear exaggerated, it is worth\nnoticing that (i) we run a theoretical exercise assuming that the availability of Bitcoin is not limited and\n(ii) under this assumption the upper bound to our strategy, corresponding to investing every day in the\nmost performing currency results in a total cumulative return of 6 \u00b7 10123 BTC (see Appendix Fig. A6).\nWe consider also the more realistic scenario of investors paying a transaction fee when selling and buying\ncurrencies (see Appendix Section A3). In most exchange markets, the fee is typically included between\n0.1% and 0.5% of the traded amount [66]. For fees up to 0.2%, all the investment methods presented\nabove lead, on average, to positive returns over the entire period (see Table A1). The best performing\nmethod, Method 3, achieves positive gains also when fees up to 1% are considered (see Table A1).\n7",
    "chunk_index": 7,
    "start_char": 17793,
    "end_char": 20839,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "return of 6 \u00b7 10123 BTC (see Appendix Fig. A6).\nWe consider also the more realistic scenario of investors paying a transaction fee when selling and buying\ncurrencies (see Appendix Section A3). In most exchange markets, the fee is typically included between\n0.1% and 0.5% of the traded amount [66]. For fees up to 0.2%, all the investment methods presented\nabove lead, on average, to positive returns over the entire period (see Table A1). The best performing\nmethod, Method 3, achieves positive gains also when fees up to 1% are considered (see Table A1).\n7\n\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n0\n10\n3\n10\n6\n10\n9\ncumulative return\nA\ngeometric mean optimization\nBaseline\nMethod 1\nMethod 2\nMethod 3\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n0\n10\n3\n10\n6\n10\n9\ncumulative return\nB\nSharpe ratio optimization\nBaseline\nMethod 1\nMethod 2\nMethod 3\nFigure 5: Cumulative returns. The cumulative returns obtained under the Sharpe Ratio optimisation\n(A) and the geometric mean optimisation (B) for the baseline (blue line), Method 1 (orange line), Method\n2 (green line) and Method 3 (red line). Analyses are performed considering prices in BTC.\nThe cumulative return in Fig. 5 is obtained by investing between January 1st, 2016 and April 24th,\n2018. We investigate the overall performance of the various methods by looking at the geometric mean\nreturn obtained in di\ufb00erent periods (see Fig. 6). Results presented in Fig. 6 are obtained under Sharpe\nratio optimisation for the baseline (Fig. 6-A), Method 1 (Fig. 6-B), Method 2 (Fig. 6-C), and Method 3\n(Fig. 6-D). Note that, while in this case the investment can start after January 1st, 2016, we optimised\nthe parameters by using data from that date on in all cases. Results are considerably better than those\nachieved using geometric mean return optimisation (see Appendix Fig. A10). Finally, we observe that\nbetter performance is achieved when the algorithms consider prices in Bitcoin rather than USD (see\nTable A2).\n8\n\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\nBaseline\nA\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 1\nB\nMethod 2\nC\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure 6: Geometric mean return obtained within di\ufb00erent periods of time. The geometric\nmean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for the\nbaseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).",
    "chunk_index": 8,
    "start_char": 20282,
    "end_char": 23112,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure 6: Geometric mean return obtained within di\ufb00erent periods of time. The geometric\nmean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for the\nbaseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).\nFeature importance\nIn Fig. 7, we illustrate the relative importance of the various features in Method 1 and Method 2.\nFor Method 1, we show the average feature importance; For Method 2, we show the average feature\nimportances for two sample currencies: Ethereum and Ripple.\n9\n\nprice, mean\nROI, last\nROI, mean\nROI, trend\nage, last\nROI, std\nvolume, mean\nprice, trend\nprice, std\nvolume, last\nvolume, trend\nvolume, std\nrank, std\nrank, trend\nprice, last\nmarket share, trend\nrank, mean\nmarket share, std\nmarket cap, trend\nmarket cap, std\nrank, last\nmarket share, mean\nmarket share, last\nmarket cap, last\nmarket cap, mean\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nfeature importance\nA\nprice, mean, Dash \nprice, mean, Augur\nprice, mean, BitShares\nprice, mean, Ardor \nprice, mean, 0x\nprice, mean, Ethereum\nprice, mean, Dogecoin \nprice, mean, Ethereum Classic\nprice, mean, Factom\nprice, mean, Litecoin\nprice, mean, ATBCoin\nprice, mean, MaidSafeCoin \nprice, mean, Aragon\nprice, mean, AdEx\nprice, mean, Achain\nprice, std, Ethereum\nprice, mean, E-Dinar Coin\nprice, last, Ethereum\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nfeature importance\nB\nEthereum\nprice, mean, Dash \nprice, mean, Augur\nprice, mean, BitShares\nprice, mean, 0x\nprice, mean, Dogecoin \nprice, mean, Ardor \nprice, mean, Ethereum\nprice, mean, EDRCoin \nprice, mean, Factom\nprice, mean, Ark\nprice, mean, MaidSafeCoin \nprice, mean, AdEx\nprice, mean, E-Dinar Coin\nprice, mean, Aragon\nprice, mean, Litecoin\nprice, mean, ATBCoin\nprice, mean, Ethereum Classic\nprice, std, Dash \nfeature importance\nC\nRipple\nFigure 7: Feature importance for Methods 1 and 2. (A) The average importance of each feature\nfor the XGBoost regression model of Method 1.\nResults are shown for w = 7 and Wtraining = 10.\n(B,C) Examples of average feature importance for the XGBoost regression model developed in Method\n2. Results are shown for w = 3, Wtraining = 10, for Ethereum (B) and Ripple (C). For visualization\npurposes, we show only the top features.\nPortfolio composition\nThe 10 most selected currencies under Sharpe Ratio optimisation are the following:\nBaseline:\nFactom (91 days), E-Dinar Coin (89 days), Ripple (76 days), Ethereum (71 days), Steem\n(70 days), Lisk (70 days), MaidSafeCoin (69 days), Monero (58 days), BitShares (55 days), EDRCoin\n(52 days).\nMethod 1: Ethereum (154 days), Dash (128 days), Monero (111 days), Factom (104 days), Ripple\n(94 days), Litecoin (93 days), Dogecoin (92 days), Maid Safe Coin (86 days), BitShares (73 days), Tether\n(59 days)\nMethod 2: Ethereum (63 days), Monero (61 days), Factom (51 days), Ripple (42 days), Dash (40\ndays), Maid Safe Coin (40 days), Siacoin (30 days), NEM (26 days), NXT (26 days), Steem (23 days).",
    "chunk_index": 9,
    "start_char": 22625,
    "end_char": 25773,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "Ripple (76 days), Ethereum (71 days), Steem\n(70 days), Lisk (70 days), MaidSafeCoin (69 days), Monero (58 days), BitShares (55 days), EDRCoin\n(52 days).\nMethod 1: Ethereum (154 days), Dash (128 days), Monero (111 days), Factom (104 days), Ripple\n(94 days), Litecoin (93 days), Dogecoin (92 days), Maid Safe Coin (86 days), BitShares (73 days), Tether\n(59 days)\nMethod 2: Ethereum (63 days), Monero (61 days), Factom (51 days), Ripple (42 days), Dash (40\ndays), Maid Safe Coin (40 days), Siacoin (30 days), NEM (26 days), NXT (26 days), Steem (23 days).\nMethod 3: Factom (48 days), Monero (46 days), Ethereum (39 days), Lisk (36 days), Maid Safe Coin\n10\n\n(32 days), E-Dinar Coin (32 days), BitShares (26 days), B3 Coin (26 days), Dash (25 days), Cryptonite\n(22 days).\nConclusion\nWe tested the performance of three forecasting models on daily cryptocurrency prices for 1, 681 currencies.\nTwo of them (Method 1 and Method 2) were based on gradient boosting decision trees and one is based\non long short-term memory recurrent neural networks (Method 3). In Method 1, the same model was\nused to predict the return on investment of all currencies; in Method 2, we built a di\ufb00erent model for\neach currency, that uses information on the behaviour of the whole market to make a prediction on that\nsingle currency; in Method 3, we used a di\ufb00erent model for each currency, where the prediction is based\non previous prices of the currency.\nWe built investment portfolios based on the predictions of the di\ufb00erent method and compared their\nperformance with that of a baseline represented by the well known simple moving average strategy. The\nparameters of each model were optimised for all but Method 3 on a daily basis, based on the outcome\nof each parameters choice in previous times. We used two evaluation metrics used for parameter opti-\nmisation: The geometric mean return and the Sharpe ratio. To discount the e\ufb00ect of the overall market\ngrowth, cryptocurrencies prices were expressed in Bitcoin. All strategies, produced pro\ufb01t (expressed\nin Bitcoin) over the entire considered period and for a large set of shorter trading periods (di\ufb00erent\ncombinations of start and end dates for the trading activity), also when transaction fees up to 0.2% are\nconsidered.\nThe three methods performed better than the baseline strategy when the investment strategy was ran\nover the whole period considered. The optimisation of parameters based on the Sharpe ratio achieved\nlarger returns. Methods based on gradient boosting decision trees (Method 1 and 2) worked best when\npredictions were based on short-term windows of 5/10 days, suggesting they exploit well mostly short-\nterm dependencies. Instead, LSTM recurrent neural networks worked best when predictions were based\non \u223c50 days of data, since they are able to capture also long-term dependencies and are very stable\nagainst price volatility. They allowed to make pro\ufb01t also if transaction fees up to 1% are considered.\nMethods based on gradient boosting decision trees allow to better interpret results.",
    "chunk_index": 10,
    "start_char": 25221,
    "end_char": 28260,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "were based on short-term windows of 5/10 days, suggesting they exploit well mostly short-\nterm dependencies. Instead, LSTM recurrent neural networks worked best when predictions were based\non \u223c50 days of data, since they are able to capture also long-term dependencies and are very stable\nagainst price volatility. They allowed to make pro\ufb01t also if transaction fees up to 1% are considered.\nMethods based on gradient boosting decision trees allow to better interpret results. We found that the\nprices and the returns of a currency in the last few days preceding the prediction were leading factors\nto anticipate its behaviour. Among the two methods based on random forests, the one considering a\ndi\ufb00erent model for each currency performed best (Method 2). Finally, it is worth noting that the three\nmethods proposed perform better when predictions are based on prices in Bitcoin rather than prices\nin USD. This suggests that forecasting simultaneously the overall cryptocurrency market trend and the\ndevelopments of individual currencies is more challenging than forecasting the latter alone.\nIt is important to stress that our study has limitations. First, we did not attempt to exploit the\nexistence of di\ufb00erent prices on di\ufb00erent exchanges, the consideration of which could open the way to sig-\nni\ufb01cantly higher returns on investment. Second, we ignored intra-day price \ufb02uctuations and considered\nan average daily price. Finally, and crucially, we run a theoretical test in which the available supply\nof Bitcoin is unlimited and none of our trades in\ufb02uence the market. Notwithstanding these simplifying\nassumptions, the methods we presented were systematically and consistently able to identify outperform-\ning currencies. Extending the current analysis by considering these and other elements of the market is\na direction for future work.\nA di\ufb00erent yet promising approach to the study cryptocurrencies consists in quantifying the impact\n11\n\nof public opinion, as measured through social media traces, on the market behaviour, in the same spirit\nin which this was done for the stock market [67]. While it was shown that social media traces can be\nalso e\ufb00ective predictors of Bitcoin [68\u201374] and other currencies [75] price \ufb02uctuations, our knowledge of\ntheir e\ufb00ects on the whole cryptocurrency market remain limited and is an interesting direction for future\nwork.\nReferences\n[1] A. ElBahrawy, L. Alessandretti, A. Kandler, R. Pastor-Satorras, and A. Baronchelli, \u201cEvolutionary\ndynamics of the cryptocurrency market,\u201d Royal Society Open Science, vol. 4, no. 11, 2017. [Online].\nAvailable: http://rsos.royalsocietypublishing.org/content/4/11/170623\n[2] G. Hileman and M. Rauchs, Global Cryptocurrency Benchmarking Study.\nCambridge Centre for\nAlternative Finance, 2017.\n[3] Binance.com. (2017) Binance. [Online]. Available: https://www.binance.com/\n[4] Dunamu. (2017) Upbit. [Online]. Available: https://upbit.com/\n[5] P. Inc. (2012) Kraken. [Online]. Available: https://www.kraken.com/\n[6] S. Foley, J. Karlsen, and T. J. Putni, \u201cSex, drugs, and bitcoin: How much illegal activity is \ufb01nanced\nthrough cryptocurrencies?\u201d SSRN, p. https://ssrn.com/abstract=3102645, 2018.\n[7] S. Nakamoto, \u201cBitcoin: A peer-to-peer electronic cash system,\u201d 2008.\n[8] R. Ali, J. Barrdear, R. Clews, and J. Southgate, \u201cThe economics of digital currencies,\u201d 2014.\n[9] The Dash Network. (2018) Dash. [Online]. Available: https://www.dash.org\n[10] Ethereum\nFoundation\n(Stiftung\nEthereum).",
    "chunk_index": 11,
    "start_char": 27784,
    "end_char": 31250,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "Available: https://www.binance.com/\n[4] Dunamu. (2017) Upbit. [Online]. Available: https://upbit.com/\n[5] P. Inc. (2012) Kraken. [Online]. Available: https://www.kraken.com/\n[6] S. Foley, J. Karlsen, and T. J. Putni, \u201cSex, drugs, and bitcoin: How much illegal activity is \ufb01nanced\nthrough cryptocurrencies?\u201d SSRN, p. https://ssrn.com/abstract=3102645, 2018.\n[7] S. Nakamoto, \u201cBitcoin: A peer-to-peer electronic cash system,\u201d 2008.\n[8] R. Ali, J. Barrdear, R. Clews, and J. Southgate, \u201cThe economics of digital currencies,\u201d 2014.\n[9] The Dash Network. (2018) Dash. [Online]. Available: https://www.dash.org\n[10] Ethereum\nFoundation\n(Stiftung\nEthereum).\n(2018)\nEthereum.\n[Online].\nAvailable:\nhttps:\n//www.ethereum.org/\n[11] Ripple. (2013) Ripple. [Online]. Available: https://ripple.com/\n[12] (2018) Iota. [Online]. Available: https://iota.org/\n[13] A. Baronchelli, \u201cThe emergence of consensus: a primer,\u201d Royal Society Open Science, vol. 5, no. 2,\n2018. [Online]. Available: http://rsos.royalsocietypublishing.org/content/5/2/172189\n[14] G. P. Dwyer, \u201cThe economics of bitcoin and similar private digital currencies,\u201d Journal of Financial\nStability, vol. 17, pp. 81\u201391, 2015.\n[15] R. B\u00a8ohme, N. Christin, B. Edelman, and T. Moore, \u201cBitcoin: Economics, technology, and gover-\nnance,\u201d Journal of Economic Perspectives, vol. 29, no. 2, pp. 213\u201338, 2015.\n[16] M. J. Casey and P. Vigna, \u201cBitcoin and the digital-currency revolution,\u201d The Wall Street Journal,\nvol. 23, 2015.\n[17] S. Trimborn and W. K. H\u00a8ardle, \u201cCrix an index for blockchain based currencies,\u201d 2016.\n[18] M. Iwamura, Y. Kitamura, and T. Matsumoto, \u201cIs bitcoin the only cryptocurrency in the town?\neconomics of cryptocurrency and friedrich a. hayek,\u201d 2014.\n12\n\n[19] M. A. Cusumano, \u201cThe bitcoin ecosystem,\u201d Communications of the ACM, vol. 57, no. 10, pp. 22\u201324,\n2014.\n[20] K. Wu, S. Wheatley, and D. Sornette, \u201cClassi\ufb01cation of crypto-coins and tokens from the dynamics\nof their power law capitalisation distributions,\u201d arXiv preprint arXiv:1803.03088, 2018.\n[21] A. Lamarche-Perrin, A. Orl\u00b4ean, and P. Jensen, \u201cCoexistence of several currencies in presence of\nincreasing returns to adoption,\u201d Physica A: Statistical Mechanics and its Applications, 2018.\n[22] P. M. Kra\ufb00t, N. Della Penna, and A. Pentland, \u201cAn experimental study of cryptocurrency market\ndynamics,\u201d arXiv preprint arXiv:1801.05831, 2018.\n[23] A. Rogojanu, L. Badea et al., \u201cThe issue of competing currencies. case study\u2013bitcoin,\u201d Theoretical\nand Applied Economics, vol. 21, no. 1, pp. 103\u2013114, 2014.\n[24] L. H. White, \u201cThe market for cryptocurrencies,\u201d Cato J., vol. 35, p. 383, 2015.\n[25] P. Ceruleo, \u201cBitcoin: a rival to \ufb01at money or a speculative \ufb01nancial asset?\u201d 2014.\n[26] M. N. Sayed and N. A. Abbas, \u201cImpact of crypto-currency on emerging market focus on gulf\ncountries,\u201d Life Science Journal, vol. 15, no. 1, 2018.\n[27] M. A. Javarone and C. S. Wright, \u201cFrom bitcoin to bitcoin cash: a network analysis,\u201d arXiv preprint\narXiv:1804.02350, 2018.\n[28] Y. Sovbetov, \u201cFactors in\ufb02uencing cryptocurrency prices: Evidence from bitcoin, ethereum, dash,\nlitcoin, and monero,\u201d 2018.\n[29] F. Parino, L. Gauvin, and M. G. Beiro, \u201cAnalysis of the bitcoin blockchain: Socio-economic factors\nbehind the adoption,\u201d arXiv preprint arXiv:1804.07657, 2018.\n[30] S. Begu\u02c7si\u00b4c, Z. Kostanj\u02c7car, H. E. Stanley, and B. Podobnik, \u201cScaling properties of extreme price\n\ufb02uctuations in bitcoin markets,\u201d arXiv preprint arXiv:1803.08405, 2018.\n[31] P. Ciaian, M. Rajcaniova, and d. Kancs, \u201cThe economics of bitcoin price formation,\u201d Applied Eco-\nnomics, vol.",
    "chunk_index": 12,
    "start_char": 30600,
    "end_char": 34158,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "preprint\narXiv:1804.02350, 2018.\n[28] Y. Sovbetov, \u201cFactors in\ufb02uencing cryptocurrency prices: Evidence from bitcoin, ethereum, dash,\nlitcoin, and monero,\u201d 2018.\n[29] F. Parino, L. Gauvin, and M. G. Beiro, \u201cAnalysis of the bitcoin blockchain: Socio-economic factors\nbehind the adoption,\u201d arXiv preprint arXiv:1804.07657, 2018.\n[30] S. Begu\u02c7si\u00b4c, Z. Kostanj\u02c7car, H. E. Stanley, and B. Podobnik, \u201cScaling properties of extreme price\n\ufb02uctuations in bitcoin markets,\u201d arXiv preprint arXiv:1803.08405, 2018.\n[31] P. Ciaian, M. Rajcaniova, and d. Kancs, \u201cThe economics of bitcoin price formation,\u201d Applied Eco-\nnomics, vol. 48, no. 19, pp. 1799\u20131815, 2016.\n[32] T. Guo and N. Antulov-Fantulin, \u201cPredicting short-term bitcoin price \ufb02uctuations from buy and\nsell orders,\u201d arXiv preprint arXiv:1802.04065, 2018.\n[33] G. Gajardo, W. D. Kristjanpoller, and M. Minutolo, \u201cDoes bitcoin exhibit the same asymmetric\nmultifractal cross-correlations with crude oil, gold and djia as the euro, great british pound and\nyen?\u201d Chaos, Solitons & Fractals, vol. 109, pp. 195\u2013205, 2018.\n[34] N. Gandal and H. Halaburda, \u201cCan we predict the winner in a market with network e\ufb00ects? com-\npetition in cryptocurrency market,\u201d Games, vol. 7, no. 3, p. 16, 2016.\n[35] H. Elendner, S. Trimborn, B. Ong, T. M. Lee et al., \u201cThe cross-section of crypto-currencies as \ufb01-\nnancial assets: An overview,\u201d Sonderforschungsbereich 649, Humboldt University, Berlin, Germany,\nTech. Rep., 2016.\n[36] D. Enke and S. Thawornwong, \u201cThe use of data mining and neural networks for forecasting stock\nmarket returns,\u201d Expert Systems with applications, vol. 29, no. 4, pp. 927\u2013940, 2005.\n13\n\n[37] W. Huang, Y. Nakamori, and S.-Y. Wang, \u201cForecasting stock market movement direction with\nsupport vector machine,\u201d Computers & Operations Research, vol. 32, no. 10, pp. 2513\u20132522, 2005.\n[38] P. Ou and H. Wang, \u201cPrediction of stock market index movement by ten data mining techniques,\u201d\nModern Applied Science, vol. 3, no. 12, p. 28, 2009.\n[39] M. Gavrilov, D. Anguelov, P. Indyk, and R. Motwani, \u201cMining the stock market (extended abstract):\nwhich measure is best?\u201d in Proceedings of the sixth ACM SIGKDD international conference on\nKnowledge discovery and data mining.\nACM, 2000, pp. 487\u2013496.\n[40] K. S. Kannan, P. S. Sekar, M. M. Sathik, and P. Arumugam, \u201cFinancial stock market forecast\nusing data mining techniques,\u201d in Proceedings of the International Multiconference of Engineers\nand computer scientists, vol. 1, 2010, p. 4.\n[41] A. F. Sheta, S. E. M. Ahmed, and H. Faris, \u201cA comparison between regression, arti\ufb01cial neural\nnetworks and support vector machines for predicting stock market index,\u201d Soft Computing, vol. 7,\np. 8, 2015.\n[42] P.-C. Chang, C.-H. Liu, C.-Y. Fan, J.-L. Lin, and C.-M. Lai, \u201cAn ensemble of neural networks for\nstock trading decision making,\u201d Emerging Intelligent Computing Technology and Applications. With\nAspects of Arti\ufb01cial Intelligence, pp. 1\u201310, 2009.\n[43] I. Madan, S. Saluja, and A. Zhao, \u201cAutomated bitcoin trading via machine learning algorithms,\u201d\n2015.\n[44] H. Jang and J. Lee, \u201cAn empirical study on modeling and prediction of bitcoin prices with bayesian\nneural networks based on blockchain information,\u201d IEEE Access, vol. 6, pp. 5427\u20135437, 2018.\n[45] S. McNally, \u201cPredicting the price of bitcoin using machine learning,\u201d Ph.D.",
    "chunk_index": 13,
    "start_char": 33542,
    "end_char": 36854,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "ensemble of neural networks for\nstock trading decision making,\u201d Emerging Intelligent Computing Technology and Applications. With\nAspects of Arti\ufb01cial Intelligence, pp. 1\u201310, 2009.\n[43] I. Madan, S. Saluja, and A. Zhao, \u201cAutomated bitcoin trading via machine learning algorithms,\u201d\n2015.\n[44] H. Jang and J. Lee, \u201cAn empirical study on modeling and prediction of bitcoin prices with bayesian\nneural networks based on blockchain information,\u201d IEEE Access, vol. 6, pp. 5427\u20135437, 2018.\n[45] S. McNally, \u201cPredicting the price of bitcoin using machine learning,\u201d Ph.D. dissertation, Dublin,\nNational College of Ireland, 2016.\n[46] K. HEGAZY and S. MUMFORD, \u201cComparitive automated bitcoin trading strategies.\u201d\n[47] A. G. Shilling, \u201cMarket timing: Better than a buy-and-hold strategy,\u201d Financial Analysts Journal,\nvol. 48, no. 2, pp. 46\u201350, 1992.\n[48] Z. Jiang and J. Liang, \u201cCryptocurrency portfolio management with deep reinforcement learning,\u201d\narXiv preprint arXiv:1612.01277, 2016.\n[49] \u201cTraderobot package,\u201d https://github.com/owocki/pytrader, 2017, accessed: 5 December 2017.\n[50] \u201cCryptobot package,\u201d https://github.com/AdeelMufti/CryptoBot, 2017, accessed:\n5 December\n2017.\n[51] \u201cCryptocurrencytrader package,\u201d https://github.com/llens/CryptoCurrencyTrader, 2017, accessed:\n5 December 2017.\n[52] \u201cBtctrading package,\u201d https://github.com/bukosabino/btctrading, 2017, accessed:\n5 December\n2017.\n[53] \u201cBitpredict package,\u201d https://github.com/cbyn/bitpredict, 2017, accessed: 5 December 2017.\n[54] \u201cBitcoin bubble burst,\u201d https://bitcoinbubbleburst.github.io/, 2017, accessed: 5 December 2017.\n14\n\n[55] J. H. Friedman, \u201cStochastic gradient boosting,\u201d Computational Statistics & Data Analysis, vol. 38,\nno. 4, pp. 367\u2013378, 2002.\n[56] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8,\npp. 1735\u20131780, 1997.\n[57] W. Brock, J. Lakonishok, and B. LeBaron, \u201cSimple technical trading rules and the stochastic prop-\nerties of stock returns,\u201d The Journal of \ufb01nance, vol. 47, no. 5, pp. 1731\u20131764, 1992.\n[58] T. Kilgallen, \u201cTesting the simple moving average across commodities, global stock indices, and\ncurrencies,\u201d The Journal of Wealth Management, vol. 15, no. 1, p. 82, 2012.\n[59] B. LeBaron, \u201cThe stability of moving average technical trading rules on the dow jones index,\u201d Deriv.\nUse Trad. Regul, vol. 5, pp. 324\u2013338, 2000.\n[60] C. A. Ellis and S. A. Parbery, \u201cIs smarter better? a comparison of adaptive, and simple moving\naverage trading strategies,\u201d Research in International Business and Finance, vol. 19, no. 3, pp.\n399\u2013411, 2005.\n[61] \u201ccoinmarketcap.com,\u201d http:https://coinmarketcap.com/, 2013, accessed: 30 May 2017.\n[62] G. T. Friedlob and F. J. Plewa Jr, Understanding return on investment.\nJohn Wiley & Sons, 1996.\n[63] T. Chen and C. Guestrin, \u201cXgboost: A scalable tree boosting system,\u201d in Proceedings of the 22nd\nacm sigkdd international conference on knowledge discovery and data mining.\nACM, 2016, pp.\n785\u2013794.\n[64] K. Inc. (2018) Kaggle. [Online]. Available: https://www.kaggle.com/\n[65] Z. C. Lipton, J. Berkowitz, and C. Elkan, \u201cA critical review of recurrent neural networks for sequence\nlearning,\u201d arXiv preprint arXiv:1506.00019, 2015.\n[66] B. Wiki. (2017) Comparison of exchanges. [Online]. Available:\nhttps://en.bitcoin.it/wiki/\nComparison of exchanges\n[67] H. S. Moat, C. Curme, A. Avakian, D. Y. Kenett, H. E. Stanley, and T. Preis, \u201cQuantifying\nwikipedia usage patterns before stock market moves,\u201d Scienti\ufb01c reports, vol. 3, p. 1801, 2013.\n[68] D. Kondor, I. Csabai, J. Sz\u00a8ule, M. P\u00b4osfai, and G. Vattay, \u201cInferring the interplay between network\nstructure and market e\ufb00ects in bitcoin,\u201d New Journal of Physics, vol.",
    "chunk_index": 14,
    "start_char": 36292,
    "end_char": 39969,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "C. Elkan, \u201cA critical review of recurrent neural networks for sequence\nlearning,\u201d arXiv preprint arXiv:1506.00019, 2015.\n[66] B. Wiki. (2017) Comparison of exchanges. [Online]. Available:\nhttps://en.bitcoin.it/wiki/\nComparison of exchanges\n[67] H. S. Moat, C. Curme, A. Avakian, D. Y. Kenett, H. E. Stanley, and T. Preis, \u201cQuantifying\nwikipedia usage patterns before stock market moves,\u201d Scienti\ufb01c reports, vol. 3, p. 1801, 2013.\n[68] D. Kondor, I. Csabai, J. Sz\u00a8ule, M. P\u00b4osfai, and G. Vattay, \u201cInferring the interplay between network\nstructure and market e\ufb00ects in bitcoin,\u201d New Journal of Physics, vol. 16, no. 12, p. 125003, 2014.\n[69] L. Kristoufek, \u201cWhat are the main drivers of the bitcoin price? evidence from wavelet coherence\nanalysis,\u201d PloS one, vol. 10, no. 4, p. e0123923, 2015.\n[70] D. Garcia and F. Schweitzer, \u201cSocial signals and algorithmic trading of bitcoin,\u201d Royal Society\nOpen Science, vol. 2, no. 9, 2015. [Online]. Available:\nhttp://rsos.royalsocietypublishing.org/\ncontent/2/9/150288\n[71] L. Kristoufek, \u201cBitcoin meets google trends and wikipedia: Quantifying the relationship between\nphenomena of the internet era,\u201d Scienti\ufb01c reports, vol. 3, p. 3415, 2013.\n[72] A. Urquhart, \u201cWhat causes the attention of bitcoin?\u201d Economics Letters, vol. 166, pp. 40 \u2013 44,\n2018. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S016517651830065X\n15\n\n[73] D. Garcia, C. J. Tessone, P. Mavrodiev, and N. Perony, \u201cThe digital traces of bubbles: feedback cy-\ncles between socio-economic signals in the bitcoin economy,\u201d Journal of the Royal Society Interface,\nvol. 11, no. 99, p. 20140623, 2014.\n[74] S. Wang and J.-P. Vergne, \u201cBuzz factor or innovation potential: What explains cryptocurrencies\nreturns?\u201d PloS one, vol. 12, no. 1, p. e0169556, 2017.\n[75] T. R. Li, A. S. Chamrajnagar, X. R. Fong, N. R. Rizik, and F. Fu, \u201cSentiment-based prediction\nof alternative cryptocurrency price \ufb02uctuations using gradient boosting tree model,\u201d arXiv preprint\narXiv:1805.00558, 2018.\n16\n\nAppendix\nA1\nParameter optimisation\nIn Fig. A1, we show the optimisation of the parameters w (A,C) and n (B,D) for the baseline strategy. In\nFig. A2, we show the optimisation of the parameters w (A,D), Wtraining (B,E), and n (C,F) for Method\n1. In Fig. A3, we show the optimisation of the parameters w (A,D), Wtraining (B,E), and n (C,F) for\nMethod 2. In Fig. A4, we show the median squared error obtained under di\ufb00erent training window\nchoices (A), number of epochs (B) and number of neurons (C), for Ethereum, Bitcoin and Ripple. In\nFig. A5, we show the optimisation of the parameter n (C,F) for Method 3.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ngeometric mean optimization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSharpe ratio optimization\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\n25\nw (days)\nA\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\nn, currencies\nB\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n20\nw (days)\nC\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\nn, currencies\nD\nFigure A1: Baseline strategy: parameters optimisation.",
    "chunk_index": 15,
    "start_char": 39364,
    "end_char": 42450,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "0.2\n0.4\n0.6\n0.8\n1.0\nSharpe ratio optimization\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\n25\nw (days)\nA\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\nn, currencies\nB\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n20\nw (days)\nC\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\nn, currencies\nD\nFigure A1: Baseline strategy: parameters optimisation. The sliding window w (A,C) and the\nnumber of currencies n (B,D) chosen over time under the geometric mean (A,B) and the Sharpe Ratio\noptimisation (C,D). Analyses are performed considering prices in BTC.\n17\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ngeometric mean optimization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSharpe ratio optimization\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n4\n6\n8\n10\nw (days)\nA\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\nWtraining(days)\nB\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n0\n10\n20\n30\n40\nn, currencies\nC\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n4\n6\n8\n10\nw (days)\nD\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\nWtraining(days)\nE\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n0\n10\n20\n30\n40\nn, currencies\nF\nFigure A2: Method 1: Parameters optimisation. The sliding window w (A,D), the training window\nWtraining (B,E) and the number of currencies n (C,F) chosen over time under the geometric mean\n(A,B,C) and the Sharpe Ratio optimisation (D,E,F). Analyses are performed considering prices in BTC.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ngeometric mean optimization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSharpe ratio optimization\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n3.0\n3.5\n4.0\n4.5\n5.0\nw (days)\nA\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\nWtraining(days)\nB\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n0\n20\n40\n60\nn, currencies\nC\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n3.0\n3.5\n4.0\n4.5\n5.0\nw (days)\nD\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n5\n10\n15\n20\nWtraining(days)\nE\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n0\n50\n100\n150\nn, currencies\nF\nFigure A3: Method 2: Parameters optimisation. The sliding window w (A,D), the training window\nWtraining (B,E) and the number of currencies n (C,F) chosen over time under the geometric mean\n(A,B,C) and the Sharpe Ratio optimisation (D,E,F). Analyses are performed considering prices in BTC.\n18\n\n10\n20\n30\n40\n50\n60\n70\nw (days)\n0.005\n0.010\n0.015\n0.020\nmse\nA\nBitcoin\nRipple\nEthereum\nAverage\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepochs\n0.004\n0.006\n0.008\n0.010\n0.012",
    "chunk_index": 16,
    "start_char": 42080,
    "end_char": 44583,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "F\nFigure A3: Method 2: Parameters optimisation. The sliding window w (A,D), the training window\nWtraining (B,E) and the number of currencies n (C,F) chosen over time under the geometric mean\n(A,B,C) and the Sharpe Ratio optimisation (D,E,F). Analyses are performed considering prices in BTC.\n18\n\n10\n20\n30\n40\n50\n60\n70\nw (days)\n0.005\n0.010\n0.015\n0.020\nmse\nA\nBitcoin\nRipple\nEthereum\nAverage\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepochs\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nmse\nB\nBitcoin\nRipple\nEthereum\nAverage\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nneurons\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nmse\nC\nBitcoin\nRipple\nEthereum\nAverage\nFigure A4: Method 3: Parameters optimisation. The median squared error of the ROI as a function\nof the window size (A), the number of epochs (B) and the number of neurons (C). Results are shown\nconsidering prices in Bitcoin.\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n1\n2\n3\n4\n5\nn, currencies\nA\ngeometric mean optimization\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\nn, currencies\nB\nSharpe ratio optimization\nFigure A5: Method 3: Parameters optimisation.\nThe number of currencies n chosen over time\nunder the geometric mean (A) and the Sharpe Ratio optimisation (B). Analyses are performed considering\nprices in BTC.\n19\n\nA2\nReturn under full knowledge of the market evolution.\nIn Fig. A6, we show the cumulative return obtained by investing every day in the top currency, supposing\none knows the prices of currencies on the following day.\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n9\n10\n27\n10\n45\n10\n63\n10\n81\n10\n99\n10\n117\ncumulative return\nBaseline\nMethod 1\nMethod 2\nMethod 3\nupper-bound\nFigure A6: Upper-bound for the cumulative return. The cumulative return obtained by investing\nevery day in the currency with highest return on the following day (black line). The cumulative return\nobtained with the baseline (blue line), Method 1 (orange line), Method 2 (green line), and Method 3\n(red line). Results are shown in Bitcoin.\nA3\nReturn obtained paying transaction fees.\nIn this section, we present the results obtained including transaction fees between 0.1% and 1% [66]. In\ngeneral, one can not trade a given currency with any given other. Hence, we consider that each day we\ntrade twice: We sell altcoins to buy Bitcoin, and we buy new altcoins using Bitcoin. The mean return\nobtained between Jan. 2016 and Apr. 2018 is larger than 1 for all methods, for fees up to 0.2% (see\nTable A1). In this period, Method 3 achieves positive returns for fees up to 1%. The returns obtained\nwith a 0.1% (see Fig. A7) and 0.2% (see Fig. A8) fee during arbitrary periods con\ufb01rm that, in general,\none obtains positive gains with our methods if fees are small enough.\nno fee\n0.1%\n0.2%\n0.3%\n0.5%\n1%\nBaseline\n1.005\n1.003\n1.001\n0.999\n0.995\n0.985\nMethod 1\n1.008\n1.006\n1.004\n1.002\n0.998\n0.988\nMethod 2\n1.005\n1.003\n1.001",
    "chunk_index": 17,
    "start_char": 44120,
    "end_char": 46958,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "up to 0.2% (see\nTable A1). In this period, Method 3 achieves positive returns for fees up to 1%. The returns obtained\nwith a 0.1% (see Fig. A7) and 0.2% (see Fig. A8) fee during arbitrary periods con\ufb01rm that, in general,\none obtains positive gains with our methods if fees are small enough.\nno fee\n0.1%\n0.2%\n0.3%\n0.5%\n1%\nBaseline\n1.005\n1.003\n1.001\n0.999\n0.995\n0.985\nMethod 1\n1.008\n1.006\n1.004\n1.002\n0.998\n0.988\nMethod 2\n1.005\n1.003\n1.001\n0.999\n0.995\n0.985\nMethod 3\n1.025\n1.023\n1.021\n1.019\n1.015\n1.005\nTable A1: Daily geometric mean return for di\ufb00erent transaction fees. Results are obtained\nconsidering the period between Jan. 2016 and Apr. 2018.\n20\n\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\nBaseline\nA\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 1\nB\nMethod 2\nC\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure A7: Daily geometric mean return obtained under transaction fees of 0.1%. The geo-\nmetric mean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for\nthe baseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).\n21\n\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\nBaseline\nA\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 1\nB\nMethod 2\nC\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure A8: Daily geometric mean return obtained under transaction fees of 0.2%. The geo-\nmetric mean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for\nthe baseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).\n22\n\nA4\nResults in USD\nIn this section, we show results obtained considering prices in USD. The price of Bitcoin in USD has\nconsiderably increased in the period considered. Hence, gains in USD (Fig. A9) are higher than those in\nBitcoin (Fig. 5). Note that, in Fig.",
    "chunk_index": 18,
    "start_char": 46521,
    "end_char": 49163,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).\n22\n\nA4\nResults in USD\nIn this section, we show results obtained considering prices in USD. The price of Bitcoin in USD has\nconsiderably increased in the period considered. Hence, gains in USD (Fig. A9) are higher than those in\nBitcoin (Fig. 5). Note that, in Fig. A9, we have made predictions and computed portfolios considering\nprices in Bitcoin. Then, gains have been converted to USD (without transaction fees). In Table A2, we\nshow instead the gains obtained running predictions considering directly all prices in USD. We \ufb01nd that,\nin most cases, better results are obtained from prices in BTC.\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n2\n10\n1\n10\n4\n10\n7\n10\n10\ncumulative return (USD)\nA\ngeometric mean optimization\nBaseline\nMethod 1\nMethod 2\nMethod 3\nMay \n 16\nNov \n 16\nMay \n 17\nNov \n 17\nMay \n 18\n10\n0\n10\n3\n10\n6\n10\n9\ncumulative return (USD)\nB\nSharpe ratio optimization\nBaseline\nMethod 1\nMethod 2\nMethod 3\nFigure A9: Cumulative returns in USD. The cumulative returns obtained under the Sharpe Ratio\noptimisation (A) and the geometric mean optimisation (B) for the baseline (blue line), Method 1 (orange\nline), Method 2 (green line) and Method 3 (red line). Analyses are performed considering prices in BTC.\nGeometric mean in USD (from BTC prices)\nGeometric mean in USD (from USD prices)\nBaseline\n1.0086\n1.0141\nMethod1\n1.0121\n1.0085\nMethod2\n1.0091\n1.0086\nMethod3\n1.0289\n1.0134\nTable A2: Geometric mean returns in USD. Results are obtained for the various methods by running\nthe algorithms considering prices in BTC (left column) and USD (right column).\nA5\nGeometric mean optimisation\nIn Fig. A10, we show the geometric mean return obtained by between two arbitrary points in time under\ngeometric mean return optimisation for the baseline (Fig. A10-A), Method 1 (Fig. A10-B), Method 2\n(Fig. A10-C), and Method 3 (Fig. A10-D).\n23\n\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\nBaseline\nA\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nend\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 1\nB\nMethod 2\nC\n03- 2016\n06- 2016\n09- 2016\n12- 2016\n03- 2017\n06- 2017\n09- 2017\n12- 2017\n03- 2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure A10: Geometric mean return obtained within di\ufb00erent periods of time. The geometric\nmean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for the\nbaseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1.",
    "chunk_index": 19,
    "start_char": 48708,
    "end_char": 51495,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "09- 2017\n12- 2017\n03- 2018\nstart\nMethod 3\nD\n-1\n-0.1\n-0.01\n0\n0.01\n0.1\n1\nG-1\nFigure A10: Geometric mean return obtained within di\ufb00erent periods of time. The geometric\nmean return computed between time \u201dstart\u201d and \u201dend\u201d using the Sharpe ratio optimisation for the\nbaseline (A), Method 1 (B), Method 2 (C) and Method 3 (D). Note that, for visualization purposes,\nthe \ufb01gure shows the translated geometric mean return G-1. Shades of red refers to negative returns and\nshades of blue to positive ones (see colour bar).\n24",
    "chunk_index": 20,
    "start_char": 51079,
    "end_char": 51593,
    "paper_title": "Anticipating cryptocurrency prices using machine l",
    "paper_category": "physics.soc-ph",
    "paper_filename": "Anticipating_cryptocurrency_prices_using_machine_l.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/physics.soc-ph/Anticipating_cryptocurrency_prices_using_machine_l.pdf"
  },
  {
    "text": "Evidence of market manipulation in the \ufb01nancial crisis\u2217\nVedant Misra, Marco Lagi, and Yaneer Bar-Yam\u2020\nNew England Complex Systems Institute\n238 Main Street Suite 319, Cambridge, Massachusetts 02142, US\n(Dated: October 29, 2018)\nAbstract\nWe provide direct evidence of market manipulation at the beginning of the \ufb01nancial crisis in\nNovember 2007. The type of market manipulation, a \u201cbear raid,\u201d would have been prevented by\na regulation that was repealed by the Securities and Exchange Commission in July 2007. The\nregulation, the uptick rule, was designed to prevent market manipulation and promote stability\nand was in force from 1938 as a key part of the government response to the 1929 market crash and\nits aftermath. On November 1, 2007, Citigroup experienced an unusual increase in trading volume\nand decrease in price. Our analysis of \ufb01nancial industry data shows that this decline coincided\nwith an anomalous increase in borrowed shares, the selling of which would be a large fraction of the\ntotal trading volume. The selling of borrowed shares cannot be explained by news events as there\nis no corresponding increase in selling by share owners. A similar number of shares were returned\non a single day six days later. The magnitude and coincidence of borrowing and returning of shares\nis evidence of a concerted e\ufb00ort to drive down Citigroup\u2019s stock price and achieve a pro\ufb01t, i.e., a\nbear raid. Interpretations and analyses of \ufb01nancial markets should consider the possibility that the\nintentional actions of individual actors or coordinated groups can impact market behavior. Markets\nare not su\ufb03ciently transparent to reveal or prevent even major market manipulation events. Our\nresults point to the need for regulations that prevent intentional actions that cause markets to\ndeviate from equilibrium value and contribute to market crashes. Enforcement actions, even if\nthey take place, cannot reverse severe damage to the economic system. The current \u201calternative\u201d\nuptick rule which is only in e\ufb00ect for stocks dropping by over 10% in a single day is insu\ufb03cient.\nPrevention may be achieved through a combination of improved transparency through availability\nof market data and the original uptick rule or other transaction process limitations.\n\u2217A report on preliminary results from this work was transmitted to the House Financial Services Committee\nand sent by Congressman Barney Frank and Congressman Ed Perlmutter to the SEC on May 25, 2010.\n\u2020 Corresponding author: yaneer@necsi.edu\n1\narXiv:1112.3095v3 [q-fin.GN] 3 Jan 2012\n\nI.\nINTRODUCTION TO BEAR RAIDS AND MARKET MANIPULATION\nOn July 6, 2007, the Securities and Exchange Commission (SEC) repealed the uptick rule,\na regulation that was speci\ufb01cally designed to prevent market manipulations that can trigger\nmarket crashes. While it is widely accepted that the causes of the crash that began later\nthat year were weaknesses in the mortgage market and \ufb01nancial sector, the close proximity\nof the repeal to the market crash suggests that market manipulation may have played a role.\nHere we present quantitative evidence of a major market manipulation, a \u201cbear raid,\u201d\nthat would not have been possible if the uptick rule were still in force.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3202,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "that was speci\ufb01cally designed to prevent market manipulations that can trigger\nmarket crashes. While it is widely accepted that the causes of the crash that began later\nthat year were weaknesses in the mortgage market and \ufb01nancial sector, the close proximity\nof the repeal to the market crash suggests that market manipulation may have played a role.\nHere we present quantitative evidence of a major market manipulation, a \u201cbear raid,\u201d\nthat would not have been possible if the uptick rule were still in force. The timing of the\nbear raid, in autumn 2007, suggests that it may have contributed to the \ufb01nancial crisis. Bear\nraids are an illegal market strategy in which investors manipulate stock prices by collectively\nselling borrowed shares. They pro\ufb01t by buying shares to cover their borrowed positions at a\nlower price. While bear raids are often blamed for market events, including \ufb01nancial crises\n[1, 2], this paper is the \ufb01rst to demonstrate the existence of a speci\ufb01c bear raid.\nThe sale of borrowed shares, called short selling, is a standard form of market trading.\nShort sellers sell borrowed shares, then buy them back later and return them to their owners.\nThis practice yields pro\ufb01ts when prices decline. In a bear raid, investors engage in short\nselling with the addition of market manipulation. Instead of pro\ufb01ting from a natural decline\nin the fundamental value of a company stock, the executors of a bear raid themselves cause\nthe price to decline. Large traders combine to sell shares in high volume, \u201cdriving\u201d the price\ndown [3, 4].\nA bear raid is pro\ufb01table if other investors are induced to sell their shares at the lower\nprice. This may happen for two reasons: margin calls and panic. Margin calls occur when\nbrokerages force investors to liquidate their positions. Investors who are con\ufb01dent in the\nrising price of a stock may buy shares on borrowed funds, called \u201cbuying on margin,\u201d using\nthe value of the shares themselves as collateral. When prices decline, so does the value of\nthe collateral and at some point brokerages issue \u201cmargin calls,\u201d requiring shares to be sold\neven though the owners would prefer not to. Panics occur when investors, fearing further\nlosses, sell their shares. The executors of a bear raid pro\ufb01t from the price decline by buying\nback the shares they borrowed\u2014\u201ccovering\u201d their short positions\u2014at the lower market price.\nIn the aftermath of the 1929 market crash, Congress created the Security and Exchange\nCommission (SEC). Recognizing the dangers of short selling, Congress speci\ufb01cally required\n2\n\nthe SEC to regulate short selling [5]. The regulation that was instituted in 1938, the uptick\nrule, states that borrowed shares may only be sold on an \u201cuptick\u201d\u2014at a price that is higher\nthan the immediately preceding price. The rule was designed to limit the intentional or\nunintentional impact of short selling in driving prices down, and speci\ufb01cally to prevent bear\nraids. The uptick rule was repealed in July, 2007 by the SEC on the basis of arguments that\nmarkets were transparent and no longer needed the protection of the uptick rule [6].",
    "chunk_index": 1,
    "start_char": 2693,
    "end_char": 5788,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "SEC to regulate short selling [5]. The regulation that was instituted in 1938, the uptick\nrule, states that borrowed shares may only be sold on an \u201cuptick\u201d\u2014at a price that is higher\nthan the immediately preceding price. The rule was designed to limit the intentional or\nunintentional impact of short selling in driving prices down, and speci\ufb01cally to prevent bear\nraids. The uptick rule was repealed in July, 2007 by the SEC on the basis of arguments that\nmarkets were transparent and no longer needed the protection of the uptick rule [6]. SEC\nclaims that the uptick rule had no signi\ufb01cant e\ufb00ect on market stability, even in absence\nof speci\ufb01c manipulation, have been refuted\n[7\u20139].\nOur results implying a bear raid in\nNovember 2007 contradict the assertion of market transparency.\nOur evidence points to a bear raid on the large \ufb01nancial services company Citigroup. On\nNovember 1, 2007, Citigroup\u2019s stock experienced an unusual increase in trading volume and\ndecrease in price. To analyze this event, we studied \ufb01nancial industry short trading data\n(see Appendix A), which reveal the total number of borrowed shares (short interest) at the\nend of each trading day. Using these data, we show that the increase in trading volume on\nNovember 1 coincides with an increase in borrowed shares. Six days later, a comparable\nnumber of short positions were closed during a single trading day. News events to which\nthese events might normally be attributed cannot account for the di\ufb00erence between trading\nin borrowed shares and trading by owners of shares. The magnitude and coincidence of short\nactivity is evidence of a concerted e\ufb00ort to drive down Citigroup\u2019s stock price and achieve\na pro\ufb01t, i.e., a bear raid.\nII.\nCITIGROUP ON NOVEMBER 1 AND 7, 2007\nOn November 1, 2007, Citigroup experienced large spikes in short selling and trading\nvolume. The number of borrowed shares\u2014short interest\u2014increased by approximately 130\nmillion shares to 3.8 times the 3-month moving average. The total trading volume jumped\nfrom 73 million shares on the previous day to 171 million shares, 3.7 times the 3-month\nmoving average. The ratio of the increase in short positions to volume was 0.77. This is the\nfraction of the total trading that day that may be attributed to short positions held until\nmarket closing. The total value of shares borrowed on November 1 was approximately $6.07\nbillion. Adjusted for the dividend issued on November 1, 2007, Citigroup stock closed on\nNovember 1 down $2.85 from the previous day, a drop of 6.9%.\n3\n\nThe number of positions closed on November 7, 202 million, was 53% larger than the\nnumber opened on November 1. The short interest before the increase on November 1 and\nafter November 7 are virtually identical, the larger decrease corresponding to an additional\nincrease in short interest between these dates. The mirror image one-day anomalies in short\ninterest change suggest that the two are linked.",
    "chunk_index": 2,
    "start_char": 5248,
    "end_char": 8171,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "positions closed on November 7, 202 million, was 53% larger than the\nnumber opened on November 1. The short interest before the increase on November 1 and\nafter November 7 are virtually identical, the larger decrease corresponding to an additional\nincrease in short interest between these dates. The mirror image one-day anomalies in short\ninterest change suggest that the two are linked. We can conservatively estimate the total\ngain from short selling by multiplying the number of short positions opened on November 1\nby the di\ufb00erence between the closing price on November 1 and closing price on November 7\n($4.82), which yields an estimated gain for the short sellers of $640 million.\nThe total decrease in short interest on November 7 exceeds the total trading volume\non that day, 121 million, by 82 million shares. This indicates that the reported decrease\nin borrowed shares is not fully accounted for by recorded trading on the markets.\nThe\ndi\ufb00erence may result from o\ufb00-market transfers, which may be advantageous to short sellers\nin not causing the price to increase. Alternatively, despite the usual coincidence of borrowing\nand selling, this may be due to shares that were borrowed and returned without being sold\nshort. Further investigation of transaction data is necessary to explain the di\ufb00erence in\nreturned shares and trading volume.\nFigure 1 shows daily stock price, volume, and short sale data for Citigroup over a two-\nyear period starting January 1, 2007. Short sale data includes short interest\u2014the number\nof shares borrowed at the end of each day\u2014and the daily change in short interest. During\nmuch of 2007-2009, the daily change in short interest did not exceed a small fraction of the\ntotal trading volume. The largest single-day increase in short interest occurred on November\n1 and is marked with arrows in Figure 1. Figure 2 shows an enlarged view of the period\naround that date.\nIn Appendix B we analyze quantitatively the probability of the events on November 1 and\nNovember 7. Often probabilities are estimated using normal (Gaussian) distributions that\nunderestimate the probability of extreme events (\u201cblack swans\u201d) that are better represented\nby long-tailed distributions [11, 12]. We directly \ufb01tted the long tails of the distributions and\nestimated the probability of the events based upon these tails to be p = 2\u00b710\u22125 and 8\u00b710\u22129,\nrespectively. Given 250 trading days in a typical year, it would take on average 200 years\nand 500 thousand years, respectively, to witness such events. Moreover, the probability of\nthese two events occurring 6 days apart is p = 1 \u00b7 10\u221212, corresponding to 4 billion years,\ncomparable to the age of the Earth. Figure 3 shows that these events are outside the general\n4\n\n10\n20\n30\n40\n50\nPrice ($)\n \nJan 2007\nApr 2007\nJul 2007\nOct 2007\nJan 2008\nApr 2008\nJul 2008\nOct 2008\nJan 2009\n20\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30\nDemand Quantity\n(tens of millions of shares)\n \nTotal Short Interest\nVolume\nChange in Short Interest\nFIG.",
    "chunk_index": 3,
    "start_char": 7783,
    "end_char": 10760,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "p = 1 \u00b7 10\u221212, corresponding to 4 billion years,\ncomparable to the age of the Earth. Figure 3 shows that these events are outside the general\n4\n\n10\n20\n30\n40\n50\nPrice ($)\n \nJan 2007\nApr 2007\nJul 2007\nOct 2007\nJan 2008\nApr 2008\nJul 2008\nOct 2008\nJan 2009\n20\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30\nDemand Quantity\n(tens of millions of shares)\n \nTotal Short Interest\nVolume\nChange in Short Interest\nFIG. 1: Market activity for Citigroup over a two-year period starting January 1, 2007. Top panel\nshows vertical bars for the daily high and low stock price. Lower panel shows total short interest\n(yellow), trading volume (gray), and daily change in short interest (red). Arrows indicate November\n1, 2007 [10].\nbehavior of the market. We emphasize that our estimates of the probabilities of these events\nre\ufb02ects the higher probabilities of extreme events in long-tailed distributions.\nChanges in investor behavior are often explained in terms of speci\ufb01c news items, without\nwhich it is expected that prices have no reason to change signi\ufb01cantly [13, 14]. The press\nattributed the drop of Citigroup\u2019s stock price on November 1 to an analyst\u2019s report that\nmorning [15, 16]. This report, by an analyst of the Canadian Imperial Bank of Commerce\n(CIBC), downgraded Citigroup to \u201csector underperform\u201d [17]. Any such news-based expla-\nnations of investor behavior on November 1 (similarly for November 7) would not account for\nthe di\ufb00erence in behavior between short sellers and other investors. Under the assumptions\nof standard [14] capital asset pricing models, all investors act to maximize expected future\nwealth [18], and should therefore respond similarly to news. Furthermore, it has been shown\nempirically that the ratio of short sales to total volume remains nearly constant, even around\n5\n\n30\n35\n40\n45\nPrice ($)\n \nSep 2007\nOct 2007\nNov 2007\nDec 2007\nJan 2008\n20\n15\n10\n5\n0\n5\n10\n15\n20\n25\n30\nDemand Quantity\n(tens of millions of shares)\n \nChange in Short Interest\nTotal Short Interest\nVolume\nFIG. 2: Market activity for Citigroup over a \ufb01ve-month period starting on August 15, 2007. Top\npanel shows bars for daily high and low stock price (adjusted for dividends). Lower panel shows\ndaily change in short interest (red bars), total short interest (yellow lines), and trading volume\n(gray bars). Arrows indicate November 1, 2007 [10].\nnews events [19]. In the literature, analysis of the residual small di\ufb00erences in the behavior\nof short and long investors has been interpreted to indicate that short sellers have an infor-\nmational advantage or that short sellers are able to anticipate lower future returns [19\u201323],\nrather than cause them. Still, these studies do not show that large di\ufb00erences in trading\ngenerally occur between short and long sellers. Thus, the existence of such a di\ufb00erence is\nindicative of speci\ufb01c trader action.\nOur evidence points to a bear raid during a period of \ufb01nancial stress [24, 25] to which\nthe Federal Reserve Bank responded in August 2007 by",
    "chunk_index": 4,
    "start_char": 10373,
    "end_char": 13332,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "short sellers have an infor-\nmational advantage or that short sellers are able to anticipate lower future returns [19\u201323],\nrather than cause them. Still, these studies do not show that large di\ufb00erences in trading\ngenerally occur between short and long sellers. Thus, the existence of such a di\ufb00erence is\nindicative of speci\ufb01c trader action.\nOur evidence points to a bear raid during a period of \ufb01nancial stress [24, 25] to which\nthe Federal Reserve Bank responded in August 2007 by announcing that they would be\n\u201cproviding liquidity to facilitate the orderly function of markets\u201d because \u201cinstitutions may\nexperience unusual funding needs because of dislocations in money and credit markets\u201d [26].\nShortly afterwards, the Dow Jones Industrial Average achieved its historical peak\u201414,167\npoints on October 9\u2014three weeks prior to November 1, the date our evidence suggests a bear\n6\n\nFIG. 3: Scatter plot of the daily volume of trading divided by the three month prior average (volume\nratio), and the increase in number of borrowed shares divided by the volume (short interest change\nratio), for Citigroup over a two-year period starting January 1, 2007. Arrows indicate Citigroup\non 1 November 2007 and 7 November 2007. These two points are well outside of the behavior of\ndaily events even during the period of the \ufb01nancial crisis in late 2007 and throughout 2008. The\ntwo measures are described in Appendix A.\nraid occurred. Bear raids may have long-term price impact if decision makers infer investor\ncon\ufb01dence from price movements and act on that basis [27, 28]. Citigroup CEO Charles\nPrince\u2019s resignation on November 4 after an emergency board meeting [29] may re\ufb02ect such\nan e\ufb00ect. The months after November 1 saw the beginning of the stock market turmoil of\n2008-2009 as well as many signi\ufb01cant events of the \ufb01nancial crisis, such as the purchase of\nBear Stearns by JP Morgan Chase in March 2008 and the bankruptcy of Lehman Brothers\nin September 2008.\nIII.\nCONCLUSIONS AND POLICY IMPLICATIONS\nThe 2007\u20132011 \ufb01nancial crisis resulted in widespread economic damage and introduced\nquestions about both our understanding of economic markets and about the practical need\nfor regulations that ensure market stability.\nThe Financial Crisis Inquiry Commission\n7\n\n(FCIC) reported that over 26 million Americans were unemployed or underemployed in\nearly 2011, and that nearly $11 trillion in household wealth evaporated.\nMoreover, the\nFCIC concluded that the crisis was avoidable and was caused in part by \u201cwidespread fail-\nures in \ufb01nancial regulation and supervision [that] proved devastating to the stability of the\nnation\u2019s \ufb01nancial markets\u201d [30]. Regulatory changes that preceded the \ufb01nancial crisis in-\nclude the June 2007 repeal of the uptick rule, which was implemented in 1938 to increase\nmarket stability and inhibit manipulation [5\u20138, 31].\nWithin the resulting deregulated environment, it is still widely believed that the crisis was\ncaused by mortgage-related \ufb01nancial instruments and credit conditions, and that individual\ntraders did not play a role [32\u201335]. Our analysis demonstrates that manipulation may have\nplayed a key role. Methods for detecting manipulation and its e\ufb00ects are necessary to both\ninform and enforce policy.",
    "chunk_index": 5,
    "start_char": 12851,
    "end_char": 16088,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "changes that preceded the \ufb01nancial crisis in-\nclude the June 2007 repeal of the uptick rule, which was implemented in 1938 to increase\nmarket stability and inhibit manipulation [5\u20138, 31].\nWithin the resulting deregulated environment, it is still widely believed that the crisis was\ncaused by mortgage-related \ufb01nancial instruments and credit conditions, and that individual\ntraders did not play a role [32\u201335]. Our analysis demonstrates that manipulation may have\nplayed a key role. Methods for detecting manipulation and its e\ufb00ects are necessary to both\ninform and enforce policy.\nWhen the SEC repealed the uptick rule on July 6, 2007, one of its main claims was that\nthe market was transparent, and that such regulations were not needed to prevent market\nmanipulation [6]. Our results suggest that, not long after the uptick rule was repealed,\na bear raid may have occurred and remained undetected and unprosecuted. Our analysis\nreinforces claims that lax regulation was an integral part of the \ufb01nancial crisis [30].\nIn response to requests for reinstatement of the uptick rule after the \ufb01nancial crash,\nthe SEC underwent extended deliberations and \ufb01nally implemented an alternative uptick\nrule, which allows a stock to fall by 10% in a single day before limitations on short selling\napply [36]. This weaker rule would not have a\ufb00ected trading of Citigroup on November 1,\n2007, as its minimum price was just 9% lower than the close on October 31. Subsequent\nday declines until November 7 were also smaller than 10%.\nThe existence of a major market manipulation should motivate changes in market models,\nanalysis, regulation and enforcement. In particular we conclude that:\n\u2022 Large traders may have a signi\ufb01cant in\ufb02uence on the market. Scienti\ufb01c analysis and\nmodels should recognize the role of large traders and consider both past events and\npotential future events they may cause. For example, market time series analysis that\ndoes not speci\ufb01cally consider the e\ufb00ect of manipulation may be unable to discover it,\nbecause manipulation events may not manifest in averages and distributions that are\nusually considered.\n8\n\n\u2022 Improved access to data can enable the detection of market manipulation. This would\nfoster transparency in the markets, which has been lauded but not realized. Regulatory\nagencies should mandate the increased availability of relevant data for the detection\nof manipulation. If these data cannot be made available in real-time or for public use,\nthey may be provided with time delays or only for scienti\ufb01c use. Data of importance\ninclude not only the opening of short positions but also their closing, as aggregate\nshort sale activity cannot be determined when only opening trade data are available.\nThese data should be made available at the transaction level.\n\u2022 Current legislation, which focuses on retroactive penalties, is ine\ufb00ective due to the\ndiscrepancy between the timescale of enforcement response and that of market manip-\nulation. Severe failures in the \ufb01nancial system may include cascading global market\ncrises and numerous takeovers and bankruptcies, making the disentanglement of indi-\nvidual events di\ufb03cult if not impossible.",
    "chunk_index": 6,
    "start_char": 15508,
    "end_char": 18673,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "short sale activity cannot be determined when only opening trade data are available.\nThese data should be made available at the transaction level.\n\u2022 Current legislation, which focuses on retroactive penalties, is ine\ufb00ective due to the\ndiscrepancy between the timescale of enforcement response and that of market manip-\nulation. Severe failures in the \ufb01nancial system may include cascading global market\ncrises and numerous takeovers and bankruptcies, making the disentanglement of indi-\nvidual events di\ufb03cult if not impossible. Regulatory agencies should adopt preventive\nmeasures such as the uptick rule, which would be more e\ufb00ective than punitive ones.\nThe uptick rule was designed to minimally restrict trader\u2019s actions while simultane-\nously providing underlying stability for the \ufb01nancial system and inhibiting particular\nforms of manipulation, including bear raids.\n\u2022 The limitations of our data prevent de\ufb01nitive conclusions about individual events or\ntheir attribution to individual investors. Enforcement agencies should perform inves-\ntigations into speci\ufb01c candidate events, including the candidate event we identi\ufb01ed on\nNovember 1, 2007.\n\u2022 Until e\ufb00ective regulations and enforcement are in place, market price changes may not\nre\ufb02ect economic news. They may re\ufb02ect market manipulation.\nThe complexity of \ufb01nancial markets and their rapid dynamics suggest that data analysis\nand market models are increasingly necessary for guiding decisions about setting market\nregulations and their enforcement [37\u201339]. Independent of the role it may play in \ufb01nancial\ncrises, understanding market manipulation may be important for characterizing market dy-\nnamics. Recent decades have seen signi\ufb01cant advances in \ufb01nancial market theory, including\nthe mean-variance portfolio theory [40], the capital asset pricing model [18], arbitrage pricing\ntheory [41], and the theory of interest rates [42]. However, the \ufb01nancial crisis and anoma-\n9\n\nlous events such as \u201c\ufb02ash crashes\u201d [43] demonstrate limitations in existing approaches. More\nrecent e\ufb00orts seek to explain market phenomena via methods such as agent-based model-\ning [44\u201349] and analysis of the long-tailed distributions of price \ufb02uctuations [11, 50\u201353].\nWhile these methods have been successful in describing some aspects of market behavior,\nthey generally do not consider the impact of individual traders who have the ability to sig-\nni\ufb01cantly impact the market [54\u201360]. Current approaches, whether analytical or statistical,\nmay not reveal isolated\u2014or even frequent\u2014instances of trader in\ufb02uence.\nAmong the possible forms of individual trader in\ufb02uence, intentional actions\u2014including\nmanipulation\u2014are of particular relevance, as they undermine the role of markets in setting\nprices so as to re\ufb02ect economic value. Market manipulation is illegal under Section 10 of\nthe Securities Exchange Commission Act of 1934 [5]. Some forms of manipulation are well\ndocumented, including indirect price manipulation through the generation of false news [61].\nDirect price manipulation through market transactions is also commonly thought to occur [1,\n2, 54], but methods for its detection that are based on statistical analysis [62, 63] are limited\nby their inability to independently account for news events and other anomalies. No direct\nevidence of recent price manipulation has been presented based upon these methods.\nThe timing of the event we identi\ufb01ed raises questions about the potential role it may have\nplayed in the \ufb01nancial crisis.",
    "chunk_index": 7,
    "start_char": 18146,
    "end_char": 21628,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "manipulation through the generation of false news [61].\nDirect price manipulation through market transactions is also commonly thought to occur [1,\n2, 54], but methods for its detection that are based on statistical analysis [62, 63] are limited\nby their inability to independently account for news events and other anomalies. No direct\nevidence of recent price manipulation has been presented based upon these methods.\nThe timing of the event we identi\ufb01ed raises questions about the potential role it may have\nplayed in the \ufb01nancial crisis. Understanding the wider impact of such an event requires that\nwe consider the vulnerability of the overall market.\nWhereas a highly stable system is not vulnerable to any but the largest impacts, a vul-\nnerable system can be destabilized by much smaller shocks [64, 65]. This is a general aspect\nof the behavior of complex interdependent systems, not just of \ufb01nancial markets. Speci\ufb01c\nevents can have large e\ufb00ects if the underlying physical, biological or social system is vul-\nnerable. For example, while mass extinctions have been shown to coincide with meteor\nstrikes [66], underlying vulnerabilities are thought to contribute to the severity of extinction\nevents [67]. Similarly, market manipulation during a period of instability and high intercon-\nnectedness, such as before the \ufb01nancial crisis [24, 25, 68], may exacerbate or even trigger a\ncollapse. The \ufb01nancial system can be expected to exhibit this general property of complex\nsystems, in which the coincidence of underlying vulnerability and extreme events can trigger\ncrises.\nWe thank Yves Smith and Matt Levine for helpful comments. This work was supported\nby the New England Complex Systems Institute.\n10\n\nAppendix A: Methodology: Data and Event Detection\nIt is generally di\ufb03cult to characterize the investments of individual traders, especially\nfor short positions. Unlike those who own large stakes in companies, those with large short\npositions are not required to report their holdings [69].\nShort interest data is publicly\navailable by ticker symbol at two-week intervals for a rolling 12-month period [70]. This\ntime resolution is too low to detect the bear raid candidate we will describe, and does not\ninclude historical data for the period of the \ufb01nancial crisis. The recent availability of o\ufb00-\nmarket transaction systems that enable large volume transactions, such as crossing networks\n[71, 72], makes it di\ufb03cult, if not impossible, to trace intentional large short sale transactions\nusing market data. A short sale transaction between cohorts on a crossing network may\nallow one trader to execute a short sale while the other trader accumulates a long position.\nThis long position can then be sold on the open market without leaving a signature of its\nshort sale origins.\nOur study is based on industry data on daily securities lending. While this data does\nnot identify the individuals borrowing the shares, the time resolution proved su\ufb03cient to\nprovide evidence of a bear raid.\nWe obtained price and volume data from Thomson Reuters Datastream. Short interest\ndata was obtained from Data Explorers and included a daily record of the value and quantity\nof loaned securities as reported by brokerages.",
    "chunk_index": 8,
    "start_char": 21087,
    "end_char": 24307,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "long position can then be sold on the open market without leaving a signature of its\nshort sale origins.\nOur study is based on industry data on daily securities lending. While this data does\nnot identify the individuals borrowing the shares, the time resolution proved su\ufb03cient to\nprovide evidence of a bear raid.\nWe obtained price and volume data from Thomson Reuters Datastream. Short interest\ndata was obtained from Data Explorers and included a daily record of the value and quantity\nof loaned securities as reported by brokerages.\nThese included separate time series for\nthe total number of borrowed securities (total demand quantity) and for daily incremental\nchanges in the number of borrowed shares. Daily incremental changes were approximately\ngiven by day-to-day di\ufb00erences in total demand quantity, with small corrections arising from\nthe addition and removal of reporting organizations from the data set. The reconstruction\nof short selling data from security lending data is an inexact process, because borrowed\nsecurities may be used for purposes other than short selling, including tax arbitrage, dividend\narbitrage, and merger arbitrage. Furthermore, reported data may be incomplete, because\nnot all lenders supply data to industry data providers. Nevertheless, because short selling\nis the predominant reason for securities lending, securities lending is a reasonable proxy\nfor short selling [73, 74]. We also were able to eliminate the possibility of the most likely\nalternative explanation to a bear raid, dividend arbitrage, as described in Appendix C.\nThe signature of a successful bear raid is an anomalous spike in the number of shares\n11\n\nof a company\u2019s stock that are sold short, followed by a price decline, then a corresponding\nlarge spike in the number of positions that are covered\u2014a decrease in the number of short\npositions. A su\ufb03ciently large increase in short selling would also increase the total volume\nof trades, so we monitored also the total daily trading volume.\nWe searched data for several prominent companies to identify candidate events, and\ncalculated two ratios, R and Q, for each trading day. R is the ratio of the change in short\ninterest to daily volume,\nR(t) = \u2206S(t)\nV (t) ,\n(1)\nwhere \u2206S(t) = S(t)\u2212S(t\u22121) is the change in short interest, V is trading volume, and t is the\ndate. A large absolute value of R indicates that a high proportion of trading is accounted\nfor by securities lending activity\u2014that the volume of borrowed shares was a substantial\nfraction of the total volume, and that short sales might have a\ufb00ected the stock price. A\nhigh positive value indicates that shares were borrowed, and a high negative value indicates\nshort covering. Note that if a large number of short positions were opened and closed on\nthe same day (i.e. an intraday bear raid), it would not be revealed by daily short interest\ndata. We cannot exclude the possibility of intraday bear raids occurring during this period.\nQ is the ratio of the trading volume to the three month moving average,\nQ(t) = V (t)\nV (t),\n(2)\nwhere V is the prior 3-month (63 trading day) moving average of volume.",
    "chunk_index": 9,
    "start_char": 23772,
    "end_char": 26895,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "positive value indicates that shares were borrowed, and a high negative value indicates\nshort covering. Note that if a large number of short positions were opened and closed on\nthe same day (i.e. an intraday bear raid), it would not be revealed by daily short interest\ndata. We cannot exclude the possibility of intraday bear raids occurring during this period.\nQ is the ratio of the trading volume to the three month moving average,\nQ(t) = V (t)\nV (t),\n(2)\nwhere V is the prior 3-month (63 trading day) moving average of volume. A value of Q\nsubstantially greater than one indicates an anomalously high trading volume. The event we\nanalyzed was identi\ufb01ed by a high absolute value of R and high value of Q, indicating that\nthe increase in borrowed shares was large in comparison to trading activity, and that total\ntrading activity increased dramatically.\nAppendix B: R and Q distributions\nIn this appendix we present our analysis of the distributions of R (the ratio of the change\nin short interest to daily volume, see Eq. 1) and Q (the ratio of the trading volume to the\nthree month moving average, see Eq. 2) for Citigroup, from January 2007 through December\n2008. The analysis allows us to obtain a probabilistic estimate of the inherent likelihood of\n12\n\nR and Q values for each day, and in particular for the events on November 1 and 7, 2007.\nThe positive and negative tail cumulative distributions for Citigroup for R are plotted in\nFig. 4. The two sides of the distribution behave di\ufb00erently: while the positive tail follows\na power law distribution (top panel), the negative tail is well described by a Laplacian\ndistribution (bottom panel). The distribution for Q, shown in Fig. 5, has a power law\ntail. November 1 and 7, 2007 are omitted in the plots, but this does not a\ufb00ect the \ufb01tted\ndistributions. From the \ufb01tted distributions we extracted the expected probabilities of the\ntwo events.\nAppendix C: Tests and Technical Notes\nWe have tested a number of alternative explanations of the data:\n\u2022 Is it possible that the borrowed shares were used to receive a dividend payment, i.e.\ndividend arbitrage?\nSometimes borrowing shares provides bene\ufb01ts of dividends to the borrower rather than\nto the owner. In such cases the borrower may not necessarily sell the shares short,\nwhich precludes a bear raid.\nThe date on which shares were borrowed, November 1, was an \u201cex-dividend\u201d date, i.e.\na date on which ownership determines dividend payments. In order for borrowers to\nreceive the bene\ufb01t of dividends they are required to hold the shares at the prior day\u2019s\nclosing. Thus, there was no dividend paid to shares borrowed on November 1.\n\u2022 Is it possible that the reported dates for borrowed shares is delayed so that the actual\ndate of borrowing is a di\ufb00erent date than what is reported (for example, could it be\nreported on the date of settlement three days after a market transaction)?\nWe veri\ufb01ed the",
    "chunk_index": 10,
    "start_char": 26366,
    "end_char": 29271,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "for borrowers to\nreceive the bene\ufb01t of dividends they are required to hold the shares at the prior day\u2019s\nclosing. Thus, there was no dividend paid to shares borrowed on November 1.\n\u2022 Is it possible that the reported dates for borrowed shares is delayed so that the actual\ndate of borrowing is a di\ufb00erent date than what is reported (for example, could it be\nreported on the date of settlement three days after a market transaction)?\nWe veri\ufb01ed the agreement of reported borrowing and short selling date by looking at\nthe period of the short sale ban starting in September 2008. The dates of the start\nand stop of borrowing coincide with the dates that they should for the ban, which\nshows that there is no delay in reporting.\n\u2022 Does commercial market transaction data corroborate the short selling?\n13\n\n0.001\n2\n4\n6\n8\n0.01\n2\n4\n6\n8\n0.1\n2\nCumulative Distribution\n2\n3\n4\n5\n6\n7 8 90.1\n2\n3\n4\n5\n6\n R\n Citigroup R (positive tail)\n Power Law\n0.001\n0.01\n0.1\n1\nCumulative Distribution\n5 6 7 0.01\n2\n3\n4\n5 6 7 0.1\n2\n3\n4\n5 6 7\n- R\n Citigroup R (negative tail)\n Laplacian\nFIG. 4: Citigroup R distribution - Cumulative distribution functions (CDF) of the short interest\nchange ratio for Citigroup, for 2007 and 2008. Top panel: Positive tail of the distribution, blue\nline is the best \ufb01t power law (CDF(R) \u223cR\u03b1, with \u03b1 = \u22121.35). Bottom panel: Negative tail of\nthe distribution, blue line is the best \ufb01t Laplacian distribution (CDF(R) \u223c1 + sign(R \u2212\u03b2)(1 \u2212\nexp(\u2212|R \u2212\u03b2|/\u03b3)), with \u03b2 = 0.11 and \u03b3 = 0.048).\n14\n\n0.001\n0.01\n0.1\n1\nCumulative Distribution\n2\n3\n4\n5\n6 7 8 9 1\n2\n3\n4\n5\n6 7 8\n Q\n Citigroup Q\n Power Law\nFIG. 5: Citigroup Q distribution - Cumulative distribution function (CDF) of the volume\nratio for Citigroup for 2007 and 2008. Blue line is the best \ufb01t power law (CDF(Q) \u223cQ\u03b1, with\n\u03b1 = \u22123.34).\nWe have studied commercially available NYSE short selling data [75] from these dates,\nand found it to be unreliable because the transactions reported are inconsistent with\nreported trade and quote data [76] at the transaction level. Despite dialog with the\nNYSE sta\ufb00we have not received an explanation of the inconsistency. For the present\nanalysis, the inconsistency inhibits our e\ufb00orts to use this data to cross-validate the\nresults in this report. More generally, it raises questions about the reliability of market\nprovided short sale data.\n\u2022 Is it possible that the analyst report downgrading Citigroup that morning was released\nin collusion with the bear raid?\nWe have no speci\ufb01c evidence, but such collusion would be consistent with strategies\nused by those who manipulate stocks [1, 2, 54, 61].\n\u2022 Is it possible that those who engaged in the bear raid also used trading in options to\nincrease their pro\ufb01ts by buying put or selling call options?\n15",
    "chunk_index": 11,
    "start_char": 28825,
    "end_char": 31558,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "market\nprovided short sale data.\n\u2022 Is it possible that the analyst report downgrading Citigroup that morning was released\nin collusion with the bear raid?\nWe have no speci\ufb01c evidence, but such collusion would be consistent with strategies\nused by those who manipulate stocks [1, 2, 54, 61].\n\u2022 Is it possible that those who engaged in the bear raid also used trading in options to\nincrease their pro\ufb01ts by buying put or selling call options?\n15\n\nOur estimate of the pro\ufb01ts made on the bear raid are conservative.\n\u2022 Is it possible that the large block trades on November 1 and 7 represented trading\nbased upon information that was not yet available to the public on November 1?\nOur evidence suggests that a single individual or group of individuals traded a large\nvolume of borrowed shares on November 1 and November 7. If this represented po-\ntentially illegal insider trading, the traders would have avoided attracting attention.\nNeither the large trading volume nor the abrupt price drop on November 1 at the\nopening of the market appear to be consistent with a low-pro\ufb01le trading approach.\nThe rapid price drop is also inconsistent with the expected behavior of insider traders,\nwhich is to maximize pro\ufb01ts by selling gradually to avoid a\ufb00ecting prices until the neg-\native news becomes public. Both the large volume of trading and the rapid drop are\nconsistent with trading intended to a\ufb00ect prices, i.e. a bear raid. While the intentions\nof traders can only be determined from a more detailed inquiry once those traders are\nidenti\ufb01ed, the available information strongly supports a bear raid over the possibility\nof insider trading per se. It is possible that traders with insider information chose to\nhelp matters along by performing a bear raid at the same time as they were trading\non insider information.\nAddendum: Additional Tests and Technical Notes\nFollowing the release of this paper, we were contacted by the NYSE with additional\ninformation about the NYSE short selling transaction data [75] described in Appendix C.\nThe new information enabled us to reconcile the short sale and trade data [76] by aggregating\nand shifting the times of multiple transfers to correspond with market transactions. There\nare residual issues with a small minority of transactions that are being resolved, but these\nissues appear to be irrelevant to conclusions about the volume of trading.\nThe additional information enables us to identify with some con\ufb01dence the reported short\nsale volume on the NYSE on November 1 and other dates. The short sale volume is not\nunusual as a proportion of total volume, constituting about one quarter of the total volume\non this market. NYSE transactions constituted 30% of the total market volume on November\n1, 2007. This limits the volume of reported short selling on the markets, and diminishes the\n16\n\nlikelihood that the reported increase in borrowed shares was directly re\ufb02ected in reported\nshort sales.\nAbsent an alternative interpretation, if shares were sold in a way that concealed their\norigin as borrowed shares the data sets would be consistent.",
    "chunk_index": 12,
    "start_char": 31115,
    "end_char": 34201,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "sale volume is not\nunusual as a proportion of total volume, constituting about one quarter of the total volume\non this market. NYSE transactions constituted 30% of the total market volume on November\n1, 2007. This limits the volume of reported short selling on the markets, and diminishes the\n16\n\nlikelihood that the reported increase in borrowed shares was directly re\ufb02ected in reported\nshort sales.\nAbsent an alternative interpretation, if shares were sold in a way that concealed their\norigin as borrowed shares the data sets would be consistent. One method to achieve this,\nusing \u201cshort to buy\u201d transactions, was reported in Senate investigations of the Pequot\nCapital hedge fund in 2009 [77]. In this approach a single trader moves shares from one\naccount to another, creating a short position in one and a long position in the other. Since\nthere is no change in bene\ufb01cial ownership, such transactions may be reported in a way that\nis not consistent with standard reporting requirements, resulting in share borrowing without\na market record. Long positions created this way may be sold on any market without being\nidenti\ufb01ed as short sales, even though in doing so a net short position is created.\nThis method appears to have been developed to hide short selling at a time when the\nuptick rule was in e\ufb00ect.\nShort to buy transactions require a close relationship with a\nbroker dealer. The necessary access to market trading systems, called \u201csponsored\u201d or \u201cdirect\nmarket\u201d access, needed to perform the short to buy transaction is not available to most\ntraders but constitutes a signi\ufb01cant fraction of reported trading [78, 79]. Only recently,\nbeginning in 2011, were brokers required to apply standard regulations to transactions of\ntraders using sponsored access [80, 81]. Previously, non-compulsory self-regulation was in\ne\ufb00ect [82]. In the absence of oversight, market data may not properly record the volume of\nshort selling.\nAn explanation in these terms for the events in November of 2007 is also consistent with\nthe observation that there was a larger volume of returned shares on November 7 than the\ntrading volume. In the \u201cshort to buy\u201d scenario, residual positions can be closed through\n\u201cback o\ufb03ce\u201d transactions and may never be recorded on the market.\nThe new information we received implies that the sale of borrowed shares re\ufb02ected in the\nincrease in borrowed shares on November 1 and the corresponding decrease on November 7\nmay have been done in a way that would not have been prevented by the uptick rule. A\nmore detailed inquiry into the means by which such selling could have been done is beyond\nthe current work.\n17\n\nWe thank Steven Poser and Wayne Jett for helpful discussions.\n[1] G. Matsumoto, Naked short sales hint fraud in bringing down Lehman, Bloomberg (March 19,\n2009) http://www.bloomberg.com/apps/news?pid=newsarchive&sid=aB1jlqmFOTCA.\n[2] G. Soros, One way to stop bear raids, Wall Street Journal (March 24, 2009) http://www.\ngeorgesoros.com/articles-essays/entry/one_way_to_stop_bear_raids/.\n[3] M. K. Brunnermeier, L. H. Pedersen, Predatory trading, The Journal of Finance 60, 1825\n(2005).\n[4] M. G.",
    "chunk_index": 13,
    "start_char": 33652,
    "end_char": 36787,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "into the means by which such selling could have been done is beyond\nthe current work.\n17\n\nWe thank Steven Poser and Wayne Jett for helpful discussions.\n[1] G. Matsumoto, Naked short sales hint fraud in bringing down Lehman, Bloomberg (March 19,\n2009) http://www.bloomberg.com/apps/news?pid=newsarchive&sid=aB1jlqmFOTCA.\n[2] G. Soros, One way to stop bear raids, Wall Street Journal (March 24, 2009) http://www.\ngeorgesoros.com/articles-essays/entry/one_way_to_stop_bear_raids/.\n[3] M. K. Brunnermeier, L. H. Pedersen, Predatory trading, The Journal of Finance 60, 1825\n(2005).\n[4] M. G. Ferri, S. E. Christophe, J. J. Angel, A short look at bear raids: Testing the bid test\nGeorgetown University Working Paper; Financial Management Association Annual Meeting,\nFall 2005 (2004).\n[5] Securities Exchange Act of 1934, 15 U.S.C. \u00a778a (2009).\n[6] Regulation SHO and Rule 10a-1, 17 CFR 240, 242 (2007) http://www.sec.gov/rules/\nfinal/2007/34-55970.pdf.\n[7] R. C. Pozen, Y. Bar-Yam, There\u2019s a better way to prevent \u2018bear raids\u2019, Wall Street Journal\n(November 18, 2008) http://online.wsj.com/article/SB122697410070336091.html.\n[8] Y. Bar-Yam, D. Harmon, V. Misra, J. Ornstein, Regulation of short selling: The uptick\nrule and market stability, report presented at the SEC Division of Trading and Mar-\nkets February 24, 2010, NECSI report #2010-02-01 (2010) http://www.necsi.edu/admin/\nNECSISECreportFeb2010.pdf.\n[9] Y. Bar-Yam, D. Harmon, Technical report on SEC uptick repeal pilot, NECSI report #2008-\n11-01 (2008).\n[10] Data Explorers (http://www.dataexplorers.com/).\n[11] R. N. Mantegna, H. E. Stanley, An Introduction to Econophysics: Correlations and complexity\nin \ufb01nance (Cambridge University Press, Cambridge, 1999).\n[12] N. N. Taleb, The Black Swan: The impact of the highly improbable (Random House, New\nYork, 2007).\n[13] E. F. Fama, E\ufb03cient capital markets: A review of theory and empirical work, The Journal of\nFinance 25, 383 (1970).\n[14] E. F. Fama, K. R. French, The capital asset pricing model: Theory and evidence, The Journal\n18\n\nof Economic Perspectives 18, 25 (2004).\n[15] Hostile reactions to CIBC\u2019s Citi report, The New York Times: Dealbook (November 5, 2007).\n[16] S. Rosenbush, The analyst who rocked Citi, Bloomberg: Business Week (November 26, 2007).\n[17] M. Whitney, Is Citigroup\u2019s dividend safe? Downgrading stock due to capital concerns, CIBC\nequity markets: Change in recommendation (October 31, 2007).\n[18] W. F. Sharpe, Capital asset prices: A theory of market equilibrium under conditions of risk,\nThe Journal of Finance 19, 425 (1964).\n[19] J. Engelberg, A. V. Reed, M. C. Ringgenberg, How are shorts informed? Short sellers, news,\nand information processing, University of North Carolina working paper (2010).\n[20] P. Asquith, L. Meulbroek, An empirical investigation of short interest, MIT Working Paper\n(1995).\n[21] A. J. Senchack, L. T. Starks, Short-sale restrictions and market reaction to short-interest\nannouncements, Journal of Financial and Quantitative Analysis 28, 2 (1993).\n[22] E. Boehmer, C. M. Jones, X. Zhang, Which shorts are informed? Journal of Finance 63, 491\n(2008).\n[23] H. Desai, K. Ramesh, S. R. Thiagarajan, B. V. Balachandran, An investigation of the in-\nformational role of short interest in the NASDAQ market, The Journal of Finance 57, 2263\n(2002).\n[24] F. A. Longsta\ufb00, The subprime credit crisis and contagion in \ufb01nancial markets, Journal of\nFinancial Economics 97, 436 (2010).",
    "chunk_index": 14,
    "start_char": 36201,
    "end_char": 39635,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "restrictions and market reaction to short-interest\nannouncements, Journal of Financial and Quantitative Analysis 28, 2 (1993).\n[22] E. Boehmer, C. M. Jones, X. Zhang, Which shorts are informed? Journal of Finance 63, 491\n(2008).\n[23] H. Desai, K. Ramesh, S. R. Thiagarajan, B. V. Balachandran, An investigation of the in-\nformational role of short interest in the NASDAQ market, The Journal of Finance 57, 2263\n(2002).\n[24] F. A. Longsta\ufb00, The subprime credit crisis and contagion in \ufb01nancial markets, Journal of\nFinancial Economics 97, 436 (2010).\n[25] R. J. Caballero, E. Farhi, P.-O. Gourinchas, Financial crash, commodity prices, and global\nimbalances, NBER Working Paper No. 14521 (2008).\n[26] Press release: August 10, 2007, Federal Reserve Board of Governors (August 16, 2007) http:\n//www.federalreserve.gov/newsevents/press/monetary/20070810a.htm.\n[27] I. Goldstein, A. Guembel, Manipulation and the allocational role of prices, Review of Economic\nStudies 75, 1 (2008).\n[28] N. Khanna, R. D. Mathews, Bear raids and short sale bans: Is government intervention\njusti\ufb01able? Michigan State University Working Paper (2009).\n[29] D. Wilchins, J. Stempel, Citigroup CEO Prince to resign: Reports, Reuters (November 2,\n2007).\n[30] Financial Crisis Inquiry Commission, The Financial Crisis Inquiry Report: Final report of the\n19\n\nnational commission on the causes of the \ufb01nancial and economic crisis in the United States\n(US Government Printing O\ufb03ce, 2011).\n[31] A. H. Pessin, Fundamentals of the Securities Industry (New York Institute of Finance, New\nYork, 1978).\n[32] M. L. Mah-Hui, Old wine in a new bottle: Subprime mortgage crisis \u2014 causes and conse-\nquences, The Levy Economics Institute of Bard College Working Paper No. 532 (2008).\n[33] O. J. Blanchard, The crisis: Basic mechanisms and appropriate policies, IMF Working Paper\nNo. 09/80 (2009).\n[34] V. V. Acharya, M. P. Richardson, Causes of the \ufb01nancial crisis, Critical Review 21, 2-3 (2009).\n[35] M. F. Hellwig, Systemic risk in the \ufb01nancial sector: An analysis of the subprime-mortgage\n\ufb01nancial crisis, De Economist 159, 2 (2009).\n[36] Amendments to Regulation SHO, 17 CFR 242 (2010) http://www.sec.gov/rules/final/\n2010/34-61595.pdf.\n[37] A. G. Haldane, R. M. May, Systemic risk in banking ecosystems, Nature 469, 351 (2011).\n[38] N. F. Johnson, Proposing policy by analogy is risky, Nature 469, 302 (2011).\n[39] T. Lux, Network theory is sorely required, Nature 469, 303 (2011).\n[40] H. M. Markowitz, Portfolio Selection: E\ufb03cient diversi\ufb01cation of investment (Wiley, New\nYork, 1959).\n[41] S. A. Ross, The arbitrage theory of capital asset pricing, Journal of Economic Theory 13, 341\n(1973).\n[42] J. C. Cox, J. E. Ingersoll, S. A. Ross, A theory of the term structure of interest rates, Econo-\nmetrica 53, 385 (1985).\n[43] A. E. Khandani, A. W. Lo, What happened to the quants in August 2007? Evidence from\nfactors and transactions data, Journal of Financial Markets 14, 1 (2011).\n[44] W. B. Arthur, J. H. Holland, B. LeBaron, R. G. Palmer, P. Tayler, Asset pricing under\nendogenous expectation in an arti\ufb01cial stock market, in The Economy as an Evolving Complex\nSystem II , W. B. Arthur, D. Lane, S. Durlauf, eds. (Addison-Wesley, Redwood City, 1997)\np.",
    "chunk_index": 15,
    "start_char": 39087,
    "end_char": 42306,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "rates, Econo-\nmetrica 53, 385 (1985).\n[43] A. E. Khandani, A. W. Lo, What happened to the quants in August 2007? Evidence from\nfactors and transactions data, Journal of Financial Markets 14, 1 (2011).\n[44] W. B. Arthur, J. H. Holland, B. LeBaron, R. G. Palmer, P. Tayler, Asset pricing under\nendogenous expectation in an arti\ufb01cial stock market, in The Economy as an Evolving Complex\nSystem II , W. B. Arthur, D. Lane, S. Durlauf, eds. (Addison-Wesley, Redwood City, 1997)\np. 1544.\n[45] T. Lux, M. Marchesi, Scaling and criticality in a stochastic multi-agent model of a \ufb01nancial\nmarket, Nature 397, 498 (1999).\n[46] E. Samanidou, E. Zschischang, D. Stau\ufb00er, T. Lux, Agent-based models of \ufb01nancial markets,\n20\n\nRep. Prog. Phys. 70, 409 (2007).\n[47] M. Levy, H. Levy, S. Solomon, A microscopic model of the stock market, Economics Letters\n45, 103 (1994).\n[48] R. Cont, J. P. Bouchaud, Herd behavior and aggregate \ufb02uctuations in \ufb01nancial markets,\nMacroeconomic Dynamics 4, 170 (2000).\n[49] R. Donangelo, K. Sneppen, Self-organization of value and demand, Physica A 276, 572 (2000).\n[50] X. Gabaix, P. Gopikrishnan, V. Pierou, H. E. Stanley, A theory of power law distributions in\n\ufb01nancial market \ufb02uctuations, Nature 423, 267 (2003).\n[51] Y. Liu, P. Gopikrishnan, P. Cizeau, M. Meyer et. al., Statistical properties of the volatility of\nprice \ufb02uctuations, Physical Review E 60, 1390 (1999).\n[52] D. Sornette, Why Stock Markets Crash: Critical events in complex \ufb01nancial systems (Prince-\nton University Press, Princeton, 2002).\n[53] S. Solomon, M. Levy, Spontaneous scaling emergence in generic stochastic systems, Interna-\ntional Journal of Modern Physics C 7, 745 (1996).\n[54] F. Allen, D. Gale, Stock-price manipulation, Review of Financial Studies 5, 503 (1992).\n[55] F. Allen, G. Gorton, Stock price manipulation, market microstructure and asymmetric infor-\nmation., European Economic Review 36, 624 (1992).\n[56] R. A. Jarrow, Market manipulation, bubbles, corners, and short squeezes, Journal of Financial\nand Quantitative Analysis 27, 311 (1992).\n[57] A. S. Kyle, Continuous auctions and insider trading, Econometrica 53, 1315 (1985).\n[58] R. Benabou, G. Laroque, Using privileged information to manipulate markets: Insiders, gurus,\nand credibility, The Quarterly Journal of Economics 107, 921 (1992).\n[59] P. Kumar, D. J. Seppi, Futures manipulation with cash settlement, Journal of Finance 47,\n1485 (1992).\n[60] R. Aggarwal, G. Wu, Stock market manipulations, Journal of Business 79, 1915 (2006).\n[61] M. T. Bradshaw, S. A. Richardson, R. G. Sloan, Pump and dump: An empirical analysis of\nthe relation between corporate \ufb01nancing activities and sell-side analyst research, University\nof Pennsylvania Working Paper (2003).\n[62] M. Minenna, The detection of market abuse on \ufb01nancial markets: A quantitative approach,\nQuaderni di Finanza 54 (2003).\n[63] R. Cholewiski, Real-time market abuse detection with a stochastic parameter model, Central\n21\n\nEuropean Journal of Economic Modelling and Econometrics 1, 261 (2009).\n[64] O. De Bandt, P. Hartmann, Systemic risk: A survey European Central Bank Working Paper\nNo. 35 (2000).\n[65] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, S.",
    "chunk_index": 16,
    "start_char": 41832,
    "end_char": 45018,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "(2003).\n[62] M. Minenna, The detection of market abuse on \ufb01nancial markets: A quantitative approach,\nQuaderni di Finanza 54 (2003).\n[63] R. Cholewiski, Real-time market abuse detection with a stochastic parameter model, Central\n21\n\nEuropean Journal of Economic Modelling and Econometrics 1, 261 (2009).\n[64] O. De Bandt, P. Hartmann, Systemic risk: A survey European Central Bank Working Paper\nNo. 35 (2000).\n[65] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, S. Havlin, Catastrophic cascade of failures\nin interdependent networks, Nature 564, 1025 (2010).\n[66] P. Schulte, L. Alegret, I. Arenillas, J. A. Arz et. al, The Chicxulub asteroid impact and mass\nextinction at the Cretaceous-Paleogene boundary, Science 327, 1214 (2010).\n[67] N. C. Arens, I. D. West, Press-pulse: a general theory of mass extinction? Paleobiology 34,\n456 (2008).\n[68] D. Harmon, B. C. Stacey, Yavni Bar-Yam, Yaneer Bar-Yam, Networks of economic market\ninterdependence and systemic risk, arXiv 1011.3707 (2010).\n[69] Schedule 13D, 17 CFR 240.13d-101 (2007) http://www.sec.gov/answers/sched13.htm.\n[70] Short interest, NASDAQ http://www.nasdaq.com/quotes/short-interest.aspx.\n[71] H. Mittal, Are you playing in a toxic dark pool? A guide to preventing information leakage,\nThe Journal of Trading 3, 20 (2008).\n[72] L. Harris, Trading and exchanges: Market microstructure for practitioners (Oxford University\nPress USA, 2002).\n[73] M. C. Faulkner, An introduction to securities lending, Handbook of Finance (2008) http:\n//onlinelibrary.wiley.com/doi/10.1002/9780470404324.hof001073/full.\n[74] US\nequity\nshort\npositions\nand\nsecurities\nlending\ndata,\nData\nExplorers\n(2011).\nhttp://www.dataexplorers.com/sites/default/files/Research%20Note%20%233%\n20US%20Equity%20Public%20Short%20Interest.pdf\n[75] TAQ NYSE Short Sales, NYSE Technologies http://www.nyxdata.com/Data-Products/\nNYSE-Short-Sales.\n[76] Daily\nTAQ\n(Trade\nand\nQuote),\nNYSE\nTechnologies\nhttp://www.nyxdata.com/\ndata-products/daily-taq.\n[77] Exhibit 8, The Firing of an SEC Attorney and the Investigation of Pequot Capital Man-\nagement. Joint report by the United States Senate Committee on Finance and the Senate\nJudiciary Committee (August 3, 2007).\n[78] N. Mehta, Sponsored Access Comes of Age, Traders Magazine (March, 2009) http://www.\ntradersmagazine.com/issues/20_292/-103504-1.html\n22\n\n[79] K. D. Freeman, Economic warfare: Risks and responses, Cross Consulting and Services, LLC\n(June, 2009).\n[80] U.S. Securities and Exchange Commission, Exchange Act Release No. 63241 (November 3,\n2010) http://www.sec.gov/rules/final/2010/34-63241.pdf\n[81] P.\nChapman,\nBrokers\nsee\nchallenges\nahead\nto\nmeet\nsponsored\naccess\nmandates\nTraders Magazine Online News (July 19, 2011) http://www.tradersmagazine.com/news/\nbrokers-sponsored-access-107877-1.html?zkPrintable=true\n[82] U.S. Securities and Exchange Commission, Exchange Act Release No. 61379 (January 19,\n2010) http://www.sec.gov/rules/proposed/2010/34-61379.pdf\n23",
    "chunk_index": 17,
    "start_char": 44549,
    "end_char": 47501,
    "paper_title": "Evidence of market manipulation in the financial c",
    "paper_category": "q-fin.GN",
    "paper_filename": "Evidence_of_market_manipulation_in_the_financial_c.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.GN/Evidence_of_market_manipulation_in_the_financial_c.pdf"
  },
  {
    "text": "Forecasting with fractional Brownian motion:\na \ufb01nancial perspective\nMatthieu Garcin\u2217\nSeptember 2, 2021\nAbstract\nThe fractional Brownian motion (fBm) extends the standard Brownian motion by intro-\nducing some dependence between non-overlapping increments. Consequently, if one considers\nfor example that log-prices follow an fBm, one can exploit the non-Markovian nature of the\nfBm to forecast future states of the process and make statistical arbitrages. We provide new\ninsights into forecasting an fBm, by proposing theoretical formulas for accuracy metrics rel-\nevant to a systematic trader, from the hit ratio to the expected gain and risk of a simple\nstrategy. In addition, we answer some key questions about optimizing trading strategies in\nthe fBm framework: Which lagged increments of the fBm, observed in discrete time, are to\nbe considered? If the predicted increment is close to zero, up to which threshold is it more\npro\ufb01table not to invest? We also propose empirical applications on high-frequency FX rates,\nas well as on realized volatility series, exploring the rough volatility concept in a forecasting\nperspective.\nKeywords \u2013 fractional Brownian motion, Hurst exponent, systematic trading, rough volatility,\nforeign-exchange rates\n1\nIntroduction\nAn fBm is a non-Markovian process. A question thus naturally arises when considering the use\nof this model in \ufb01nance for describing the dynamic of prices or log-prices: Does the fBm induce\npure arbitrages or not? This question has led to numerous articles [7]. In particular, it has been\nshown that arbitrage opportunities exist when trading in continuous time is allowed [38]. But the\nanswer is di\ufb00erent when one trades in discrete time, even at a very high frequency [13]. From\nthe perspective of a \ufb01nancial market practitioner, beyond the strong mathematical interest of the\nquestion, the distinction is not so clear between continuous time and discrete time with arbitrary\nhigh frequency.\nTherefore, like others before, we believe that this debate about the existence\nof arbitrages implied by this model is mainly motivated by theoretical convenience, what is not\nenough to exclude the fBm as a model of log-prices [17].\nA more recent literature deals with the use of fBm in statistical arbitrage [31, 30]. Exploiting\nthe autocovariance structure of the fBm indeed makes it possible to forecast future states of the\n\u2217L\u00b4eonard de Vinci P\u02c6ole Universitaire, Research center, 92916 Paris La D\u00b4efense, France, matthieu.garcin@m4x.org.\nThe author is grateful to Martino Grasselli, Cha\ufb01c Merhy, and Konstantinos Tsianos for valuable discussions and\ncomments.\n1\narXiv:2105.09140v3 [q-fin.MF] 1 Sep 2021\n\nprocess [36].\nThis simple idea has been used since in algorithmic trading to build systematic\nstrategies.\nSuch strategies do not lead to a certain gain as do the pure arbitrages previously\nmentioned, but they are pro\ufb01table in average. All the contributions on this subject highlight the\nasymmetry of the performance of these strategies with respect to H = 1/2, where H is the Hurst\nexponent.\nAsymmetry with respect to H = 1/2 is also underlined in the econophysics literature. Indeed,\nwhen H > 1/2, the increments of the fBm are positively correlated and the process also has a long\nmemory.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3253,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "Such strategies do not lead to a certain gain as do the pure arbitrages previously\nmentioned, but they are pro\ufb01table in average. All the contributions on this subject highlight the\nasymmetry of the performance of these strategies with respect to H = 1/2, where H is the Hurst\nexponent.\nAsymmetry with respect to H = 1/2 is also underlined in the econophysics literature. Indeed,\nwhen H > 1/2, the increments of the fBm are positively correlated and the process also has a long\nmemory. By contrast, when H < 1/2, non-overlapping increments are negatively correlated and\nthere is no long memory, in the sense that the autocovariance function decreases exponentially.\nMany researchers equate the notion of market e\ufb03ciency with the property of long memory, leading\nthem to consider that the market is e\ufb03cient if and only if H > 1/2 [19, 12, 20, 4, 21, 29]. We\nindeed aknowledge that when H < 1/2, the performance of the fBm-based predictor does not\nalways lead to satisfying empirical results [22]. But this practical limitation does not come from\na particular property of the fBm itself.\nIt comes instead from the fact that the model is not\nwell speci\ufb01ed or from the related di\ufb03culties to estimate the parameters properly, in a nutshell\nfrom model risk [23, 24]. Besides the identi\ufb01cation of market e\ufb03ciency to long-range dependence,\nanother branch of the econophysics literature considers that the market is e\ufb03cient for H = 1/2\nand that its ine\ufb03ciency gradually increases as H gets away from 1/2 [33, 34, 32, 10, 5]. Indeed,\npredictions do not rely on the long-range dependence property but, instead, on the autocovariance\nof the process, which is di\ufb00erent from zero as soon as H \u0338= 1/2.\nWhile the fBm is widespread in econophysics and more recently in mainstream quantitative \ufb01nance,\nnot everything has been said yet on forecasting the fBm in a \ufb01nancial perspective. First, the theo-\nretical evaluation of the quality of the forecast invoked in the literature is rarely very appropriate\nfor systematic traders. For example, the mean squared error (MSE), put forward in several pa-\npers [36, 28], albeit statistically relevant, is not related to the performance and risk of a trading\nstrategy. In this perspective, other metrics have to be applied in the fBm setting. Moreover, while\nmany articles focus on the fBm in continuous time [31, 30], the reality of trading, which is a\ufb00ected\nby liquidity frictions, is in discrete time, whether for the observation of the price process or for the\ninstants at which one is able to trade. And this limitation has some consequences. For example,\ndetermining which lagged price returns should be used as input of the predictor is overriding in\norder to optimize an fBm-based forecast and related systematic trading strategies.\nWe answer these questions in the present paper.\nWe introduce several accuracy metrics for\ncovariance-based predictors: a hit ratio, an average gain, a risk de\ufb01ned as a lower semi-deviation,\nand the resulting risk-adjusted performance.",
    "chunk_index": 1,
    "start_char": 2769,
    "end_char": 5767,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "one is able to trade. And this limitation has some consequences. For example,\ndetermining which lagged price returns should be used as input of the predictor is overriding in\norder to optimize an fBm-based forecast and related systematic trading strategies.\nWe answer these questions in the present paper.\nWe introduce several accuracy metrics for\ncovariance-based predictors: a hit ratio, an average gain, a risk de\ufb01ned as a lower semi-deviation,\nand the resulting risk-adjusted performance. The risk measure we have chosen di\ufb00erentiates our\nwork from the traditional mean-variance framework [31, 30]. While the variance is a widespread\ndeviation risk measure, we indeed think that a lower semi-deviation more appropriately depicts\ndownside risk. We provide theoretical expressions for all the aforementioned metrics, using the\nfBm assumption. All these expressions depict the link between the Hurst exponent H and the\nforecasting ability of the fBm: the closer H is to 1/2, the worst is the quality of the forecast, and,\nasymmetrically, the forecast is better for H > 1/2 than for 1 \u2212H. We also use the theoretical\nexpressions mentioned above to build tools which are useful in optimizing trading strategies. We\ndo this in two directions. First, we \ufb01nd numerically a general expression for the duration of the\noptimal time lags of price returns used as input variables of the predictor. Second, we de\ufb01ne an\noptimal threshold under which we consider that the forecast is too close to zero for an investment\nto positively impact the risk-adjusted pro\ufb01t. This last idea \ufb01nally leads to reducing the trading\nfrequency and thus to take into account liquidity constraints.\nWe also propose two empirical applications highlighting the relevance of predictions based on the\nfBm in \ufb01nance. In particular, we maximize an ex-ante risk-adjusted performance of a trading\nstrategy on FX rates, where log-prices follow an fBm. We also investigate another application of\n2\n\nthe fBm in \ufb01nance than the sole log-price process and for which a performing forecast method\nis useful for trading desks.\nWe indeed study series of realized volatilities, exploring the rough\nvolatility and how one can optimize the forecast of volatility in this context, focusing on hit ratios.\nThe rest of the paper is organized as follows.\nIn Section 2, we recall some basic and useful\nproperties of the fBm. Section 3 introduces the art of forecasting an fBm, namely the covariance-\nbased predictor, its accuracy measured by a hit ratio, and the optimal selection of its input. In\nSection 4, we focus on statistical arbitrage, providing both a simple systematic trading strategy\nand related accuracy metrics. In Section 5, we present empirical results on high-frequency FX\nrates and on daily \ufb01nancial series, including realized volatility. Section 6 concludes.\n2\nModel description: preliminaries on the fBm\nThe fBm, introduced by Mandelbort and van Ness in 1968, can follow several equivalent de\ufb01ni-\ntions [35]. Among them, we can cite the integral-based de\ufb01nition.\nDe\ufb01nition 1.",
    "chunk_index": 2,
    "start_char": 5275,
    "end_char": 8315,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "strategy\nand related accuracy metrics. In Section 5, we present empirical results on high-frequency FX\nrates and on daily \ufb01nancial series, including realized volatility. Section 6 concludes.\n2\nModel description: preliminaries on the fBm\nThe fBm, introduced by Mandelbort and van Ness in 1968, can follow several equivalent de\ufb01ni-\ntions [35]. Among them, we can cite the integral-based de\ufb01nition.\nDe\ufb01nition 1. An fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0 is a stochastic\nprocess Xt such that, \u2200t \u2208R,\nXt =\n\u03c3\n\u0393\n\u0000H + 1\n2\n\u0001\nZ \u221e\n\u2212\u221e\n\u0010\n(t \u2212s)H\u22121/2\n+\n\u2212(\u2212s)H\u22121/2\n+\n\u0011\ndWs,\nwhere Ws is a standard Brownian motion.\nIn De\ufb01nition 1, the fBm is to be seen as a weighted average of a Gaussian white noise. The weights\nare de\ufb01ned by the kernel (s, t) 7\u2192\n\u0010\n(t \u2212s)H\u22121/2\n+\n\u2212(\u2212s)H\u22121/2\n+\n\u0011\n, whose right part, \u2212(\u2212s)H\u22121/2\n+\n, is\nsimply intended to make the process Xt equal to zero when t = 0, which is a feature also stated in\nother de\ufb01nitions of the fBm. One major motivation for introducing the fBm was to de\ufb01ne a process\nwith other scaling properties than the standard Brownian motion. This alternative scaling clearly\nappears in De\ufb01nition 2, in the expression of the variance of an increment of the process Xt, but it\nis not so obvious in the integral-based de\ufb01nition. The idea of Mandelbrot and van Ness to obtain\na speci\ufb01c scaling was to use fractional calculus. Indeed, De\ufb01nition 1 corresponds to the fractional\nderivative (respectively integral) of order 1/2\u2212H (resp. H \u22121/2) of a standard Brownian motion,\nif H < 1/2 (resp. H > 1/2), the case H = 1/2 corresponding to the standard Brownian motion\nitself. The scaling feature characterized by a given Hurst exponent is thus to be related to a speci\ufb01c\nautocovariance of the increments of the process in the fBm model.\nSince we will work in discrete time in all the paper, we introduce another de\ufb01nition of the fBm,\nwhich is equivalent to De\ufb01nition 1.\nDe\ufb01nition 2. An fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0 is a stochastic\nGaussian process Xt such that X0 = 0, E{Xt} = 0, and\nE{(Xt \u2212Xs)2} = \u03c32|t \u2212s|2H\nfor all s, t \u2208R.\nThanks to De\ufb01nition 2, a particular property of scaling, or selfsimilarity, clearly appears for the\nfBm. This is the purpose of Proposition 1.\nProposition 1. Let Xt be an fBm of Hurst exponent H \u2208(0, 1). Then, the process Xt is statistically\nH-selfsimilar, that is to say, \u2200c > 0, the random variables Xct and cHXt, for a given t \u2208R, follow\nthe same distribution.\n3\n\nProof. According to De\ufb01nition 2, the two variables Xct and cHXt are Gaussian, have both a mean\nequal to zero and a variance equal to \u03c32c2H|t|2H. They thus follow exactly the same distribution.\nThis scaling property of the fBm also leads to a speci\ufb01c autocovariance of the process and of its\nincrements, as exposed in Proposition 2.",
    "chunk_index": 3,
    "start_char": 7907,
    "end_char": 10702,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "the process Xt is statistically\nH-selfsimilar, that is to say, \u2200c > 0, the random variables Xct and cHXt, for a given t \u2208R, follow\nthe same distribution.\n3\n\nProof. According to De\ufb01nition 2, the two variables Xct and cHXt are Gaussian, have both a mean\nequal to zero and a variance equal to \u03c32c2H|t|2H. They thus follow exactly the same distribution.\nThis scaling property of the fBm also leads to a speci\ufb01c autocovariance of the process and of its\nincrements, as exposed in Proposition 2.\nProposition 2. Let Xt be an fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0.\nThen, the covariance of Xt and Xs is\nE{XtXs} = \u03c32\n2 (|t|2H + |s|2H \u2212|t \u2212s|2H)\n(1)\nand the covariance between increments Xt \u2212Xs and Xv \u2212Xu is\nE{(Xt \u2212Xs)(Xv \u2212Xu)} = \u03c32\n2 (|u \u2212t|2H + |v \u2212s|2H \u2212|v \u2212t|2H \u2212|u \u2212s|2H).\n(2)\nProof. Using De\ufb01nition 2 and in particular its consequence that E{X2\nt } = E{(Xt\u2212X0)2} = \u03c32|t|2H,\nthe proof of Proposition2 is straightforward:\nE{XtXs}\n=\n\u22121\n2\n\u0000E{(Xt \u2212Xs)2} \u2212E{X2\nt } \u2212E{X2\ns}\n\u0001\n=\n\u22121\n2\n\u0000\u03c32|t \u2212s|2H \u2212\u03c32|t|2H \u2212\u03c32|s|2H\u0001\n=\n\u03c32\n2 (|t|2H + |s|2H \u2212|t \u2212s|2H)\nand\nE{(Xt \u2212Xs)(Xv \u2212Xu)}\n=\nE{XtXv} \u2212E{XtXu} \u2212E{XsXv} + E{XsXu}\n=\n\u03c32\n2 (|u \u2212t|2H + |v \u2212s|2H \u2212|v \u2212t|2H \u2212|u \u2212s|2H).\nFrom Proposition 2, one sees that the fBm is not a stationary process but its increments are\nstationary, exactly like the standard Brownian motion. This means that, for making forecasts, we\nwill more easily work with increments than with the process itself.\nIn all what follows, the process Xt is an fBm of Hurst exponent H and volatility parameter \u03c3 > 0.\nIn the empirical part of the paper, Xt will either depict a log-price or the volatility process of a\nprice. We thus cover the two main applications of the fBm in \ufb01nance. The \ufb01rst one has been\nput forward by econophysicists and the second one has known a recent interest in mathematical\n\ufb01nance with the rise of the literature about rough volatility. For simpli\ufb01cation, we will sometimes\nevoke log-prices in the theoretical developments, but the approaches can also often be applied to\nvolatilities and to any other dynamic described by an fBm, in particular in Section 3. Besides,\neven though volatility is not tradable, its forecast value is used in many trading algorithms, making\nthe forecast of rough volatility a relevant challenge in \ufb01nance. However, Section 4.2, in which we\ndetermine the pro\ufb01t and risk of a systematic strategy, is only appropriate for log-prices.\n3\nForecasting an fBm\nIn this section, we present the standard formula for the forecast of the process in discrete time\nalong with various accuracy metrics.\n4\n\n3.1\nCovariance-based forecast\nWe want to predict the value of the process Xt for a time horizon h, that is Xt+h, conditionally\nto observations at some times I = {t1, ..., tn}, such that \u2200s \u2208I, s \u2264t, where t is the current time.",
    "chunk_index": 4,
    "start_char": 10214,
    "end_char": 13001,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "a systematic strategy, is only appropriate for log-prices.\n3\nForecasting an fBm\nIn this section, we present the standard formula for the forecast of the process in discrete time\nalong with various accuracy metrics.\n4\n\n3.1\nCovariance-based forecast\nWe want to predict the value of the process Xt for a time horizon h, that is Xt+h, conditionally\nto observations at some times I = {t1, ..., tn}, such that \u2200s \u2208I, s \u2264t, where t is the current time.\nWriting Y = (Xt1 ... Xtn)T , the forecast minimizing the MSE is \u02c6Xt+h|Y = E{Xt+h|Y }. We recall\nthat Xt is an fBm. For this reason, we can easily determine the covariance between Xs and Xt\nat any times s, t \u2208R, thanks to equation (1), and we can even build covariance matrices for Xt+h\nand the vector Y . This makes it possible to express explicitly \u02c6Xt+h|Y as well as its MSE. This is\nthe purpose of Proposition 3, for which we do not detail the proof since it is a direct consequence\nof the Gaussian conditioning theorem [36].\nProposition 3. Let Xt be an fBm, h > 0, Y = [Xt1 ... Xtn]T , with \u2200i < j, ti \u0338= 0 and ti < tj. The\nestimator of Xt+h minimizing the MSE conditionally to Y is\n\u02c6Xt+h|Y = E{Xt+h|Y } = \u03a3XY \u03a3\u22121\nY Y\nand the corresponding MSE is\nE{( \u02c6Xt+h|Y \u2212Xt+h)2} = \u03a3X \u2212\u03a3XY \u03a3\u22121\nY \u03a3T\nXY ,\nwhere \u03a3Y = E{Y Y T }, \u03a3X = E{X2\nt+h}, and \u03a3XY = E{Xt+hY T } can be explicitly expressed thanks\nto equation (1).\nWe also stress the fact that using existing results about forecasting an fBm in continuous time\nin our framework is less relevant. Indeed, the adaptation of the continuous-time formulas to the\ndiscrete-time framework needs the discretization of an integral and therefore only leads to an\napproximation of E{Xt+h|Y }. On the contrary, exploiting the covariance matrix of an fBm is both\neasier and more accurate.\nIt is also well known that, with the fBm model, the prediction at time horizon h is optimal when\nthe only past observation considered has a time lag also equal to h. In particular, the optimality\ncan be understood as the minimization of the MSE. In other words, for n = 2, considering t1 = t\nand t2 = t \u2212h is the choice of dates which minimizes the MSE in Proposition 3.\n3.2\nHit ratio\nThe MSE, while being a useful statistical tool, may be less relevant in \ufb01nance than other statistics,\nsuch as the hit ratio, which is the probability to make a good forecast of the sign of a future price\nreturn. Indeed, in order to make an investment decision, a trader forecasts at time t the price\nreturn Rt,t+h between t and t + h, de\ufb01ned by\nRt,t+h = Xt+h \u2212Xt,\n(3)\nby a speci\ufb01c predictor \u02c6Rt,t+h. If \u02c6Rt,t+h > 0 (respectively < 0), the trader must be long (resp.",
    "chunk_index": 5,
    "start_char": 12556,
    "end_char": 15171,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "relevant in \ufb01nance than other statistics,\nsuch as the hit ratio, which is the probability to make a good forecast of the sign of a future price\nreturn. Indeed, in order to make an investment decision, a trader forecasts at time t the price\nreturn Rt,t+h between t and t + h, de\ufb01ned by\nRt,t+h = Xt+h \u2212Xt,\n(3)\nby a speci\ufb01c predictor \u02c6Rt,t+h. If \u02c6Rt,t+h > 0 (respectively < 0), the trader must be long (resp.\nshort) between t and t + h in order to expect a gain at time t + h. Therefore, the hit ratio is\nimplicitly related to the performance of a trading strategy. Every trader is thus able to interpret\nthe value of a hit ratio and to discard any forecast with a hit ratio lower than 50%. On the contrary,\nthe interpretation of the MSE is not as clear and this metric is only useful for comparing several\nforecasting methods on the same dataset, not for judging the intrinsic quality of a given predictor.\nIn our framework, log-prices follow an fBm. Therefore, log-returns are simply increments of the\nfBm. This will ease the calculation and the analysis of the hit ratio, since increments of the fBm\nare stationary whereas the fBm itself is not stationary. Moreover, the forecast is a weighted sum\n5\n\nof past observations. If one considers prices instead of returns, the weights will depend on t and\nnot only on the time lags. This is striking for example if t = h. Indeed, as exposed above, when\nconsidering two observations for forecasting, the optimal time lag in the past should be h and thus\none should forecast Xt+h using Xt and Xt\u2212h = X0 = 0. But, in this case, the matrix \u03a3Y is not\ninvertible and we cannot apply Proposition 3.\nBefore determining the hit ratio, we need to detail the predictor of the true future log-return Rt,t+h.\nWe forecast Rt,t+h conditionally to a vector S of adjacent past price returns. The vector S is built\nwith the help of the set of time lags \u2206= {\u03b40, ..., \u03b4n}, with n \u22651 and \u2200i < j, \u03b4i < \u03b4j and \u03b40 \u22650.\nMore precisely, we have\nS = [Rt\u2212\u03b41,t\u2212\u03b40 ... Rt\u2212\u03b4n,t\u2212\u03b4n\u22121]T .\nFor convenience, we set \u03b40 = 0, because it is natural to consider the current state in our forecast, but\nfor the theoretical results exposed below, this constraint is not mandatory. We note our predictor\n\u02c6Rt,t+h|\u2206. The extension of Proposition 3 to the increments of the process instead of the process\nitself is straightforward, since it simply consists in replacing the matrices of covariance of the fBm\nby the matrices of covariance of its increments.\nProposition 4. Let Xt be an fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0,\nh > 0, S = [Rt\u2212\u03b41,t\u2212\u03b40 ... Rt\u2212\u03b4n,t\u2212\u03b4n\u22121]T , with \u2200i < j, \u03b4i < \u03b4j, \u03b4i \u22650, and R.,. expressed by\nequation (3).",
    "chunk_index": 6,
    "start_char": 14766,
    "end_char": 17419,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "instead of the process\nitself is straightforward, since it simply consists in replacing the matrices of covariance of the fBm\nby the matrices of covariance of its increments.\nProposition 4. Let Xt be an fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0,\nh > 0, S = [Rt\u2212\u03b41,t\u2212\u03b40 ... Rt\u2212\u03b4n,t\u2212\u03b4n\u22121]T , with \u2200i < j, \u03b4i < \u03b4j, \u03b4i \u22650, and R.,. expressed by\nequation (3). The estimator of Rt,t+h minimizing the MSE conditionally to S is\n\u02c6Rt,t+h|\u2206= \u03a3RS\u03a3\u22121\nS S\n(4)\nand the corresponding MSE is\nE{( \u02c6Rt,t+h|\u2206\u2212Rt,t+h)2} = \u03a3R \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS,\nwhere \u03a3S = E{SST }, \u03a3R = E{R2\nt,t+h} = \u03c32h2H, and \u03a3RS = E{Rt,t+hST } can be explicitly\nexpressed thanks to equation (2).\nIt is worth noting that \u02c6Rt,t+h|\u2206does only depend on h, not on t. In what follows, we keep t in most\nsubscripts, but the hit ratios, risk measures, and expected performance displayed in the coming\ntheorems and propositions in fact only depend on the forecast horizon h.\nOne can write equation (4) di\ufb00erently, as a weighted sum of past returns:\n\u02c6Rt,t+h|\u2206=\nn\nX\ni=1\n\u03b2iRt\u2212\u03b4i,t\u2212\u03b4i\u22121,\nwhere the \u03b2i must be consistent with the covariance approach exposed in Proposition 4:\n[\u03b21 ... \u03b2n] = \u03a3RS\u03a3\u22121\nS .\nIn the particular case where n = 1, that is when the predicted return only relies on one past return,\nnamely \u02c6Rt,t+h|\u2206= \u03b21Rt\u2212\u03b41,t, we have the following expression for \u03b21:\n\u03b21 = Cov(Rt,t+h, Rt\u2212\u03b41,t)\nVar(Rt\u2212\u03b41,t)\n,\naccording to Proposition 4. We note that the covariance and variance appearing in the equation\nabove are not estimated with their empirical version. The \u03b21 parameter is indeed model-dependent.\nThe fBm assumption will in particular make it possible to rely on a parsimonious representation of\nthe dependence structure among price returns, even when considering a big number of time lags,\n6\n\nand thus to avoid over\ufb01tting in the forecast. This speci\ufb01c feature of parsimony will be studied\nempirically in Section 5.1. Taking into account the fBm assumption, the expression for \u03b21, when\nn = 1 is:\n\u03b21 = 1\n2\n\"\u0012 h\n\u03b41\n+ 1\n\u00132H\n\u2212\n\u0012 h\n\u03b41\n\u00132H\n\u22121\n#\n,\n(5)\naccording to equation (2). It is worth noting that \u03b21 is positive when H > 1/2, equal to zero for\nH = 1/2, and negative when H < 1/2. It is consistent with properties of the fBm: increments\nare positively correlated when H > 1/2, non correlated when H = 1/2, and negatively correlated\nwhen H < 1/2. In particular, we note that when \u03b41 = h, \u03b21 tends toward 1 when H tends toward\n1: the persistence is such that the best prediction of the future return is the past return of same\nduration. Alternatively, in the anti-persistence case, when H tends toward 0, whatever the value\nof \u03b41, \u03b21 tends toward \u22121/2.\nIn the general case, that is for n not necessarily equal to 1, we want to determine the theoretical\nhit ratio of our predictor.",
    "chunk_index": 7,
    "start_char": 17044,
    "end_char": 19783,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "particular, we note that when \u03b41 = h, \u03b21 tends toward 1 when H tends toward\n1: the persistence is such that the best prediction of the future return is the past return of same\nduration. Alternatively, in the anti-persistence case, when H tends toward 0, whatever the value\nof \u03b41, \u03b21 tends toward \u22121/2.\nIn the general case, that is for n not necessarily equal to 1, we want to determine the theoretical\nhit ratio of our predictor. As written above, it is the probability to forecast properly the sign of\nthe future price return Rt,t+h. Two de\ufb01nitions are in fact possible, depending on the conditioning\nof the probability to past returns or not.\nDe\ufb01nition 3. Let \u02c6Rt,t+h|\u2206be a predictor of Rt,t+h based on the vector S of past returns, as de\ufb01ned\nby equation (4). The conditional hit ratio of this predictor is\n\u03c1c(y) = P\nh\n\u02c6Rt,t+h|\u2206Rt,t+h \u22650 |S = y\ni\nand the non-conditional hit ratio is\n\u03c1 = P\nh\n\u02c6Rt,t+h|\u2206Rt,t+h \u22650\ni\n.\nIn the fBm framework, the vector S admits a Gaussian density gS of mean zero and variance \u03a3S.\nThe non-conditional hit ratio is the weighted average of the conditional hit ratio:\n\u03c1 =\nZ\nR\n\u03c1c(y)gS(y)dy = E {\u03c1c(S)} .\nTheorem 1 provides a theoretical expression both for \u03c1c and \u03c1, when the price returns are assumed\nto be increments of an fBm.\nTheorem 1. Let Xt be an fBm of Hurst exponent H \u2208(0, 1) and volatility parameter \u03c3 > 0. Let\nh > 0 and \u02c6Rt,t+h|\u2206be a predictor of Rt,t+h based on the vector S of past returns, as de\ufb01ned by\nequation (4). The conditional hit ratio is\n\u03c1c(y) = N\n\uf8eb\n\uf8ed\n\f\f\u03a3RS\u03a3\u22121\nS y\n\f\f\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\uf8f6\n\uf8f8\nand the non-conditional hit ratio is, for H \u0338= 1/2,\n\u03c1 = 1 \u22121\n\u03c0 arctan\n s\n\u03c32h2H\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u22121\n!\n,\n(6)\nwhere \u03a3S and \u03a3RS are the same as in Proposition 4 and N is the standard Gaussian cumulative\ndistribution function.\n7\n\nThe proof of Theorem 1 is postponed in Appendix B.\nThe case H = 1/2, for which the fBm is a standard Brownian motion, is of particular interest. In\nthis case, the 1\u00d71 matrix \u03a3R contains the element \u03c32h, the 1\u00d7n matrix \u03a3RS only contains zeros,\nand the n\u00d7n matrix \u03a3S is diagonal equal to diag(\u03c32(\u03b41 \u2212\u03b40), ..., \u03c32(\u03b4n \u2212\u03b4n\u22121)). As a consequence,\naccording to Theorem 1, the conditional hit ratio \u03c1c(y) is 0.5, whatever y. This is consistent with\nthe fact that the standard Brownian motion is a martingale. Besides, any value of H di\ufb00erent\nfrom 1/2 trivially leads to \u03c1c > 0.5 since it is the image by N of a positive number. Regarding\nthe non-conditional hit ratio \u03c1, equation 6 is not de\ufb01ned for H = 1/2, but we can easily determine\nthat the limit of \u03c1, when H \u21921/2, is also 0.5.",
    "chunk_index": 8,
    "start_char": 19354,
    "end_char": 21897,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "with\nthe fact that the standard Brownian motion is a martingale. Besides, any value of H di\ufb00erent\nfrom 1/2 trivially leads to \u03c1c > 0.5 since it is the image by N of a positive number. Regarding\nthe non-conditional hit ratio \u03c1, equation 6 is not de\ufb01ned for H = 1/2, but we can easily determine\nthat the limit of \u03c1, when H \u21921/2, is also 0.5. Once, again, any value of H di\ufb00erent from 1/2\nleads to \u03c1 > 0.5, that is to more frequent good predictions than bad predictions, as one can see in\nFigure 1.\nFigure 1: Theoretical non-conditional hit ratio for the fBm with one\ntime lag and \u03b41 = h (continuous line) and for four time lags and\n(\u03b40, ..., \u03b44) = (0, 0.5, 1, 2, 3) (dotted line).\nIn the rest of the paper, the theoretical hit ratio on which we focus is \u03c1, the non-conditional hit\nratio. Indeed, in a practical application, we are inclined to choose a speci\ufb01c forecast setting, that\nis a number of lags and a duration for these lags. The maximization of a theoretical hit ratio ex\nante must lead the choice of this setting. This choice is not that simple if one wants to maximize\nthe conditional hit ratio, insofar as one must numerically test the best combination of lagged\nobservations to \ufb01nally retain the one maximizing \u03c1c. A legitimate question then arises: Why not\nretaining all the tested lagged observations? If one uses \u03c1 instead of \u03c1c, the choice of time lags is\nsimpler and will always be the same, provided that the estimated H is also the same, because the\nchoice of time lags in this case does not depend on the particular value of each past observation\nbut only on the estimated H.\nWe see in Figure 1 that Hurst exponents closer to 0 or 1 than to 1/2 lead to higher hit ratios. Hurst\nexponents H above 1/2 also lead to more accurate predictions than when the Hurst exponent is\n1 \u2212H. In particular, H close to 1, that is a very persistent series of returns, leads to the highest\npossible hit ratio, whereas H close to 0 only leads to a hit ratio of 0.67 if the forecast is based\non one past return (n = 1). In other words, it is globally more di\ufb03cult to make good predictions\nwhen H < 1/2 than when H > 1/2 but predictions are however good in average, with a hit ratio\n8\n\nhigher than 0.5 in both cases. This contradicts a part of the econophysics literature which tends\nto consider that predictions are relevant only for H > 1/2. We can even stress that predictions\nwill be more accurate for H = 0.2 than for H = 0.6.\nIn the particular case where n = 1, for which we have already provided an explicit expression for\n\u03b21 in equation 5, the hit ratio also has a simpler expression:\n\u03c1 = 1 \u22121\n\u03c0 arctan\n\uf8eb\n\uf8ed\ns\n1\n\u03b22\n1\n\u0012 h\n\u03b41\n\u00132H\n\u22121\n\uf8f6\n\uf8f8\nbecause \u03a3RS\u03a3\u22121\nS \u03a3T\nRS, in this case, is equal to \u03b21\u03a3T\nRS = \u03b22\n1\u03a3S = \u03c32\u03b22\n1\u03b42H\n1 .",
    "chunk_index": 9,
    "start_char": 21558,
    "end_char": 24280,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "in both cases. This contradicts a part of the econophysics literature which tends\nto consider that predictions are relevant only for H > 1/2. We can even stress that predictions\nwill be more accurate for H = 0.2 than for H = 0.6.\nIn the particular case where n = 1, for which we have already provided an explicit expression for\n\u03b21 in equation 5, the hit ratio also has a simpler expression:\n\u03c1 = 1 \u22121\n\u03c0 arctan\n\uf8eb\n\uf8ed\ns\n1\n\u03b22\n1\n\u0012 h\n\u03b41\n\u00132H\n\u22121\n\uf8f6\n\uf8f8\nbecause \u03a3RS\u03a3\u22121\nS \u03a3T\nRS, in this case, is equal to \u03b21\u03a3T\nRS = \u03b22\n1\u03a3S = \u03c32\u03b22\n1\u03b42H\n1 .\nIn a systematic investment perspective, a forecast based on only one past return is often not\nenough to generate a pro\ufb01table strategy because of too low hit ratios, in particular because the\nHurst exponent in \ufb01nance is almost never in the interval (0.75, 1), where we observe the highest\ntheoretical hit ratios. In order to build a systematic strategy, the prediction is to be improved by\nincluding several past returns. Even without optimizing the duration of these past returns, one\nsees in Figure 1 that adding arbitrary time lags in the set \u2206indeed increases the hit ratio. This\ne\ufb00ect is very limited when the Hurst exponent is higher than 1/2, but it is promising for values of\nH below 1/2.\nIn this investment perspective, the question of the best choice for n, the number of past returns\nto be included in the predictor, is an important topic. Theoretically, the optimal n maximizing\nthe theoretical hit ratio is +\u221e. However, considering a predictor based on a big number of past\nreturns leads in general to two pitfalls: a higher computation time and over\ufb01tting. The \ufb01rst point\nis a major issue in high-frequency trading. To avoid the second drawback, one prefers minimizing\ninformation criteria, such as AIC or BIC, instead of simply maximizing an ex-ante hit ratio. Such\ncriteria are based on the likelihood of the model and penalize the number of parameters. Regarding\nthe likelihood, reality is always more complex than any model, so adding time lags will \ufb01rst improve\nthe likelihood but it may also deteriorate it for a too big number of time lags due to a discrepancy\nbetween reality and the fBm speci\ufb01cation. Regarding the number of parameters, it does not depend\non the number of time lags if the model is an fBm. Indeed, the weights \u03b2i applied to past returns\nall depend on the parameters of the fBm.\nThe choice of the n minimizing AIC or BIC thus depends on the dataset. We will see an illustration\nof this problem in the empirical part of the paper, in Section 5.1. Once n is selected, the duration\nof the n past price returns is to be selected too. We will see in the next subsection how this can\nbe achieved using the theoretical hit ratio.",
    "chunk_index": 10,
    "start_char": 23758,
    "end_char": 26441,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "the parameters of the fBm.\nThe choice of the n minimizing AIC or BIC thus depends on the dataset. We will see an illustration\nof this problem in the empirical part of the paper, in Section 5.1. Once n is selected, the duration\nof the n past price returns is to be selected too. We will see in the next subsection how this can\nbe achieved using the theoretical hit ratio.\n3.3\nOptimal duration of time lags\nThe question of the optimal duration of time lags is to be related to the question of the optimal\nsampling of high-frequency observations of asset prices, with which the econometric literature has\nalready dealt without any fBm assumption [6, 37]. Observations are equally spaced in time in this\neconometric approach. This will not be the case in our fBm-based predictor.\nWe consider a \ufb01xed number n \u22651 of past returns to be used in the predictor de\ufb01ned in Proposi-\ntion 4. We write them Rt\u2212\u03b41,t\u2212\u03b40, ..., Rt\u2212\u03b4n,t\u2212\u03b4n\u22121, with \u2200i < j, \u03b4i < \u03b4j, \u03b4i \u22650, and \u03b40 = 0. Given\na \ufb01xed prediction horizon h > 0, the vector of time lags [\u03b40 ... \u03b4n]T maximizing the hit ratio \u03c1 of\nTheorem 1 is noted [\u03b4\u22c6\n0 ... \u03b4\u22c6\nn]T . A numerical optimization of \u03c1 provides these optimal time lags.\n9\n\nThis numerical study leads to the following observation:\n\u2200i \u2208J1, nK, \u03b4\u22c6\ni =\nh2\n\u03b4\u22c6\nn+1\u2212i\n.\nWe brie\ufb02y focus on the case n = 1. Figure 2 displays the value of the hit ratio as a function of\n\u03b41/h. The optimum in \u03b41/h = 1 is clear and it also appears that the hit ratio is the same for \u03b41/h\nand h/\u03b41. Therefore, the optimal time lag for the past return used in the forecast is equal to the\nforecast horizon: \u03b41 = h. In other words, to forecast the future daily return, one has to consider\nthe last observed daily return. For a monthly return to be forecast, the last observed monthly\nreturn is the optimal input of the fBm-based prediction model. This is consistent with the existing\nliterature [36].\nFigure 2: Hit ratio with H = 0.65 (black) and H = 0.15 (grey) for\nthe fBm, for various values of the ratio \u03b41/h and n = 1.\nOther cases than n = 1 provide new insights. Increasing n of one unit does indeed not mean adding\none time lag to the set of time lags optimized in the case n \u22121. For example, if n = 2, h is not one\nof the optimal time lags. Instead, if we consider h = 1 for simplifying and H = 0.65, the optimal\nlags are (\u03b41, \u03b42) = (0.29, 3.45). For a monthly forecast, so for a forecast horizon of 22 days, the\noptimal time lags to be considered are thus roughly 6 and 76 days. For a daily forecast, intraday\ndata are to be taken into account. If n is odd (respectively even), one has to consider (n \u22121)/2\n(resp.",
    "chunk_index": 11,
    "start_char": 26071,
    "end_char": 28663,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "2, h is not one\nof the optimal time lags. Instead, if we consider h = 1 for simplifying and H = 0.65, the optimal\nlags are (\u03b41, \u03b42) = (0.29, 3.45). For a monthly forecast, so for a forecast horizon of 22 days, the\noptimal time lags to be considered are thus roughly 6 and 76 days. For a daily forecast, intraday\ndata are to be taken into account. If n is odd (respectively even), one has to consider (n \u22121)/2\n(resp. n/2) time lags lower than h, the same quantity bigger than h, and one (resp. zero) time lag\nequal to h. We gather in Tables 1 and 2 the optimal lags for two values of H and various n. We\nprovide them for h = 1. Multiplying them by h gives the optimal time lags for a forecast horizon\nequal to h.\nAnother question is important for de\ufb01ning a systematic trading strategy: what is the best between\nde\ufb01ning each price return on the one hand between t \u2212\u03b4i and t \u2212\u03b4i\u22121 and, on the other hand, be-\ntween t\u2212\u03b4i and t? It is worth noting that if we compare the two vectors [Rt\u2212\u03b41,t\u2212\u03b40 ... Rt\u2212\u03b4n,t\u2212\u03b4n\u22121]T\nand [Rt\u2212\u03b41,t ... Rt\u2212\u03b4n,t]T , the weight vector [\u03b21 ... \u03b2n]T associated to each vector of price returns\nis di\ufb00erent, but the accuracy of the corresponding predictor, de\ufb01ned as the non-conditional hit\nratio, is the same. Consequently, the optimal lags are also the same between the two versions.\nWhatever the way one splits the time lags among the di\ufb00erent returns, provided that there is no\nredundancy, the non-conditional quality of the forecast will be the same. All the information used\n10\n\nn\n\u03b4\u22c6\n1\n\u03b4\u22c6\n2\n\u03b4\u22c6\n3\n\u03b4\u22c6\n4\n\u03b4\u22c6\n5\n\u03b4\u22c6\n6\n\u03c1\n1\n1.000\n57.42%\n2\n0.289\n3.454\n58.14%\n3\n0.127\n1.000\n7.896\n58.72%\n4\n0.067\n0.458\n2.185\n14.979\n58.90%\n5\n0.039\n0.253\n1.000\n3.949\n25.407\n58.99%\n6\n0.025\n0.156\n0.562\n1.780\n6.411\n39.919\n59.05%\nTable 1: For various numbers n of lagged returns, optimal time lags\nand corresponding theoretical hit ratio. The dynamic is an fBm of\nHurst exponent 0.65 and the forecast horizon is h = 1.\nn\n\u03b4\u22c6\n1\n\u03b4\u22c6\n2\n\u03b4\u22c6\n3\n\u03b4\u22c6\n4\n\u03b4\u22c6\n5\n\u03b4\u22c6\n6\n\u03c1\n1\n1.000\n62.56%\n2\n0.367\n2.726\n64.34%\n3\n0.193\n1.000\n5.168\n65.72%\n4\n0.120\n0.539\n1.856\n8.365\n66.29%\n5\n0.081\n0.341\n1.000\n2.933\n12.347\n66.66%\n6\n0.058\n0.236\n0.637\n1.570\n4.241\n17.170\n66.91%\nTable 2: For various numbers n of lagged returns, optimal time lags\nand corresponding theoretical hit ratio. The dynamic is an fBm of\nHurst exponent 0.15 and the forecast horizon is h = 1.\nby the predictor comes in fact from the list of time lags. One must thus concentrate one\u2019s e\ufb00orts\non the selection of \u2206instead of on its speci\ufb01c division as bounds of all the price returns.\n4\nStatistical arbitrage: to predict or not to predict\nThe theoretical results in Section 3, about hit ratios for predictors of an fBm, are encouraging.",
    "chunk_index": 12,
    "start_char": 28248,
    "end_char": 30902,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "corresponding theoretical hit ratio. The dynamic is an fBm of\nHurst exponent 0.15 and the forecast horizon is h = 1.\nby the predictor comes in fact from the list of time lags. One must thus concentrate one\u2019s e\ufb00orts\non the selection of \u2206instead of on its speci\ufb01c division as bounds of all the price returns.\n4\nStatistical arbitrage: to predict or not to predict\nThe theoretical results in Section 3, about hit ratios for predictors of an fBm, are encouraging.\nIndeed, many values of the Hurst exponent lead to an acceptable hit ratio in the perspective of\nbuilding a trading strategy. However a more precise look at the predicted increments show some\nforecasts close to zero which may a\ufb00ect the performance of a trading strategy for two reasons.\nFirst, the smaller the predictions in absolute value, the higher the incertitude about the sign of\nthe future return. The second reason is almost a tautology: the smaller the future price returns of\nan asset in absolute value, the lower the \ufb01nancial performance a trader can expect by following an\ninvestment strategy using this asset. Because of these two reasons, a trader may want to discard\npredictions which are close to zero. We are thus interested in understanding how the quality of\nthe forecast evolves when one applies a thresholding to the predictor introduced in equation (4).\nNext, we want not to limit our analysis to hit ratios but to introduce more \ufb01nancial criteria, such\nas expected performance and risk of a simple trading strategy based on the thresholding concept.\nThis strategy consists in buying the asset if the predicted return is signi\ufb01cantly above zero, short\nselling the asset if it is signi\ufb01cantly below, and keeping a neutral position if the prediction is too\nclose to zero for the trader to be con\ufb01dent in the predicted sign of the future return.\n11\n\n4.1\nThreshold and hit ratio\nWe still suppose that log-prices follow an fBm. Thanks to this assumption, we can build predictors\nfollowing equation (4), in which we can input a number n of lagged observed returns. We want\nto determine the theoretical forecasting performance of such a predictor when one discards the\npredicted values which are close to zero. In other words, one de\ufb01nes a threshold \u03b8 \u22650 and ignores\nthe predictions lower than \u03b8 in absolute value. We thus introduce a thresholding function\nf\u03b8 : x \u2208R 7\u2192x1{|x|\u2265\u03b8},\nso that the considered predictor is now f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\n.\nThe hit ratio we used in Section 3 is not appropriate in this new framework. Indeed, the hit ratio\nonly supposes two states: on the one hand, the good prediction of the sign of the future return\nand, on the other hand, the bad prediction. We now use a ternary classi\ufb01cation of predictions:\ngood, bad, and zero prediction. The statistics depicting the forecast accuracy of the new predictor\ncan be based on the probability of each of these three states.\nDe\ufb01nition 4.",
    "chunk_index": 13,
    "start_char": 30444,
    "end_char": 33321,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "is not appropriate in this new framework. Indeed, the hit ratio\nonly supposes two states: on the one hand, the good prediction of the sign of the future return\nand, on the other hand, the bad prediction. We now use a ternary classi\ufb01cation of predictions:\ngood, bad, and zero prediction. The statistics depicting the forecast accuracy of the new predictor\ncan be based on the probability of each of these three states.\nDe\ufb01nition 4. Let \u02c6Rt,t+h|\u2206be a predictor of Rt,t+h based on the vector S of past returns, as de\ufb01ned\nby equation (4).\nGiven \u03b8 \u22650, we transform \u02c6Rt,t+h|\u2206in a new predictor f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\n.\nThe\nnon-conditional probability of good sign forecast is\np+(\u03b8) = P\nh\nf\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\nRt,t+h > 0\ni\n,\nthe non-conditional probability of bad sign forecast is\np\u2212(\u03b8) = P\nh\nf\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\nRt,t+h < 0\ni\n,\nand the non-conditional probability of zero forecast is\np0(\u03b8) = P\nh\nf\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\n= 0\ni\n.\nWe stress the fact that we are only interested here in non-conditional probabilities. Taking into\naccount conditional probabilities may lead to better predictors, in which more appropriate weights\nare used for lagged returns, and thus to a better performance.\nNevertheless, the conditional\napproach is very speci\ufb01c to the data observed and is a useless re\ufb01nement in our perspective. The\npurpose of this paper is indeed not the \ufb01ne tuning of a trading strategy but, instead, it is about\nsetting general results concerning the relevance of fBm-based predictions in \ufb01nance.\nThe formula for the probabilities introduced in De\ufb01nition 4 are expressed in Theorem 2, in which\nthe fBm assumption for log-prices is used.\nTheorem 2. Let Xt be an fBm of Hurst exponent H \u2208(0, 1/2) \u222a(1/2, 1) and \u03b8 \u22650. Let h > 0 and\n\u02c6Rt,t+h|\u2206be a predictor of Rt,t+h based on the vector S of past returns, as de\ufb01ned by equation (4).\nThe non-conditional probabilities of sign forecast, as introduced in De\ufb01nition 4, are provided by a\nTaylor expansion, when \u03b8 is in the neighbourhood of 0, for p+(\u03b8):\np+(\u03b8) = 1 \u2212N\n\u0012\u03b8\na\n\u0013\n+ 1\n\u03c0 arctan\n\u0010a\nb\n\u0011\n\u2212\n\u03b82\n2\u03c0ab +\n\u0012 1\nab3 + 3\na4b\n\u0013 \u03b84\n24\u03c0 + O(\u03b86)\nand for p\u2212(\u03b8):\np\u2212(\u03b8) = 1 \u2212N\n\u0012\u03b8\na\n\u0013\n\u22121\n\u03c0 arctan\n\u0010a\nb\n\u0011\n+\n\u03b82\n2\u03c0ab \u2212\n\u0012 1\nab3 + 3\na4b\n\u0013 \u03b84\n24\u03c0 + O(\u03b86),\n12\n\nas well as by the following exact formula for p0(\u03b8):\np0(\u03b8) = \u22121 + 2N\n\u0012\u03b8\na\n\u0013\n,\nwhere a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, b =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, and where \u03a3S and \u03a3RS are the same as\nin Proposition 4.\nThe proof of Theorem 2 is postponed in Appendix C.\nTheorem 2 provides only Taylor expansions of p+(\u03b8) and p\u2212(\u03b8), whereas we have an exact formula\nfor p0(\u03b8). However, beyond the expansions, we can describe how p+(\u03b8) and p\u2212(\u03b8) evolve with \u03b8,\nwhatever H.",
    "chunk_index": 14,
    "start_char": 32891,
    "end_char": 35491,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "p0(\u03b8) = \u22121 + 2N\n\u0012\u03b8\na\n\u0013\n,\nwhere a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, b =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, and where \u03a3S and \u03a3RS are the same as\nin Proposition 4.\nThe proof of Theorem 2 is postponed in Appendix C.\nTheorem 2 provides only Taylor expansions of p+(\u03b8) and p\u2212(\u03b8), whereas we have an exact formula\nfor p0(\u03b8). However, beyond the expansions, we can describe how p+(\u03b8) and p\u2212(\u03b8) evolve with \u03b8,\nwhatever H. Indeed, both p+ and p\u2212are trivially decreasing functions. Moreover, p+ decreases\nmore rapidly than p\u2212, as stated in Proposition 5.\nProposition 5. With the assumptions of Theorem 2, for \u03b8 \u22650 we have:\ndp+\nd\u03b8 (\u03b8) \u2264dp\u2212\nd\u03b8 (\u03b8) \u22640.\nProof. We remark from equations (12) and (13), in the proof of Theorem 2, that the derivative of\np+ and p\u2212is:\n\u001a\nd\nd\u03b8p+(\u03b8)\n=\n\u22122\naN\n\u0000 \u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\nd\nd\u03b8p\u2212(\u03b8)\n=\n\u22122\na\n\u00001 \u2212N\n\u0000 \u03b8\nb\n\u0001\u0001\ng\n\u0000 \u03b8\na\n\u0001\n.\nThe two derivatives are negative and the ratio of the second one over the \ufb01rst one is N(\u03b8/b)\u22121 \u22121,\nwhich is in the interval [0, 1] for \u03b8 \u22650, meaning that the derivative of p+ with respect to \u03b8 \u22650 is\ngreater in absolute value than the one of p\u2212.\nIn Theorem 2, when the threshold is \u03b8 = 0, the zero forecast probability is p0(0) = 0.\nThis\nparticular case corresponds to the binary classi\ufb01cation of predictions of Section 3.\nThe non-\nconditional hit ratio \u03c1 introduced in De\ufb01nition 3 corresponds to p+(0), whose formula in Theorem 2\nmatches the formula of \u03c1 in Theorem 1.\nThe greater the threshold \u03b8, the greater the value of p0(\u03b8), whose limit, when \u03b8 \u2192+\u221e, is 1. As a\nconsequence, if \u03b8 \u0338= 0, we do not have a binary classi\ufb01cation between good and bad forecast but a\nternary one, the third state being the zero prediction because of the thresholding. The hit ratio is\nthus inappropriate. One can think of several alternatives exploiting p+(\u03b8), p\u2212(\u03b8), and p0(\u03b8).\nThe \ufb01rst natural alternative to the hit ratio consists in considering the proportion of good predic-\ntions with respect to the non-zero predictions, that is p+(\u03b8)/(p+(\u03b8) + p\u2212(\u03b8)). This ratio converges\nasymptotically towards its maximal value, 1. Indeed, from equations (12) and (13) in the proof of\nTheorem 2, we have p+(\u03b8) + p\u2212(\u03b8) = 2N(\u2212\u03b8/a) and\np+(\u03b8)\np+(\u03b8)+p\u2212(\u03b8)\n=\n1\n2 +\n1\n2N(\u2212\u03b8/a)\nR +\u221e\n\u03b8/a\n\u0000N\n\u0000 a\nb u\n\u0001\n\u2212N\n\u0000\u2212a\nb u\n\u0001\u0001\ng(u)du\n\u03b8\u2192+\u221e\n\u223c\n1\n2 +\n1\n2N(\u2212\u03b8/a)\nR +\u221e\n\u03b8/a g(u)du\n=\n1\n2 +\n1\n2N(\u2212\u03b8/a) (1 \u2212N(\u03b8/a))\n=\n1.\nIn a \ufb01nancial perspective, this accuracy metric is not suitable, because maximizing it leads to\nselecting \u03b8 = +\u221eand thus to discarding all the predictions.\nIn order to penalize high values of the zero forecast probability, on can consider the alternative\nratio p+(\u03b8)/(p+(\u03b8) + p\u2212(\u03b8) + p0(\u03b8)) = p+(\u03b8), but it is monotonically decreasing in \u03b8, as stated by\n13",
    "chunk_index": 15,
    "start_char": 35103,
    "end_char": 37728,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "a\nb u\n\u0001\n\u2212N\n\u0000\u2212a\nb u\n\u0001\u0001\ng(u)du\n\u03b8\u2192+\u221e\n\u223c\n1\n2 +\n1\n2N(\u2212\u03b8/a)\nR +\u221e\n\u03b8/a g(u)du\n=\n1\n2 +\n1\n2N(\u2212\u03b8/a) (1 \u2212N(\u03b8/a))\n=\n1.\nIn a \ufb01nancial perspective, this accuracy metric is not suitable, because maximizing it leads to\nselecting \u03b8 = +\u221eand thus to discarding all the predictions.\nIn order to penalize high values of the zero forecast probability, on can consider the alternative\nratio p+(\u03b8)/(p+(\u03b8) + p\u2212(\u03b8) + p0(\u03b8)) = p+(\u03b8), but it is monotonically decreasing in \u03b8, as stated by\n13\n\nProposition 5, so that the optimal threshold would be \u03b8 = 0, whatever H. This solution thus does\nnot address the problem introduced in this section about discarding the less certain predictions.\nAlternatively, p+(\u03b8) \u2212p\u2212(\u03b8) is a signed forecast accuracy metric. This metric corresponds to the\naverage number of coins earned in a game in which one earns one coin for a good forecast, loses\none for a bad one, and there is no coin transaction if one restrains from playing this forecasting\ngame. According to Proposition 5, this ratio is monotonically decreasing in \u03b8, like the previous\nratio.\nAll the natural extensions of the hit ratio we can think of are inappropriate insofar as they lead to an\noptimal threshold \u03b8 either equal to 0 or to in\ufb01nity. We propose in the next subsection an alternative\naccuracy metric more consistent with a \ufb01nancial application. Indeed, all the extended hit ratios\nproposed above only rely on probabilities of making a good, a bad, or a small forecast. In particular,\nthey do not consider the amplitude of the predicted price returns. In an investment perspective,\nforecasting properly large price returns is though more fruitful than forecasting properly small\nones.\n4.2\nOptimal threshold and risk-adjusted performance\nWe now consider the \ufb01nancial performance corresponding to a prediction. A trader is more eager\nto forecast accurately large price variations than small ones.\nTherefore, the trader\u2019s objective\nis not minimizing an MSE or maximizing a hit ratio, but maximizing instead a risk-adjusted\nperformance of an investment based on these predictions. A proper evaluation of the predictor\nmust indeed weight the 0-1 contribution of each prediction, which appears in the hit ratio, with\nthe amplitude of the prediction.\nThe trading strategy we study here is the ternary strategy exposed above: buying a \ufb01xed quantity\nof the asset if one predicts a positive price return, short selling the same quantity if one predicts a\nnegative return, not investing if the expected return of the asset is below the threshold \u03b8 in absolute\nvalue. The trading frequency of the strategy is equal to h > 0, the forecast horizon. Using the\nsame notations as above and given an amount of money hold by the trader and totally invested in\nthis strategy, the return at horizon h will thus be:\nRstrat\nt,t+h(\u03b8) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n= 0\nRt,t+h\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n> 0\n\u2212Rt,t+h\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n< 0.",
    "chunk_index": 16,
    "start_char": 37267,
    "end_char": 40165,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "below the threshold \u03b8 in absolute\nvalue. The trading frequency of the strategy is equal to h > 0, the forecast horizon. Using the\nsame notations as above and given an amount of money hold by the trader and totally invested in\nthis strategy, the return at horizon h will thus be:\nRstrat\nt,t+h(\u03b8) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n= 0\nRt,t+h\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n> 0\n\u2212Rt,t+h\nif f\u03b8\n\u0010\n\u02c6Rt,t+h|\u2206|\n\u0011\n< 0.\nThis return represents the performance to be maximized. At time t, one only knows an expectation\nof this return:\neRt,t+h(\u03b8) = E[Rstrat\nt,t+h(\u03b8)].\n(7)\nRegarding the risk statistic of this investment, we consider a lower absolute semi-deviation risk\nmeasure. It is de\ufb01ned as the average absolute deviation below zero of the return of the strategy,\nin other words it is the average loss:\ne\u03c3\u2212\nt,t+h(\u03b8) = \u2212E\n\u0002\nmin(0, Rstrat\nt,t+h(\u03b8))\n\u0003\n.\n(8)\nSuch a risk measure is less widespread in the asset management industry than the volatility, but it\nis more relevant since it does not incorporate positive deviations which, by de\ufb01nition, are not risky.\nThis risk measure appears for example at the denominator of the kappa ratio of order one, which\nis a general risk-performance ratio including the popular Sharpe ratio as a particular case [40].\nTheorem 3 provides a formula for eRt,t+h(\u03b8) and for e\u03c3\u2212\nt,t+h(\u03b8) when the log-prices follow an fBm.\n14\n\nTheorem 3. Let Xt be an fBm of parameters H \u2208(0, 1/2) \u222a(1/2, 1) and \u03c3 > 0. Let h > 0 and\n\u02c6Rt,t+h|\u2206be a predictor of Rt,t+h based on the vector S of past returns, as de\ufb01ned by equation (4).\nGiven \u03b8 \u22650, the average return of the ternary strategy, eRt,t+h(\u03b8), and the corresponding lower\nabsolute semi-deviation risk measure, e\u03c3\u2212\nt,t+h(\u03b8), de\ufb01ned in equations (7) and (8), are equal to:\neRt,t+h(\u03b8) = 2ag\n\u0012\u03b8\na\n\u0013\nand\ne\u03c3\u2212\nt,t+h(\u03b8) = \u22122aN\n\u0012\n\u2212\u03b8\nb\n\u0013\ng\n\u0012\u03b8\na\n\u0013\n+\nr\n2\n\u03c0 \u03c3hHN\n \n\u2212\u03b8\nr\n1\na2 + 1\nb2\n!\n,\nwhere a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, b =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, and g is the Gaussian probability density\nfunction.\nThe proof of Theorem 3 is postponed in Appendix D.\nIn the case where \u03b8 = 0, that is when the trader does not discard any forecast, the performance\nand the risk of the strategy are simpli\ufb01ed:\n\uf8f1\n\uf8f2\n\uf8f3\neRt,t+h(0)\n=\na\nq\n2\n\u03c0\ne\u03c3\u2212\nt,t+h(0)\n=\n\u03c3hH\u2212a\n\u221a\n2\u03c0 ,\nin which we recognize, for eRt,t+h(0), the expected value of a|U|, where U \u223cN(0, 1). Both eRt,t+h(\u03b8)\nand e\u03c3\u2212\nt,t+h(\u03b8) are monotonically decreasing in \u03b8: discarding predictions tends to diminish both the\nexpected gain and the risk. At the limit, when \u03b8 \u2192+\u221e, both eRt,t+h(\u03b8) and e\u03c3\u2212\nt,t+h(\u03b8) tend to\nzero: the threshold is so high that no investment decision is made by the trader.",
    "chunk_index": 17,
    "start_char": 39758,
    "end_char": 42324,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "t,t+h(0)\n=\n\u03c3hH\u2212a\n\u221a\n2\u03c0 ,\nin which we recognize, for eRt,t+h(0), the expected value of a|U|, where U \u223cN(0, 1). Both eRt,t+h(\u03b8)\nand e\u03c3\u2212\nt,t+h(\u03b8) are monotonically decreasing in \u03b8: discarding predictions tends to diminish both the\nexpected gain and the risk. At the limit, when \u03b8 \u2192+\u221e, both eRt,t+h(\u03b8) and e\u03c3\u2212\nt,t+h(\u03b8) tend to\nzero: the threshold is so high that no investment decision is made by the trader.\nIn order to \ufb01nd a good balance between performance and risk, we de\ufb01ne an expected risk-adjusted\nperformance, which simply consists in the expected return of the strategy, penalized by the corre-\nsponding risk:\neR\u03bb(\u03b8) = eRt,t+h(\u03b8) \u2212\u03bbe\u03c3\u2212\nt,t+h(\u03b8).\n(9)\nIn equation (9), we omit the subscript in t and t + h for the risk-adjusted performance eR\u03bb(\u03b8), in\norder not to overload the notations. But one has to keep in mind that eR\u03bb(\u03b8) depends on h. The\nparameter \u03bb in equation (9) plays the role of a risk aversion when its value is positive. A negative\nvalue of \u03bb would indicate a risk-seeking behaviour. In particular, if \u03bb = \u22121, eR\u03bb(\u03b8) is simply the\nupper absolute semi-deviation, E[max(0, Rstrat\nt,t+h(\u03b8))]. In what follows, we focus on the case \u03bb \u22650.\nBecause of the particular risk measure we have chosen, the risk-adjusted performance corresponds\nto the expected performance of the strategy in a distorted probability in which bad outcomes, that\nis to say negative returns, are overweighted. Indeed,\neR\u03bb(\u03b8) =\nZ 1\n0\nQRstrat\nt,t+h(\u03b8)(p)d\u00b5(p),\nwhere QZ(p) is the quantile of probability p of a variable Z and \u00b5 is the probability distortion\nfunction de\ufb01ned by\n\u00b5 : x \u2208[0, 1] 7\u2212\u2192\n(\n(1+\u03bb)x\n1+\u03bbp\u2212(\u03b8)\nif x < p\u2212(\u03b8)\nx+\u03bbp\u2212(\u03b8)\n1+\u03bbp\u2212(\u03b8)\nelse.\nIn particular, if \u03bb = 1, the probability distortion doubles the probability of negative returns,\nrelatively to the probability of positive returns.\n15\n\nFigure 3 displays the risk-adjusted performance for various values of risk aversion \u03bb and threshold\n\u03b8. Since both the expected return and the risk measure tend toward zero when \u03b8 \u2192+\u221e, eR\u03bb(\u03b8) also\ntends toward zero for high threshold values. When \u03bb = 0, the maximum risk-adjusted performance\nis reached for \u03b8 = 0. For \u03bb > 0, we observe a global maximum for a value \u03b8 > 0. This threshold\nis important insofar as it constitutes an optimal threshold for a trader whose aim would be the\nmaximization of the risk-adjusted performance:\n\u03b8\u22c6\n\u03bb = argmax\n\u03b8\u22650\neR\u03bb(\u03b8).\nFigure 3: Theoretical penalized expected return eR\u03bb(\u03b8) as a function\nof the threshold \u03b8, for various values of risk aversion \u03bb, namely, from\nfatter to thinner, 0, 0.25, 0.5, 0.75, 1. The fBm has the parameters\nH = 0.65 and \u03c3 = 1. The forecast horizon is h = 1 and the number\nof lagged returns in the predictor is n = 1.\nFigure 4 illustrates the link between \u03bb and \u03b8\u22c6\n\u03bb. The higher the risk aversion \u03bb, the higher the\noptimal threshold \u03b8\u22c6\n\u03bb. Moreover, a higher threshold means that the decision not to invest is more\nfrequent.",
    "chunk_index": 18,
    "start_char": 41921,
    "end_char": 44784,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "values of risk aversion \u03bb, namely, from\nfatter to thinner, 0, 0.25, 0.5, 0.75, 1. The fBm has the parameters\nH = 0.65 and \u03c3 = 1. The forecast horizon is h = 1 and the number\nof lagged returns in the predictor is n = 1.\nFigure 4 illustrates the link between \u03bb and \u03b8\u22c6\n\u03bb. The higher the risk aversion \u03bb, the higher the\noptimal threshold \u03b8\u22c6\n\u03bb. Moreover, a higher threshold means that the decision not to invest is more\nfrequent. Figure 4 thus shows p0(\u03b8\u22c6\n\u03bb) as an increasing function of \u03bb.\nWe also observe that the optimal threshold strongly depends on the value of the Hurst exponent,\nas displayed in Figure 5. In particular, it is asymmetric with respect to H = 1/2. Whatever the\nrisk aversion, the highest optimal thresholds are reached for Hurst exponents close to 1/2, but\nthe optimal threshold decreases toward 0 when H is close to 1, whereas a threshold signi\ufb01cantly\ndi\ufb00erent from 0 is indicated when H is close to 0. This shows a relatively higher di\ufb03culty to make\n\ufb01nancially performing predictions when H < 1/2 than when H > 1/2. Figure 5 also shows p0(\u03b8\u22c6\n\u03bb)\nas a function of the Hurst exponent. This zero forecast probability reaches huge values when H\nis close to 1/2. For instance, for the risk aversion considered, \u03bb = 0.1, p0(\u03b8\u22c6\n\u03bb) is twice as large for\nH = 0.55 (40%) than for H = 0.6 (20%).\nGuasoni and co-authors provide an fBm-based risk-adjusted performance which di\ufb00ers from our\napproach for several reasons: their risk is a variance, whereas ours is a lower absolute semi-\ndeviation, and they consider trading in continuous time, whereas we more realistically work in\ndiscrete time [31, 30]. They notice an asymmetry of their risk-adjusted performance with respect\nto H = 1/2. While the shape of our metric is obviously di\ufb00erent from theirs, we also obtain an\nasymmetry of our risk-adjusted performance, as illustrated by Figure 6: the worst risk-adjusted\n16\n\nFigure 4: Theoretical optimal threshold \u03b8\u22c6\n\u03bb (left) and corresponding\nnon-conditional zero forecast probability p0(\u03b8\u22c6\n\u03bb) (right) as functions\nof the risk aversion \u03bb. The fBm has the parameters \u03c3 = 1 and, for\nH, respectively from the top curve to the bottom curve, 0.6, 0.7, and\n0.8. The forecast horizon is h = 1 and the number of lagged returns\nin the predictor is n = 1.\nFigure 5: Theoretical optimal threshold \u03b8\u22c6\n\u03bb (left) and corresponding\nnon-conditional zero forecast probability p0(\u03b8\u22c6\n\u03bb) (right) as functions\nof the Hurst exponent H. The fBm has a volatility parameter \u03c3 = 1.\nThe risk aversion is \u03bb = 0.1, the forecast horizon is h = 1, and the\nnumber of lagged returns in the predictor is n = 1.\n17\n\nperformance is for H = 1/2 and the improvement of this metric is stronger for H > 1/2 than\nfor H < 1/2.",
    "chunk_index": 19,
    "start_char": 44360,
    "end_char": 47047,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "functions\nof the Hurst exponent H. The fBm has a volatility parameter \u03c3 = 1.\nThe risk aversion is \u03bb = 0.1, the forecast horizon is h = 1, and the\nnumber of lagged returns in the predictor is n = 1.\n17\n\nperformance is for H = 1/2 and the improvement of this metric is stronger for H > 1/2 than\nfor H < 1/2. In the case where n = 1 and with an optimal lag \u03b41 = h, we obtain, thanks to\nequation (5),\na\n=\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n=\np\n\u03b22\n1\u03c32\u03b42H\n1\n=\n\f\f\u03b21\u03c3\u03b4H\n1\n\f\f\n=\n\u03c3hH \f\f22H\u22121 \u22121\n\f\f ,\nand \ufb01nally the risk-adjusted performance without thresholding small predictions is equal to\neR\u03bb(0)\n=\na\n\u0010q\n2\n\u03c0 +\n\u03bb\n\u221a\n2\u03c0\n\u0011\n\u2212\u03bb\u03c3hH\n\u221a\n2\u03c0\n=\n\u03c3hH\n\u221a\n2\u03c0\n\u0002\f\f22H\u22121 \u22121\n\f\f (2 + \u03bb) \u2212\u03bb\n\u0003\n.\nWe see in Figure 6 that introducing the optimal thresholding tends to set a \ufb02oor at 0 for the risk-\nadjusted performance. As a consequence, when H is close to 1/2, eR\u03bb(\u03b8\u22c6\n\u03bb) is close to the performance\nof the strategy without risk adjustment or thresholding, eR0(0). In this case, the thresholding thus\ntends to erase the risk. However, when H is close to 0 or 1, eR\u03bb(\u03b8\u22c6\n\u03bb) is close to eR\u03bb(0) and the e\ufb00ect\nof the thresholding is not signi\ufb01cant.\nFigure 6: Theoretical penalized expected return eR0(0) (black line),\neR\u03bb(0) (continuous grey line), and eR\u03bb(\u03b8\u22c6\n\u03bb) (dotted grey line), as a\nfunction of the Hurst exponent, for \u03bb = 0.1 (left) and \u03bb = 0.5 (right).\nThe fBm has a volatility parameter \u03c3 = 1, the forecast horizon is\nh = 1 and the number of lagged returns in the predictor is n = 1.\n5\nEmpirical application\nWe now apply the theoretical results exposed above to two kinds of samplings of \ufb01nancial time\nseries: daily sampling of various \ufb01nancial dynamics and high-frequency prices of foreign exchange\nrates. With daily sampled data, we simply check whether the fBm is a relevant model and we\ncompare the forecasting performance of this model with an autoregressive (AR) model. However,\nthe daily sampling does not make it possible to optimize the time lags in the forecasting procedure\nas detailed in Section 3.3. For this purpose, we then use intraday prices and analyse the extent to\nwhich the systematic strategy exposed in Section 4 is performing.\n18\n\n5.1\nDaily sampling: fBm vs autoregressive model\nWe want to see that the fBm is appropriate for various \ufb01nancial series with a daily time step. We\n\ufb01rst brie\ufb02y focus on series of prices of some assets and then we present a more promising extension\nto series of realized volatilities.\n5.1.1\nDaily series of prices\nRegarding the price series, we consider two stock indices, the CAC 40 index and the S&P 500 index\n(SPX), as well as one FX rate, the GBPUSD. The series of daily prices starts in January 2000 for\nthe two stock indices and in December 2003 for the GBPUSD.",
    "chunk_index": 20,
    "start_char": 46742,
    "end_char": 49410,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "series with a daily time step. We\n\ufb01rst brie\ufb02y focus on series of prices of some assets and then we present a more promising extension\nto series of realized volatilities.\n5.1.1\nDaily series of prices\nRegarding the price series, we consider two stock indices, the CAC 40 index and the S&P 500 index\n(SPX), as well as one FX rate, the GBPUSD. The series of daily prices starts in January 2000 for\nthe two stock indices and in December 2003 for the GBPUSD. They all end the 12th April 2021.\nWe want to determine if the fBm-based predictor introduced in Proposition 4 is accurate for the\ntime series of the logarithm of the prices, compared to a predictor based on an AR model. We\nwant to make a one-day forecast using daily increments of log-prices, namely n lagged observations,\nfor n varying between one up to six days. In particular, we do not try to optimize the time lags,\ncontrary to the developments of Section 3.3, in order to make a fair comparison between fBm and\nAR models. For each value of n, the fBm-based forecast is the weighted sum of past increments, as\nin equation (4). The weights depend both on n and on H, the Hurst exponent. For this purpose,\nwe need to estimate a Hurst exponent at each date. We use an estimator exploiting the scaling\nproperties of the fBm in a rolling window of size T, without regularizing the obtained time-varying\nHurst exponent [15, 8, 22]1. More precisely, if we note pt the series of log-prices, supposed to\nfollow an fBm of parameters H and \u03c3 in a speci\ufb01c sub-sample, any increment pt \u2212pt\u2212\u03c4 of duration\n\u03c4 > 0 contained in the sub-sample has a variance \u03c32|\u03c4|2H. Therefore, comparing the empirical\nlog-variance of increments of two distinct durations, \u03c41 and \u03c42, leads to a simple estimator of the\nHurst exponent using data in the interval [t \u2212T, t]:\nbHt =\n1\n2 log(\u03c41/\u03c42) log\n \n(T \u2212\u03c42) PT \u2212\u03c41\ni=0\n(pt\u2212i \u2212pt\u2212i\u2212\u03c41)2\n(T \u2212\u03c41) PT \u2212\u03c42\ni=0\n(pt\u2212i \u2212pt\u2212i\u2212\u03c42)2\n!\n.\n(10)\nFor the three daily series, the fBm-based forecast outperforms the forecast with the AR model, if\nwe consider information criteria, such as AIC or BIC. However, the hit ratio for both methods is\nvery close to 50%, whatever the lag. We assess the signi\ufb01cance of the hit ratio with respect to 50%\nthrough a binomial test. If we consider three one-day time lags, the hit ratio is 51.5% (signi\ufb01cantly\nabove 50% with a con\ufb01dence of 98.3%) for the fBm and 51.8% (conf. 99.4%) for the AR model,\nfor the SPX series. For the CAC series, the hit ratio is 52.1% (conf. 99.8%) for the fBm and\n50.8% (conf. 86.7%) for the AR model. For the GBPUSD series, the signal is not signi\ufb01cantly\ndi\ufb00erent from noise, with a hit ratio of 50.2% for the fBm and 50.1% for the AR model. We will\nsee in Section 5.2 how such low hit ratios can be improved by optimizing the time lags and thus\nby considering intraday data.",
    "chunk_index": 21,
    "start_char": 48958,
    "end_char": 51754,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "98.3%) for the fBm and 51.8% (conf. 99.4%) for the AR model,\nfor the SPX series. For the CAC series, the hit ratio is 52.1% (conf. 99.8%) for the fBm and\n50.8% (conf. 86.7%) for the AR model. For the GBPUSD series, the signal is not signi\ufb01cantly\ndi\ufb00erent from noise, with a hit ratio of 50.2% for the fBm and 50.1% for the AR model. We will\nsee in Section 5.2 how such low hit ratios can be improved by optimizing the time lags and thus\nby considering intraday data. However, some \ufb01nancial time series lead to high hit ratios even with\na daily sampling. It is the case for example of the series of realized volatilities, which we study in\nthe next section.\n5.1.2\nRough volatility\nThe use of the fBm in \ufb01nance is not limited to log-prices and one of the most popular applications\nof the fBm is for volatility modelling. Even though accuracy metrics directly related to the perfor-\nmance and risk of a trading strategy, as developed in Section 4.2, are not appropriate in this case,\n1Precisely, in our empirical analysis, T = 504 business days.\n19\n\nforecasting a volatility is also useful in algorithmic trading, and one can properly use the hit ratio\nto evaluate the quality of the fBm-based forecast. If the idea of depicting the volatility with the\nfBm is not new [16], the recent paradigm is rough volatility, that is the fBm used for modelling\nvolatility has a very low Hurst exponent, close to 0.15, so that the series of volatility is strongly\nantipersistent [3, 28]. This model seems to currently dominate the research on stochastic volatility\nand leads to a rich literature, regarding for example some simpli\ufb01cations of the model to make it\ncomputationally more e\ufb03cient [1, 2], or some re\ufb01nements to make it more realistic [26]. We also\nstress the fact that the rough framework relies on empirical observations of scaling rules of volatil-\nity increments and not on the analysis of the dependence between distant increments of volatility.\nThis analysis of the long-range dependence seems to reveal a slowly decaying correlation [14], so\nthat modelling volatility with a Hurst exponent higher than 1/2 may also be relevant in certain\ncases [27].\nThe data used in our analysis are daily realized volatilities computed with a \ufb01ve-minute discretiza-\ntion of prices, imported from the Oxford-Man Institute of Quantitative Finance Realized Library\n2. We focus on the realized volatility of eight stock indices: the AEX index, the CAC 40 index, the\nFTSE 100 index, the Nasdaq 100 index (IXIC), the Nikkei 225 index (N225), the Oslo Exchange\nAll-share index (OSEAX), the Madrid General index (SMSI), and the S&P 500 index. The series\nstarts on January 2000, except N225, which starts in February 2000, OSEAX in September 2001\nand SMSI in July 2005. The end date of our sample is on the 12th April 2021.\nLike for the series of prices above, we want to compare fBm-based predictions of log-volatilities\nwith the AR model.",
    "chunk_index": 22,
    "start_char": 51288,
    "end_char": 54206,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "100 index, the Nasdaq 100 index (IXIC), the Nikkei 225 index (N225), the Oslo Exchange\nAll-share index (OSEAX), the Madrid General index (SMSI), and the S&P 500 index. The series\nstarts on January 2000, except N225, which starts in February 2000, OSEAX in September 2001\nand SMSI in July 2005. The end date of our sample is on the 12th April 2021.\nLike for the series of prices above, we want to compare fBm-based predictions of log-volatilities\nwith the AR model. The Hurst exponent, for the fBm approach, is estimated like in equation (10),\nin which the log-price pt is to be replaced by the logarithm of the realized volatility. We display the\nestimated time-varying Hurst exponent of the realized log-volatility of SPX in Figure 7. We observe\nthat, consistently with the literature on rough volatility, bHt is mainly in the interval [0.05, 0.25].\nFigure 7: Estimated time-varying Hurst exponent of the realized log-\nvolatility of SPX.\nBefore presenting the accuracy of the forecast with the fBm approach for the log-volatilities series,\nwe give more details about the benchmark alternative predictor. The alternative approach is an\nAR model with Gaussian residuals and with the same number n of lagged daily increments of log-\nvolatility as in the fBm predictor. The two models are estimated in the same rolling windows. The\n2Available at http://realized.oxford-man.ox.ac.uk/data/download.\n20\n\nfBm and the AR models are very close since both are Gaussian and make linear forecast. The only\ndi\ufb00erence is that the weights are not calculated in the same way. For the AR model, each weight\nis a parameter. When increasing n, we add new parameters, which will maximize the in-sample\naccuracy. For the fBm approach, the n weights all depend on the parameters H and \u03c3 of the fBm,\nso that increasing the number of lags will not increase the number of parameters. This last model\nis thus more parsimonious and less subject to over\ufb01tting than the AR approach. In other words,\nwe expect the AR model to be more accurate in sample and the fBm to have better forecasting\nability, at least when n is big enough. The accuracy metric we present below will con\ufb01rm this\nintuition.\nWe determine the likelihood of each model in sample. Logically, this likelihood, calculated on\nthe estimation set, is higher for the AR model. But this is not a relevant indicator of forecasting\nability. One often prefers using information criteria, such as AIC and BIC, which take into account,\nnegatively, the log-likelihood and, positively, the number of parameters as well as, for the BIC, the\nnumber of data used for the estimation. Since the fBm model has less parameters than the AR\nmodel, it is less penalized and we can expect to have a lower AIC and BIC for it.\nEmpirical results con\ufb01rm the intuition in the sense that the fBm predictor has always a lower AIC\nwhen n = 6, as one can see in Table 33. But the results are even stronger.",
    "chunk_index": 23,
    "start_char": 53742,
    "end_char": 56653,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "as well as, for the BIC, the\nnumber of data used for the estimation. Since the fBm model has less parameters than the AR\nmodel, it is less penalized and we can expect to have a lower AIC and BIC for it.\nEmpirical results con\ufb01rm the intuition in the sense that the fBm predictor has always a lower AIC\nwhen n = 6, as one can see in Table 33. But the results are even stronger. Indeed, when n = 1,\nthat is when the variation of log-volatility is forecast only from the increment observed the last\nday, the fBm predictor has a slight advantage over the AR one, except for IXIC. We recall that the\nvalue of the information criterion cannot be interpreted in absolute value but that it is only useful\nfor comparisons: for a same dataset, the best model is the one with the lowest criterion. For SPX,\nit seems from Figure 8, that the optimal number of lagged observations for the AR predictor is\nn = 4, at least regarding the BIC, whereas it is n \u22656 for the fBm predictor. Indeed, adding more\nlagged observations does not increase the number of parameters only in the fBm predictor.\nindex\nfBm(1)\nfBm(6)\nAR(1)\nAR(6)\nAEX\n875.9\n812.9\n877.7\n823.6\nCAC 40\n795.0\n734.8\n796.7\n742.1\nFTSE 100\n886.4\n826.8\n888.0\n837.7\nIXIC\n920.1\n856.6\n916.8\n861.2\nN225\n770.9\n716.8\n772.8\n727.4\nOSEAX\n937.5\n838.2\n939.1\n845.5\nSMSI\n1025.4\n931.1\n1027.4\n940.6\nSPX\n933.6\n873.9\n935.4\n880.9\nTable 3: AIC for the eight volatility series, for four di\ufb00erent predic-\ntors: the fBm and the AR model both with 1 and 6 lagged observa-\ntions. In bold is the lowest AIC for each series.\nInformation criteria provide in sample a hint of which model will have the higher out-of-sample\naccuracy. In order to check this intuition, we determine the empirical out-of-sample hit ratio for\neach model. Results for n = 1 and n = 6 are gathered in Table 4. For n = 1, the hit ratio is\nalways higher for the fBm predictor than for the AR one, except for AEX index. For n = 6, the\nfBm is always more accurate than any other model tested in the table. More details are displayed\nin Figure 8 for the SPX. It shows that the di\ufb00erence of accuracy between fBm and AR predictors\nbecomes stronger from n = 3. For both models, adding lagged observation has a strong impact on\nthe accuracy until n = 3. Our recommendation would thus be to use the fBm predictor with at\nleast three lagged observations.\nThis analysis of realized volatility series shows that the fBm provides a good predictor, which is\n3The results with BIC, not displayed in a table, are similar.\n21\n\nFigure 8: On the left: AIC (\ufb01lled dots) and BIC (empty dots) for the\nfBm predictor (black) and the AR one (grey). On the right: out-of-\nsample hit ratio for the fBm predictor (black) and the AR one (grey).",
    "chunk_index": 24,
    "start_char": 56278,
    "end_char": 58977,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "thus be to use the fBm predictor with at\nleast three lagged observations.\nThis analysis of realized volatility series shows that the fBm provides a good predictor, which is\n3The results with BIC, not displayed in a table, are similar.\n21\n\nFigure 8: On the left: AIC (\ufb01lled dots) and BIC (empty dots) for the\nfBm predictor (black) and the AR one (grey). On the right: out-of-\nsample hit ratio for the fBm predictor (black) and the AR one (grey).\nThe number of lagged observations in the input of the two predictors\nis n and the forecast series is the log-volatility of SPX.\nindex\nfBm(1)\nfBm(6)\nAR(1)\nAR(6)\nAEX\n62.4%\n65.8%\n62.5%\n65.7%\nCAC 40\n63.0%\n66.8%\n62.8%\n66.0%\nFTSE 100\n64.8%\n67.5%\n64.3%\n67.4%\nIXIC\n62.2%\n64.6%\n62.1%\n64.3%\nN225\n62.9%\n67.4%\n62.8%\n67.1%\nOSEAX\n64.0%\n67.9%\n63.8%\n67.6%\nSMSI\n61.6%\n64.0%\n61.4%\n63.7%\nSPX\n63.0%\n66.2%\n62.9%\n65.7%\nTable 4: Empirical out-of-sample hit ratio for the eight volatility\nseries, for four di\ufb00erent predictors: the fBm and the AR model both\nwith 1 and 6 lagged observations. In bold is the highest hit ratio for\neach series.\n22\n\nmore accurate than an AR model because of its parsimony. It con\ufb01rms previous \ufb01ndings in the\nliterature, where the superiority of the fBm over the AR model was assessed with the help of\nanother accuracy indicator, namely the MSE [28]. It highlights the relevance of the fBm in \ufb01nance\nin a forecasting perspective.\n5.2\nHigh-frequency FX rates\nWe now focus on time series of FX prices for three pairs: EURGBP, EURUSD, and GBPUSD. We\nconsider high-frequency observations, with one price every minute. The FX market is continuously\nopen during almost one week. We thus focus on one week in order to limit the e\ufb00ects of closing\nmarkets: from the 23rd June 2019 to the 28th June 2019.\nWe transform the price series in a series of log-prices and we assume it follows an fBm. We do\nnot investigate here the relevance of the fBm for modelling FX log-prices, because many papers\nalready deal with this question [11, 39, 22] and also propose some extensions of the fBm to match\nother stylized facts such as stationarity [23], high kurtosis [41, 25, 5], multiscaling [18], or even\nrandomness of the Hurst exponent [9, 25]. We understand that several additional parameters may\nbe relevant to perfectly depict the dynamic of FX log-prices, but, if our reference is the standard\nBrownian motion, the use of the fBm is a good step towards a realistic representation of this\n\ufb01nancial series.\nUsing a 12-hour window and equation (10), we estimate the Hurst exponent every minute for each\nseries, as represented in Figure 9. Then, we forecast the evolution of the log-price at a one-hour\nhorizon. We base this forecast on the formulas provided in Section 3. In particular, we take into\naccount two lagged returns in each forecast. We propose either a naive choice of these lagged\nreturns, or an optimal one.",
    "chunk_index": 25,
    "start_char": 58533,
    "end_char": 61386,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "of this\n\ufb01nancial series.\nUsing a 12-hour window and equation (10), we estimate the Hurst exponent every minute for each\nseries, as represented in Figure 9. Then, we forecast the evolution of the log-price at a one-hour\nhorizon. We base this forecast on the formulas provided in Section 3. In particular, we take into\naccount two lagged returns in each forecast. We propose either a naive choice of these lagged\nreturns, or an optimal one. The naive choice consists in the one-hour and two-hour past returns.\nAlternatively, the optimal choice of lags is theoretically non-conditionally optimal4 and depends\non the Hurst exponent, as exposed in Section 3.3. For example, when H = 0.35, the lagged returns\nconsidered are the last 21-minute and 176-minute returns.\nAs displayed in Table 5, the hit ratios for the EURUSD pair are very close between the naive and\nthe optimal approaches. But for the two other pairs, the optimal lags outperform the naive lags,\nfor which the hit ratio is close to 50%. It thus sounds that using carefully the lags in the predictor\ncan transform an equiprobable coin toss in a performing strategy. It is worth noting that increasing\nthe number of lags, and not only choosing them properly, should also increase the hit ratio and\nmake this predictor more appealing for practitioners.\nWe then implement the ternary strategy described in Section 4. We consider a threshold \u03b8 either\nequal to zero or to the theoretically optimal value determined numerically in Section 4.2 for a given\nrisk aversion. Whatever the FX pair, we observe in Table 5 a sharp decrease of the risk when using\nthe optimal \u03b8. For EURGBP and GBPUSD, it also leads to the highest risk-adjusted performance.\nThe EURUSD case is particular. We noticed the close hit ratios for the two kinds of lags, but the\nnaive approach in fact leads to a better risk-adjusted performance. Three causes can explain this\nsuperiority of the naive approach for this FX pair. First, the optimal choice of lags is intended to\nmaximize a theoretical non-conditional hit ratio, not a risk-adjusted performance. Second, the non-\nconditional framework used to de\ufb01ne optimal lags works in average, but in some cases a conditional\napproach may be preferable. Third and last, our formulas rely on a model, the fBm, which is only\na simpli\ufb01cation of reality, so that a wise practical implementation of the proposed predictor should\nconstantly check the relevance of the model and challenge it with other approaches in order to\nminimize model risk.\n4This optimal threshold is not to be confused with an ex-post optimal threshold.\n23\n\nFigure 9: Estimated time-varying Hurst exponent of the log-prices of\nEURGBP (black), EURUSD (dark grey), GBPUSD (light grey).\nFX rate\nMethod\nReturn\nRisk\nRisk-adj. return\nc\np+\nc\np\u2212\nd\np0\nNaive lags, \u03b8 = 0\n-0.51\n2.47\n-2.97\n49.8%\n50.2%\n0.0%\nEURGBP\nOptimal lags, \u03b8 = 0\n3.66\n2.26\n1.40\n52.9%\n47.1%\n0.0%\nOptimal lags and \u03b8\n3.09\n1.50\n1.59\n30.2%\n26.7%\n43.1%\nNaive lags, \u03b8",
    "chunk_index": 26,
    "start_char": 60948,
    "end_char": 63904,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "optimal threshold is not to be confused with an ex-post optimal threshold.\n23\n\nFigure 9: Estimated time-varying Hurst exponent of the log-prices of\nEURGBP (black), EURUSD (dark grey), GBPUSD (light grey).\nFX rate\nMethod\nReturn\nRisk\nRisk-adj. return\nc\np+\nc\np\u2212\nd\np0\nNaive lags, \u03b8 = 0\n-0.51\n2.47\n-2.97\n49.8%\n50.2%\n0.0%\nEURGBP\nOptimal lags, \u03b8 = 0\n3.66\n2.26\n1.40\n52.9%\n47.1%\n0.0%\nOptimal lags and \u03b8\n3.09\n1.50\n1.59\n30.2%\n26.7%\n43.1%\nNaive lags, \u03b8 = 0\n2.82\n1.85\n0.97\n54.6%\n45.4%\n0.0%\nEURUSD\nOptimal lags, \u03b8 = 0\n1.52\n1.91\n-0.39\n54.3%\n45.7%\n0.0%\nOptimal lags and \u03b8\n1.37\n1.20\n0.17\n34.4%\n29.6%\n36.0%\nNaive lags, \u03b8 = 0\n1.97\n2.42\n-0.45\n50.6%\n49.4%\n0.0%\nGBPUSD\nOptimal lags, \u03b8 = 0\n4.22\n2.31\n1.91\n52.4%\n47.6%\n0.0%\nOptimal lags and \u03b8\n4.58\n1.63\n2.96\n33.5%\n28.8%\n37.7%\nTable 5: Empirical average one-hour return, risk (lower absolute semi-\ndeviation), and risk-adjusted return (with \u03bb = 0.1) multiplied respec-\ntively by 105, 104, and 105. The three columns on the right are the\ncorresponding empirical probabilities of good forecast, bad forecast,\nand zero forecast, following De\ufb01nition 4. As soon as \u03b8 = 0, c\np+ is\nsimply the hit ratio.\n24\n\nNevertheless for the optimal lags, we observe in Table 5, that introducing the optimal threshold\ninstead of \u03b8 = 0 improves the risk-adjusted performance of the strategy for the three pairs, including\nEURUSD. This was precisely the purpose of the thresholding.\nWe also note that the optimal\nthreshold leads not to take an investment decision roughly 40% of the time.\n6\nConclusion\nIn this paper, we have provided theoretical formulas for accuracy metrics related to forecasts of the\nfBm in discrete time. We focused on metrics that we consider to be meaningful for a systematic\ntrader, namely the hit ratio, the average performance and risk. We have used these theoretical\nexpressions to derive optimal time lags and an optimal threshold under which the prediction is\nto be discarded. Finally, we have demonstrated empirically the practical relevance of all these\nconsiderations on two kinds of time series: realized volatilities and high-frequency FX rates. We\nthink that systematic traders should \ufb01nd in this contribution some elements for improving their\nstrategies based on the fBm.\nReferences\n[1] E. Abi Jaber. Lifting the Heston model. Quantitative \ufb01nance, 19(12):1995\u20132013, 2019.\n[2] E. Abi Jaber and O. El Euch. Multifactor approximation of rough volatility models. SIAM\njournal on \ufb01nancial mathematics, 10(2):309\u2013349, 2019.\n[3] E. Al`os, J.A. Le\u00b4on, and J. Vives. On the short-time behavior of the implied volatility for jump-\ndi\ufb00usion models with stochastic volatility. Finance and stochastics, 11(4):571\u2013589., 2007.\n[4] J. Alvarez-Ramirez, J. Alvarez, E. Rodriguez, and G. Fernandez-Anaya.\nTime-varying\nHurst exponent for US stock markets. Physica A: statistical mechanics and its applications,\n387(24):6159\u20136169, 2008.\n[5] A. Ammy-Driss and M. Garcin. E\ufb03ciency of the \ufb01nancial markets during the COVID-19\ncrisis: time-varying parameters of fractional stable dynamics. arxiv preprint, 2020.\n[6] F.M. Bandi and J.R. Russell. Microstructure noise, realized variance, and optimal sampling.",
    "chunk_index": 27,
    "start_char": 63464,
    "end_char": 66584,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "the short-time behavior of the implied volatility for jump-\ndi\ufb00usion models with stochastic volatility. Finance and stochastics, 11(4):571\u2013589., 2007.\n[4] J. Alvarez-Ramirez, J. Alvarez, E. Rodriguez, and G. Fernandez-Anaya.\nTime-varying\nHurst exponent for US stock markets. Physica A: statistical mechanics and its applications,\n387(24):6159\u20136169, 2008.\n[5] A. Ammy-Driss and M. Garcin. E\ufb03ciency of the \ufb01nancial markets during the COVID-19\ncrisis: time-varying parameters of fractional stable dynamics. arxiv preprint, 2020.\n[6] F.M. Bandi and J.R. Russell. Microstructure noise, realized variance, and optimal sampling.\nThe review of economic studies, 75(2):339\u2013369, 2008.\n[7] C. Bender, T. Sottinen, and E. Valkeila. Arbitrage with fractional Brownian motion? Theory\nof stochastic processes, 13(1):23\u201334, 2007.\n[8] S. Bianchi and A. Pantanella. Pointwise regularity exponents and market cross-correlations.\nInternational review of business research papers, 6(2):39\u201351, 2010.\n[9] S. Bianchi, A. Pantanella, and A. Pianese. Modeling and simulation of currency exchange\nrates using multifractional process with random exponent. International journal of modeling\nand optimization, 2(3):309\u2013314, 2012.\n[10] S. Bianchi and A. Pianese.\nTime-varying Hurst-H\u00a8older exponents and the dynamics of\n(in)e\ufb03ciency in stock markets. Chaos, solitons & fractals, 109:64\u201375, 2018.\n[11] M. Bohdalov\u00b4a and M. Gregu\u02c7s. Fractal analysis of forward exchange rates. Acta polytechnica\nhungarica, 7(4):57\u201369, 2010.\n25\n\n[12] D.O. Cajueiro and B.M. Tabak. The Hurst exponent over time: testing the assertion that\nemerging markets are becoming more e\ufb03cient. Physica A: statistical mechanics and its appli-\ncations, 336(3-4):521\u2013537, 2004.\n[13] P. Cheridito.\nArbitrage in fractional Brownian motion models.\nFinance and stochastics,\n7(4):533\u2013553, 2003.\n[14] A. Chronopoulou and F.G. Viens.\nEstimation and pricing under long-memory stochastic\nvolatility. Annals of \ufb01nance, 8(2-3):379\u2013403, 2012.\n[15] J.-F. Coeurjolly. Identi\ufb01cation of multifractional Brownian motion. Bernoulli, 11(6):987\u20131008,\n2005.\n[16] F. Comte and E. Renault.\nLong memory in continuous-time stochastic volatility models.\nMathematical \ufb01nance, 8(4):291\u2013323, 1998.\n[17] R. Cont. Long range dependence in \ufb01nancial markets. In J. L\u00b4evy-V\u00b4ehel and E. Lutton, editors,\nFractals in engineering, pages 159\u2013179. Springer, 2005.\n[18] T. Di Matteo. Multi-scaling in \ufb01nance. Quantitative \ufb01nance, 7(1):21\u201336, 2007.\n[19] T. Di Matteo, T. Aste, and M.M. Dacorogna.\nScaling behaviors in di\ufb00erently developed\nmarkets. Physica A: statistical mechanics and its applications, 324(1-2):183\u2013188, 2003.\n[20] T. Di Matteo, T. Aste, and M.M. Dacorogna. Long-term memories of developed and emerging\nmarkets: Using the scaling analysis to characterize their stage of development. Journal of\nbanking & \ufb01nance, 29(4):827\u2013851, 2005.\n[21] C. Eom, S. Choi, G. Oh, and W.S. Jung. Hurst exponent and prediction based on weak-\nform e\ufb03cient market hypothesis of stock markets. Physica A: statistical mechanics and its\napplications, 387(18):4630\u20134636, 2008.\n[22] M. Garcin. Estimation of time-dependent Hurst exponents with variational smoothing and\napplication to forecasting foreign exchange rates. Physica A: statistical mechanics and its\napplications, 483:462\u2013479, 2017.\n[23] M. Garcin. Hurst exponents and delampertized fractional Brownian motions. International\njournal of theoretical and applied \ufb01nance, 22(5):1950024, 2019.\n[24] M. Garcin. A comparison of maximum likelihood and absolute moments for the estimation of\nHurst exponents in a stationary framework. arxiv preprint, 2020.\n[25] M. Garcin. Fractal analysis of the multifractality of foreign exchange rates. Mathematical\nmethods in economics and \ufb01nance, 13-14(1):49\u201373, 2020.",
    "chunk_index": 28,
    "start_char": 65963,
    "end_char": 69707,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "exponents with variational smoothing and\napplication to forecasting foreign exchange rates. Physica A: statistical mechanics and its\napplications, 483:462\u2013479, 2017.\n[23] M. Garcin. Hurst exponents and delampertized fractional Brownian motions. International\njournal of theoretical and applied \ufb01nance, 22(5):1950024, 2019.\n[24] M. Garcin. A comparison of maximum likelihood and absolute moments for the estimation of\nHurst exponents in a stationary framework. arxiv preprint, 2020.\n[25] M. Garcin. Fractal analysis of the multifractality of foreign exchange rates. Mathematical\nmethods in economics and \ufb01nance, 13-14(1):49\u201373, 2020.\n[26] M. Garcin and M. Grasselli. Long vs short time scales: the rough dilemma and beyond. arxiv\npreprint, 2020.\n[27] J. Garnier and K. S\u00f8lna. Option pricing under fast-varying long-memory stochastic volatility.\nMathematical \ufb01nance, 29(1):39\u201383, 2019.\n[28] J. Gatheral, T. Jaisson, and M. Rosenbaum.\nVolatility is rough.\nQuantitative \ufb01nance,\n18(6):933\u2013949, 2018.\n[29] M.S. Granero, J.T. Segovia, and J.G. P\u00b4erez. Some comments on Hurst exponent and the long\nmemory processes on capital markets. Physica A: statistical mechanics and its applications,\n387(22):5543\u20135551, 2008.\n26\n\n[30] P. Guasoni, Y. Mishura, and M. R\u00b4asonyi. High-frequency trading with fractional Brownian\nmotion. Finance and stochastics, 25:277\u2013310, 2021.\n[31] P. Guasoni, Z. Nika, and M. R\u00b4asonyi. Trading fractional Brownian motion. SIAM journal on\n\ufb01nancial mathematics, 10(3):769\u2013789, 2019.\n[32] L. Kristoufek.\nOn Bitcoin markets (in)e\ufb03ciency and its evolution.\nPhysica A: statistical\nmechanics and its applications, 503:257\u2013262, 2018.\n[33] L. Kristoufek and M. Vosvrda. Measuring capital market e\ufb03ciency: Global and local correla-\ntions structure. Physica A: statistical mechanics and its applications, 392(1):184\u2013193, 2013.\n[34] L. Kristoufek and M. Vosvrda. Gold, currencies and market e\ufb03ciency. Physica A: statistical\nmechanics and its applications, 449:27\u201334, 2016.\n[35] B. Mandelbrot and J. van Ness. Fractional Brownian motions, fractional noises and applica-\ntions. SIAM review, 10(4):422\u2013437, 1968.\n[36] C.J. Nuzman and H.V. Poor. Linear estimation of self-similar processes via Lamperti\u2019s trans-\nformation. Journal of applied probability, 37(2):429\u2013452, 2000.\n[37] R.C.A. Oomen. Properties of realized variance under alternative sampling schemes. Journal\nof business & economic statistics, 24(2):219\u2013237, 2006.\n[38] L.C.G. Rogers. Arbitrage with fractional Brownian motion. Mathematical \ufb01nance, 7(1):95\u2013\n105, 1997.\n[39] D. Surgailis, G. Teyssi`ere, and M. Vai\u02c7ciulis. The increment ratio statistic. Journal of multi-\nvariate analysis, 99(3):510\u2013541, 2008.\n[40] W. Van Harlow. Asset allocation in a downside-risk framework. Financial analysts journal,\n47(5):28\u201340, 1991.\n[41] A. Weron, K. Burnecki, S. Mercik, and K. Weron. Complete description of all self-similar\nmodels driven by L\u00b4evy stable noise. Physical review E, 71(1):016113, 2005.\nA\nLemmas used for the proof of the theorems\nLet g and N be respectively the probability density function and the cumulative distribution\nfunction of a standard Gaussian variable. We introduce the following lemmas.\nLemma 1. For all \u03b1 \u2208R, we have\nR \u221e\n0\nN(\u03b1x)g(x)dx = 1\n4 +\n1\n2\u03c0 arctan(\u03b1).\nProof. Let f(\u03b1) =\nR \u221e\n0\nN(\u03b1x)g(x)dx. Then f is di\ufb00erentiable and\nf \u2032(\u03b1)\n=\nR \u221e\n0\nxg(\u03b1x)g(x)dx\n=\nR \u221e\n0\nx\n2\u03c0 exp\n\u0010\n\u2212x2\n2 (1 + \u03b12)\n\u0011\ndx\n=\nh\n\u2212\n1\n2\u03c0(1+\u03b12) exp\n\u0010\n\u2212x2\n2 (1 + \u03b12)\n\u0011i\u221e\nx=0\n=\n1\n2\u03c0(1+\u03b12).",
    "chunk_index": 29,
    "start_char": 69075,
    "end_char": 72506,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "cumulative distribution\nfunction of a standard Gaussian variable. We introduce the following lemmas.\nLemma 1. For all \u03b1 \u2208R, we have\nR \u221e\n0\nN(\u03b1x)g(x)dx = 1\n4 +\n1\n2\u03c0 arctan(\u03b1).\nProof. Let f(\u03b1) =\nR \u221e\n0\nN(\u03b1x)g(x)dx. Then f is di\ufb00erentiable and\nf \u2032(\u03b1)\n=\nR \u221e\n0\nxg(\u03b1x)g(x)dx\n=\nR \u221e\n0\nx\n2\u03c0 exp\n\u0010\n\u2212x2\n2 (1 + \u03b12)\n\u0011\ndx\n=\nh\n\u2212\n1\n2\u03c0(1+\u03b12) exp\n\u0010\n\u2212x2\n2 (1 + \u03b12)\n\u0011i\u221e\nx=0\n=\n1\n2\u03c0(1+\u03b12).\nAs a consequence, f can be written in the form f(\u03b1) = \u03b3 +\n1\n2\u03c0 arctan(\u03b1), with \u03b3 a constant. The\nconstant \u03b3 is equal to f(0) = 1/4. This proves the lemma.\n27\n\nLemma 2. For all \u03b1, a \u2208R, we have the following Taylor expansion in the neighbourhood of a = 0:\nZ \u221e\na\nN(\u03b1x)g(x)dx = 1\n4 + 1\n2\u03c0 arctan(\u03b1) \u2212\na\n2\n\u221a\n2\u03c0 \u2212\u03b1a2\n4\u03c0 +\na3\n12\n\u221a\n2\u03c0 + (\u03b13 + 3\u03b1)a4\n48\u03c0\n\u2212\na5\n80\n\u221a\n2\u03c0 + O(a6).\nProof. Let f(a) =\nR \u221e\na N(\u03b1x)g(x)dx. First, getting the derivatives of g is straightforward thanks\nto Hermite polynomials:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\ng(0)\n=\n1\n\u221a\n2\u03c0\ng(1)(0)\n=\n0\ng(2)(0)\n=\n\u2212\n1\n\u221a\n2\u03c0\ng(3)(0)\n=\n0\ng(4)(0)\n=\n3\n\u221a\n2\u03c0.\nThe function f is di\ufb00erentiable and, using Leibniz formula, we get:\n1. f (1)(a) = \u2212N(\u03b1a)g(a) and f (1)(0) = \u2212\n1\n2\n\u221a\n2\u03c0,\n2. f (2)(a) = \u2212\u03b1g(\u03b1a)g(a) \u2212N(\u03b1a)g(1)(a) and f (2)(0) = \u2212\u03b1\n2\u03c0,\n3. f (3)(a) = \u2212\u03b12g(1)(\u03b1a)g(a) \u22122\u03b1g(\u03b1a)g(1)(a) \u2212N(\u03b1a)g(2)(a) and f (3)(0) =\n1\n2\n\u221a\n2\u03c0,\n4. f (4)(a) = \u2212\u03b13g(2)(\u03b1a)g(a)\u22123\u03b12g(1)(\u03b1a)g(1)(a)\u22123\u03b1g(\u03b1a)g(2)(a)\u2212N(\u03b1a)g(3)(a) and f (4)(0) =\n\u03b13+3\u03b1\n2\u03c0\n,\n5. f (5)(a) = \u2212\u03b14g(3)(\u03b1a)g(a) \u22124\u03b13g(2)(\u03b1a)g(1)(a) \u22126\u03b12g(1)(\u03b1a)g(2)(a) \u22124\u03b1g(\u03b1a)g(3)(a) \u2212\nN(\u03b1a)g(4)(a) and f (5)(0) = \u2212\n3\n2\n\u221a\n2\u03c0.\nWe conclude with Taylor\u2019s theorem applied to f in a = 0, using Lemma 1 for f(0).\nLemma 3. For all \u03b1, a \u2208R, we have\nZ +\u221e\na\nuN(\u03b1u)g(u)du = N(\u03b1a)g(a) +\n\u03b1\np\n2\u03c0(1 + \u03b12)\n\u0010\n1 \u2212N\n\u0010\na\np\n1 + \u03b12\n\u0011\u0011\n.\nProof. By noting that \u2212g is a primitive function of u 7\u2192ug(u), we integrate by part the following\nequation:\nR b\na uN(\u03b1u)g(u)du\n=\n[\u2212N(\u03b1u)g(u)]b\na + \u03b1\nR b\na g(\u03b1u)g(u)du\n=\n\u2212N(\u03b1b)g(b) + N(\u03b1a)g(a) +\n\u03b1\n\u221a\n2\u03c0\nR b\na g\n\u0000u\n\u221a\n1 + \u03b12\u0001\ndu\n=\n\u2212N(\u03b1b)g(b) + N(\u03b1a)g(a) +\n\u03b1\n\u221a\n2\u03c0(1+\u03b12)\n\u0000N\n\u0000b\n\u221a\n1 + \u03b12\u0001\n\u2212N\n\u0000a\n\u221a\n1 + \u03b12\u0001\u0001\nafter a change of variable between the second and the third line.\nBy doing b \u2192+\u221e, we get\nLemma 3.\nB\nProof of Theorem 1\nProof. We treat separately the conditional and the non-conditional hit ratios. We begin with the\nnon-conditional case.\n28\n\n1. According to equation (4), the predicted price return is the random variable\n\u02c6Rt,t+h|\u2206= \u03a3RS\u03a3\u22121\nS S.",
    "chunk_index": 30,
    "start_char": 72141,
    "end_char": 74428,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "\u0000b\n\u221a\n1 + \u03b12\u0001\n\u2212N\n\u0000a\n\u221a\n1 + \u03b12\u0001\u0001\nafter a change of variable between the second and the third line.\nBy doing b \u2192+\u221e, we get\nLemma 3.\nB\nProof of Theorem 1\nProof. We treat separately the conditional and the non-conditional hit ratios. We begin with the\nnon-conditional case.\n28\n\n1. According to equation (4), the predicted price return is the random variable\n\u02c6Rt,t+h|\u2206= \u03a3RS\u03a3\u22121\nS S.\nIts mean is zero and its variance is:\nVar\n\u0010\n\u02c6Rt,t+h|\u2206\n\u0011\n=\nE\nh\n\u03a3RS\u03a3\u22121\nS S\n\u0000\u03a3RS\u03a3\u22121\nS S\n\u0001T i\n=\n\u03a3RS\u03a3\u22121\nS E\n\u0002\nSST \u0003 \u0000\u03a3T\nS\n\u0001\u22121 \u03a3T\nRS\n=\n\u03a3RS\u03a3\u22121\nS \u03a3S\u03a3\u22121\nS \u03a3T\nRS\n=\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS,\nwhere we have used the fact that \u03a3S is symmetric. The covariance of \u02c6Rt,t+h|\u2206with Rt,t+h\nis:\nCov( \u02c6Rt,t+h|\u2206, Rt,t+h)\n=\nE\n\u0002\n\u03a3RS\u03a3\u22121\nS SRt,t+h\n\u0003\n=\n\u03a3RS\u03a3\u22121\nS E [SRt,t+h]\n=\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS.\nThen, the covariance matrix \u0393 of the Gaussian vector [ \u02c6Rt,t+h|\u2206Rt,t+h]T is:\n\u0393 =\n\u0012 \u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u03c32h2H\n\u0013\n,\nwhich we can also write \u0393 = \u03a3\u03a3T , with \u03a3 a lower triangular matrix obtained by Cholesky\ndecomposition:\n\u03a3 =\n\u0012 a\n0\na\nb\n\u0013\n,\nwith a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS and b =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS.\nTherefore, there exists two\nGaussian variables U, V \u223cN(0, 1), independent from each other, such that\n[ \u02c6Rt,t+h|\u2206Rt,t+h]T = \u03a3[U V ]T .\n(11)\nThe non-conditional hit ratio can thus be written as a sum of joint probabilities of the two\nincrements \u02c6Rt,t+h|\u2206and Rt,t+h, which can be expressed as a combination of U and V :\n\u03c1\n=\nP\nh\n\u02c6Rt,t+h|\u2206\u22650, Rt,t+h \u22650\ni\n+ P\nh\n\u02c6Rt,t+h|\u2206\u22640, Rt,t+h \u22640\ni\n=\nP [aU \u22650, aU + bV \u22650] + P [aU \u22640, aU + bV \u22640]\n=\nP [U \u22650, V \u2265\u2212(a/b)U] + P [U \u22640, V \u2264\u2212(a/b)U]\n=\nR +\u221e\n0\nR +\u221e\n\u2212au/b (g(u)g(v))dvdu +\nR 0\n\u2212\u221e\nR \u2212au/b\n\u2212\u221e\n(g(u)g(v))dvdu\n=\nR +\u221e\n0\n\u00001 \u2212N\n\u0000\u2212a\nb u\n\u0001\u0001\ng(u)du +\nR 0\n\u2212\u221eN\n\u0000\u2212a\nb u\n\u0001\ng(u)du\n=\n1\n2 \u2212\nR +\u221e\n0\nN\n\u0000\u2212a\nb u\n\u0001\ng(u)du +\nR +\u221e\n0\nN\n\u0000 a\nb u\n\u0001\ng(u)du\n=\n1\n2 \u22121\n4 \u2212\n1\n2\u03c0 arctan\n\u0000\u2212a\nb\n\u0001\n+ 1\n4 +\n1\n2\u03c0 arctan\n\u0000 a\nb\n\u0001\n=\n1\n2 + 1\n\u03c0 arctan\n\u0000 a\nb\n\u0001\n.\nwhere g and N are respectively the probability density function and the cumulative distri-\nbution function of a standard Gaussian variable and where we used Lemma 1, which is in\nAppendix A. Moreover, we note that\na\nb =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n=\n\u0012\n\u03c32h2H\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u22121\n\u0013\u22121/2\n,\n29",
    "chunk_index": 31,
    "start_char": 74054,
    "end_char": 76155,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "b\n\u0001\n.\nwhere g and N are respectively the probability density function and the cumulative distri-\nbution function of a standard Gaussian variable and where we used Lemma 1, which is in\nAppendix A. Moreover, we note that\na\nb =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n=\n\u0012\n\u03c32h2H\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u22121\n\u0013\u22121/2\n,\n29\n\nso that\n\u03c1 = 1\n2 + 1\n\u03c0 arctan\n \u0014\n\u03c32h2H\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u22121\n\u0015\u22121/2!\n= 1 \u22121\n\u03c0 arctan\n s\n\u03c32h2H\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS\n\u22121\n!\n.\n2. We now deal with the conditional case. Thanks to the above Cholesky decomposition and\nto equation (11), we know that, conditionally to S, the variable Rt,t+h is Gaussian of mean\nE{Rt,t+h|S} = \u02c6Rt,t+h|\u2206and of variance b2. Therefore, the conditional hit ratio is, where\nV \u223cN(0, 1):\n\u03c1c(y)\n=\nP\nh\n\u02c6Rt,t+h|\u2206\u22650, \u02c6Rt,t+h|\u2206+ bV \u22650|S = y\ni\n+ P\nh\n\u02c6Rt,t+h|\u2206< 0, \u02c6Rt,t+h|\u2206+ bV < 0|S = y\ni\n=\n1{\u03a3RS\u03a3\u22121\nS y\u22650}P\n\u0002\nV \u2265\u2212\u03a3RS\u03a3\u22121\nS y/b\n\u0003\n+ 1{\u03a3RS\u03a3\u22121\nS y<0}P\n\u0002\nV < \u2212\u03a3RS\u03a3\u22121\nS y/b\n\u0003\n=\n1{\u03a3RS\u03a3\u22121\nS y\u22650}N\n\u0000\u03a3RS\u03a3\u22121\nS y/b\n\u0001\n+ 1{\u03a3RS\u03a3\u22121\nS y<0}N\n\u0000\u2212\u03a3RS\u03a3\u22121\nS y/b\n\u0001\n=\nN\n\u0000\f\f\u03a3RS\u03a3\u22121\nS y\n\f\f /b\n\u0001\n.\nC\nProof of Theorem 2\nProof. Using the same approach as in the proof of Theorem 1, we can write:\np+(\u03b8)\n=\nP\nh\n\u02c6Rt,t+h|\u2206\u2265\u03b8, Rt,t+h > 0\ni\n+ P\nh\n\u02c6Rt,t+h|\u2206\u2264\u2212\u03b8, Rt,t+h < 0\ni\n=\nP [aU \u2265\u03b8, aU + bV > 0] + P [aU \u2264\u2212\u03b8, aU + bV < 0]\n=\nP [U \u2265\u03b8/a, V > \u2212(a/b)U] + P [U \u2264\u2212\u03b8/a, V < \u2212(a/b)U]\n=\nR +\u221e\n\u03b8/a\nR +\u221e\n\u2212au/b (g(u)g(v))dvdu +\nR \u2212\u03b8/a\n\u2212\u221e\nR \u2212au/b\n\u2212\u221e\n(g(u)g(v))dvdu\n=\nR +\u221e\n\u03b8/a\n\u00001 \u2212N\n\u0000\u2212a\nb u\n\u0001\u0001\ng(u)du +\nR \u2212\u03b8/a\n\u2212\u221e\nN\n\u0000\u2212a\nb u\n\u0001\ng(u)du\n=\n1 \u2212N(\u03b8/a) \u2212\nR +\u221e\n\u03b8/a N(\u2212a\nb u)g(u)du +\nR +\u221e\n\u03b8/a N( a\nb u)g(u)du,\n(12)\nby symmetry of g, the Gaussian density, with a change of variable in the second integral, and\nwhere U, V \u223cN(0, 1) are random variables independent from each other, a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, and\nb =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS. Using Lemma 2, which is in Appendix A, we obtain:\np+(\u03b8) = 1 \u2212N(\u03b8/a) + 1\n\u03c0 arctan(a\nb ) \u2212\n\u03b82\n2\u03c0ab +\n\u0012 1\nab3 + 3\na4b\n\u0013 \u03b84\n24\u03c0 + O(\u03b86).\nSimilarly, we can write\np\u2212(\u03b8)\n=\nP [aU \u2264\u2212\u03b8, aU + bV > 0] + P [aU \u2265\u03b8, aU + bV < 0]\n=\nR +\u221e\n\u03b8/a N(\u2212a\nb u)g(u)du +\nR \u2212\u03b8/a\n\u2212\u221e\n(1 \u2212N(\u2212a\nb u))g(u)du\n=\n1 \u2212N(\u03b8/a) +\nR +\u221e\n\u03b8/a N(\u2212a\nb u)g(u)du \u2212\nR +\u221e\n\u03b8/a N( a\nb u)g(u)du.\n(13)\nUsing again Lemma 2, we get:\np\u2212(\u03b8) = 1 \u2212N(\u03b8/a) \u22121\n\u03c0 arctan(a\nb ) +\n\u03b82\n2\u03c0ab \u2212\n\u0012 1\nab3 + 3\na4b\n\u0013 \u03b84\n24\u03c0 + O(\u03b86).",
    "chunk_index": 32,
    "start_char": 75851,
    "end_char": 77985,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "write\np\u2212(\u03b8)\n=\nP [aU \u2264\u2212\u03b8, aU + bV > 0] + P [aU \u2265\u03b8, aU + bV < 0]\n=\nR +\u221e\n\u03b8/a N(\u2212a\nb u)g(u)du +\nR \u2212\u03b8/a\n\u2212\u221e\n(1 \u2212N(\u2212a\nb u))g(u)du\n=\n1 \u2212N(\u03b8/a) +\nR +\u221e\n\u03b8/a N(\u2212a\nb u)g(u)du \u2212\nR +\u221e\n\u03b8/a N( a\nb u)g(u)du.\n(13)\nUsing again Lemma 2, we get:\np\u2212(\u03b8) = 1 \u2212N(\u03b8/a) \u22121\n\u03c0 arctan(a\nb ) +\n\u03b82\n2\u03c0ab \u2212\n\u0012 1\nab3 + 3\na4b\n\u0013 \u03b84\n24\u03c0 + O(\u03b86).\nFrom equations (12) and (13), we also obtain:\np0(\u03b8) = 1 \u2212p+(\u03b8) \u2212p\u2212(\u03b8) = \u22121 + 2N\n\u0012\u03b8\na\n\u0013\n.\nThis proves Theorem 2.\n30\n\nD\nProof of Theorem 3\nProof. By de\ufb01nition,\neRt,t+h(\u03b8) = E\nh\nRt,t+h\n\u0010\n1{ \u02c6\nRt,t+h|\u2206\u2265\u03b8} \u22121{ \u02c6\nRt,t+h|\u2206\u2264\u2212\u03b8}\n\u0011i\n.\nLike in the proof of Theorems 1 and 2, we can write it:\neRt,t+h(\u03b8) = E\n\u0002\n(aU + bV )\n\u00001{aU\u2265\u03b8} \u22121{aU\u2264\u2212\u03b8}\n\u0001\u0003\n,\nwhere U, V \u223cN(0, 1) are two independent Gaussian variables, a =\nq\n\u03a3RS\u03a3\u22121\nS \u03a3T\nRS, and b =\nq\n\u03c32h2H \u2212\u03a3RS\u03a3\u22121\nS \u03a3T\nRS. Then\neRt,t+h(\u03b8)\n=\nR +\u221e\n\u03b8/a\n\bR\nR (au + bv) g(v)g(u)dv\n \ndu \u2212\nR \u2212\u03b8/a\n\u2212\u221e\n\bR\nR (au + bv) g(v)g(u)dv\n \ndu\n=\nR +\u221e\n\u03b8/a aug(u)du \u2212\nR \u2212\u03b8/a\n\u2212\u221e\naug(u)du\n=\n2ag(\u03b8/a),\nusing the symmetry of g and the fact that ug(u) = \u2212g\u2032(u). Similarly,\ne\u03c3\u2212\nt,t+h(\u03b8)\n=\n\u2212E\nh\nRt,t+h\n\u0010\n1{ \u02c6\nRt,t+h|\u2206\u2265\u03b8,Rt,t+h<0} \u22121{ \u02c6\nRt,t+h|\u2206\u2264\u2212\u03b8,Rt,t+h>0}\n\u0011i\n=\n\u2212E\n\u0002\n(aU + bV )\n\u00001{aU\u2265\u03b8,aU+bV <0} \u22121{aU\u2264\u2212\u03b8,aU+bV >0}\n\u0001\u0003\n=\n\u2212\nR +\u221e\n\u03b8/a\nnR \u2212au/b\n\u2212\u221e\n(au + bv) g(v)g(u)dv\no\ndu\n+\nR \u2212\u03b8/a\n\u2212\u221e\nnR +\u221e\n\u2212au/b (au + bv) g(v)g(u)dv\no\ndu\n=\n\u2212\nR +\u221e\n\u03b8/a\n\u0002\nauN\n\u0000\u2212au\nb\n\u0001\n\u2212bg\n\u0000\u2212au\nb\n\u0001\u0003\ng(u)du\n+\nR \u2212\u03b8/a\n\u2212\u221e\n\u0002\nau\n\u00001 \u2212N\n\u0000\u2212au\nb\n\u0001\u0001\n+ bg\n\u0000\u2212au\nb\n\u0001\u0003\ng(u)du\n=\n\u22122a\nR +\u221e\n\u03b8/a uN\n\u0000\u2212au\nb\n\u0001\ng(u)du + 2b\nR +\u221e\n\u03b8/a g\n\u0000 au\nb\n\u0001\ng(u)du,\nafter a change of variable u 7\u2192\u2212u in the last line. This leads to the general formula, where we use\nLemma 3 (which is postponed in Appendix A) in the \ufb01rst integral, the fact that g(au/b)g(u) =\ng(u\np\n1 + (a/b)2)/\n\u221a\n2\u03c0 and a change of variable u 7\u2192u\np\n1 + (a/b)2 in the second integral:\ne\u03c3\u2212\nt,t+h(\u03b8)\n=\n\u22122a\n\u0014\nN\n\u0000\u2212\u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\n\u2212\na\n\u221a\n2\u03c0(a2+b2)\n\u0010\n1 \u2212N\n\u0010\n\u03b8\nq\n1\na2 + 1\nb2\n\u0011\u0011\u0015\n+\n2b\n\u221a\n2\u03c0\u221a\n1+(a/b)2\n\u0010\n1 \u2212N\n\u0010\n\u03b8\nq\n1\na2 + 1\nb2\n\u0011\u0011\n=\n\u22122aN\n\u0000\u2212\u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\n+\n2a2+2b2\n\u221a\n2\u03c0(a2+b2)\n\u0010\n1 \u2212N\n\u0010\n\u03b8\nq\n1\na2 + 1\nb2\n\u0011\u0011\n=\n\u22122aN\n\u0000\u2212\u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\n+\nq\n2(a2+b2)\n\u03c0\nN\n\u0010\n\u2212\u03b8\nq\n1\na2 + 1\nb2\n\u0011\n.",
    "chunk_index": 33,
    "start_char": 77680,
    "end_char": 79631,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "q\n1\na2 + 1\nb2\n\u0011\u0011\u0015\n+\n2b\n\u221a\n2\u03c0\u221a\n1+(a/b)2\n\u0010\n1 \u2212N\n\u0010\n\u03b8\nq\n1\na2 + 1\nb2\n\u0011\u0011\n=\n\u22122aN\n\u0000\u2212\u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\n+\n2a2+2b2\n\u221a\n2\u03c0(a2+b2)\n\u0010\n1 \u2212N\n\u0010\n\u03b8\nq\n1\na2 + 1\nb2\n\u0011\u0011\n=\n\u22122aN\n\u0000\u2212\u03b8\nb\n\u0001\ng\n\u0000 \u03b8\na\n\u0001\n+\nq\n2(a2+b2)\n\u03c0\nN\n\u0010\n\u2212\u03b8\nq\n1\na2 + 1\nb2\n\u0011\n.\nBy noting that a2 + b2 = \u03c32h2H, we obtain the results displayed in Theorem 3.\n31",
    "chunk_index": 34,
    "start_char": 79426,
    "end_char": 79712,
    "paper_title": "Forecasting with fractional Brownian motion a fina",
    "paper_category": "q-fin.MF",
    "paper_filename": "Forecasting_with_fractional_Brownian_motion_a_fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.MF/Forecasting_with_fractional_Brownian_motion_a_fina.pdf"
  },
  {
    "text": "A Comparative Anatomy of REITs and Residential Real Estate Indexes: \nReturns, Risks and Distributional Characteristics \n \nby \n \nJohn Cotter and Richard Roll \n \n February 2011 \n \nAbstract \n \nReal Estate Investment Trusts (REITs) are the only truly liquid assets related to real estate \ninvestments. We study the behavior of U.S. REITs over the past three decades and document \ntheir return characteristics. REITs have somewhat less market risk than equity; their betas \nagainst a broad market index average about .65. Decomposing their covariances into principal \ncomponents reveals several strong factors. REIT characteristics differ to some extent from those \nof the S&P/Case-Shiller (SCS) residential real estate indexes. This is partly attributable to \nmethods of index construction. Our examination of REITs suggests that investment in real estate \nis far more risky than what might be inferred from the widely-followed SCS series. REITs, \nunlike SCS series are forward looking, and this helps them in the prediction of SCS returns. \nREIT forecasts of SCS returns are reasonably precise over a number of periods. \n \n \nAuthors\u2019 Coordinates \n \nCotter \nRoll \nAddress \nUniversity College Dublin \nCentre for Financial Markets \nSchool of Business \nCarysfort Avenue \nBlackrock, County Dublin \nIreland \nUCLA Anderson \n110 Westwood Plaza \nLos Angeles, CA 90095-1481 \nUSA \nVoice \n353 1 716 8900 \n1 310 825 6118 \nE-Mail \njohn.cotter@ucd.ie \nrroll@Anderson.ucla.edu \n \nAcknowledgement \n \nWe have benefited greatly from discussions with Brad Case, Steve Cauley, Jerry Coakley, Joao \nCocco, Craig A. Depken II, Stuart Gabriel, Joe Gyuorko, Cal Muckley, Brendan Murphy, \nKathleen Smalley, participants at the 2011 American Real Estate and Urban Economics Society \nAnnual Conference, and seminar participants at the University of Manchester and University \nCollege Dublin. We thank the Ziman Center for Real Estate at UCLA for providing REIT data \nand for a research grant supporting this project. Cotter acknowledges the support of Science \nFoundation Ireland under Grant Number 08/SRC/FM1389. \n \n\n1 \n \nA Comparative Anatomy of REITs and Residential Real Estate Indexes: \nReturns, Risks and Distributional Characteristics \n \nI. Introduction. \n \nAlthough real estate probably represents the dominant fraction of non-human capital for most \nAmericans, it is very illiquid. Transaction costs and search costs are high when selling or buying \na single- or a multi-family residence. Residential real estate indexes such as those published by \nS&P/Case-Shiller\u00ae (SCS) are widely followed, but they are reported only monthly and are \nsubject to some unavoidable difficulties.1\n \n \nThe only truly liquid real estate related vehicle with high-frequency observability is the Real \nEstate Investment Trust or REIT. REITs are listed on major exchanges and are traded \ncontinually. Hence, their features should be of great interest to those who want to keep frequent \ntrack of their major investments. As proxies, REITs offer the opportunity to observe the likely \nbehavior of all real estate prices if they were only observable. In this paper, we provide \ncomprehensive documentation for REIT return characteristics and compare them to the Case-\nShiller indexes of residential real estate. \n \nInvestors in real estate often are often motivated by a belief that unlike equities, real estate offers \nhigh returns and low risk possibilities.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3415,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "track of their major investments. As proxies, REITs offer the opportunity to observe the likely \nbehavior of all real estate prices if they were only observable. In this paper, we provide \ncomprehensive documentation for REIT return characteristics and compare them to the Case-\nShiller indexes of residential real estate. \n \nInvestors in real estate often are often motivated by a belief that unlike equities, real estate offers \nhigh returns and low risk possibilities. In a series of papers, Karl Case and Robert Shiller report \ntabulated responses from a questionnaire survey of home buyers (for example see Case and \nShiller, 2003). Here large proportions of respondents suggest that their investment is based on \nlarge positive price movement possibilities that carry negligible risk. In order to address whether \nreal estate is in fact a low risk and high return investment we empirically invest these \ncharacteristics of related price indexes using both SCS and REIT series. \n \nWe document a strong return performance for REITs in comparison to both the US stock market \nand, in particular, the SCS real estate series. Furthermore, we find that compared to thebroad-\nbased equity index, the S&P500, REITs as a group has lower market risk (beta) but comparable \ntotal volatility. SCS risk features are negligible compared to the other series. REITs return \n \n1 Later in the paper, we document some of the measurement difficulties for these indexes and analyze their \nconsequences. \n\n2 \n \ndistributions mostly display moderate negative skewness but very high kurtosis, which implies \nthat they are emphatically non-normal, a result that we verify with formal tests. Their monthly \nreturns are not very auto-correlated. These characteristics are similar to those noted for equity \nreturns. \n \nIn contrast, SCS index returns are highly auto-correlated, probably because they are constructed \nas three-month moving averages. They also have much smaller total volatility, roughly one-fifth \nthat of REITs. Reconstructed REIT series as a three-month moving average leads to a far \ngreater alignment of the risk/return outcomes between the two sets of real estate series. This \nreconstruction reduces volatility dramatically and increases autocorrelation , thus suggesting that \nthe unadjusted REITs are much more in line with the performance of actual real estate markets. \nHowever, SCS returns like REITs, are moderately left-skewed and non-normal, but their betas \nare very small, only 1/20 as large as the betas of REITs (again resulting to an extent from the \nconstruction method followed by the SCS series).2\n \n We report further divergences in the real \nestate indicators, where for instance, to explain 90% of REIT return variance, five to six \nprincipal components are required. In contrast, only three principal components explain 90% of \nthe variance in SCS index. \nMuch of the recent financial crises had focused on the pivotal role of real estate and its poor \nperformance. Various explanations including the sub-prime mortgage meltdown, inadequate \nfinancial legislation, increased availability of mortgage credit, and a real estate bubble heavily \ninter twine with the collapse in real estate markets (for a discussion of these and other drivers of \nthe crises see Mian and Sufi (2009) and Roll (2011)).",
    "chunk_index": 1,
    "start_char": 2944,
    "end_char": 6262,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "components are required. In contrast, only three principal components explain 90% of \nthe variance in SCS index. \nMuch of the recent financial crises had focused on the pivotal role of real estate and its poor \nperformance. Various explanations including the sub-prime mortgage meltdown, inadequate \nfinancial legislation, increased availability of mortgage credit, and a real estate bubble heavily \ninter twine with the collapse in real estate markets (for a discussion of these and other drivers of \nthe crises see Mian and Sufi (2009) and Roll (2011)). Real estate is a fundamentally important \nasset class and represents a large proportion of the net worth of individuals, firms and the overall \neconomy.3\n \n2 Although Scholes-Williams (1977) betas are somewhat larger, (e.g., for Los Angeles the beta increases from 0.027 \nto 0.0624), they remain small in comparison to those of REITS. \n Whilst detailing past performance can help us understand the role of risk in the crises, \nthe ability to predict future real estate can help us avoid repeating past failures. It is thus \n3 For instance, in 2007, the value of U.S. residential real estate totalled US$ 22.5 trillion compared to US$ 19.9 \ntrillion in domestic equities. \n\n3 \n \nimportant to determine if we can predict future real estate returns. Taking our two surrogates of \nreal estate we examine their predictability on a stand-alone basis and in conjunction with each \nother. The forecast from the smoothed SCS series will shrink large possible price movements of \nfuture real estate prices in comparison to REITs. But is this how real estate prices actually \nbehave? Due to the lack of observability of true prices, this is a question that cannot be directly \nanswered. However, we can determine if the indicators can help in predicting future values of \neach other, and more importantly, identify which series has a better ability in doing so. Thus we \ncan see if REITS can help in predicting the often cited and heavily used SCS series. We can also \nmeasure the extent that both series respond to innovations in each other and whether this can aid \nforecasting future real estate returns. \n \nIn addressing these issues, we find evidence of real estate predictability using the REIT series. \nThe forward looking investor orientated REIT series can predict future SCS index returns. We \nfind much of the future variation of SCS returns can be predicted by the inclusion of movements \nin the REIT series. Thus, adding REIT series help in forecasting future real estate returns as \nproxied by the SCS series. We also report evidence of Granger causality between the series on a \nstand-alone basis suggesting that, for example, changes in REITs in the current period impacts \nchanges in the SCS series in future periods. Moreover, REITs help to offer reasonable precise \nforecasts of SCS series over a number of periods. As the SCS series is developed using a \nmoving average construction we report that predictability of itself is strong and remains for long \nlags compared to the REIT series.",
    "chunk_index": 2,
    "start_char": 5707,
    "end_char": 8760,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "causality between the series on a \nstand-alone basis suggesting that, for example, changes in REITs in the current period impacts \nchanges in the SCS series in future periods. Moreover, REITs help to offer reasonable precise \nforecasts of SCS series over a number of periods. As the SCS series is developed using a \nmoving average construction we report that predictability of itself is strong and remains for long \nlags compared to the REIT series. Notwithstanding this, both impulse response and forecast \nerror variance decomposition statistics suggest that innovations in REIT series have additional \ninformation for forecasting future SCS returns. \nThese and other details are reported numerically below after we first describe the data sources \nand provide summary statistics covering the two separate indicators of real estate investments. \nWe begin however with a discussion of the literature on our proposed proxy for real estate, \nREITS, detailing their risk characteristics and evidence on predictability. \n \n\n4 \n \nII. Literature Review \n \nThere is considerable literature focusing on the risk profiles of REITS. Chan, Hendershott and \nSanders (1990) use the macro-economic factors identified for equities by Chen, Roll and Ross \n(1986) and find that changes in risk, term structure and unexpected inflation drive both REIT and \nequity returns. Peterson and Hsieh (1997) explain REIT returns with the three Fama-French \n(1993) factors and Derwall, Huij, Brounen and Marquering (2009) add a momentum factor. See \nalso Lizieri, Satchall and Zhang, (2007). However, there is some indication that broad equity \nfactors are becoming less important for REITS and are being replaced by specific real estate \nfactors (Clayton and MacKinnon, 2003). \n \nIdiosyncratic risk is important for real estate in general and for REITs in particular. Investors in \nreal estate tend to hold small undiversified portfolios due to the localized and segmented nature \nof the asset. Clayton and MacKinnon (2003) find that idiosyncratic risk for REITS is large and \nincreasing over time. Ooi, Wang and Webb (2009) find that there is a positive risk and return \nrelationship associated with idiosyncratic risk of REITs so the expected returns of real estate \ninvestors increase with idiosyncratic risk. Moreover REITS tend to have low levels of \nsystematic risk, although this varies across assets (Gyuorko and Nelling, 1996; Peterson and \nHsieh, 1997). \n \nREITs indexes are forward looking as they represent investor behavior relating to real estate \nmarkets. Real estate returns by themselves, especially commercial, are mean reverting and \nfollow cycles, and are found to be predictable. REITs have strong predictive power for future \nREITS (Chui, Titman and Wei, 2003). However, there is mixed to weak evidence that REITs \nprices can predict future real estate prices. This is somewhat surprising as REITs offer liquidity \nand transparency for real estate investors who through their trading actions should process and \nsignal information about future values of real estate. In a similar vein, the forecastabilty of \nequity returns is mixed (for a review see Goyal and Welch, 2007). \n \nOf the evidence in support of REIT predictability, Gyuorko and Keim (1992) find that lagged \nvalues of REIT returns can predict returns on appraisal-based real estate indexes. They note that",
    "chunk_index": 3,
    "start_char": 8311,
    "end_char": 11675,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "that REITs \nprices can predict future real estate prices. This is somewhat surprising as REITs offer liquidity \nand transparency for real estate investors who through their trading actions should process and \nsignal information about future values of real estate. In a similar vein, the forecastabilty of \nequity returns is mixed (for a review see Goyal and Welch, 2007). \n \nOf the evidence in support of REIT predictability, Gyuorko and Keim (1992) find that lagged \nvalues of REIT returns can predict returns on appraisal-based real estate indexes. They note that \n\n5 \n \nappraisal-based indexes incorporate market fundamentals at a lag. Analogously the use of past \nmoving average based index values drive the development of transaction based series such as \nCase Shiller. However REITs were unable to predict the decline in real estate during the recent \ncrises, and Pavlov and Wachter (2009) suggest that this may be due to cheap credit being offered \nand accessed by REIT investors. \nAppraisal based methods (similar to SCS series) are heavily reliant on smoothing based \ntechniques and there is a substantial divergence between these returns and REITS (Muhlhofer, \n2008). Since REITs are not subject to the dampening of volatility associated with real estate \nappraisals (Ross and Zisler, 1991) of the moving average based SCS series, an excellent case can \nbe made that REITs are among the best available short-term proxies for real estate generally, \neven residential real estate. \n \nREIT returns are non-normal and exhibit volatility clustering. Young and Graff (1995) find \nsupport for a mixtures distribution where the variance is not constant in contrast to the normal \ndistribution. Lizieri, Satchall and Zhang (2007) identify and model excess kurtosis in REIT \nreturns. (Excess kurtosis is related to fat-tails where the probability distribution for large returns \nexceeds that of the normal distribution.) Booth and Broussard (2002) find support for fitting the \nfat-tailed Fr\u00e9chet distribution to REIT returns. In line with equities REITS returns exhibit \nconditional heteroskedasticity and volatility has been successfully modeled with many GARCH \nmodels (see long memory modeling with FIGARCH by Cotter and Stevenson, 2008). \n \n \nIII. Data \n \nMonthly return data and information about investment strategies are available for individual \nREITs from the UCLA Ziman Center for real estate. This information is also available now on \nthe CRSP database from the Center for Research in Securities Prices at the University of \nChicago. The data are available daily since 1969 but since we want to compare REITs and the \nSCS indexes, our analysis will be based mainly on monthly observations. As we are using \nrelatively low frequency REIT data we avoid problems of thin trading (although the \nCRSP/Ziman Centre database incorporates used prices to overcome this for daily data). \n\n6 \n \n \nREITs come in various flavors. As reported in Table 1, there are three broad categories, equity, \nmortgage, and hybrid. Equity REITs are devoted to direct purchases of real estate requiring at \nleast 75% of their total assets in income producing real estate properties.",
    "chunk_index": 4,
    "start_char": 11110,
    "end_char": 14278,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "avoid problems of thin trading (although the \nCRSP/Ziman Centre database incorporates used prices to overcome this for daily data). \n\n6 \n \n \nREITs come in various flavors. As reported in Table 1, there are three broad categories, equity, \nmortgage, and hybrid. Equity REITs are devoted to direct purchases of real estate requiring at \nleast 75% of their total assets in income producing real estate properties. Mortgage REITs hold \nportfolios of loans backed by real estate collateral with at least 75% of their assets in residential \nmortgages, short and long term construction loans and mortgages in commercial properties. \nHybrid REITs are a combination of the two, investing in both properties and real-estate related \nloans. Within these categories, the REITs are further categorized by the main property types \nheld or financed such as residential property. \n \nOur sample includes all REITs that have traded on the NYSE, AMEX and NASDAQ exchanges \nduring this time. Table 1 also reveals the enormous growth of the REIT sector; from the \nbeginning of 1980 through May, 2009 the total market capitalization of REITs had grown 80-\nfold, almost 17% per year (compounded) over more than 28 years. This growth has been in \nassets and in the number and aggregate market value of equity REITs. The numbers of mortgage \nand hybrid REITs have actually declined as well as their relative market value. Equity REITs \nrepresent over 90% of total value at the end of the sample and hybrid REITs now have negligible \nsize. \n \nREITs are generally highly leveraged assets although there is considerable variation across REIT \ntypes. Overall they tend to have higher debt than ordinary firms. Mortgage REITs have higher \ndebt than equity REITS. Leverage levels tend to be strongly related to the ratings of the debt \nwith an increase in the number of investment grade debt offerings resulting in higher levels of \ndebt. Jaffe (1991) offers further explanation for the high levels of REIT debt. He suggests \nREITs tend to be relatively small firms but their cost of debt is lower than similarly-sized \nordinary firms. The investments of REITs in real property, which is associated with a high level \nof debt capacity, leads to more debt financing. \n \n\n7 \n \nOver time, a number of legislative acts have responded to the changing investment environment \nand have allowed REITS more flexibility to meet new challenges.4\n \n Although REITS were \nintroduced initially as an investment vehicle that allowed and encouraged small investor \ninvolvement, the role of institutional investors, and especially pension funds, has increased over \ntime. For instance, prior to 1980, 50% of REIT shareholding could not be held by groups of five \nor fewer individual investors, known as the 5/50 rule. However, these rules were relaxed over \ntime and the average institutional ownership increased from 10% to 39% between 1981 and \n2009. This included the Omnibus Budget Reconciliation Act (1993) that allowed pension funds \nto overcome the 5/50 rule (prior to the legislation they were treated as a single investor) by \ncounting their own investors as individual investors.",
    "chunk_index": 5,
    "start_char": 13868,
    "end_char": 17009,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "REIT shareholding could not be held by groups of five \nor fewer individual investors, known as the 5/50 rule. However, these rules were relaxed over \ntime and the average institutional ownership increased from 10% to 39% between 1981 and \n2009. This included the Omnibus Budget Reconciliation Act (1993) that allowed pension funds \nto overcome the 5/50 rule (prior to the legislation they were treated as a single investor) by \ncounting their own investors as individual investors. As a consequence REITs capitalization \ngrew in the 1990s accompanied by increased interest from institutional investors. \nREITS as a real estate investment vehicle offer some advantages. In contrast to other entities, \nthey are effectively exempt from corporate taxes. However, there are other rules that may offset \nthis tax benefit. REITS must distribute a high proportion, usually over 90%, of their taxable \nincome. This restricts their ability to grow from internal funds. Also, some REITS that have \nbeen set up with pre-determined finite lives; these generally rely less on additional external \nfunding and tend to have limited growth. \n \nREITS developed a pronounced tendency to use external advisors during the 1980s to manage \ntheir assets. This is probably inherited from the Real Estate Investment Trust Act (1960), the \noriginal legislation that introduced REITs, which defined REITs as having multiple trustees as \nmanagers. \n \nPerhaps the most confining restriction is a limit on the type of income a REIT can earn and the \ntype of asset it can hold. But despite such limits, REITs have been generally profitable since the \ninception. \n \nIII.A. Descriptive Statistics. \n \n4 These include the REIT Modernisation Act (1999) and the Economic Recovery Act (1981); (see Chan, Erickson \nand Wang [2003] for a review). \n\n8 \n \n \nTable 2 reports summary statistics for individual REITs by calendar year from 1980 through \n2008. Although mean returns are positive on average, there are substantial cross-sectional \ndifferences among individual REIT returns in every calendar year. The cross-sectional standard \ndeviation of individual REIT means \u2013 the mean of the standard deviation of returns by calendar \nyear - is generally six to ten times larger than the mean itself. On average, betas are less than \nunity, thereby indicating smaller systematic risk than equities in general. Again, however, there \nis substantial cross-sectional variation. Also, it should be noted that betas exceed 1.0 in each of \nthe last five sample years. This might be due to equity REITs becoming larger in number \nrelative to presumably less risky mortgage and hybrid REITs. The betas for Equity REITs are \ngreater than one for the last five sample years and are larger than corresponding values for \nHybrid and Mortgage REITs (with the exception of 2004 where beta for Mortgage REITs is \nlarger). \n \nTo study the general characteristics of REITs and REITs in various categories, we form each \nmonth several value-weighted portfolio of individual REIT returns weighted by the market \ncapitalization at the end of the prior month. One index covers all REITs, three others are for the \nbroad equity, mortgage and hybrid groups, and six others were composed of specific property \ntypes.",
    "chunk_index": 6,
    "start_char": 16528,
    "end_char": 19779,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "sample years and are larger than corresponding values for \nHybrid and Mortgage REITs (with the exception of 2004 where beta for Mortgage REITs is \nlarger). \n \nTo study the general characteristics of REITs and REITs in various categories, we form each \nmonth several value-weighted portfolio of individual REIT returns weighted by the market \ncapitalization at the end of the prior month. One index covers all REITs, three others are for the \nbroad equity, mortgage and hybrid groups, and six others were composed of specific property \ntypes. The first observation is for January, 1980 and the last available observation is in May, \n2009.5\n \n \nThe S&P/Case-Shiller\u00ae (SCS) Indexes were collected from their inception in February 1987 \nthrough May 2009. SCS indexes are created by combining all transactions on a quality-adjusted \nbasis for repeat sales on single family homes (for full details see Standard and Poors, 2009). \nMany forms of real estate are explicitly excluded, for example condominiums and multi-family \ndwellings and this may affect the risk and return performance of the indexes. What will certainly \ndampen return and volatility statistics is the downward adjustment that is followed for extreme \n \n5 The returns reported are nominal but excess returns and inflation-adjusted returns provide a similar picture. They \nare available from the authors upon request. \n\n9 \n \nprice movements, compared to average market price movements.6 The price change included in \nthe index is for two arms-length sales of the same home. The SCS indexes are reported monthly \nwith each index point representing a three-month moving average of the current and past two \nmonths values. Given the lack of observable real estate prices at high frequencies the three-\nmonth moving average is an attempt to have a sufficiently large sample size with meaningful \nprice changes.7\n \n \nA value-weighted index that operates in an analogous fashion to a market capitalization weighted \nequity index is created that distinguishes the real-estate unit traded across three price tiers, low, \nmedium and high. Case and Shiller (1994) have noted that there is a value effect across the three \ntiers with low tiers having better return performance compared to the other tiers. They compare \nthis to similar size based investment strategies followed in equity markets. There were 14 city \nindexes available over the entire time period, 1987-20098\n \n and a \u201cNational\u201d index constructed as \nan aggregation of 10 of the major metropolitan areas. In addition to the 10 cities (Boston, \nChicago, Denver, Las Vegas, Los Angeles, Miami, New York, San Diego, San Francisco, \nWashington DC) representing a ten-city composite, data is also available for Charlotte, \nCleveland, Portland and Tampa. \nTable 3 presents descriptive statistics for both our set of REIT indexes and for the SCS indexes.9\n \n6 Standard and Poors (2009) provide illustrations of cases where the weighting of extreme price changes would be \nadjusted downwards, including transactions for homes that were not well maintained and thus deviating from a \nrepresentative market real estate unit. \n \nTime series plots of a sample of the series returns and conditional volatility (using a GARCH (1, \n1) model) are given in Figures 1 and 2.",
    "chunk_index": 7,
    "start_char": 19238,
    "end_char": 22510,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "Cleveland, Portland and Tampa. \nTable 3 presents descriptive statistics for both our set of REIT indexes and for the SCS indexes.9\n \n6 Standard and Poors (2009) provide illustrations of cases where the weighting of extreme price changes would be \nadjusted downwards, including transactions for homes that were not well maintained and thus deviating from a \nrepresentative market real estate unit. \n \nTime series plots of a sample of the series returns and conditional volatility (using a GARCH (1, \n1) model) are given in Figures 1 and 2. Except for the Lodging Resort category, REIT returns \nare, on average, somewhat higher than the returns of the SCS indexes. This is particularly \nimportant because means are virtually immune to such problems as moving average-induced \n7 There are many other adjustments in the creation of the indexes to try and ensure meaningful prices. For example, \nif a transaction occurs too quickly on a unit they can be removed if they are not felt to represent market prices. The \ncut-off point is six-months. In contrast, if the arm-length transaction was over a very long holding period they \nwould be a given a lower weight in the database than a transaction over a shorter (six-month) period (the interval \nadjustment process is further detailed in Case and Shiller, 1987). \n8 Twenty SCS cities are now included in their indexes, but only since 2000. \n9 REIT results are presented first by REIT type and then by Property type. For REITS a condensed set of property \ntypes outlined in Table 1 are analysed as there is no price data available for Unknown property type and Mortgage \nproperty type. Moreover data is only available for Health Care property type since March 1984 and for Self Storage \nproperty type since November 1982. \n\n10 \n \nautocorrelation and spuriously low volatility. That is, over a long time period, the general drift \nin prices should be a valid indication of investment performance, regardless of the nature of \nshort-term fluctuations. For example, the residential REIT index has a mean monthly return of \n1.005%, slightly more than 12% per annum, while the SCS composite has a monthly mean of \n0.372%, slightly more than one-third as large. This is something of a puzzle. In addition to the \ndiverging magnitudes of returns for the respective real estate series, the smoothness of returns for \nthe SCS series resulting from the moving average construction is further evidenced in Figure 1. \n \nREIT volatilities are much larger than SCS volatilities. Figure 2 illustrates the divergence in \nvolatilities especially during the recent bear market. The ratio of volatility for All REITs relative \nto the SCS composite is 4.916/.907 = 5.42 for the full sample. This is a large difference but is \npartly attributable to the moving average construction method of SCS. Clearly, the dramatically \nlarger auto-correlation of the SCS returns have arisen, at least in part, for the same reason. To \nillustrate, applying the SCS method of a 3-month moving averages to the All REIT series brings \na reduction in volatility to 2.871.",
    "chunk_index": 8,
    "start_char": 21972,
    "end_char": 25049,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "The ratio of volatility for All REITs relative \nto the SCS composite is 4.916/.907 = 5.42 for the full sample. This is a large difference but is \npartly attributable to the moving average construction method of SCS. Clearly, the dramatically \nlarger auto-correlation of the SCS returns have arisen, at least in part, for the same reason. To \nillustrate, applying the SCS method of a 3-month moving averages to the All REIT series brings \na reduction in volatility to 2.871. Moreover, the auto-correlation of the moving average of the \nAll REIT series increases to 0.712, similar to the SCS series.10\n \n \nThe All REITs return has a beta of 0.625 against the S&P500, thereby indicating considerably \nsmaller systematic market risk. Most REIT sub-categories are similar; indeed, their betas are all \nin the 60% range with the exception of Hybrid REITs (0.511) and Lodging Resort REITs \n(0.824). In sharp contrast, SCS betas are tiny, but again, this is attributable to some extent by \ntheir construction. For instance the beta from using a moving average of the All REIT series \nreduces to 0.211. \n \nExcept for Lodging Resort, all the REIT indexes have a moderate amount of negative skewness. \nThis characteristic is shared by every one of the SCS indexes, one of the few statistics between \nthe two sets of real estate indicators that are more or less in agreement. Kurtosis is moderately \nhigher for REITs than for the SCS indexes, which is also reflected in the Jarque-Bera tests of \n \n10 Other possible causes of the divergence between the outcomes of REIT and SCS series are the different forms of \nleverage associated with investing in REITs compared to investing in single-family properties that constitute the \nSCS series. REITs would tend to have higher leverage amplifying the magnitude of price movements during boom \nand crises periods. \n\n11 \n \nnormality. Every series is significantly non-normal and all have the thick tails typical of most \nasset returns. \n \nIII.B. Principal Components \nUsing the six REIT index series and separately the fourteen SCS series, we extract principal \ncomponents from the covariance matrix of their concurrent monthly returns.11\n \n Figures 3 and 4 \npresent \u201cscree\u201d plots for the narrow (disaggregated) set of REITs and SCS indexes, respectively. \nA scree plots depicts the variance explained by successive principal components (PCs) ranked \nfrom left to right in descending order of variance explained. The bar gives the variance \nexplained by the PC and the cumulative variance explained in percent is printed above each bar. \nComparing the figures, these two sets of real estate indicators again display some differences. \nThe first PC explains quite a bit more of REIT returns than it does for SCS returns; the same is \ntrue for the second PC. Moreover, it takes six PCs to cumulatively explain about 90% of SCS \nvolatility while only three PCs are required to reach the same level of cumulative explanation for \nREITs. One possible explanation is that REITs are less heterogenous than residential real estate \nmeasured by the SCS indexes.",
    "chunk_index": 9,
    "start_char": 24576,
    "end_char": 27659,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "some differences. \nThe first PC explains quite a bit more of REIT returns than it does for SCS returns; the same is \ntrue for the second PC. Moreover, it takes six PCs to cumulatively explain about 90% of SCS \nvolatility while only three PCs are required to reach the same level of cumulative explanation for \nREITs. One possible explanation is that REITs are less heterogenous than residential real estate \nmeasured by the SCS indexes. The SCS indexes are geographically more diverse than REITs, \nwhich mix in properties from all over the country. On the other hand, REITs do include \ncommercial properties of various types and also mortgages. So the empirical contrast is far from \na foregone conclusion. \n \nTable 4 gives the factor loadings on the first 2 PCs for the separate indexes and the variance \nexplained by the first 3 PCs for REITS and first 6 PCs for SCS indexes. The number of factors \nis chosen to explain 90% of the variance of the series. Across the REIT indexes, there is only \nminor variation in the first factor loading and in explaining the variance. The SCS numbers \nshow more variation across cities. For example, Charlotte has only a 0.090 loading on the first \nPC while San Francisco\u2019s loading is 0.388. Charlotte\u2019s R-square of 0.566 (and its relatively \n \n11 Principal Component Analysis (PCA) is actually completed twice. In the first case, we use all series to obtain \nresiduals after removing common factors, thereby allowing for an analysis of the idiosyncratic risk associated with \neach series. Secondly, we examine the structure of the returns with PCA applied to a narrower set of series that \nexcludes any aggregated series such as the SCS Composite 10. Except for the discussion involving residuals, we are \nreferring to the latter PCA. \n\n12 \n \nsmall loading on the first PC) seems to suggest that its real estate market is partially segmented \nfrom the markets in other urban centers. \n \nCorrelations between these REIT factors and various candidate variables raises hope that the \nfactors can be identified. There is a strong and significant correlation between the first REIT \nfactor and the general stock market (corr = 0.591) as proxied by the S&P500 index. In contrast, \nthe first SCS factor series is more related to industrial production growth (corr = 0.311), than to \nthe stock market (corr = 0.170). The factors (2nd for REITs and 4th for SCS) for both sets of \nseries respond negatively to changes in interest rates proxied by the 3-month US Treasury bill \nrate. The impact of interest rates is stronger for the SCS series (corr = -0.356 compared to corr = \n-0.194 for REITs). \n \nIII.C. Risk of Loss Measures. \nWe now turn our attention to risk of loss measures. Value-at-Risk is a widely used indicator of \nloss likelihood. It is mandated in some countries for financial reporting by institutions such as \nbanks, in which case it covers the entire asset/liability portfolio. It can also be adapted to \nindividual asset series such as our REIT indexes and the SCS residential indexes.",
    "chunk_index": 10,
    "start_char": 27223,
    "end_char": 30256,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "interest rates is stronger for the SCS series (corr = -0.356 compared to corr = \n-0.194 for REITs). \n \nIII.C. Risk of Loss Measures. \nWe now turn our attention to risk of loss measures. Value-at-Risk is a widely used indicator of \nloss likelihood. It is mandated in some countries for financial reporting by institutions such as \nbanks, in which case it covers the entire asset/liability portfolio. It can also be adapted to \nindividual asset series such as our REIT indexes and the SCS residential indexes. \n \nTo obtain a risk of loss measure, one must first fit a probability distribution to the empirical \nseries. When Value-at-Risk was first used, the Gaussian or normal was commonly employed as \nthe fitted distribution; it is convenient since it is fully characterized by just two parameters, the \nmean and variance, which can be estimated easily from a sample of observations. \n \nWe now know, however, that the normal distribution is a poor model for most financial asset \nreturns and is particularly prone to large errors in Value-at-Risk. The main reason is that asset \nreturns often have thicker tails than the normal and hence larger probability of extreme loss. As \nwe have already seen, REITs and SCS depart significantly from the normal in this respect. \nConsequently, we resort here to three alternative models of the return distribution based on \nfitting the actual historical observations. \n \n\n13 \n \nThe first model will be called EM for the Efficient Maximization algorithm fit to the \nunconditional distribution. As shown in Table 3, excess kurtosis is present in all of the series. \nThis characteristic is typical of Gaussian mixtures with extensive regimes of small returns \ninterspersed with occasional extremely large returns, thus giving rise to fat-tails. Each Gaussian \ndistribution in the mixture would have its own mean and standard deviation while a fifth \nparameter is probability of being in one regime or the other (see Hamilton, 1994, pp. 685-689). \nThe EM algorithm uses maximum likelihood estimates to determine the parameters (Dempster, \nLaird and Rubin, 1977) and the Bayesian Information Criterion (BIC) to determine model \nselection. In all cases except 4 (San Diego, Washington, Miami and Tampa) two mixture were \nidentified as the optimal model incorporating regimes of low (high) volatility with a high (low) \nprobability of occurrence. \n \nThe second model is the Generalized Pareto Distribution (GPD), which is often used to fit thick-\ntailed empirical phenomena. The GPD relies on Extreme Value Theory. We adopt a Peaks over \nThreshold (POT) approach for its fit; (See Embrechts, Kluppelberg and Mikosch, 1997).12\n \n This \napproach utilizes the realizations of a random variable X in excess of a high threshold u; such a \nrealization is called an \u201cexceedence.\u201d When u is large, as it would be for tail realizations of \nfinancial time series, the distribution of exceedances tends to a GPD. The GPD parameters, the \nshape and scale parameters of exceedences, are estimated using maximum likelihood. The GPD \nhas proven successful in modeling the fat-tails characteristics for a number of financial \ninstruments such as currencies and equity returns (e.g., Cotter and Dowd, 2006).",
    "chunk_index": 11,
    "start_char": 29749,
    "end_char": 32971,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "realizations of a random variable X in excess of a high threshold u; such a \nrealization is called an \u201cexceedence.\u201d When u is large, as it would be for tail realizations of \nfinancial time series, the distribution of exceedances tends to a GPD. The GPD parameters, the \nshape and scale parameters of exceedences, are estimated using maximum likelihood. The GPD \nhas proven successful in modeling the fat-tails characteristics for a number of financial \ninstruments such as currencies and equity returns (e.g., Cotter and Dowd, 2006). \nThe third model is based on the GARCH (1,1) (Bollerslev, 1986) fit to non-stationary volatility \nin the return series. It makes some sense to fit a GARCH model to our series because non-\nstationarity in the parameters, particularly the variance, is often given as an explanation for the \nappearance of thick tails. For example, it is easy to show that a mixture of Gaussian distributions \nthat cycles through several volatility regimes will have an unconditional thick-tailed distribution. \nAlso we find significant ARCH effects for all series reported in Table 1. \n \n12 Alternatively, extreme tail returns could be modelled by Generalised Extreme Value (GEV) theory, which deals \nwith the distribution of the sample maxima. The GEV and POT approaches are analogous in the limit, but we prefer \nto use the POT approach because it (generally) uses one less parameter, and because the GEV approach does not \nutilise all extreme returns if extremes occur in clusters. \n\n14 \n \n \nGARCH differs from EM in that it models some persistence in the conditional volatility. EM, in \ncontrast, assumes that the volatility regime shifts randomly from period to period. \n \nFor each of these three models, we fit the empirical sample and derive distributional parameters \n(which are not reported for reasons of space.) Then, the risk of loss is calculated by examining \nthe left tails of the fitted distributions and parameters. \n \nWe report two loss statistics. The first is simply the fractile of the loss distribution; i.e., it is the \nloss that is met or exceeded with a particular probability such as 1%. The 1% probability of loss \nis the 99% fractile of the fitted loss distribution in the sense that 99% of the time the return will \nbe larger.13\n \n The second statistic is the average loss given that a particular loss fractile has been \nbreached. This is the expected value in the left tail of the return distribution conditional on the \nreturn being lower than a particular fractile. \nTable 5 has the results. The numbers there are percentage losses so they are the negatives of the \nmonthly realized returns. For example, the Generalized Pareto Distribution (GPD) produces an \nestimate of -27.539 percent for the average loss in the All REITs index given that its return has \nbeen less than or equal to -21.658%, the .999 loss fractile. \n \nThus, according to GPD, the All REITs index return will be less than -21.658% one month in \nevery thousand (83 years and 4 months) and when this happens the average return will be -",
    "chunk_index": 12,
    "start_char": 32438,
    "end_char": 35489,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "are the negatives of the \nmonthly realized returns. For example, the Generalized Pareto Distribution (GPD) produces an \nestimate of -27.539 percent for the average loss in the All REITs index given that its return has \nbeen less than or equal to -21.658%, the .999 loss fractile. \n \nThus, according to GPD, the All REITs index return will be less than -21.658% one month in \nevery thousand (83 years and 4 months) and when this happens the average return will be -\n27.539% Table 3 shows that the mean monthly return of this index is 0.884% and its return \nstandard deviation is 4.916%. Hence, a loss of -21.658% is 4.4 standard deviations below the \nmean. It should be noted that the .001 fractile of the Normal distribution is only 3.09 standard \ndeviations below the mean, which would imply a return of \u201conly\u201d -14.3%. The reality is much \nworse than that depicted by an assumption that returns are Normally distributed. Note also the \nEM results tend to be small relative to the GPD and GARCH estimates. This is a result of the \nparameters of the mixtures distributions where the high volatility distribution has a very low \nprobability of occurrence and results in the overall risk measures having lower magnitudes. \n \n13 The fractile of the loss distribution is 1 minus the fractile of the return distribution. \n\n15 \n \nPretty much the same story can be told for all the REIT indexes and for the SCS indexes as well \n(though the latter, of course, have much lower volatility.) \n \nMuch of the volatility in real estate is explained by broad market factors; indeed, as we have \nalready seen in Table 4, principal component factors explain 80 to 90 percent for both REIT and \nSCS indexes, with a few exceptions among the latter. However, individual real estate assets \nwould also be exposed to idiosyncratic risk, particularly for many families whose home is the \nmain, and undiversified, investment. Consequently, it is worthwhile to examine the distributions \nof residual or idiosyncratic risks, after common variation is excluded. To this end, we first \ncalculate the residuals after regressing REIT returns on three PC factors and SCS returns on six \nPC factors, (based on the findings above.) \n \nThe remaining residual risk is important for many reasons. In the future, it may be possible to \neliminate common variation using real estate futures. Even now, long short strategies commonly \nemployed by hedge funds can be engineered to eliminate systematic risk. But residual risk \nremains in both cases. \n \nThe issue we examine here is the impact of the shape of tail distributions on real estate residual \nrisk. This would be relevant, for instance, in computing value-at-risk for a hedge fund that \nfollows a tailored long/short strategy. The same methods can be employed. Table 6 has the \nresults. \n \nTo illustrate with an example, let\u2019s take the Unclassified REIT index, which Table 6 reports has \na Generalized Pareto Loss Distribution .95 fractile of 3.462.",
    "chunk_index": 13,
    "start_char": 35025,
    "end_char": 37992,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "impact of the shape of tail distributions on real estate residual \nrisk. This would be relevant, for instance, in computing value-at-risk for a hedge fund that \nfollows a tailored long/short strategy. The same methods can be employed. Table 6 has the \nresults. \n \nTo illustrate with an example, let\u2019s take the Unclassified REIT index, which Table 6 reports has \na Generalized Pareto Loss Distribution .95 fractile of 3.462. Table 3 shows that this index has a \ntotal return standard deviation of 5.242 and a mean of 0.730 while Table 4 reports that its R-\nsquare on the PC factors is .903. The residuals\u2019 standard deviation is thus 5.242(1-.903)1/2 or \napproximately 1.63, so the .95 fractile in Table 6 is 3.462/1.63 = 2.12 standard deviations below \nthe mean residual (which is zero.) Five percent of the months, Unclassified REITs will have \nidiosyncratic (non-market) returns less than 3.462 percent but the Normal distribution would \nhave indicated losses of only 1.65(1.63) = 2.69 percent. Again, as with raw returns, residual \n\n16 \n \nreturns have thick tails relative to the Normal distribution although the magnitude of the risk is \nconsiderably smaller. \n \nIII.D. Return Predictability \nGiven that REIT prices represent investor trading activity in real estate assets, it is interesting to \nascertain whether they have any predictive power for future real estate prices. The liquidity and \navailability of high frequency forward looking investor prices may have predictive power. As \nwe have already seen, REITs and SCS are correlated, and especially if one constructs the REIT \nseries in the same manner as followed by the SCS series. \n \nTo motivate the predictability analysis, we show plots of the time series of a selection of real \nestate log price series in Figure 5 between 1987 and 2009. We use an MA(3) of the REIT series \nto align it with adjustments made in the development made in constructing the SCS series \nalthough the conclusions are equally relevant where no adjustment is made. The positive \nperformance of all real estate series is evidenced where a similar pattern occurs for SCS and \nREIT series. Much of the sample has an upward trend for all series that appears to begin earlier \nfor REITs, followed by a sharp decline in real estate prices in recent times. \n \nTo avoid a spurious regression from these similar trends we fit a VAR to the returns data and \nresults are reported in Table 7. In Panel A we confirm that the log price series exhibit a unit root \nwhereas log returns are stationary. Next we report that REITs have predictive power for other \nreal estate series. Taking the Residential REIT series as an example, we see from the VAR \nmodel coefficients in Panel B that it has predictive power for the SCS series at lag 3 as well as \nhaving forecasting power for itself. Similarily the SCS series is able to predict the REIT series. \nThe VAR fit indicates strong explanatory power with over 90% of the variation in SCS returns \nbeing forecastable ahead of time with the REIT series.",
    "chunk_index": 14,
    "start_char": 37569,
    "end_char": 40593,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "predictive power for other \nreal estate series. Taking the Residential REIT series as an example, we see from the VAR \nmodel coefficients in Panel B that it has predictive power for the SCS series at lag 3 as well as \nhaving forecasting power for itself. Similarily the SCS series is able to predict the REIT series. \nThe VAR fit indicates strong explanatory power with over 90% of the variation in SCS returns \nbeing forecastable ahead of time with the REIT series. Also we seea good fit for the VAR \nbetween actual and response values in Figure 6, and especially for the SCS series. We report \nmodel coefficients from a VAR (5) chosen using both the AIC and BIC criterion although we \nalso examined different lag specifications of the VAR and similar results are observed. It is clear \nthat the All REITs series also has predictive power for the SCS series. \n \n\n17 \n \nWe also examine causality. To motivate the causality, tests we examine autocorrelations and \ncross correlations of the REIT and SCS series. An illustration for the Composite10 and two \nREITs series, the Residential and All REIT series are given in Figure 7. Whilst all series are \npositively autocorrelated, significant effects remain at longer lags, more than 20, for the SCS \nseries. This is to be expected given the moving average construction of the SCS series. Both \nREITs show, however, lesser predictability in themselves but follow a similar pattern to each \nother. The cross autocorrelations provide evidence of correlation between the Composite10 and \nlags of the REIT series. Both residential and All REIT series are significantly impacted by \nmovements in the SCS series and vice versawith a positive sign for a number of lags,. \n \nFormally testing for the REIT series being able to help in forecasting the SCS series we find \nevidence of Granger causality. In Table 7, panel C, we report significant F-statistics that suggest \nthat we should not reject the null hypothesis that the SCS series with its lags have an ability to \nforecast the REIT series and similarity for the null that the REIT series forecasts SCS returns at \nconventional levels. This direction is bi-directional suggesting feedback between the proxy real \nestate series. \n \nIn panel D of Table 7 we present multi-period forecasts of the respective returns series. Lags of \nREITs and the SCS series are able to aid the forecasting of the other real estate surrogate series \nfor a number of periods. The forecast returns for all real estate indicators are less that the mean \nreturns reported in Table 3 for most periods. For example, the return of 0.230% at the beginning of \nthe forecast horizon is less than 0.327% for the SCS series in Table 3. Overall, the VAR gives us \nreasonably precise forecasts of future SCS series although the standard errors of the REIT series \nforecasts are relatively much larger. \n \nWe also look at the evidence of impulse response functions and forecasted variance \ndecomposition for forecasting real estate returns.",
    "chunk_index": 15,
    "start_char": 40127,
    "end_char": 43127,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "in Table 3 for most periods. For example, the return of 0.230% at the beginning of \nthe forecast horizon is less than 0.327% for the SCS series in Table 3. Overall, the VAR gives us \nreasonably precise forecasts of future SCS series although the standard errors of the REIT series \nforecasts are relatively much larger. \n \nWe also look at the evidence of impulse response functions and forecasted variance \ndecomposition for forecasting real estate returns. The impulse responses suggest that both REIT \nand SCS series respond positively to innovations in the other series. Impulse response functions \nare presented in Figure 8 for the VAR where we order the variables as Composite10 followed by \nthe REIT series (although ordering has no qualitative effect on responses). For both the REIT \nand Composite 10 series, there are significant responses to innovations in the other series and \n\n18 \n \nthese remain for approximately six months. The pattern in impulse responses is similar for both \nREIT series. We also see that the own-series response to innovations for SCS is stronger than \nboth REIT series with both a larger effect and one that decays more slowly, significant for \ntwenty four months. \n \nTo see how much of the forecast of the future error variance is explained by innovations in the \nreal estate series we utilize our VAR output and present these in Figure 9. Whilst, the forecast \nerror variance decomposition suggests that the main response of both REIT and SCS series is \nfrom the orthogonal innovations in the respective series, there is also a considerable fraction \nexplained by innovations in the other series. Innovations in both sets of series have explanatory \npower of over 5% for the multi-step forecast error variance of the other series within a few lags \nwith a similar pattern of results for both REIT series. Thus, overall we report a large spectrum of \nresults that support predictability in real estate indicators. \n \nConclusion \nWe study return data for U.S. Real Estate Investment Trusts (REITs) over the past three decades. \nREITs have somewhat less market risk than other equity classes; their betas against a broad \nmarket index average about .65. In contrast, the more commonly used real estate indicator, the \nS&P/Case-Shiller (SCS) residential real estate indexes have a much lower beta, although this in \npart is due to its moving average construction. \n \nREIT characteristics differ to some extent from those of the SCSindexes. Broad REIT indexes \nare about five times more volatile than the SCS indexes and have three times higher returns on \naverage. The associated risk of loss measures for REITS are also considerably higher than for \nthe SCS series. Also, unlike SCS returns, REIT returns exhibit have very little autocorrelation. \nExtracting principal components from REIT and SCS returns reveals another difference; six \nfactors are required to explain 90% of the volatility in SCS returns while only three factors are \nrequired for REIT returns. These distinguishing features must be partly attributable to differing \nmethods of index construction.",
    "chunk_index": 16,
    "start_char": 42670,
    "end_char": 45772,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "of loss measures for REITS are also considerably higher than for \nthe SCS series. Also, unlike SCS returns, REIT returns exhibit have very little autocorrelation. \nExtracting principal components from REIT and SCS returns reveals another difference; six \nfactors are required to explain 90% of the volatility in SCS returns while only three factors are \nrequired for REIT returns. These distinguishing features must be partly attributable to differing \nmethods of index construction. \n \n\n19 \n \nREITs prices are forward looking and constructed based on investment transactions whereas in \ncontrast, SCS series are obtained using a moving average of previous index values. In turn, the \nREIT series returns have predictive power for the SCS series. Inclusion of REITs improves the \nforecastability of the commonly cited SCS series. Forward looking REIT returns provide \nreasonable precise forecasts of future SCS returns over a number of periods. Moreover, there is \ncausality between the series, suggesting that analysis of current REIT return values is beneficial \nfor predicting future SCS returns. \n \n \n\n20 \n \nReferences \n \nBollerslev, Tim, 1986. Generalized autoregressive conditional heteroskedasticity, Journal of \nEconometrics, vol. 31(3), pages 307-327. \nCase, Karl E. and Robert J. Shiller 1987, Prices of Single-Family Homes Since 1970: New \nIndexes for Four Cities, New England Economic Review, September/October, 46\u201356. \nCase, Karl E. and Robert J. Shiller, 1994, A Decade of Boom and Bust in Prices of Single \nFamily Homes: Boston and Los Angeles, 1983 to 1993, New England Economic Review, \nMarch/April, 39-51. \nCase, Karl E. & Robert J. Shiller, 2003. Is There a Bubble in the Housing Market?, Brookings \nPapers on Economic Activity, Economic Studies Program, 34(2003-2), pp 299-362. \nChan, Su Han, John Erickson, and Ko Wang, 2003, Real Estate Investment Trusts: Structure, \nPerformance and Investment Opportunities, New York: Oxford University Press. \nChan, K. C., Patric H. Hendershott and Anthony B. Sanders, 1990, Risk and Return on Real \nEstate: Evidence from Equity REITs, AREUEA Journal, Vol. 18, No. 4, 1990 \nChen, Nai-Fu, Richard. Roll and Stephen. A. Ross. 1986, Economic Forces and the Stock \nMarket: Testing the APT and Alternative Asset Pricing Theories. Journal of Business 59, 383-\n403. \nChui, Andy C.W., Sheridan Titman, and K.C. John Wei, 2003, The Cross-Section of Expected \nREIT Returns, Real Estate Economics, 31, 451\u2013479. \nClayton Jim and MacKinnon Greg, 2003, The relative importance of stock, bond and real estate \nfactors in explaining REIT returns, Journal of Real Estate Finance and Economics 27, 39-60. \nCotter, John, and Kevin Dowd, 2006, Extreme Spectral Risk Measures: An Application to \nFutures Clearinghouse Margin Requirements, Journal of Banking and Finance, 30, 3469-3485. \nCotter, John, and Simon Stevenson, 2008, Modelling Long Memory in REITs, Real Estate \nEconomics, 36, 533-554. \nDempster, A. P., N. M. Laird, D. B. Rubin, 1977, Maximum likelihood from incomplete data via \nthe EM algorithm, Journal of the Royal Statistical Society, Series B, vol. 39, pages 1-38. \nDerwall, Jeroen., Joop Huij, Dirk Brounen, and Wessel Marquering, 2009, REIT Momentum and \nthe Performance of Real Estate Mutual Funds, Financial Analysts Journal, 65, 5, 24-34 \nEmbrechts, Paul., Kluppelberg Claudio., Mikosch, Thomas., 1997, Modelling Extremal Events \nfor Insurance and Finance.",
    "chunk_index": 17,
    "start_char": 45289,
    "end_char": 48705,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "Simon Stevenson, 2008, Modelling Long Memory in REITs, Real Estate \nEconomics, 36, 533-554. \nDempster, A. P., N. M. Laird, D. B. Rubin, 1977, Maximum likelihood from incomplete data via \nthe EM algorithm, Journal of the Royal Statistical Society, Series B, vol. 39, pages 1-38. \nDerwall, Jeroen., Joop Huij, Dirk Brounen, and Wessel Marquering, 2009, REIT Momentum and \nthe Performance of Real Estate Mutual Funds, Financial Analysts Journal, 65, 5, 24-34 \nEmbrechts, Paul., Kluppelberg Claudio., Mikosch, Thomas., 1997, Modelling Extremal Events \nfor Insurance and Finance. Berlin: Springer Verlag. \nFama, Eugene F., and Kenneth R. French. 1993, Common Risk Factors in the Returns on \nStocksand Bonds, Journal of Financial Economics, 33, 3\u201356. \nGoyal, Amit and Ivo Welch, 2008, A Comprehensive Look at The Empirical Performance of \nEquity Premium Prediction, Review of Financial Studies, 21, 1455-1508. \nGyuorko, Joseph and Donald B. Keim, 1992, What Does the Stock Market Tell Us About Real \nEstate Returns?, Journal of the American Real Estate and Urban Economics Association, 20, \n457-85. \n\n21 \n \nGyourko, Joseph, and Edward Nelling, 1996, Systematic risk and diversification in the equity \nREIT market, Real Estate Economics 24, 493-515. \nHamilton, James D. 1994, Time series analysis, Princeton: Princeton University Press. \nJaffe, Jeffrey, 1991, Taxes and the capital structure of partnerships, REITs, and related entities, \nJournal of Finance, 46, 401-407. \nLizieri Colin, Satchell Stephen, Zhang Qi, 2007, The underlying return-generating factors for \nREIT returns: An application of independent component analysis, Real Estate Economics, 35, \n569-598. \nMian, Atif R. and Sufi, Amir, 2009, The Consequences of Mortgage Credit Expansion: Evidence \nfrom the U.S. Mortgage Default Crisis, Quarterly Journal of Economics, 124(4), 1449-1496. \nMuhlhofer, T, 2008, Why do REIT Returns Poorly Reflect Property Returns? Unrealizable \nAppreciation Gains due to Trading Constraints as the Solution to the Short-Term Disparity, \nWorking Paper, Kelley School of Business, Indiana University. \nOoi, Joseph T. L. Jingliang Wang and James R. Webb, 2009, Idiosyncratic Risk and REIT \nReturns, The Journal of Real Estate Finance and Economics 38, 420-442 \nPavlov and Wachter (2009) REITs and Underlying Real Estate Markets: Is There a Link?, \nWorking Paper. \nPeterson, James D. and Cheng-Ho Hsieh, 1997, Do Common Risk Factors in the Returns on \nStocks and Bonds Explain Returns on REITs?. Real Estate Economics, 25, 321-345. \nRoll, Richard, 2011, The Possible Misdiagnosis of a Crises, Financial Analysts Journal, \nForthcoming. \nRoss, Stephen A. and Randall C. Zisler 1991, Risk and return in real estate, The Journal of Real \nEstate Finance and Economics, 4, 175-190. \nScholes, Myron and Joseph T. Williams, 1977, Estimating betas from nonsynchronous data, \nJournal of Financial Economics 5, 309-327. \nStandard and Poors, 2009, S&P/Case-Shiller home price indices \u2013 index methodology, \nNovember. \nYoung Michael S. and Richard A. Graff, 1995, Real-Estate is not Normal - A fresh look at Real-\nEstate Return Distributions, Journal Of Real Estate Finance And Economics, 10, 225-259. \n \n \n \n \n \n \n \n \n\n22 \n \n \nTable 1: Individual REITS \n \n \nJanuary \n1980 \nMay \n2009 \nTotal Market \nCapitalization \n($Millions) \n$2,453 \n$194,993 \n \n \n \nNumber of REITs \nAll \n90 \n148 \nEquity \n53 \n115 \nMortgage \n29 \n26 \nHybrid \n18 \n7 \n \n \n \nProperty Type \nUnknown \n8 \n0 \nUnclassified",
    "chunk_index": 18,
    "start_char": 48131,
    "end_char": 51583,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "5, 309-327. \nStandard and Poors, 2009, S&P/Case-Shiller home price indices \u2013 index methodology, \nNovember. \nYoung Michael S. and Richard A. Graff, 1995, Real-Estate is not Normal - A fresh look at Real-\nEstate Return Distributions, Journal Of Real Estate Finance And Economics, 10, 225-259. \n \n \n \n \n \n \n \n \n\n22 \n \n \nTable 1: Individual REITS \n \n \nJanuary \n1980 \nMay \n2009 \nTotal Market \nCapitalization \n($Millions) \n$2,453 \n$194,993 \n \n \n \nNumber of REITs \nAll \n90 \n148 \nEquity \n53 \n115 \nMortgage \n29 \n26 \nHybrid \n18 \n7 \n \n \n \nProperty Type \nUnknown \n8 \n0 \nUnclassified \n23 \n13 \nDiversified \n15 \n12 \nHealth Care \n0 \n13 \nIndustrial/Office \n8 \n25 \nLodging/Resorts \n3 \n12 \nMortgage \n19 \n27 \nResidential \n8 \n16 \nRetail \n6 \n25 \nSelf Storage \n0 \n5 \n \n \n \n\n23 \n \nTable 2: Summary cross-sectional statistics for individual REITs \n \nAnnual summary statistics are presented for individual REITs that have a full year of data, 1980 \nthrough 2008. The cross-sectional mean and standard deviation (mean of the standard \ndeviations) of return are in percent per month. Betas are computed against the SP500 index. \n \n \nNumber \nof REITs \nReturn (%/month) \nBeta \n \nMean \nStandard \nDeviation \nMean \nStandard \nDeviation \n1980 \n80 \n0.764 \n10.876 \n1.044 \n0.611 \n1981 \n72 \n-0.681 \n9.211 \n0.109 \n0.248 \n1982 \n70 \n1.230 \n8.691 \n0.470 \n0.825 \n1983 \n70 \n0.742 \n8.884 \n0.536 \n0.910 \n1984 \n73 \n-0.109 \n6.423 \n0.342 \n0.615 \n1985 \n71 \n-0.351 \n7.524 \n0.413 \n0.670 \n1986 \n91 \n0.183 \n7.655 \n0.325 \n0.514 \n1987 \n106 \n-2.455 \n9.458 \n0.466 \n0.375 \n1988 \n111 \n-0.202 \n6.988 \n0.487 \n0.741 \n1989 \n120 \n-2.268 \n9.941 \n0.471 \n0.698 \n1990 \n123 \n-4.132 \n13.219 \n0.277 \n0.817 \n1991 \n120 \n0.439 \n11.404 \n0.343 \n0.867 \n1992 \n137 \n-0.679 \n10.754 \n-0.060 \n2.270 \n1993 \n140 \n1.280 \n11.173 \n0.479 \n2.419 \n1994 \n185 \n-0.653 \n7.697 \n0.302 \n0.867 \n1995 \n220 \n0.765 \n7.228 \n0.227 \n3.507 \n1996 \n207 \n1.770 \n6.717 \n0.147 \n1.156 \n1997 \n191 \n0.736 \n7.232 \n0.274 \n0.608 \n1998 \n200 \n-2.238 \n8.512 \n0.489 \n0.700 \n1999 \n211 \n-1.442 \n8.346 \n0.180 \n0.657 \n2000 \n202 \n-0.154 \n8.675 \n0.018 \n0.709 \n2001 \n192 \n0.829 \n9.268 \n0.296 \n0.680 \n2002 \n185 \n0.026 \n7.173 \n0.140 \n0.390 \n2003 \n177 \n1.915 \n6.492 \n0.484 \n0.972 \n2004 \n173 \n1.260 \n7.887 \n1.129 \n1.259 \n2005 \n195 \n-0.464 \n6.444 \n1.069 \n0.988 \n2006 \n185 \n1.262 \n6.308 \n1.088 \n1.236 \n2007 \n161 \n-3.238 \n9.606 \n1.515 \n1.331 \n2008 \n152 \n-6.812 \n20.808 \n1.849 \n1.172 \n \n \n \n \n\n24 \n \nTable 3. Descriptive Return Statistics for REIT Indexes and S&P/Case-Shiller\u00ae Indexes \n \nREIT results are presented first by REIT type and then by Property type. The mean and standard \ndeviation are in percent per month. Auto-correlation is first order. Beta is against the S&P500. \nExcess Kurtosis is relative to the normal distribution. Normality is the Jarque-Bera statistic \nwhose 5% critical value is 5.99; hence all series are non-normal. The sample period is January \n1980 through May 2009 for REITs and the S&P500 and February 1987 through May 2009 for \nthe SCS indexes. \n \n \nMean \nStandard \nDeviation Skewness \nExcess \nKurtosis Normality \nAuto \ncorrelation \nBeta \nValue-weighted REIT Indexes \nAll REITs \n0.884 \n4.916 \n-0.918 \n11.999 \n1237.3 \n0.138 \n0.625 \nEquity \n0.933 \n5.062 \n-0.898 \n12.974 \n1506.3 \n0.118 \n0.640 \nMortgage \n0.527 \n6.317 \n-1.027 \n5.794 \n176.4 \n0.103 \n0.631 \nHybrid \n0.635 \n5.380 \n-0.969 \n7.832 \n397.5 \n0.189 \n0.511 \nUnclassified \n0.730 \n5.242 \n-0.505 \n6.402 \n184.7 \n0.135 \n0.602 \nDiversified \n0.972 \n5.727 \n-0.444 \n12.174 \n1246.0 \n0.148 \n0.642",
    "chunk_index": 19,
    "start_char": 51013,
    "end_char": 54447,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "through May 2009 for REITs and the S&P500 and February 1987 through May 2009 for \nthe SCS indexes. \n \n \nMean \nStandard \nDeviation Skewness \nExcess \nKurtosis Normality \nAuto \ncorrelation \nBeta \nValue-weighted REIT Indexes \nAll REITs \n0.884 \n4.916 \n-0.918 \n11.999 \n1237.3 \n0.138 \n0.625 \nEquity \n0.933 \n5.062 \n-0.898 \n12.974 \n1506.3 \n0.118 \n0.640 \nMortgage \n0.527 \n6.317 \n-1.027 \n5.794 \n176.4 \n0.103 \n0.631 \nHybrid \n0.635 \n5.380 \n-0.969 \n7.832 \n397.5 \n0.189 \n0.511 \nUnclassified \n0.730 \n5.242 \n-0.505 \n6.402 \n184.7 \n0.135 \n0.602 \nDiversified \n0.972 \n5.727 \n-0.444 \n12.174 \n1246.0 \n0.148 \n0.642 \nIndustrial Office 0.717 \n6.387 \n-0.639 \n11.504 \n1084.7 \n0.042 \n0.656 \nLodging Resort 0.281 \n8.576 \n0.961 \n14.353 \n1944.7 \n0.164 \n0.824 \nResidential \n1.005 \n5.549 \n-0.731 \n6.836 \n247.2 \n0.084 \n0.636 \nRetail \n1.077 \n5.672 \n-0.546 \n17.321 \n3025.5 \n0.102 \n0.632 \n \nS&P500 \n0.594 \n4.539 \n-0.962 \n6.353 \n219.2 \n0.076 \n1.000 \n \nS&P/Case-Shiller\u00ae (SCS) Indexes \nComposite10 \n0.327 \n0.907 \n-0.869 \n4.348 \n16.381 \n0.962 \n0.018 \nBoston \n0.276 \n0.874 \n-0.288 \n3.145 \n3.432 \n0.802 \n-0.002 \nCharlotte \n0.235 \n0.544 \n-1.043 \n7.256 \n36.381 \n0.673 \n0.018 \nChicago \n0.309 \n0.877 \n-1.617 \n9.929 \n231.531 \n0.864 \n0.032 \nCleveland \n0.227 \n0.748 \n-1.945 \n14.443 \n321.325 \n0.539 \n0.030 \nDenver \n0.333 \n0.694 \n-0.912 \n5.058 \n17.876 \n0.805 \n0.019 \nLas Vegas \n0.197 \n1.425 \n-0.461 \n7.833 \n8.145 \n0.938 \n0.028 \nLos Angeles \n0.370 \n1.302 \n-0.528 \n4.007 \n14.335 \n0.954 \n0.027 \nMiami \n0.283 \n1.180 \n-1.312 \n6.061 \n20.654 \n0.931 \n0.017 \nNew York \n0.310 \n0.783 \n-0.247 \n3.235 \n14.555 \n0.908 \n0.000 \nPortland \n0.477 \n0.807 \n-0.722 \n6.090 \n20.298 \n0.876 \n0.024 \nSan Diego \n0.364 \n1.238 \n-0.293 \n4.553 \n4.729 \n0.928 \n0.023 \nSan Francisco \n0.349 \n1.388 \n-0.828 \n5.273 \n9.645 \n0.916 \n0.046 \nTampa \n0.223 \n1.020 \n-1.043 \n6.541 \n17.548 \n0.894 \n0.034 \nWashington \n0.359 \n0.997 \n-0.338 \n4.065 \n6.301 \n0.931 \n0.018 \n \n \n \n\n25 \n \n \nTable 4. Principal Components Analysis of Real Estate Returns. \nPrincipal Components are extracted from the covariance matrix of 6 REIT Indexes and 14 \nS&P/Case-Shiller\u00ae (SCS) residential real estate indexes. Loadings on the first two principal \ncomponents and R-squares from regressions on the first three principal components for the REIT \nseries and first six principal components for SCS series are reported. \n \n \nLoading on \nFirst PC \nLoading on \nSecond PC \nAdjusted \nR-Square \n \nValue-weighted REIT Indexes \nUnclassified \n0.299 \n0.208 \n0.903 \nDiversified \n0.396 \n0.209 \n0.873 \nIndustrial Office \n0.421 \n0.228 \n0.913 \nLodging Resort \n0.562 \n-0.817 \n0.998 \nResidential \n0.340 \n0.359 \n0.793 \nRetail \n0.381 \n0.253 \n0.854 \n \n \n \n \n \nS&P/Case-Shiller\u00ae (SCS) Indexes \nBoston \n0.189 \n-0.396 \n0.918 \nCharlotte \n0.090 \n-0.112 \n0.566 \nChicago \n0.208 \n-0.103 \n0.742 \nCleveland \n0.142 \n-0.263 \n0.772 \nDenver \n0.109 \n-0.244 \n0.775 \nLas Vegas \n0.376 \n0.578 \n0.983 \nLos Angeles \n0.380 \n-0.053 \n0.933 \nMiami \n0.321 \n0.370 \n0.943 \nNew York \n0.189 \n-0.074 \n0.892 \nPortland \n0.160 \n0.135 \n0.833 \nSan Diego \n0.351 \n-0.125 \n0.941 \nSan Francisco \n0.388 \n-0.359 \n0.925 \nTampa \n0.272 \n0.204 \n0.915 \nWashington \n0.289 \n-0.087 \n0.877 \n \n \n \n \n\n26 \n \nTable 5. Measures of the Risk of Loss for Real Estate Index Returns \nRisk of loss estimates are reported for three different models of the distribution of real estate \nindex returns: (1) EM, the Efficient Maximization algorithm fit to the unconditional distribution; \n(2) The Generalized Pareto Distribution (GPD); and (3) GARCH (1,1). \u201cLoss\u201d is the negative of \nthe loss distribution\u2019s fractile for the probability reported, which indicates the likelihood that the \nmonthly observed return will exceed minus this level.",
    "chunk_index": 20,
    "start_char": 53857,
    "end_char": 57487,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "Risk of Loss for Real Estate Index Returns \nRisk of loss estimates are reported for three different models of the distribution of real estate \nindex returns: (1) EM, the Efficient Maximization algorithm fit to the unconditional distribution; \n(2) The Generalized Pareto Distribution (GPD); and (3) GARCH (1,1). \u201cLoss\u201d is the negative of \nthe loss distribution\u2019s fractile for the probability reported, which indicates the likelihood that the \nmonthly observed return will exceed minus this level. \u201cAverage loss\u201d is the negative of the \nexpected return conditional on the loss exceeding the reported loss fractile. \n \n \n \nEM \nGPD \nGARCH \n \nLoss \nFractile \nLoss \nAverage \nLoss \nLoss \nAverage \nLoss \nLoss \nAverage \nLoss \nValue-weighted REIT Indexes \nAll REITs \n0.95 \n5.689 \n8.391 \n7.539 \n10.613 \n6.565 \n9.639 \n0.99 \n8.461 \n10.842 \n12.269 \n16.283 \n11.296 \n15.310 \n0.999 \n11.569 \n13.697 \n21.658 \n27.539 \n20.685 \n26.566 \nEquity \n0.95 \n5.877 \n8.540 \n7.515 \n10.895 \n6.480 \n9.860 \n0.99 \n8.699 \n11.034 \n12.552 \n17.560 \n11.517 \n16.525 \n0.999 \n11.861 \n13.940 \n24.250 \n33.036 \n23.215 \n32.001 \nMortgage \n0.95 \n8.272 \n11.035 \n9.079 \n12.150 \n8.379 \n11.450 \n0.99 \n11.918 \n14.258 \n14.006 \n17.154 \n13.307 \n16.454 \n0.999 \n16.004 \n18.013 \n21.270 \n24.531 \n20.570 \n23.831 \nHybrid \n0.95 \n7.184 \n9.806 \n8.213 \n11.320 \n7.300 \n10.407 \n0.99 \n10.424 \n12.670 \n13.043 \n16.910 \n12.130 \n15.996 \n0.999 \n14.055 \n16.007 \n22.076 \n27.365 \n21.163 \n26.452 \nUnclassified \n0.95 \n7.073 \n9.785 \n8.739 \n11.818 \n7.956 \n11.035 \n0.99 \n10.306 \n12.643 \n13.705 \n16.732 \n12.921 \n15.949 \n0.999 \n13.929 \n15.973 \n20.665 \n23.621 \n19.882 \n22.838 \nDiversified \n0.95 \n6.829 \n9.783 \n8.708 \n12.782 \n7.732 \n11.807 \n0.99 \n10.061 \n12.641 \n14.988 \n20.269 \n14.013 \n19.293 \n0.999 \n13.684 \n15.970 \n27.338 \n34.990 \n26.362 \n34.014 \nIndustrial Office \n0.95 \n7.794 \n10.956 \n9.157 \n14.955 \n8.219 \n14.017 \n0.99 \n11.414 \n14.156 \n17.416 \n27.251 \n16.478 \n26.312 \n0.999 \n15.471 \n17.884 \n40.146 \n61.091 \n39.208 \n60.153 \nLodging Resort \n0.95 \n11.715 \n15.043 \n11.607 \n19.807 \n10.655 \n18.855 \n0.99 \n16.685 \n19.437 \n23.469 \n36.801 \n22.517 \n35.849 \n0.999 \n22.256 \n24.556 \n54.421 \n81.145 \n53.469 \n80.193 \nResidential \n0.95 \n7.117 \n10.185 \n9.634 \n12.590 \n8.679 \n11.634 \n0.99 \n10.482 \n13.160 \n14.448 \n17.083 \n13.492 \n16.128 \n0.999 \n14.254 \n16.626 \n20.444 \n22.682 \n19.488 \n21.726 \nRetail \n0.95 \n6.314 \n9.268 \n8.205 \n12.392 \n6.934 \n11.121 \n0.99 \n9.376 \n11.975 \n14.355 \n20.860 \n13.084 \n19.589 \n0.999 \n12.808 \n15.128 \n29.512 \n41.727 \n28.241 \n40.456 \n \n \n \n \n \n \n \n \nSP500 \n0.95 \n6.009 \n8.280 \n7.315 \n8.950 \n6.629 \n8.265 \n0.99 \n8.744 \n10.698 \n10.023 \n11.134 \n9.337 \n10.448 \n0.999 \n11.810 \n13.515 \n12.460 \n13.100 \n11.775 \n12.414 \n\n27 \n \nS&P/Case-Shiller\u00ae (SCS) Indexes \nComposite10 \n \n0.95 \n0.826 \n1.446 \n1.625 \n1.868 \n1.071 \n1.314 \n0.99 \n1.303 \n1.868 \n2.028 \n2.165 \n1.474 \n1.611 \n0.999 \n1.839 \n2.360 \n2.319 \n2.379 \n1.765 \n1.825 \nBoston \n \n0.95 \n1.011 \n1.614 \n1.507 \n1.954 \n1.196 \n1.643 \n0.99 \n1.544 \n2.085 \n2.173 \n2.833 \n1.863 \n2.522 \n0.999 \n2.141 \n2.634 \n3.714 \n4.864 \n3.403 \n4.553 \nCharlotte \n \n0.95 \n0.543 \n0.976 \n1.027 \n1.289 \n0.771 \n1.032 \n0.99 \n0.866 \n1.261 \n1.448 \n1.707 \n1.192 \n1.450 \n0.999 \n1.227 \n1.593 \n2.042 \n2.296 \n1.786 \n2.040 \nChicago \n \n0.95 \n0.862 \n1.469 \n1.528 \n1.933 \n1.105 \n1.510 \n0.99 \n1.348 \n1.898 \n2.193 \n2.513 \n1.770 \n2.090 \n0.999 \n1.892 \n2.398 \n2.910 \n3.139 \n2.487 \n2.715 \nCleveland \n \n0.95 \n0.784 \n1.268 \n1.183 \n1.544 \n0.847 \n1.208 \n0.99 \n1.203 \n1.638 \n1.756 \n2.156 \n1.420 \n1.820 \n0.999 \n1.672 \n2.069 \n2.685 \n3.148 \n2.349 \n2.813 \nDenver \n \n0.95 \n0.636 \n1.215 \n1.325 \n1.569 \n0.957 \n1.201 \n0.99 \n1.037 \n1.570 \n1.727 \n1.910 \n1.359 \n1.541 \n0.999 \n1.487 \n1.983 \n2.133 \n2.254 \n1.765 \n1.886 \nLas Vegas \n \n0.95 \n1.536 \n2.173 \n1.829 \n4.062 \n1.390 \n2.974 \n0.99",
    "chunk_index": 21,
    "start_char": 56992,
    "end_char": 60675,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "2.042 \n2.296 \n1.786 \n2.040 \nChicago \n \n0.95 \n0.862 \n1.469 \n1.528 \n1.933 \n1.105 \n1.510 \n0.99 \n1.348 \n1.898 \n2.193 \n2.513 \n1.770 \n2.090 \n0.999 \n1.892 \n2.398 \n2.910 \n3.139 \n2.487 \n2.715 \nCleveland \n \n0.95 \n0.784 \n1.268 \n1.183 \n1.544 \n0.847 \n1.208 \n0.99 \n1.203 \n1.638 \n1.756 \n2.156 \n1.420 \n1.820 \n0.999 \n1.672 \n2.069 \n2.685 \n3.148 \n2.349 \n2.813 \nDenver \n \n0.95 \n0.636 \n1.215 \n1.325 \n1.569 \n0.957 \n1.201 \n0.99 \n1.037 \n1.570 \n1.727 \n1.910 \n1.359 \n1.541 \n0.999 \n1.487 \n1.983 \n2.133 \n2.254 \n1.765 \n1.886 \nLas Vegas \n \n0.95 \n1.536 \n2.173 \n1.829 \n4.062 \n1.390 \n2.974 \n0.99 \n2.254 \n2.808 \n4.288 \n10.059 \n4.733 \n5.042 \n0.999 \n3.058 \n3.547 \n15.880 \n38.326 \n5.321 \n5.494 \nLos Angeles \n \n0.95 \n1.578 \n2.442 \n2.223 \n2.797 \n1.690 \n2.264 \n0.99 \n2.384 \n3.156 \n3.126 \n3.796 \n2.593 \n3.264 \n0.999 \n3.289 \n3.987 \n4.687 \n5.524 \n4.155 \n4.992 \nMiami \n \n0.95 \n0.741 \n1.284 \n1.987 \n2.322 \n1.664 \n1.999 \n0.99 \n0.887 \n1.660 \n2.542 \n2.691 \n2.220 \n2.369 \n0.999 \n1.641 \n2.097 \n2.848 \n2.895 \n2.525 \n2.572 \nNew York \n \n0.95 \n0.916 \n1.537 \n1.509 \n1.782 \n1.314 \n1.588 \n0.99 \n1.423 \n1.986 \n1.956 \n2.186 \n1.762 \n1.992 \n0.999 \n1.993 \n2.509 \n2.477 \n2.657 \n2.282 \n2.462 \nPortland \n \n0.95 \n0.685 \n1.458 \n1.720 \n2.154 \n1.182 \n1.615 \n0.99 \n1.166 \n1.883 \n2.432 \n2.777 \n1.894 \n2.238 \n0.999 \n1.706 \n2.379 \n3.205 \n3.453 \n2.666 \n2.914 \nSan Diego \n \n0.95 \n1.239 \n2.010 \n2.147 \n2.915 \n1.506 \n2.273 \n0.99 \n1.765 \n2.597 \n3.322 \n4.347 \n2.681 \n3.706 \n0.999 \n2.647 \n3.280 \n5.720 \n7.270 \n5.078 \n6.629 \nSan Francisco \n \n0.95 \n1.635 \n2.488 \n2.435 \n3.013 \n1.950 \n2.528 \n0.99 \n2.457 \n3.215 \n3.391 \n3.799 \n2.906 \n3.314 \n0.999 \n3.378 \n4.062 \n4.291 \n4.539 \n3.806 \n4.053 \nTampa \n \n0.95 \n0.939 \n1.458 \n1.791 \n2.214 \n1.539 \n1.962 \n0.99 \n1.038 \n1.883 \n2.493 \n2.755 \n2.241 \n2.502 \n0.999 \n1.960 \n2.379 \n3.058 \n3.189 \n2.806 \n2.937 \nWashington \n \n0.95 \n0.810 \n1.466 \n1.915 \n2.369 \n1.745 \n2.199 \n0.99 \n1.209 \n1.894 \n2.657 \n3.040 \n2.487 \n2.870 \n0.999 \n1.837 \n2.393 \n3.521 \n3.820 \n3.351 \n3.650 \n \n\n28 \n \nTable 6. Measures of the Risk of Loss for Real Estate Index Residuals \nRisk of loss estimates are reported for three different models of the distribution of real estate \nindex residuals: (1) EM, the Efficient Maximization algorithm fit to the unconditional \ndistribution; (2) The Generalized Pareto Distribution (GPD); and (3) GARCH (1,1). \u201cLoss\u201d is \nthe negative of the loss distribution\u2019s fractile for the probability reported, which indicates the \nlikelihood that the monthly observed return will exceed minus this level. \u201cAverage loss\u201d is the \nnegative of the expected return given conditional on the loss exceeding the reported loss fractile. \nThe residuals are obtained by fitting returns to the first three principal components for the REIT \nseries and first six principal components for SCS series. \n \n \n \nEM \nGPD \nGARCH \n \nLoss \nFractile \nLoss \nAverage \nLoss \nLoss \nAverage \nLoss \nLoss \nAverage \nLoss \nValue-weighted REIT Indexes \nAll REITs \n0.95 \n1.331 \n1.669 \n1.503 \n1.866 \n1.511 \n1.874 \n0.99 \n1.883 \n2.157 \n2.106 \n2.299 \n2.115 \n2.308 \n0.999 \n2.501 \n2.725 \n2.515 \n2.593 \n2.523 \n2.602 \nEquity \n0.95 \n1.205 \n1.511 \n1.200 \n1.641 \n1.204 \n1.646 \n0.99 \n1.704 \n1.952 \n1.910 \n2.353 \n1.914 \n2.358 \n0.999 \n2.264 \n2.466 \n2.931 \n3.377 \n2.936 \n3.381 \nMortgage \n0.95 \n2.537 \n3.165 \n3.027 \n4.240 \n2.992 \n4.205 \n0.99 \n3.583 \n4.090 \n5.013 \n6.028 \n4.978 \n5.993 \n0.999 \n4.755 \n5.167 \n7.304 \n8.091 \n7.269 \n8.056 \nHybrid \n0.95 \n3.029 \n3.798 \n3.038 \n3.903 \n3.043 \n3.908 \n0.99 \n4.284 \n4.908 \n4.457 \n5.158 \n4.462 \n5.162 \n0.999 \n5.690 \n6.200 \n6.032",
    "chunk_index": 22,
    "start_char": 60113,
    "end_char": 63575,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "1.866 \n1.511 \n1.874 \n0.99 \n1.883 \n2.157 \n2.106 \n2.299 \n2.115 \n2.308 \n0.999 \n2.501 \n2.725 \n2.515 \n2.593 \n2.523 \n2.602 \nEquity \n0.95 \n1.205 \n1.511 \n1.200 \n1.641 \n1.204 \n1.646 \n0.99 \n1.704 \n1.952 \n1.910 \n2.353 \n1.914 \n2.358 \n0.999 \n2.264 \n2.466 \n2.931 \n3.377 \n2.936 \n3.381 \nMortgage \n0.95 \n2.537 \n3.165 \n3.027 \n4.240 \n2.992 \n4.205 \n0.99 \n3.583 \n4.090 \n5.013 \n6.028 \n4.978 \n5.993 \n0.999 \n4.755 \n5.167 \n7.304 \n8.091 \n7.269 \n8.056 \nHybrid \n0.95 \n3.029 \n3.798 \n3.038 \n3.903 \n3.043 \n3.908 \n0.99 \n4.284 \n4.908 \n4.457 \n5.158 \n4.462 \n5.162 \n0.999 \n5.690 \n6.200 \n6.032 \n6.551 \n6.037 \n6.556 \nUnclassified \n0.95 \n3.249 \n4.075 \n3.462 \n5.531 \n3.463 \n5.604 \n0.99 \n4.596 \n5.265 \n6.654 \n9.325 \n6.816 \n8.068 \n0.999 \n6.105 \n6.652 \n12.901 \n16.750 \n9.095 \n9.449 \nDiversified \n0.95 \n2.846 \n3.569 \n3.174 \n4.060 \n3.180 \n4.065 \n0.99 \n4.025 \n4.612 \n4.638 \n5.267 \n4.644 \n5.273 \n0.999 \n5.347 \n5.826 \n6.028 \n6.414 \n6.033 \n6.420 \nIndustrial Office \n0.95 \n2.117 \n2.655 \n2.062 \n2.919 \n1.999 \n2.856 \n0.99 \n2.994 \n3.431 \n3.407 \n4.419 \n3.345 \n4.357 \n0.999 \n3.978 \n4.334 \n5.767 \n7.051 \n5.704 \n6.988 \nLodging Resort \n0.95 \n2.577 \n3.231 \n2.747 \n4.268 \n2.713 \n4.234 \n0.99 \n3.644 \n4.175 \n5.135 \n6.931 \n5.101 \n6.897 \n0.999 \n4.841 \n5.275 \n9.320 \n11.597 \n9.286 \n11.563 \nResidential \n0.95 \n1.792 \n2.247 \n1.696 \n2.505 \n1.796 \n2.605 \n0.99 \n2.534 \n2.903 \n2.927 \n4.031 \n3.028 \n4.131 \n0.999 \n3.367 \n3.668 \n5.509 \n7.230 \n5.609 \n7.330 \nRetail \n0.95 \n2.100 \n2.633 \n2.112 \n2.671 \n2.083 \n2.642 \n0.99 \n2.969 \n3.402 \n3.034 \n3.443 \n3.005 \n3.415 \n0.999 \n3.944 \n4.298 \n3.942 \n4.204 \n3.913 \n4.175 \n \n \n \n \n \n \n \n \nSP500 \n0.95 \n2.894 \n3.629 \n3.079 \n4.439 \n3.126 \n4.516 \n0.99 \n4.093 \n4.690 \n5.221 \n6.798 \n4.924 \n6.270 \n0.999 \n5.437 \n5.924 \n8.894 \n10.845 \n7.524 \n8.450 \nS&P/Case-Shiller\u00ae (SCS) Indexes \nComposite10 \n \n0.95 \n0.111 \n0.138 \n1.625 \n1.868 \n1.071 \n1.314 \n0.99 \n0.156 \n0.179 \n2.028 \n2.165 \n1.474 \n1.611 \n0.999 \n0.207 \n0.226 \n2.319 \n2.379 \n1.765 \n1.825 \nBoston \n \n0.95 \n0.372 \n0.466 \n1.507 \n1.954 \n1.196 \n1.643 \n0.99 \n0.526 \n0.602 \n2.173 \n2.833 \n1.863 \n2.522 \n0.999 \n0.698 \n0.760 \n3.714 \n4.864 \n3.403 \n4.553 \nCharlotte \n \n0.95 \n0.607 \n0.762 \n1.027 \n1.289 \n0.771 \n1.032 \n0.99 \n0.859 \n0.984 \n1.448 \n1.707 \n1.192 \n1.450 \n0.999 \n1.141 \n1.244 \n2.042 \n2.296 \n1.786 \n2.040 \nChicago \n \n0.95 \n0.581 \n0.728 \n1.528 \n1.933 \n1.105 \n1.510 \n0.99 \n0.821 \n0.941 \n2.193 \n2.513 \n1.770 \n2.090 \n0.999 \n1.091 \n1.189 \n2.910 \n3.139 \n2.487 \n2.715 \n\n29 \n \nCleveland \n \n0.95 \n0.528 \n0.662 \n1.183 \n1.544 \n0.847 \n1.208 \n0.99 \n0.747 \n0.855 \n1.756 \n2.156 \n1.420 \n1.820 \n0.999 \n0.992 \n1.081 \n2.685 \n3.148 \n2.349 \n2.813 \nDenver \n \n0.95 \n0.453 \n0.568 \n1.325 \n1.569 \n0.957 \n1.201 \n0.99 \n0.641 \n0.734 \n1.727 \n1.910 \n1.359 \n1.541 \n0.999 \n0.851 \n0.928 \n2.133 \n2.254 \n1.765 \n1.886 \nLas Vegas \n \n0.95 \n0.263 \n0.330 \n0.289 \n0.387 \n0.278 \n0.411 \n0.99 \n0.372 \n0.426 \n0.449 \n0.533 \n0.457 \n0.544 \n0.999 \n0.494 \n0.538 \n0.639 \n0.706 \n0.591 \n0.618 \nLos Angeles \n \n0.95 \n0.416 \n0.520 \n0.419 \n0.546 \n0.427 \n0.554 \n0.99 \n0.587 \n0.671 \n0.625 \n0.746 \n0.634 \n0.754 \n0.999 \n0.780 \n0.848 \n0.901 \n1.012 \n0.909 \n1.020 \nMiami \n \n0.95 \n0.412 \n0.518 \n1.987 \n2.322 \n1.664 \n1.999 \n0.99 \n0.583 \n0.669 \n2.542 \n2.691 \n2.220 \n2.369 \n0.999 \n0.775 \n0.845 \n2.848 \n2.895 \n2.525 \n2.572 \nNew York \n \n0.95 \n0.400 \n0.500 \n1.509 \n1.782 \n1.314 \n1.588 \n0.99 \n0.565 \n0.647 \n1.956 \n2.186 \n1.762 \n1.992 \n0.999 \n0.750 \n0.817 \n2.477 \n2.657 \n2.282 \n2.462 \nPortland \n \n0.95 \n0.580 \n0.727 \n1.720 \n2.154 \n1.182 \n1.615 \n0.99 \n0.820 \n0.939 \n2.432 \n2.777 \n1.894 \n2.238 \n0.999 \n1.089 \n1.186 \n3.205 \n3.453 \n2.666 \n2.914 \nSan Diego \n \n0.95 \n0.481 \n0.604 \n2.147 \n2.915 \n1.506 \n2.273 \n0.99 \n0.680 \n0.780",
    "chunk_index": 23,
    "start_char": 63019,
    "end_char": 66571,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "1.020 \nMiami \n \n0.95 \n0.412 \n0.518 \n1.987 \n2.322 \n1.664 \n1.999 \n0.99 \n0.583 \n0.669 \n2.542 \n2.691 \n2.220 \n2.369 \n0.999 \n0.775 \n0.845 \n2.848 \n2.895 \n2.525 \n2.572 \nNew York \n \n0.95 \n0.400 \n0.500 \n1.509 \n1.782 \n1.314 \n1.588 \n0.99 \n0.565 \n0.647 \n1.956 \n2.186 \n1.762 \n1.992 \n0.999 \n0.750 \n0.817 \n2.477 \n2.657 \n2.282 \n2.462 \nPortland \n \n0.95 \n0.580 \n0.727 \n1.720 \n2.154 \n1.182 \n1.615 \n0.99 \n0.820 \n0.939 \n2.432 \n2.777 \n1.894 \n2.238 \n0.999 \n1.089 \n1.186 \n3.205 \n3.453 \n2.666 \n2.914 \nSan Diego \n \n0.95 \n0.481 \n0.604 \n2.147 \n2.915 \n1.506 \n2.273 \n0.99 \n0.680 \n0.780 \n3.322 \n4.347 \n2.681 \n3.706 \n0.999 \n0.904 \n0.985 \n5.720 \n7.270 \n5.078 \n6.629 \nSan Francisco \n \n0.95 \n0.323 \n0.404 \n2.435 \n3.013 \n1.950 \n2.528 \n0.99 \n0.456 \n0.522 \n3.391 \n3.799 \n2.906 \n3.314 \n0.999 \n0.606 \n0.660 \n4.291 \n4.539 \n3.806 \n4.053 \nTampa \n \n0.95 \n0.450 \n0.564 \n1.791 \n2.214 \n1.539 \n1.962 \n0.99 \n0.637 \n0.728 \n2.493 \n2.755 \n2.241 \n2.502 \n0.999 \n0.845 \n0.920 \n3.058 \n3.189 \n2.806 \n2.937 \nWashington \n \n0.95 \n0.515 \n0.647 \n1.915 \n2.369 \n1.745 \n2.199 \n0.99 \n0.729 \n0.837 \n2.657 \n3.040 \n2.487 \n2.870 \n0.999 \n0.968 \n1.057 \n3.521 \n3.820 \n3.351 \n3.650 \n \n \n \n \n\n30 \n \nTable 7. Prediction of Real Estate Index Returns \nPredictions for a sample of series are presented: All REITs Value Weighted Index, REITs \nResidential Property Value Weighted Index respectively with the SCS Composite-10 index. A \nMA(3) of both REIT series is used. The unit root tests are reported in Panel A. ADF is the \nAugmented Dickey Fuller test, PP is the Phillips-Perron test and Statitionarity is the KPSS test. \nEach unit root test statistic is followed by p-values. The symbol * indicates significance at the \n5% level and the symbol ** indicates significance at the 1% level. The VAR model coefficients \nare reported in Panel B. For the VAR, the optimal lag order is 5 based on both the AIC and BIC \ncriterion. VAR Model coefficients are followed by respective t-statistics. Granger causality tests \nare reported in Panel C with test statistics followed by p-values. The VAR model forecasts are \nreported in Panel D for 6 periods with coefficients followed by standard errors. \n \nPanel A: Unit Root Tests \nADF \nPP \nStationarity \nLog Prices \nResidential \n-0.412 \n0.838 \n0.765** \n( -0.987) \n(-1.000) \nAll REITs \n-0.258 \n1.234 \n0.589** \n(-0.991) \n(-1.000) \nComposite \n-2.036 \n-0.35 \n0.794** \n(-0.578) \n(-0.989) \nLog Returns \nResidential \n-4.374 \n-5.285 \n0.610* \n(0.000) \n(0.000) \nAll REITs \n-4.822 \n-5.736 \n0.594* \n(0.000) \n(0.000) \nComposite \n-2.746 \n-3.532 \n0.45 \n(-0.068) \n(-0.008) \n \nPanel B: VAR Model Coefficients \n \nComposite10 Residential \n \nComposite10 \nAll \nREITs \nIntercept \n0.004 \n0.018 \nIntercept \n0.005 \n0.016 \n(1.110) \n(1.624) \n(1.267) \n(2.013) \nComposite10 Lag1 \n1.268 \n-0.056 \nComposite10 Lag1 \n1.232 \n0.118 \n(20.392) \n(-0.282) \n(19.909) \n(0.825) \nResidential Lag1 \n0.007 \n1.105 \nAll REITs Lag1 \n0.035 \n1.011 \n(0.359) \n(17.115) \n(1.179) \n(14.630) \n\n31 \n \nComposite10 Lag2 \n-0.033 \n0.614 \nComposite10 Lag2 \n0.006 \n0.121 \n(-0.319) \n(1.890) \n(0.057) \n(0.525) \nResidential Lag2 \n0.052 \n-0.247 \nAll REITs Lag2 \n0.078 \n-0.140 \n(1.902) \n(-2.819) \n(1.891) \n(-1.466) \nComposite10 Lag3 \n-0.400 \n-0.713 \nComposite10 Lag3 \n-0.399 \n-0.147 \n(-3.856) \n(-2.156) \n(-3.980) \n(-0.636) \nResidential Lag3 \n-0.068 \n-0.447 \nAll REITs Lag3 \n-0.094 \n-0.445 \n(-2.644) \n(-5.486) \n(-2.457) \n(-5.037) \nComposite10 Lag4 \n-0.154 \n0.235 \nComposite10 Lag4 \n-0.152 \n-0.151 \n(-1.423) \n(0.679) \n(-1.442) \n(-0.622) \nResidential Lag4 \n-0.005 \n0.627 \nAll REITs Lag4 \n-0.019 \n0.565 \n(-0.196) \n(7.332) \n(-0.481) \n(6.094) \nComposite10 Lag5 \n0.262 \n-0.100 \nComposite10 Lag5 \n0.257",
    "chunk_index": 24,
    "start_char": 66017,
    "end_char": 69602,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "-0.033 \n0.614 \nComposite10 Lag2 \n0.006 \n0.121 \n(-0.319) \n(1.890) \n(0.057) \n(0.525) \nResidential Lag2 \n0.052 \n-0.247 \nAll REITs Lag2 \n0.078 \n-0.140 \n(1.902) \n(-2.819) \n(1.891) \n(-1.466) \nComposite10 Lag3 \n-0.400 \n-0.713 \nComposite10 Lag3 \n-0.399 \n-0.147 \n(-3.856) \n(-2.156) \n(-3.980) \n(-0.636) \nResidential Lag3 \n-0.068 \n-0.447 \nAll REITs Lag3 \n-0.094 \n-0.445 \n(-2.644) \n(-5.486) \n(-2.457) \n(-5.037) \nComposite10 Lag4 \n-0.154 \n0.235 \nComposite10 Lag4 \n-0.152 \n-0.151 \n(-1.423) \n(0.679) \n(-1.442) \n(-0.622) \nResidential Lag4 \n-0.005 \n0.627 \nAll REITs Lag4 \n-0.019 \n0.565 \n(-0.196) \n(7.332) \n(-0.481) \n(6.094) \nComposite10 Lag5 \n0.262 \n-0.100 \nComposite10 Lag5 \n0.257 \n0.092 \n(3.982) \n(-0.477) \n(3.998) \n(0.623) \nResidential Lag5 \n0.011 \n-0.239 \nAll REITs Lag5 \n-0.011 \n-0.275 \n(0.521) \n(-3.640) \n(-0.346) \n(-3.714) \nR-squared \n0.927 \n0.741 \nR-squared \n0.927 \n0.684 \nAdjusted R-squared \n0.924 \n0.730 \nAdjusted R-squared \n0.924 \n0.671 \n \nPanel C: Granger Causality \nComposite10 Residential \nComposite10 \nAll \nREITs \nComposite10 \n \n3.354 \nComposite10 \n \n5.569 \n \n(0.006) \n \n(0.000) \nResidential \n3.191 \n \nAll REITs \n8.673 \n \n(0.008) \n \n(0.000) \n \n \nPanel D: VAR Model Forecasts \n \nComposite10 \nResidential \n \nComposite10 \nAll REITs \n1-step-ahead \n0.230 \n0.602 \n1-step-ahead \n0.225 \n0.530 \n(0.051) \n(0.162) \n(0.050) \n(0.115) \n2-step-ahead \n0.349 \n0.654 \n2-step-ahead \n0.399 \n0.759 \n(0.082) \n(0.240) \n(0.079) \n(0.164) \n3-step-ahead \n0.345 \n0.431 \n3-step-ahead \n0.444 \n0.434 \n(0.116) \n(0.290) \n(0.113) \n(0.195) \n4-step-ahead \n0.280 \n0.317 \n4-step-ahead \n0.446 \n0.445 \n(0.142) \n(0.298) \n(0.139) \n(0.201) \n\n32 \n \n5-step-ahead \n0.172 \n0.330 \n5-step-ahead \n0.350 \n0.333 \n(0.158) \n(0.306) \n(0.156) \n(0.207) \n6-step-ahead \n0.073 \n0.354 \n6-step-ahead \n0.243 \n0.371 \n(0.166) \n(0.310) \n(0.165) \n(0.210) \n \n \n \n \n \n \n\n33 \n \nFigure 1. Time Series plots of Returns Series \n \nMonthly returns for a sample of series are presented: All REITs Value Weighted Index, REITs \nResidential Property Value Weighted Index and SCS Composite-10 index. The sample period is \nJanuary 1980 through May 2009 for REITs and February 1987 through May 2009 for the SCS \nindex. \n \n \nREIT VW Index\nPrice Change\nTime\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n-20\n0\n20\nResidential Property VW Index\nPrice Change\nTime\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n-25\n-5\n15\nComposite-10\nPrice Change\nTime\n1987\n1989\n1991\n1993\n1995\n1997\n1999\n2001\n2003\n2005\n2007\n2009\n-2.5\n1.5\n \n\n34 \n \n \nFigure 2. Time Series plots of Conditional Volatility Series \n \nMonthly conditional volatility (from fitting a GARCH (1, 1) model) for a sample of series are \npresented: All REITs Value Weighted Index, REITs Residential Property Value Weighted Index \nand SCS Composite-10 index. The sample period is January 1980 through May 2009 for REITs \nand February 1987 through May 2009 for the SCS index. \n \nREIT VW Index\nConditional Volatility\nTime\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n4\n8\n12\nResidential Property VW Index\nConditional Volatility\nTime\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n4\n8\n12\nComposite-10\nConditional Volatility\nTime\n1987\n1989\n1991\n1993\n1995\n1997\n1999\n2001\n2003\n2005\n2007\n2009\n0.5\n2.5\n \n \n \n\n35 \n \nFigure 3. Scree plot for REIT Indexes \nFor the six REIT indexes whose descriptive statistics are reported in Table 1, excluding the \naggregated series, a covariance matrix is formed using their concurrent monthly returns, January \n1980 through May 2009. The scree plots depict the variance explained by successive principal \ncomponents (PCs) with PCs ranked from left to right in descending order of variance explained.",
    "chunk_index": 25,
    "start_char": 68938,
    "end_char": 72481,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "2007\n2009\n0.5\n2.5\n \n \n \n\n35 \n \nFigure 3. Scree plot for REIT Indexes \nFor the six REIT indexes whose descriptive statistics are reported in Table 1, excluding the \naggregated series, a covariance matrix is formed using their concurrent monthly returns, January \n1980 through May 2009. The scree plots depict the variance explained by successive principal \ncomponents (PCs) with PCs ranked from left to right in descending order of variance explained. \nThe bar gives the variance explained by the PC and the cumulative variance explained in percent \nis printed above each bar. \nF.1\nF.2\nF.3\n0\n50\n100\n150\nPrincipal Components [F.1:F.3]\nVariances\n0.726\n0.849\n0.907\n \n \n \n \n \n \n \n \n \n\n36 \n \nFigure 4. Scree plot for S&P/Case-Shiller\u00ae (SCS) Indexes \nFor the 14 SCS indexes whose descriptive statistics are reported in Table 1 excluding the \ncomposite series, a covariance matrix is formed using their concurrent monthly returns, February \n1987 through May 2009. The scree plots depict the variance explained by successive principal \ncomponents (PCs) with PCs ranked from left to right in descending order of variance explained. \nThe bar gives the variance explained by the PC and the cumulative variance explained in percent \nis printed above each bar. \n \nF.1\nF.2\nF.3\nF.4\nF.5\nF.6\n0\n2\n4\n6\n8\n10\nPrincipal Components [F.1:F.6]\nVariances\n0.666\n0.743\n0.805\n0.851\n0.886\n0.912\n \n \n \n\n37 \n \nFigure 5. Time Series plots of Log Price Series \n \nMonthly log prices for a sample of series are presented: All REITs Value Weighted Index, \nREITs Residential Property Value Weighted Index and SCS Composite-10 index. The sample \nperiod is February 1987 through May 2009 for the three series. \n \nREIT VW Index\nPrice\nTime\n1987\n1989\n1991\n1993\n1995\n1997\n1999\n2001\n2003\n2005\n2007\n2009\n16.0\n18.0\nResidential Property VW Index\nPrice\nTime\n1987\n1989\n1991\n1993\n1995\n1997\n1999\n2001\n2003\n2005\n2007\n2009\n13.5\n17.5\nComposite-10\nPrice\nTime\n1987\n1989\n1991\n1993\n1995\n1997\n1999\n2001\n2003\n2005\n2007\n2009\n4.2\n5.0\n \n \n \n\n38 \n \n \nFigure 6. Time Series plots of Response and Fitted Values \n \nMonthly response and fitted values from the VAR model for a sample of series are presented: All \nREITs Value Weighted Index, REITs Residential Property Value Weighted Index respectively \nwith the SCS Composite-10 index. The sample period is February 1987 through May 2009 for \nthe three series. \n-3\n-2\n-1\n0\n1\n2\n0\n50\n100\n150\n200\n250\nComposite10\n-10\n0\n10\n20\n0\n50\n100\n150\n200\n250\nResidential\nResponse and Fitted Values\n \n \n \n\n39 \n \nFigure 7. Autocorrelation and cross correlation of Real Estate Index Returns \n \nAutocorrelation and cross correlation with confidence bands for a sample of series are presented: \nAll REITs Value Weighted Index, REITs Residential Property Value Weighted Index \nrespectively with the SCS Composite-10 index. The sample period is February 1987 through \nMay 2009 for the three series. \n Composite10 \nACF\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Composite10 and Residential\n0\n5\n10\n15\n20\n-0.10\n-0.05\n0.0\n0.05\n0.10\n0.15\n Residential and Composite10\nLag\nACF\n-20\n-15\n-10\n-5\n0\n-0.10\n-0.05\n0.0\n0.05\n0.10\n Residential \nLag\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nComposite 10 and Residential REITs",
    "chunk_index": 26,
    "start_char": 72031,
    "end_char": 75185,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "bands for a sample of series are presented: \nAll REITs Value Weighted Index, REITs Residential Property Value Weighted Index \nrespectively with the SCS Composite-10 index. The sample period is February 1987 through \nMay 2009 for the three series. \n Composite10 \nACF\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Composite10 and Residential\n0\n5\n10\n15\n20\n-0.10\n-0.05\n0.0\n0.05\n0.10\n0.15\n Residential and Composite10\nLag\nACF\n-20\n-15\n-10\n-5\n0\n-0.10\n-0.05\n0.0\n0.05\n0.10\n Residential \nLag\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nComposite 10 and Residential REITs\n \n\n40 \n \n Composite10 \nACF\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Composite10 and All.REITs\n0\n5\n10\n15\n20\n-0.1\n0.0\n0.1\n0.2\n All.REITs and Composite10\nLag\nACF\n-20\n-15\n-10\n-5\n0\n-0.10\n0.0\n0.05\n0.10\n0.15\n0.20\n All.REITs \nLag\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nComposite 10 and All REITs\n \n \n \n\n41 \n \nFigure 8. Impulse Response Functions for Real Estate Returns \n \nImpulse response functions and asymptotic standard errors from the VAR model for a sample of \nseries are presented: All REITs Value Weighted Index, REITs Residential Property Value \nWeighted Index respectively with the SCS Composite-10 index. The sample period is February \n1987 through May 2009 for the three series. \n \n \n-0.02\n0.02\n0.06\n0\n5\n10\n15\n20\nInno.: Composite10\nResp.: Residential\n0.0\n0.05\n0.10\n0.15\nInno.: Residential\nResp.: Residential\n0.0\n0.02\n0.04\n0.06\n0.08\nInno.: Composite10\nResp.: Composite10\n-0.01\n0.0\n0.01\n0.02\n0\n5\n10\n15\n20\nInno.: Residential\nResp.: Composite10\nSteps\nImpulse Response\nOrthogonal Impulse Response Function\n \n\n42 \n \n-0.02\n0.0\n0.02\n0.04\n0\n5\n10\n15\n20\nInno.: Composite10\nResp.: All.REITs\n0.0\n0.05\n0.10\nInno.: All.REITs\nResp.: All.REITs\n0.02\n0.04\n0.06\n0.08\nInno.: Composite10\nResp.: Composite10\n-0.02\n0.0\n0.02\n0\n5\n10\n15\n20\nInno.: All.REITs\nResp.: Composite10\nSteps\nImpulse Response\nOrthogonal Impulse Response Function\n \n \n \n\n43 \n \nFigure 9. Forecast Error Variance Decompositions for Real Estate Returns \n \nForecast error variance decompositions and asymptotic standard errors from the VAR model for \na sample of series are presented: All REITs Value Weighted Index, REITs Residential Property \nValue Weighted Index respectively with the SCS Composite-10 index. The sample period is \nFebruary 1987 through May 2009 for the three series. \n \n0.0\n0.05\n0.10\n0.15\n5\n10\n15\n20\nInno.: Composite10\nResp.: Residential\n0.85\n0.90\n0.95\n1.00\nInno.: Residential\nResp.: Residential\n0.96\n0.97\n0.98\n0.99\n1.00\nInno.: Composite10\nResp.: Composite10\n0.0\n0.01\n0.02\n0.03\n0.04\n5\n10\n15\n20\nInno.: Residential\nResp.: Composite10\nForecast Steps\nProportion of Variance Contribution\nForecast Error Variance Decomposition\n \n\n44 \n \n0.0\n0.05\n0.10\n0.15\n5\n10\n15\n20\nInno.: Composite10\nResp.: All.REITs\n0.85\n0.90\n0.95\n1.00\nInno.: All.REITs\nResp.: All.REITs\n0.88\n0.92\n0.96\n1.00\nInno.: Composite10\nResp.: Composite10\n0.0\n0.04\n0.08\n0.12\n5\n10\n15\n20\nInno.: All.REITs\nResp.: Composite10\nForecast Steps\nProportion of Variance Contribution\nForecast Error Variance Decomposition",
    "chunk_index": 27,
    "start_char": 74640,
    "end_char": 77608,
    "paper_title": "A Comparative Anatomy of REITs and Residential Rea",
    "paper_category": "q-fin.PM",
    "paper_filename": "A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/A_Comparative_Anatomy_of_REITs_and_Residential_Rea.pdf"
  },
  {
    "text": "Fast Times, Slow Times: Timescale Separation in Financial\nTimeseries Data\nJan Rosenzweig\nAbstract\nFinancial time series exhibit multiscale behavior, with interaction between multiple processes\noperating on different timescales. This paper introduces a method for separating these processes\nusing variance and tail stationarity criteria, framed as generalized eigenvalue problems. The approach\nallows for the identification of slow and fast components in asset returns and prices, with applications\nto parameter drift, mean reversion, and tail risk management. Empirical examples using currencies,\nequity ETFs and treasury yields illustrate the practical utility of the method.\n1\nStationarity in Finance\nThe study of multiscale processes is well-established in physics, with notable examples ranging from fluid\ndynamics to protein folding, where fast and slow dynamics interact [1].\nThe typical physical example involves fast oscillations inside a slowly moving envelope, where the\ncumulative effect of fast oscillations drives changes in the envelope [1, 2].\nIn finance, we can see multiple examples of similar multiscale behavior. For example, the S&P 500\nexhibits mean reversion over miliseconds (due to low-latency arbitrage between e-mini S&P futures,\nETFs and stock baskets) and over years (relative to gold or bonds).\nA notable example of feedback from high frequency to macro is of course the Flash Crash of 2010,\nwhere multiple feedback loops in high and mid frequency trading lead to the S&P 500 losing 9% of its\nvalue, only to recover later [3].\nTherefore identification of such processes operating on different timescales, and the nature of their\ninteraction, is of clear importance both to market practitioners and to economists.\nThe key questions in this context are:\n\u2022 Can we separate these processes?\n\u2022 Can we estimate their relaxation timescales?\n\u2022 Can we assess stationarity in a meaningful way?\nThis paper addresses these questions by introducing a framework for timescale separation in financial\ntime series, building on methods from signal processing and variational principles.\nOn the trading side, financial strategies rely on calibrated parameters that drift over time, leading to\nstrategy degradation.\nTwo types of stationarity are relevant in this context:\n\u2022 Stationarity of Returns: Implies stable distribution of returns, leading to stable parameters.\n\u2022 Stationarity of Prices: Implies mean reversion in price levels.\nThe same mathematical tools can be applied to both, though their interpretations differ.\n2\nTypes of Stationarity\nThe textbook definition of stationary processes involves the stationarity of the underlying distribution.\n1\narXiv:2601.11201v1 [q-fin.PM] 16 Jan 2026\n\nWhile this definition is attractive, it does not lend itself straightforwardly to data driven analysis.\nPrimarily, this is due to the fact that we only have a finite amount of data available, and that any\nestimation of the underlying distribution is therefore fuzzy and imprecise.\nRather than aiming for the entire distribution, it is then more practical to focus on particular aspects\nof the distribution.\nThe initial promising target is the covariance matrix, i.e. measuring the stationarity of variances and\ncovariances in the basket.\nHowever non-Gaussianity of financial time series means that we have to look further than just to the\nGaussian MLE, into the stationarity of the tail behaviour of the relevant timeseries.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3433,
    "paper_title": "Fast Times Slow Times Timescale Separation in Fina",
    "paper_category": "q-fin.PM",
    "paper_filename": "Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf"
  },
  {
    "text": "we only have a finite amount of data available, and that any\nestimation of the underlying distribution is therefore fuzzy and imprecise.\nRather than aiming for the entire distribution, it is then more practical to focus on particular aspects\nof the distribution.\nThe initial promising target is the covariance matrix, i.e. measuring the stationarity of variances and\ncovariances in the basket.\nHowever non-Gaussianity of financial time series means that we have to look further than just to the\nGaussian MLE, into the stationarity of the tail behaviour of the relevant timeseries.\nWe therefore consider two complimentary approaches to timescale separation:\n\u2022 Variance Timescales: Stationarity and drift in the second moment.\n\u2022 Tail Timescales: Stability in higher moments, relevant for tail risk.\n2.1\nVariance Timescales\nLet Xt be an n-dimensional column vector process (e.g., prices or returns). We seek a matrix W of\nn-dimensional column weight vectors wi such that the drift of variance is minimized for a fixed unit of\nvariance:\nd\ndtVar(XtwT\ni ) \u2192min, i = 1..n\n(1)\nVar(WXT\nt ) = 1\n(2)\nHere, (1) and (2) are both instantaneous at time t and hence they do not contradict each other.\nThis leads to the generalized eigenvalue problem:\nE[(dXt)T Xt + XT\nt dXt]w = \u03b2 E[XT\nt Xt]w\n(3)\nE[wiXT\nt XtwT\nj ] = \u03b4ij\n(4)\nwhere \u03b2 is the Lagrange multiplier.\nDiscretising dXt as\ndXt \u22481\nT (dXt+T \u2212dXt) ,\n(3) becomes\nC(t, T)w = \u03bb C(t, 0)w\n(5)\nwhere C(t, T) is the autocovariance of Xt with lag T,\nC(t, T) = 1\n2E[XT\nt+T Xt + XT Xt+T ]\nand\n\u03bb = 1 + \u03b2\n2T\nThe eigenvalue problem (5) is known as tICA [4], although our derivation is different from the usual\ntICA derivation; the usual derivation assumes that the discretised process follows a Hidden Markov\nModel\nXt+T = AXt + \u03f5t\n(6)\nfor some unknown linear operator A and noise \u03f5t. Multiplying both sides of (6) by XT\nt and taking the\nexpectation, this yields\nE[XT\nt Xt+T ] = A C(t, 0)\nwhere the autocovariance term E[XT\nt Xt+T ] is usually symmetrized to give the eigenvalue problem (5).\nNote that the matrix C(t, 0) on the right hand side of equation (5) is the covariance matrix, and\nas such it is real, symmetric and positive definite. This makes (5) straightforward to solve numerically,\nusing e.g. LAPACK\u2019s gvd driver or its various interfaces such as Python scipy.linalg.eigh.\nThe eigenvectors of (5) correspond to the directions of fastest (slowest) decay in non-stationarity.\nThe eigenvalues \u03bbi are their autocorrelations, as seen from (5), and are therefore in the range [\u22121, 1].\nThey are related to decay time scales through\nti = \u22122T\n\u03bbi \u22121\n(7)\n2\n\n2.2\nTail Timescales\nFor tail timescales, we minimize the drift of higher order moments [6]. The moment of order 2k is given\nas\nM2k = 1\n2k E(XtwT )2k\nand the minimization problem becomes\nd\ndt\n1\n2k E(XtwT\ni )2k \u2192min, i = 1..n",
    "chunk_index": 1,
    "start_char": 2853,
    "end_char": 5661,
    "paper_title": "Fast Times Slow Times Timescale Separation in Fina",
    "paper_category": "q-fin.PM",
    "paper_filename": "Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf"
  },
  {
    "text": "in non-stationarity.\nThe eigenvalues \u03bbi are their autocorrelations, as seen from (5), and are therefore in the range [\u22121, 1].\nThey are related to decay time scales through\nti = \u22122T\n\u03bbi \u22121\n(7)\n2\n\n2.2\nTail Timescales\nFor tail timescales, we minimize the drift of higher order moments [6]. The moment of order 2k is given\nas\nM2k = 1\n2k E(XtwT )2k\nand the minimization problem becomes\nd\ndt\n1\n2k E(XtwT\ni )2k \u2192min, i = 1..n\n(8)\nE[wiXT\nt XtwT\nj ] = \u03b4ij\n(9)\nThe problem (8), (9) is nonlinear and it does not reduce to a simple eigenvalue problem.\nThe constrained minimum satisfies\n(2k \u22121)E[(XtwT )2k\u22122(dXtwT )Xt] + E[(XtwT )2k\u22121dXt] = \u03b2w\n(10)\nwT w = 1\n(11)\nwhere \u03b2 is the Lagrange multiplier as before.\nMultiplying the right hand side of (10) by wT and using (11), we can solve for \u03b2 as\n\u03b2 = 2kE[(XtwT )2k\u22121(dXtwT )] = 2k d\ndtM2k\n(12)\nFrom (12), we get the generalization of the equivalent of \u03bb from the previous section as\n\u03bb = E[(XtwT )2k\u22121(Xt+T wT )],\n(13)\nso the eigenvalue is now the tail autocorrelation of order k, i.e. the tail correlation of order k between\nXt and Xt+T [6].\nPlugging (12) into (9), we get\n(2k \u22121)E[(XtwT )2k\u22122(dXtwT )Xt] + E[(XtwT )2k\u22121dXt] = 2kE[(XtwT )2k\u22121(dXtwT )]w\n(14)\nNumerically, it is easier to solve (8),(9) using the Fixed Point Iteration of [5], known as FastICA. The\niteration procedure is\nw \u2190E[Xtd(wtXt)2k\u22121)] \u2212(2k \u22121)E[(wT Xt)2k\u22122)]w\nw \u2190w/|w|\n(15)\nwhen iterating a single component, or\nW \u2190E[Xtd(WT Xt)2k\u22121)] \u2212(2k \u22121)E[(WT Xt)2k\u22122)]W\nW \u2190\n\u0000WXT\nt XtWT \u0001\u22121/2 W\n(16)\nwhen iterating all the components in parallel.\n3\nEmpirical Examples\nWe have applied the time-driven decomposition to three separate baskets, namely the G10 currencies,\nfive multifactor equity ETFs, and US Treasuries.\nFor G10 currencies, we extracted the five slowest components from 2006\u20132010 and projected them\nonto 2010\u20132025.\nFor factor ETFs (IFSU, IUSZ, IUVL, IUMO, IUQA), we extracted the five slowest components over\n2021\u20132022 and projected them through 2022\u20132025.\nFinally, for US Treasuries, we fitted over 1994-1998, and continued over 1998-2025.\nWe fitted each basket with the liner tICA (k = 1, section 2.1) and nonlinear tICA (k = 4, section\n2.2).\nThe results are shown in Figures 1, 2, 3.\n3\n\n(a). Linear tICA, k = 1\n(b). Nonlinear tICA, k = 4\nFigure 1: G10 currencies: Blind extraction of slowest components over 2006-2010 (green background), contin-\nuation over 2010-2025 (white background). Weights are rescaled to unit gross exposure.\nWhile retaining some similarity between the weight profiles in each case, the timeseries for nonlinear\ntICA results are considerably different from the corresponding linear decompositions. Increasing the\nexponent k further, and thus increasing the penalty to tails relative to volatility, did not produce further\nmeaningful changes.\nThis seems to imply that there are overall two choices for timescale separation; namely linear, governed\nby volatility, and nonlinear, governed by the tails.",
    "chunk_index": 2,
    "start_char": 5244,
    "end_char": 8175,
    "paper_title": "Fast Times Slow Times Timescale Separation in Fina",
    "paper_category": "q-fin.PM",
    "paper_filename": "Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf"
  },
  {
    "text": "gross exposure.\nWhile retaining some similarity between the weight profiles in each case, the timeseries for nonlinear\ntICA results are considerably different from the corresponding linear decompositions. Increasing the\nexponent k further, and thus increasing the penalty to tails relative to volatility, did not produce further\nmeaningful changes.\nThis seems to imply that there are overall two choices for timescale separation; namely linear, governed\nby volatility, and nonlinear, governed by the tails.\nWe can note that the relaxation timescales, at least for the slowest components, appear to be remark-\nably robust out of sample, with no visible deterioration accompanying the transition from in sample to\nout of sample.\nEven more remarkably, where higher frequencies visibly enter the slow components such as, say, due\nto the global financial crisis 2008-2010 or geopolitical events in 2023, the relevant components revert\nback to their original, slow timescale once the relevant events pass.\nThis robustness does not carry over to orthogonality of components, which decays as expected out\nof sample.\n4\nConclusion\nFinancial time series contain a hierarchy of processes: stationary, slow, and fast.\nUsing generalized\neigenvalue problems, we can separate these components, providing insight into parameter drift, mean\nreversion regimes and/or tail risk stability.\nThe method is computationally tractable and can be applied to diverse asset classes, offering a\npowerful tool for risk and strategy management. The conceptual framework, moving from small to large\nstructures, mirrors approaches used in physical sciences.\nThe remarkable property of the methods described in this paper is their ability to persist the in-sample\ntime scales of the selected components out of sample.\nReferences\n[1] Hinch, E.J. (1991). Perturbation Methods. https://doi.org/10.1017/CBO9781139172189, Cambridge\nUniversity Press.\n4\n\n(a). Linear tICA, k = 1\n(b). Nonlinear tICA, k = 4\nFigure 2: Factor ETF: Blind extraction of slowest components over 2021-2022 (green background), continuation\nover 2022-2025 (white background). Weights are rescaled to unit gross exposure.\n[2] P\u00b4erez-Hern\u00b4andez, G., Paul, F., Giorgino, T., De Fabritiis, G., No\u00b4e, F. (2013) Identification of\nslow molecular order parameters for Markov model construction. J. Chem. Phys 139, 015102.\nhttps://doi.org/10.1063/1.4811489\n[3] 2010 flash crash, Wikipedia https://en.wikipedia.org/wiki/2010 flash crash\n[4] Schmid, P.J. (2010) Dynamic mode decomposition of numerical and experimental data. Journal of\nfluid mechanics 656:5\u201328.\n[5] Hyv\u00a8arinen,\nA.\n(1999).\nFast\nand\nrobust\nfixed-point\nalgorithms\nfor\nindependent\ncomponent\nanalysis.\nIEEE Transactions on Neural Networks\n10\n(3):\n626\u2013634.\nhttps://www.cs.helsinki.fi/u/ahyvarin/papers/TNN99new.pdf\n[6] Rosenzweig, J. (2023) A tale of tail covariances. https://arxiv.org/abs/2302.13646\n5\n\n(a). Linear tICA, k = 1\n(b). Nonlinear tICA, k = 4\nFigure 3: US Treasuries: Blind extraction of slowest components over 1994-1998 (green background), continu-\nation over 1998-2025 (white background). Weights are rescaled to unit gross exposure.\n6",
    "chunk_index": 3,
    "start_char": 7669,
    "end_char": 10802,
    "paper_title": "Fast Times Slow Times Timescale Separation in Fina",
    "paper_category": "q-fin.PM",
    "paper_filename": "Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Fast_Times_Slow_Times_Timescale_Separation_in_Fina.pdf"
  },
  {
    "text": "Machine Learning Portfolio Allocation\nMichael Pinelis* and David Ruppert**\n*Department of Economics, Cornell University, mdp93@cornell.edu\n**Department of Statistics & Data Science and School of Operations Research and\nInformation Engineering, Cornell University, dr24@cornell.edu\nMarch 2, 2020\nAbstract\nWe \ufb01nd economically and statistically signi\ufb01cant gains when using machine learning\nfor portfolio allocation between the market index and risk-free asset. Optimal portfolio\nrules for time-varying expected returns and volatility are implemented with two Random\nForest models. One model is employed in forecasting monthly excess returns with\nmacroeconomic factors including payout yields. The second is used to estimate the\nprevailing volatility. Reward-risk timing with machine learning provides substantial\nimprovements over the buy-and-hold in utility, risk-adjusted returns, and maximum\ndrawdowns. This paper presents a unifying framework for machine learning applied to\nboth return- and volatility-timing.\nKeywords: portfolio allocation, machine learning, random forest, elastic net,\nmarket timing, reward-risk timing, volatility estimation, equity return predictability\nJEL Classi\ufb01cation: G11, G12, C13\n1\narXiv:2003.00656v5 [q-fin.PM] 4 Nov 2021\n\n1\nIntroduction\nWe use machine learning to \ufb01nd the optimal portfolio weights between the market index and\nthe risk-free asset. The timing strategy is generated from the utility maximization principle\nand gives optimal portfolio weights estimated monthly with two Random Forest models. The\nmarket weight is proportional to the reward factor, which is a forecast of the excess market\nreturn1, and is inversely proportional to the risk factor, an estimate of prevailing squared\nvolatility. This procedure is simultaneously return- and volatility-timing the market and\ncan be called \u2019reward-risk timing\u20192. Our method found that a portfolio allocation strategy\nemploying machine learning to reward-risk time the market gave signi\ufb01cant improvements\nin investor utility and Sharpe ratios and earned a large alpha of 3.4%. We motivate our\nanalysis from the vantage point of a utility-maximizing investor, who adjusts the portfolio\nallocation according to the attractiveness of the risk-reward trade-o\ufb00.\nA number of papers have been written on predicting returns and volatilities with\nmachine learning and large numbers of features. See as a review (Henrique et al., 2019).\nMachine learning methods have been shown to be suitable and advantageous for the di\ufb03cult\ntask of identifying the regimes in the markets (Gu et al., 2020). Gu et al. \ufb01nd a bene\ufb01t of\nusing machine learning for market timing with return forecasts of 26% and 18% increases in\nSharpe ratios with neural networks and Random Forest, respectively, relative to that of the\nbuy-hold. Yet none predict returns and volatilities with machine learning in combination.\nOur results document a 28% increase in Sharpe ratios when using Random Forest for both\nreturns and volatilities in combination. Taking advantage of the allowance for nonlinear\npredictor interactions in machine learning models gives better return and volatility forecasts\nbased on market conditions. An approach with machine learning that considers both expected\nreturn- and volatility-timing leads to a pro\ufb01table trading strategy, without an extensive set\nof predictors. This paper studies how the machine learning methods of Random Forest and\n1We refer to excess market return as the excess over the risk-free rate in this paper.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3502,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "for both\nreturns and volatilities in combination. Taking advantage of the allowance for nonlinear\npredictor interactions in machine learning models gives better return and volatility forecasts\nbased on market conditions. An approach with machine learning that considers both expected\nreturn- and volatility-timing leads to a pro\ufb01table trading strategy, without an extensive set\nof predictors. This paper studies how the machine learning methods of Random Forest and\n1We refer to excess market return as the excess over the risk-free rate in this paper.\n2This term is from Kirby and Ostdiek (2012), who propose weighting by individual price of risks in a\nmulti-asset portfolio. Our paper focuses on the portfolio with the market index and risk-free asset. Another\ndi\ufb00erence is Kirby and Ostdiek (2012) use several-year-long rolling window estimates of the conditional mean\nand volatility while we look at short-term windows for machine learning strategies.\n2\n\nElastic Net can forecast the excess return with the major conditioning variables proposed\nso far in the literature and summarized by Goyal and Welch (2008) as well as an enhanced\nmeasure of the payout yield (see Boudoukh et al. (2007)). Then separate Random Forest\nand Elastic Net models are employed to predict next month\u2019s volatility with the similar set\nof variables. Comparing the performance of a standard linear model for reward-risk timing,\nwe show that the machine learning models outperform by a signi\ufb01cant margin.\nExpected-return or reward-timing involves adjusting the portfolio allocation according to\nbeliefs about future asset returns. This is akin to benchmark timing, the active management\ndecision to vary the managed portfolio\u2019s beta with respect to the benchmark (Grinold and\nKahn, 1999). Merton (1981) derived the economic value of return forecasts. Campbell and\nThompson (2008) show that many predictive regressions beat the historical average return,\nonce weak restrictions are imposed on the signs of coe\ufb03cients and return forecast.\nVolatility- or risk-timing is a newer idea. While there is a wide array of volatility-\nbased portfolio allocation strategies, this paper\u2019s trading rule is derived from the utility\nmaximization principle and naturally depends on both the return and volatility. With\nthis methodology, the portfolio weight in the risky asset is inversely proportional to the\nrecent squared volatility, which is a similar to the assumption in Moreira and Muir (2017).\nIntuitively, by avoiding high-volatility times the investor avoids risks, but if the risk-return\ntrade-o\ufb00is strong one also sacri\ufb01ces expected returns, leaving the volatility timing strategy\nwith no edge. Commonly, the volatility estimator is the realized volatility for the past few\nmonths. We propose a forward-looking model-based volatility estimate. The results show\nthat the bene\ufb01ts from volatility-timing are enhanced when using this proposed measure for\nvolatility.\nReward-risk timing is the combination of both return- and volatility-timing. Return-\ntiming can be pro\ufb01table with superior forecasting ability, yet ignoring the risk associated\nwith a high return, for instance, would lead to poor risk-adjusted performance. The incorrect\nforecasts are not mitigated by their risk. On the other hand, volatility-timing is advantageous\nif the risk is not compensated fully by the reward, yet there may be cases when in fact\n3",
    "chunk_index": 1,
    "start_char": 2950,
    "end_char": 6342,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "estimate. The results show\nthat the bene\ufb01ts from volatility-timing are enhanced when using this proposed measure for\nvolatility.\nReward-risk timing is the combination of both return- and volatility-timing. Return-\ntiming can be pro\ufb01table with superior forecasting ability, yet ignoring the risk associated\nwith a high return, for instance, would lead to poor risk-adjusted performance. The incorrect\nforecasts are not mitigated by their risk. On the other hand, volatility-timing is advantageous\nif the risk is not compensated fully by the reward, yet there may be cases when in fact\n3\n\nthe reward overcompensates the risk. Timing the market with both the expected return\nand volatility addresses the drawbacks of these individual approaches. The role of machine\nlearning is to provide more accurate estimates by taking advantage of complex non-linear\nrelationships between market variables and help make optimal decisions. With this, we\nprovide a novel unifying framework for return- and volatility-timing as well as machine\nlearning in portfolio allocation.\nAn outline of the paper follows. Section 2 reviews the literature. Section 3 describes\nthe portfolio allocation methodology, including the utility-maximization problem and models.\nSection 4 demonstrates the results of using the machine learning portfolio allocation strategy,\nand Section 5 concludes.\n2\nLiterature\nAbundant work can be found on two strands of market timing, via expected returns\nand volatilites. Work can also be found on approaches combining the two, yet none to our\nknowledge integrate machine learning.\nThere is a long literature on expected-return timing. Kandel and Stambaugh (1996)\nexamine equity return predictability and \ufb01nd that the optimal stock-versus-cash allocation\ncan depend importantly on a predictor variable such as the dividend yield. Goyal and Welch\n(2008) comprehensively examine the performance of variables that have been suggested by the\nacademic literature to be good predictors of the equity premium and \ufb01nd contradictory results.\nJohannes et al. (2014), however, \ufb01nd strong evidence that investors can use predictability\nto improve out-of-sample portfolio performance provided they incorporate time-varying\nvolatility and estimation risk into their optimal portfolio problems.\nThere has also been a sizable interest in volatility-timing. Moreira and Muir (2017)\nshowed volatility-managed factors outperform their buy-and-hold counterparts, modeling the\noptimal weight as a constant over the realized volatility for the previous month. Fleming et\nal. (2001) discussed the economic value of volatility timing, and Moreira and Muir (2019)\n4\n\nfound that investors who volatility time earn 2.4% more annually than those who do not.\nNumerous papers have been written in response. Liu et al. (2019) found that the strategy in\nMoreira and Muir (2017) is subject to look-ahead bias since they choose the constant based\non the full sample and that it is not easy to outperform the market with volatility timing\nalone. One \ufb01nding in this paper is that simply replacing the constant with the expanding\nestimate of the unconditional mean excess return, which stays close to the constant chosen\nby Moreira, leads to similar performance3.\nOur main aim is to simultaneously perform expected return- and volatility-timing.\nMarquering and Verbeek (2004) study the economic value of predicting stock index returns\nand volatility.",
    "chunk_index": 2,
    "start_char": 5757,
    "end_char": 9170,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "look-ahead bias since they choose the constant based\non the full sample and that it is not easy to outperform the market with volatility timing\nalone. One \ufb01nding in this paper is that simply replacing the constant with the expanding\nestimate of the unconditional mean excess return, which stays close to the constant chosen\nby Moreira, leads to similar performance3.\nOur main aim is to simultaneously perform expected return- and volatility-timing.\nMarquering and Verbeek (2004) study the economic value of predicting stock index returns\nand volatility. They \ufb01nd that using simple linear models can lead to economically pro\ufb01table\nperformance in the monthly sample from 1970 to 2001. Our period is more recent. Also,\nJohannes et al. (2014) \ufb01nd statistically and economically signi\ufb01cant out-of-sample portfolio\nbene\ufb01ts for an investor who uses models of return predictability when forming optimal\nportfolios, if accounting for estimation risk and allowing for time-varying volatility. We\nstudy a similar problem as these authors, however, not only with typical regression-based\napproaches but with machine learning models.\nKirby and Ostdiek (2012) develop volatility- and reward-risk-timing strategies for the\nportfolio with many assets. Our paper considers the problem for the risk-free asset and the\nmarket while applying machine learning.\nGu et al. (2020) showed the bene\ufb01t from using machine learning for empirical asset\npricing, tracing the predictive gains to the allowance of non-linear predictor interactions.\nTrees and neural nets were the most successful in predicting returns.\nAn article by Nystrup et al. (2016) proposes dynamic asset allocation using Hidden\nMarkov Models that is based on detection of change points without \ufb01tting a model with\na \ufb01xed number of regimes to the data, without estimating any parameters, and without\nassuming a speci\ufb01c distribution of the data. Our machine learning approach also does not\nassume a number of regimes, yet it does not discretize the portfolio weights.\n3Our weight is constrained by a 150% leverage limit so the alphas are not the same in the main results.\n5\n\nTo our knowledge, this is the \ufb01rst paper written on a machine learning approach to\nsimultaneous return- and volatility-timing.\n3\nMethodology\nWe perform two tasks with machine learning that give the weight of the market index\nin our portfolio. First, we predict the market excess return next month with well-known\nmacroeconomic and \ufb01nancial variables. Second, we estimate the prevailing volatility with\na similar set of predictors. The weight of the equity index is proportional to the expected\nexcess return and inversely proportional to the squared volatility estimate. The initial data\nthe excess return and volatility models are trained on are from 1927 to 1957. The strategies\nare then optimized on out-of-sample data from 1958 to 1988 in a procedure called validation.\nEach month, the training data grows by one past observation and the models are re\ufb01t. One\nset of models for each hyperparameter combination is kept. We select the combination of\nhyperparameters for Random Forest and Elastic Net that attains the highest predictive\naccuracy measured by R2 over this validation period.",
    "chunk_index": 3,
    "start_char": 8617,
    "end_char": 11820,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "volatility models are trained on are from 1927 to 1957. The strategies\nare then optimized on out-of-sample data from 1958 to 1988 in a procedure called validation.\nEach month, the training data grows by one past observation and the models are re\ufb01t. One\nset of models for each hyperparameter combination is kept. We select the combination of\nhyperparameters for Random Forest and Elastic Net that attains the highest predictive\naccuracy measured by R2 over this validation period. Then the Random Forest and Elastic\nNet strategies are tested on a holdout set from 1989 to 2019, data that provides a \ufb01nal\nestimate of the models\u2019 performance after they have been validated, to prevent against\nbacktest-over\ufb01tting (Bailey et al., 2015)4. Only one attempt on the holdout set is made. The\ngeneral portfolio allocation approach is the following. For each month, update the machine\nlearning models with the data only before that month, forecast the excess return and the\nvolatility, and recompute the optimal weights. This gives us a time series of out-of-sample\nforecasts, portfolio returns, and corresponding performance metrics.\nUsing these time series, comprehensive summary statistics are computed to summarize\nthe model and portfolio performance. We also conduct an array of tests to evaluate the\nrobustness of our results. A key result is that the typical investor can bene\ufb01t from reward-\nrisk timing even if subject to realistic transaction costs and tight leverage constraints. A\n4Holdout sets are never used to make decisions about which algorithms to use or for improving or tuning\nalgorithms. Therefore, the performance on the holdout set is indicative of investment performance if an\ninvestor starts trading with the models and strategy today.\n6\n\ncomparison of the Sharpe ratios and certainty equivalent (CE) yields of similar strategies that\ndo not employ machine learning \ufb01nds less impressive performance. Furthermore, examining\nthe results of a series of time-series regressions gives evidence for positive alphas even after\napplying realistic transaction costs. The next section establishes the optimal trading rules\nfollowed.\n3.1\nPortfolio Allocation\nConsider a power utility investor of terminal wealth Wt+\u2206t.\nU(Wt+\u2206t) = W (1\u2212\u03b3)\nt+\u2206t \u22121\n1 \u2212\u03b3\n,\n(1)\nwhere \u03b3 > 0 is the coe\ufb03cient of relative risk-aversion and as \u03b3 \u2212\u21921, U(Wt+\u2206t) = ln Wt+\u2206t.\nThe investment universe with a risky asset with time-varying mean and variance and riskless\nasset constrained by a budget is de\ufb01ned by\nrt = \u00b5t + \u03c3t \u00b7 zt\n(2)\nWt = Wt\u22121\n\u0010\nwt \u00b7 exp(rt) + (1 \u2212wt) \u00b7 exp(rf\nt )\n\u0011\n,\n(3)\nwhere \u00b5t is the expected log return on the risky asset, \u03c3t is the time-varying volatility, zt is\na standard normal random variable, Wt is the investor\u2019s wealth at time t, rf\nt is the risk-free\nasset log return, and wt is the portfolio weight in the risky asset at time t. In general form,\nwe describe an asset\u2019s excess return and volatility as additive prediction error models:\n\u00b5t = gt (\u20d7xt\u22121) + \u03f5t\n(4)\nlog(\u03c32\nt ) = ht (\u20d7vt\u22121) + st,\n(5)",
    "chunk_index": 4,
    "start_char": 11341,
    "end_char": 14339,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "expected log return on the risky asset, \u03c3t is the time-varying volatility, zt is\na standard normal random variable, Wt is the investor\u2019s wealth at time t, rf\nt is the risk-free\nasset log return, and wt is the portfolio weight in the risky asset at time t. In general form,\nwe describe an asset\u2019s excess return and volatility as additive prediction error models:\n\u00b5t = gt (\u20d7xt\u22121) + \u03f5t\n(4)\nlog(\u03c32\nt ) = ht (\u20d7vt\u22121) + st,\n(5)\nwhere \u20d7xt\u22121 is the vector of predictor variables for the excess return model, \u20d7vt\u22121 is the\nvector for the volatility model, \u03f5t, and st are potentially correlated normal random variables,\nE[\u03f5t|Ft\u22121] = 0, and E[st|Ft\u22121] = 0. Functions gt and ht are to be estimated and can be\n7\n\nnon-linear. The well-known optimal weight5 is\nw\u2217\nt = E[Rt \u2212Rf\nt |Ft\u22121]\n\u00af\u03b3 \u00b7 var[Rt|Ft\u22121] ,\n(6)\nwhere Rt = exp(rt) \u22121.\nWith this portfolio allocation framework in mind, we examine a number of di\ufb00erent\nvariations of reward-risk timing for the utility-maximizing investor. One strategy is reward-\nrisk timing with an expanding window estimate of the expected return and the last month\u2019s\nrealized volatility as the prevailing volatility, referred to as the \u2019base\u2019 strategy. The investor\nrelies on volatility clustering and has a simple estimate for the excess market return at time t.\nSpeci\ufb01cally, in this strategy, volatility is computed from the daily returns for the past month\nbut the risk premia with the full monthly sample until time t \u22121. The strategy\u2019s weights\non the index are given by\n1\nt\u22121\nPt\u22121\ni=1(Rt \u2212Rf\nt )/(\u00af\u03b3 \u00b7 \u03c32\nt\u22121), a simple estimate of the optimal\nweight. Our conditionally mean-variance e\ufb03cient or optimal reward-risk timing strategies\nemploy machine learning and standard linear models models to 1) forecast the expected\nexcess return for the next month with macroeconomic and \ufb01nancial variables and 2) estimate\nnext month\u2019s volatility with a similar set of variables. Lastly, trading rules are examined that\nonly use the return or volatility model forecast, with the other factor from the base strategy.\nOur results support that machine learning models give more accurate estimates of the\nexpected return than the simple unconditional mean, and the volatility estimates relative to\nthe last month\u2019s realized volatility are similarly enhanced. We employ eleven macroeconomic\nand \ufb01nancial predictors for all the statistical models following the variable de\ufb01nitions detailed\nin Goyal and Welch (2008), including the dividend-price ratio (dp), earnings-price ratio (ep),\nbook-to-market ratio (bm), net equity expansion (ntis), Treasury-bill rate (tbl), term spread\n(tms), default spread (dfy), in\ufb02ation (in\ufb02), the high-quality corporate bond rate (corpr), long\nterm rate of return (ltr), and stock variance (svar). An additional variable is the one-month\nlagged excess return. Lastly, for the expected return models, we also use one- to three-month\n5The derivation is shown in the appendix.\n8\n\nlags of an enhanced measure of the payout yield from Boudoukh et al. (2007). Likewise, for\nthe volatility models one- to three-month lags of the realized squared monthly volatilities are\nincluded.",
    "chunk_index": 5,
    "start_char": 13919,
    "end_char": 17024,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "(tms), default spread (dfy), in\ufb02ation (in\ufb02), the high-quality corporate bond rate (corpr), long\nterm rate of return (ltr), and stock variance (svar). An additional variable is the one-month\nlagged excess return. Lastly, for the expected return models, we also use one- to three-month\n5The derivation is shown in the appendix.\n8\n\nlags of an enhanced measure of the payout yield from Boudoukh et al. (2007). Likewise, for\nthe volatility models one- to three-month lags of the realized squared monthly volatilities are\nincluded.\nGiven accurate estimates of the two moments, the reward-risk timing strategies are able\nto avoid investing during most periods of low market reward and high risk. It is not surprising\nthat even the performance of the simple reward-risk timing strategy is better relative to the\nbuy-and-hold given that it is an extension of the risk-managed portfolio literature discussed\nin the next subsection. It has been shown that only using the recent volatility as a proxy\nfor the near-future forecast has utility bene\ufb01ts (Moreira and Muir, 2019). The strategies\nemploying machine learning, however, achieve the best results. Next, we look more closely at\nthe volatility-timing strategy in the literature and the modi\ufb01cation that is made to arrive at\nthe base reward-risk timing strategy.\n3.1.1\nVolatility-Timing\nMoreira and Muir (2017) examine a volatility-managed portfolio constructed by scaling\nthe portfolio weight of the market or factor wt by the inverse of the past month\u2019s realized daily\nreturn variance. The strategy is motivated by their observation that changes in volatility\nover time are not o\ufb00set by proportional changes in returns. The authors \ufb01nd that this\nvolatility-timing strategy improves investment performance relative to the original market\nindex and a wide range of asset pricing factors by reducing risk exposure when volatility\nis high (Liu et al., 2019). In this volatility-managed portfolio, the weight in the index is\ninversely proportional to the squared realized volatility,\nwt =\nc\n\u02c6\u03c32\nt\u22121\n,\n(7)\nwhere c is a constant and \u02c6\u03c32\nt\u22121 is the realized return variance in month t\u22121. \u02c6\u03c32\nt\u22121 is computed\nfrom the 22 average daily returns over the month\n\u02c6\u03c32\nt (f) = RV 2\nt (f) =\n1\nX\nd=1/22\n \nfD\nt+d \u2212\nP1\nd=1/22 fD\nt+d\n22\n!2\n,\n(8)\n9\n\nwhere fD is the daily excess return. The constant c is set in Moreira and Muir (2017)\nsuch that the strategy\u2019s standard deviation matches that of the buy-and hold for ease of\ninterpretation. Liu et al. (2019) point out that choosing c based on the unconditional\nvolatility over the entire period is an in-sample approach and is thus subject to look-ahead\nbias. While this is correct, simply using the historical average excess return instead of the\nconstant gives similar weights and performance over time. This is not surprising since the\nhistorical mean divided by the commonly used risk-aversion coe\ufb03cient \u00af\u03b3 = 6, for instance,\nproduces a numerator that stays consistently close to the exact value of c, the constant\nwhich makes the standard deviation of the volatility-managed strategy equal to that of the\nbuy-and-hold6.",
    "chunk_index": 6,
    "start_char": 16499,
    "end_char": 19596,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "entire period is an in-sample approach and is thus subject to look-ahead\nbias. While this is correct, simply using the historical average excess return instead of the\nconstant gives similar weights and performance over time. This is not surprising since the\nhistorical mean divided by the commonly used risk-aversion coe\ufb03cient \u00af\u03b3 = 6, for instance,\nproduces a numerator that stays consistently close to the exact value of c, the constant\nwhich makes the standard deviation of the volatility-managed strategy equal to that of the\nbuy-and-hold6. Figure 1 shows the e\ufb00ects on the portfolio weights and plots the di\ufb00erence\nbetween the two weights c/\u02c6\u03c32\nt\u22121 and\n1\nt\u22121\nPt\u22121\ni=1(Rt \u2212Rf\nt )/(6 \u00b7 \u02c6\u03c32\nt\u22121) from 1989 to 2019.\nThe two weights stay close to each other over the period and the di\ufb00erence generally does\nnot exceed 10% in absolute value.\nThe discussion above provides an intuition for why this modi\ufb01ed version of volatility-\ntiming, or base reward-risk timing, achieves investment performance for the market portfolio\nsimilar to volatility-timing in Moreira and Muir (2017). The results are discussed in Section\n4. To come to the full strategy, we \ufb01rst look at the standard linear and machine learning\nmodels in the next sections.\n3.2\nElastic Net\nStarting with a standard linear model,\nyt = \u00b5 +\nm\nX\ni=1\n\u03b2ixi,t\u22121 + \u03f5t\n(9)\nwhere yt can be either the log excess return rt \u2212rf\nt or the volatility \u03c3t, we can consider\nvarious forms of regression regularization to deal with the high dimensionality of the predictor\nset. This gives alternate procedures to estimate the model coe\ufb03cients from OLS. First we\n6Because our data has a di\ufb00erent sample period, the value here does not match that in the papers above.\n10\n\nFigure 1: Volatility-timing with a constant versus the expanding window estimate of\nexcess return. The constant c, which gives the volatility-timing strategy the same ending standard\ndeviation as the buy-and-hold, over last month\u2019s realized volatility is plotted less the weight with an\nexpanding excess return mean and a risk-aversion coe\ufb03cient \u00af\u03b3 = 6.\n11\n\ndescribe LASSO, penalized regression that is designed to prevent over\ufb01tting with shrinkage.\nTo \ufb01t a model, minimize the objective function\nmin\n\u00b5,\u03b21,...,\u03b2m\n1\nT\nT\nX\nt=1\n\uf8eb\n\uf8edyt \u2212\u00b5 \u2212\nm\nX\nj=1\n\u03b2jxj,t\u22121\n\uf8f6\n\uf8f8\n2\n+ \u03bb\nm\nX\nj=1\n|\u03b2j|,\n(10)\nwhere \u03bb \u22650 is the shrinkage parameter on the l1 penalty. A higher value of \u03bb places a higher\npenalty on the coe\ufb03cients\u2019 absolute values, selectively shrinking them, and a high enough \u03bb\ncan make coe\ufb03cients zero. This produces a looser \ufb01t on the training data but less chance of\nover-\ufb01tting in terms of out-of-sample forecasts. Setting \u03bb = 0 gives the same coe\ufb03cients as\nOLS. To select the optimal value, validation is typically done by testing the performance for\na range of values on an out-of-sample data set. The parameter value that gives the maximum\npredictive accuracy is then used in the model on a distinct out-of-sample set for which results\nare reported.",
    "chunk_index": 7,
    "start_char": 19053,
    "end_char": 22007,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "\u03bb\ncan make coe\ufb03cients zero. This produces a looser \ufb01t on the training data but less chance of\nover-\ufb01tting in terms of out-of-sample forecasts. Setting \u03bb = 0 gives the same coe\ufb03cients as\nOLS. To select the optimal value, validation is typically done by testing the performance for\na range of values on an out-of-sample data set. The parameter value that gives the maximum\npredictive accuracy is then used in the model on a distinct out-of-sample set for which results\nare reported.\nWhile the LASSO \ufb01tting method typically improves predictions relative to the OLS\nmodel, it can sometimes select one predictor arbitrarily from a group of correlated predictors.\nZou and Hastie (2005) proposed Elastic Net, regression with both l1 and l2 loss, which adds\na second parameter and makes variable selection more robust. The objective function is\narg\u00b5,\u03b21,...,\u03b2m\n1\nT\nT\nX\ni=1\n(yt \u2212\u00b5 \u2212\nm\nX\nj=1\n\u03b2jxj,t\u22121)2 + \u03bb(\u03b1\nm\nX\nj\n|\u03b2j| + 1\n2(1 \u2212\u03b1)\nm\nX\nj=1\n\u03b22\nj ).\n(11)\nThe parameter 0 \u2264\u03b1 \u22641 controls the blending of the l1 and l2 loss. Using \u03b1 > 0 results in\na stronger tendency to select groups of correlated predictors. The parameters \u03b1 and \u03bb for\nthe Elastic Net model are chosen with the sample from 1958 to 1988 with cross validation as\ndescribed in Section 3. The out-of-sample predictions for LASSO or Elastic Net are given by\n\u02c6yt+1 = \u02c6\u00b5 +\nm\nX\ni=1\n\u02c6\u03b2ixi,t.\n(12)\nThe predictions, like for a standard OLS linear model, are a weighted sum of variables.\n12\n\nThe next subsection discusses the machine learning model Random Forest, which relies on\nrecursive partitioning of the feature space to make predictions, and why it can perform better\nthan linear models in our portfolio allocation problem.\n3.3\nRandom Forest\nRandom Forest is an ensemble machine learning algorithm developed by Breiman (2001).\nThe prediction by a Random Forest model is the majority vote across all the individual\ndecision tree learners (Hastie et al., 2017). The default tree bagging procedure draws B\ndi\ufb00erent bootstrap samples of the training data and \ufb01ts a separate classi\ufb01cation tree to\nthe bth sample. The forecast is the average of the trees\u2019 individual forecasts. Trees for a\nbootstrap sample are usually deep and over\ufb01t, meaning each has low bias but is ine\ufb03ciently\nvariable. Averaging over the B predictions reduces the variance and stabilizes the trees\u2019\nforecast performance. Algorithm 2 gives the procedure used to construct a Random Forest\nwith the implementation by Liaw and Wiener (2002).\nAlgorithm 1: Random Forest\nResult: The ensemble of trees {Tb}B\nfor b = 1 to B do\n1. Draw a bootstrap sample Z\u2217of size n from the training data.\n2. Grow a random-forest tree Tb to the bootstrapped data by\nrecursively repeating the following steps for each terminal node\nof the tree, until the minimum node size fraction smin or the maximum\nnumber of terminal nodes kmax are reached.\n(a) Select m variables at random from the p variables\n(b) Pick the best variable/split-point among the m.",
    "chunk_index": 8,
    "start_char": 21527,
    "end_char": 24466,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "Random Forest\nResult: The ensemble of trees {Tb}B\nfor b = 1 to B do\n1. Draw a bootstrap sample Z\u2217of size n from the training data.\n2. Grow a random-forest tree Tb to the bootstrapped data by\nrecursively repeating the following steps for each terminal node\nof the tree, until the minimum node size fraction smin or the maximum\nnumber of terminal nodes kmax are reached.\n(a) Select m variables at random from the p variables\n(b) Pick the best variable/split-point among the m.\n(c) Split the node into two child nodes.\nThe prediction at a new point, \u20d7x, is\n\u02c6f(\u20d7x) = 1\nB\nB\nX\nb=1\n\u02c6Tb(\u20d7x),\n(13)\n13\n\nthe average of all the individual trees\u2019 predictions.\nRandom forests give an improvement over bagging with a variation designed to reduce\nthe correlation among trees grown from di\ufb00erent bootstrap samples. If most of the bootstrap\nsamples are similar, the trees trained on these sample sets will be highly correlated. The\naverage estimators of similar decision trees do not perform much better than a single decision\ntree. If, for example, among the variables, last month\u2019s dividend yield is the dominant\npredictor of the return, then most of the bagged trees will have low-depth splits on the most\nrecent yield, resulting in a large correlation among their predictions. Trees are de-correlated\nwith a method known as \"random subspace\" or \"attribute bagging,\" which considers only a\nrandom subset of m predictors out of p for splitting at each potential branch. In the example,\nattribute bagging will ensure early branches for some trees will split on predictors other than\nthe most recent dividend yield. Since each tree is grown with di\ufb00erent sets of predictors,\nthe average correlation among trees further decreases and the variance reduction relative to\nstandard bagging is larger (Gu et al. 2020)7. The number of variables randomly sampled\nas candidates at each split, m, the number of bootstrap samples, B, the minimum fraction\nof observations in the terminal nodes, smin, and kmax are the tuning parameters optimized\nwith validation. A detailed algorithm for classi\ufb01cation trees can be found in the Appendix.\nThe parameters m, smin, kmax, and B are tuned with the sample from 1958 to 1988. To\ntest against parameter over-\ufb01tting, the \ufb01nal values are kept on the holdout time period from\n1989 to 2019, for which results are reported, and only one attempt is made on the period.\n3.3.1\nWhy Apply Random Forest to Portfolio Allocation?\nWith an understanding of the Random Forest model, we can discuss why this it is\npreferred over alternative machine learning methods for this portfolio allocation problem.\nTree-based learning models like Random Forest have certain desirable characteristics\nsuch as being non-metric, meaning there are no inherent assumptions of distributions in data.\nDecision trees are also scale invariant; rescaling the features by nonzero numbers do not\n7Because this makes Random Forest a non-deterministic algorithm, we average the results for multiple\ndi\ufb00erent seeds.\n14",
    "chunk_index": 9,
    "start_char": 23992,
    "end_char": 26981,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "this it is\npreferred over alternative machine learning methods for this portfolio allocation problem.\nTree-based learning models like Random Forest have certain desirable characteristics\nsuch as being non-metric, meaning there are no inherent assumptions of distributions in data.\nDecision trees are also scale invariant; rescaling the features by nonzero numbers do not\n7Because this makes Random Forest a non-deterministic algorithm, we average the results for multiple\ndi\ufb00erent seeds.\n14\n\nchange their predictions. The number of parameters typically optimized in Random Forest is\nfewer than many other machine learning models. Deep neural networks, for example, can\nhave hundreds of parameters to estimate, and the possible con\ufb01gurations of hidden layers\nand neurons are practically uncountable.\nThere is also the problem that \ufb01nancial data are notoriously noisy. Risk premia are\ndi\ufb03cult to forecast as market e\ufb03ciency diminishes the signal-to-noise ratio in well-known\nvariables. The risk premia estimation problem is further complicated by potential shifts\nin the data distributions. If a model mostly relies on idiosyncratic relationships in past\ndata, the out-of-sample performance will signi\ufb01cantly su\ufb00er when those patterns fade over\ntime. Random Forest can both \ufb01nd complex signals and mitigate the e\ufb00ect of changing\nrelationships between predictors and the target variable such as excess returns with the\nrandom subspace method. If one tree is grown to capture the relationship between expected\nreturns and the in\ufb02ation and term spread variables, the tree may accurately predict the\nexpected excess returns in some market environments, but not in all. In certain time periods,\nthe dividend yield, for instance, may be more strongly correlated with excess market returns.\nSince Random Forest grows many trees with di\ufb00erent variables, if there are changes in\nthe data distributions, some of the trees might not perform well, but the results of the\nforest should largely remain unchanged. In other words, while a single tree may capture the\nrelationships in the training data well, it is less stable. In general, a forest model can be used\nto reduce the e\ufb00ect of noisy data.\n3.4\nConditional Excess Return and Volatility Estimation\nForecasting individual stock returns is explored extensively in Gu et al. (2020). We\nfocus on the aggregate market excess returns, yet the general methodology for both excess\nreturns and volatility could be used on speci\ufb01c stocks too. This is left as a subject for future\nresearch.\nFor optimal portfolio construction, the weight of the market index should increase when\nthe investor expects a greater excess return, holding all else constant. To estimate the excess\n15\n\nreturn each month, we borrow from the standard literature which commonly employs the\nvariables from Goyal and Welch (2008). Additionally, we use a variation of lagged dividend\nyields as predictors. The importance of the dividend yield in the allocation is robust to\nthe \"data-mining\" consideration (Kandel and Stambaugh, 1996), and it has been shown to\nexplain equity return predictability in Johannes et al. (2004) for example. In traditional\ntheory, the dividend yield can explain equity prices since prices are the discounted future\ncash \ufb02ows. Boudoukh et al.",
    "chunk_index": 10,
    "start_char": 26491,
    "end_char": 29762,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "standard literature which commonly employs the\nvariables from Goyal and Welch (2008). Additionally, we use a variation of lagged dividend\nyields as predictors. The importance of the dividend yield in the allocation is robust to\nthe \"data-mining\" consideration (Kandel and Stambaugh, 1996), and it has been shown to\nexplain equity return predictability in Johannes et al. (2004) for example. In traditional\ntheory, the dividend yield can explain equity prices since prices are the discounted future\ncash \ufb02ows. Boudoukh et al. (2007) research a measure of net payout yield incorporating both\nshare repurchases and issuances which, compared to dividend yields, can have a stronger\nassociation with returns as \ufb01rms have shifted the ways they distribute earnings to their\nshareholders. We use net payout yields in lieu of traditional dividend yields and, in line\nwith previous \ufb01ndings by Boudoukh et al., observe better predictive ability in the linear\nand machine learning models. Higher order lags of the payout yield up to three months\nstill contain valuable information. In traditional literature, a higher past month\u2019s dividend\nyield is indicative of a higher chance of a positive excess return (Fama and French, 1988).\nYet the yield two months ago still has information about the overall trend in the market.\nWe trace the predictive gains of our approach to the presence of interaction e\ufb00ects between\npayout yields at di\ufb00erent months and the other macroeconomic and \ufb01nancial variables, which\nRandom Forest can detect.\nA feature of our approach is the exclusion of outliers. We omit the top decile of returns\nin absolute value, with this cuto\ufb00best performing in the validation set. We use the same\ncuto\ufb00in the test period. While trimming achieves better predictive accuracy, one could\npoint out that this may limit the return model\u2019s ability to identify extreme market events.\nOur second model, however, which forecasts volatility is better at anticipating months with\nextreme market conditions. It can be seen that in combination the two models balance each\nother and improve portfolio performance.\nVolatility has a central role in optimal portfolio selection, derivatives pricing, and risk\nmanagement. These applications motivate an extensive literature on volatility modeling.\nStarting with Engle (1982), researchers have \ufb01t a variety of autoregressive conditional\n16\n\nheteroskedasticity (ARCH), generalized ARCH (Bollerslev, 1986), and stochastic volatility\nmodels to asset returns (Fleming et al., 2001). GARCH models are widely used for their\nability to permit a wide range of behavior, in particular, more persistent periods of high\nor low volatility than seen in an ARCH process (Ruppert and Matteson, 2015). We model\nthe volatility as a function of macroeconomic and \ufb01nancial variables as well as past realized\nvolatilities.\nWe use the variables described in Section 3.1. The realized daily return variance for\na month is given by Eq. 8. The variance is highly persistent, as using simply the previous\nmonth\u2019s is su\ufb03cient for an out-of-sample nearing 50% R2.\nEmploying lagged realized\nvolatilities as predictors in our machine learning models achieves even higher accuracies.",
    "chunk_index": 11,
    "start_char": 29238,
    "end_char": 32422,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "volatility as a function of macroeconomic and \ufb01nancial variables as well as past realized\nvolatilities.\nWe use the variables described in Section 3.1. The realized daily return variance for\na month is given by Eq. 8. The variance is highly persistent, as using simply the previous\nmonth\u2019s is su\ufb03cient for an out-of-sample nearing 50% R2.\nEmploying lagged realized\nvolatilities as predictors in our machine learning models achieves even higher accuracies.\n4\nEmpirical Results\n4.1\nData Description\nThis paper uses monthly time series from Kenneth French\u2019s8 website on the market\nreturn (Mkt) and risk-free asset return (Rf) from 1927 to 2019, with 1927-1957 as the initial\ntraining period, 1958-1988 the validation period, and 1989-2019 the test period. Daily returns\nare retrieved to compute the realized volatilities. The monthly data for the conditioning\nfactors are from Amit Goyal\u2019s website9.\nThe payout yield data is from Michael Robert\u2019s website10, which is updated to cover\nJanuary 2011 to December 2019 and is derived from all \ufb01rms continuously listed on the\nNYSE, AMEX, or NASDAQ indices. For the updated data, CRSP monthly data at the\n\ufb01rm-level and the same aggregation procedure to form the payout yields as by Boudoukh et al.\n(2007) is used. This payout yield is a more inclusive measure of total payouts than standard\ndividend yields and is achieved via the \u2018net payout\u2019 of Boudoukh et al. (2007). It includes\nshare issuances and repurchases in addition to the traditional cash dividend yields. In recent\n8http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n9http://www.hec.unil.ch/agoyal\n10http://finance.wharton.upenn.edu/~mrrobert/styled-9/styled-13/index.html\n17\n\nyears share repurchases have played a more important role in total payouts to shareholders.\nFor example, Boudoukh, Richardson, and Whitelaw (2006) report a signi\ufb01cantly higher\nforecast R2 when using various measures of the payout yield (i.e. including repurchases)\nthan the dividend yield.\n4.2\nPredictive Performance\nTo assess the predictive performance for the simple, linear, and machine learning models,\nwe measure their out-of-sample R2 and directional accuracies. The out-of-sample R2 for\nexcess returns is calculated as\nR2\nos = 1 \u2212\nP\nt\u2208T (ft+1 \u2212d\nft+1)2\nP\nt\u2208T (ft+1 \u2212ft+1)2\n(14)\nwhere T denotes the set of points not used for model training and f are the monthly market\nexcess returns, \u02c6f are the model forecasts, and the mean excess return ft+1 is the competing\nforecast. The notation for excess return, f, is for readability and also re\ufb02ects that reward-risk\ntiming can be applied to factors other than the market. The R2 for the volatility models is\ncomputed in the same way.\nTable 1 contains the R2 values and directional accuracies for each forecasting model for\nexcess returns and volatility.\nRandom Forest is the best performing method for excess returns and attains the only positive\nR2, 0.52%, and correctly identi\ufb01es the correct sign of the excess return 64.52% of the time.\nThe expanding window mean estimate is slightly negative, and the linear models including\nElastic Net do not beat the simple mean. For the Random Forest excess return models the\noptimal values we \ufb01nd for smin, kmax, the number of trees, and the number of variables\nto select from at each split (m) are 0.95, 2, 500, and 4, respectively.",
    "chunk_index": 12,
    "start_char": 31968,
    "end_char": 35295,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "for\nexcess returns and volatility.\nRandom Forest is the best performing method for excess returns and attains the only positive\nR2, 0.52%, and correctly identi\ufb01es the correct sign of the excess return 64.52% of the time.\nThe expanding window mean estimate is slightly negative, and the linear models including\nElastic Net do not beat the simple mean. For the Random Forest excess return models the\noptimal values we \ufb01nd for smin, kmax, the number of trees, and the number of variables\nto select from at each split (m) are 0.95, 2, 500, and 4, respectively. The parameters have\nvarying degrees of in\ufb02uence on the model. The larger the value of smin the more shallow the\ntrees will be in general, lessening the chance to over\ufb01t. The excess return data sets have\nsigni\ufb01cant noise so a large value is not surprising. Generally, once a su\ufb03cient number of\n18\n\nTable 1: Out-of-Sample Forecasting Accuracy\nIn this table are the out-of-sample R2 and directional accuracies from 1989 to 2019 for the various\nexcess return and volatility models. The directions for volatility are based o\ufb00the mean.\nModel\nR2\nDirectional Accuracy (%)\nExcess Returns\nPrevailing Mean\n-0.0012\n64.25\nLinear Model\n-0.0351\n63.17\nElastic Net\n-0.0273\n63.17\nRandom Forest\n0.0052\n64.52\nVolatility\nPrevious Realized Volatility\n0.4437\n78.49\nLinear Model\n0.5469\n79.84\nElastic Net\n0.5451\n80.65\nRandom Forest\n0.5008\n80.91\ntrees has been reached tuning is not necessary. The maximum number of terminal nodes also\ncontrols the depth of trees, but more directly. Reducing m reduces the correlation between\ntrees. The shrinking parameter \u03bb is 0.07 and blending parameter \u03b1 is 0.1 for Elastic Net.\nFor volatility forecasting, the linear model and elastic net attain the highest R2 values\nof 54.69% and 54.51%, respectively. Random Forest produces an R2 above 50% as well and\nthe highest directional accuracy of 80.91%. For the volatility Random Forest models, the\nrespective values for smin, kmax, the number of trees, and the number of variables to select\nfrom at each split (m) are 0.01, 12, 500, and 4. The trees are grown much deeper than for the\nexcess return models as volatility is more predictable. For Elastic Net, \u03bb is 0.3 and \u03b1 is 0.1.\nNext, we show variable importance for the models measured by estimated Shapley\nvalues (Shapley, 1953).\nWe use an algorithm called Kernel SHAP to approximate the\nvalues (Lundberg and Lee, 2017).\nSHAP calculates the impact of each feature on the\npredictions made by the learned model. Given an input vector \u20d7x and a trained model f,\nSHAP approximates f with a simple model g that can easily explain the contribution of each\nfeature value. The Kernel SHAP algorithm involves the following steps:\n1. Sample S coalitions \u20d7zk \u2208{0, 1}M from 2M \u22122 total, where M is the number of variables,\n0 indicates a variable is absent, and 1 indicates it is present11.\n11We use a sample of 1,000 cases.\n19",
    "chunk_index": 13,
    "start_char": 34739,
    "end_char": 37622,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "approximates f with a simple model g that can easily explain the contribution of each\nfeature value. The Kernel SHAP algorithm involves the following steps:\n1. Sample S coalitions \u20d7zk \u2208{0, 1}M from 2M \u22122 total, where M is the number of variables,\n0 indicates a variable is absent, and 1 indicates it is present11.\n11We use a sample of 1,000 cases.\n19\n\n2. Convert \u20d7zk into the original space by replacing absent feature values with either sampled\nor reference values12 and compute predictions f(h(\u20d7zk)) for each sampled \u20d7z, where h is\nthe conversion function.\n3. Fit the weighted linear model g(\u20d7z) = \u03c60+PM\nj=1 \u03c6jzj by minimizing L = P\n\u20d7z\u2208Z[f(h(\u20d7z)\u2212\ng(\u20d7z)]2\u03c0(\u20d7z), where \u03c0(\u20d7z) = (M \u22121)/(\n\u0000M\n|\u20d7z|\n\u0001\n|\u20d7z|(M \u2212|\u20d7z|)) is the SHAP kernel.\n4. Return estimated Shapley values \u03c6j, the coe\ufb03cients from the linear model.\nIn Figure 2 we show the average factor contributions to the excess return predictions\nover the 1989 to 2019 period and in Figure 3 is the same for volatility forecasts. The darkest\nblue cells indicate the strongest positive factor contributions to an excess-return forecast,\nand the lightest cells mean the strongest negative factor contribution.\nFor the excess return Random Forest model, high values of net payout yield lagged by\none and three months, excess return, and the long-term rate of return are the most indicative\nof a higher forecasted excess return, on average. Earnings to price is the top contributor for\na smaller forecast.\nThe volatility Random Forest, Elastic Net, and Linear Models have the most recent\nrealized volatilities and net issuance as the largest contributors for a higher forecasted volatility\nnext month. The dividend-price ratio is the largest negative contributor for Random Forest.\nWith these forecasting characteristics in mind, we next discuss the risk-adjusted perfor-\nmance of the strategies and models.\n4.3\nRisk-Adjusted Returns\nThis section discusses the out-of-sample investment performance for machine learning\ncalibrated reward-risk timing and makes the relevant comparisons. We invest $1 in the\nstart of 1989 as an investor with a coe\ufb03cient of relative-risk aversion \u00af\u03b3 = 4 and plot the\ncumulative returns to each strategy on a log scale in Figures 4 and 5 without short-selling\n12We use variable means as the reference values.\n20\n\nFigure 2: SHAP values for excess return factors. This \ufb01gure shows the overall importance\nof factors for the excess return models for the 1989 to 2019 period sorted by most positive positive\ncontribution for Random Forest. Variable names are de\ufb01ned in Section 3.1. npy indicates net payout\nyield and the number refer to the order of the lag.\n21\n\nFigure 3: SHAP values for volatility factors. This \ufb01gure shows the overall importance of\nfactors for the volatility models for the 1989 to 2019 period sorted by most positive contribution for\nRandom Forest. Variable names are de\ufb01ned in Section 3.1. vol indicates monthly realized volatility\nand the number refer to the order of the lag.\n22",
    "chunk_index": 14,
    "start_char": 37272,
    "end_char": 40241,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "number refer to the order of the lag.\n21\n\nFigure 3: SHAP values for volatility factors. This \ufb01gure shows the overall importance of\nfactors for the volatility models for the 1989 to 2019 period sorted by most positive contribution for\nRandom Forest. Variable names are de\ufb01ned in Section 3.1. vol indicates monthly realized volatility\nand the number refer to the order of the lag.\n22\n\nFigure 4: Cumulative returns of reward-risk timing to market index (200% leverage\nlimit). This \ufb01gure plots the cumulative returns of the base reward-risk timing strategy in blue and\nRandom Forest reward-risk timing in black against the market index in green from 1989 to 2019.\nThe vertical axis is in log-scale.\nand with 100% and 50% leverage constraints, respectively13. For the rest of the paper, we\nimpose the more realistic portfolio constraint, preventing the investor from taking more than\n50% leverage as in Campbell and Thompson (2008): that is, con\ufb01ning the portfolio weight\non the market index to lie between 0% and 150%.\nThe investments that reward-risk time realize relatively steady gains. The \ufb01nal wealth\naccumulates to around $46 and $28 at the end of the sample for the Random Forest and base\n(expanding sample mean reward estimate and previous month realized volatility risk estimate)\nstrategies, respectively, versus about $23 for the buy-and-hold. During stable periods of high\nmarket returns, the Random Forest models aptly forecast higher excess returns and lower\nvolatility, leading to greater performance. The models also lead to better performance during\n13The \ufb01gures and tables in this section are all with \u00af\u03b3 = 4 except for Table 3. The results do not change\nsigni\ufb01cantly for other values.\n23\n\nFigure 5: Cumulative returns of reward-risk timing to market index (150% leverage\nlimit). This \ufb01gure plots the cumulative returns of the base reward-risk timing strategy in blue and\nRandom Forest reward-risk timing in black against the market index in green from 1989 to 2019.\nThe vertical axis is in log-scale.\n24\n\nFigure 6: Drawdowns of reward-risk timing to market index. This \ufb01gure plots the drawdown\nof the base reward-risk timing strategy in blue, machine learning reward-risk timing in black against\nthe market index in green from 1989 to 2019.\nrecessions and avoid as high allocations as the passive strategy. The \u2019break-away\u2019 moment\nfor Random Forest from the base reward-risk timing strategy is around 2000. During periods\nof market expansion, the Random Forest portfolio takes more risk which leads to steadily\nincreasing outperformance relative to the base portfolio. Since all the model parameters are\nchosen with data before 1989, the results cannot be easily explained by the particular choice\nof machine learning model parameters.\nIt is also valuable to look at the drawdowns for the strategies. Figure 6 plots the\ndrawdown starting from 1989 of the two strategies relative to the market, which helps us\nunderstand when our strategies lose money relative to the buy-and-hold. The base reward-\nrisk strategy takes relatively less risk when volatility is high (e.g., the 2000s) and thus,\nnot surprisingly, it diminishes the largest markets losses concentrated in those times.",
    "chunk_index": 15,
    "start_char": 39860,
    "end_char": 43052,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "data before 1989, the results cannot be easily explained by the particular choice\nof machine learning model parameters.\nIt is also valuable to look at the drawdowns for the strategies. Figure 6 plots the\ndrawdown starting from 1989 of the two strategies relative to the market, which helps us\nunderstand when our strategies lose money relative to the buy-and-hold. The base reward-\nrisk strategy takes relatively less risk when volatility is high (e.g., the 2000s) and thus,\nnot surprisingly, it diminishes the largest markets losses concentrated in those times. The\nmachine learning analog has a pattern of losses similiar to simple reward-risk timing, yet it\n25\n\ndiminishes the severity of many losses and to a high degree for some of the most extreme\nnegative returns. For the sharp market losses starting in 2007, the \ufb01rst major drawdown, the\nRandom Forest models\u2019 response is delayed, due to the very sudden drop. Yet for the other\nmajor drawdown in 2001, our Random Forest models are able to recognize the incoming\nnegative returns because the drops are more staggered, cutting the losses felt by investors\ngreatly. This is seen clearly in the Dot-com recession from 2000 to 2002, where using machine\nlearning allows investors to more than halve losses during this time. In the last recession of\n2007\u20132008, due to the extremely sharp onset, our return machine learning model reduces\nrisk exposure slightly too late, yet the information in the volatility estimate still correctly\nsteers market exposure down. Reward-risk timing never has a drawdown greater than 30%\nof the portfolio value and greatly mitigates losses during severe recessions.\nThe risk-adjusted returns from machine learning portfolio allocation are substantially\nhigher than simple reward-risk timing and the buy-and-hold. Table 2 displays the Sharpe\nratios for each portfolio allocation strategy for the sample from 1989 to 2019. We run the\ntrading rules on it with the same parameters and seeds as the 1958\u20131988 sample after they\nare \ufb01nalized.\nTable 2: Sharpe Ratios\nIn this table are the out-of-sample annual returns, standard deviations, and Sharpe ratios for the\ntest period from 1989 to 2019 for the trading rules. Mkt denotes the buy-and-hold.\nStrategy\nAnnual Return (%)\nStandard Deviation (%)\nSharpe Ratio\nMkt\n11.21\n14.57\n0.57\nBase\n11.63\n13.03\n0.67\nLinear Model Optimal\n13.84\n16.30\n0.67\nLinear Model Returns\n13.09\n15.40\n0.66\nLinear Model Volatility\n12.08\n13.07\n0.71\nElastic Net Optimal\n14.01\n16.10\n0.69\nElastic Net Returns\n13.15\n15.12\n0.68\nElastic Net Volatility\n12.08\n13.18\n0.70\nRandom Forest Optimal\n13.42\n14.39\n0.73\nRandom Forest Returns\n12.64\n13.57\n0.72\nRandom Forest Volatility\n12.08\n13.54\n0.68\n26\n\nAll the active strategies outperform the buy-and-hold on a risk-adjusted basis for the\nout-of-sample period. Reward-risk timing with Random Forest, using Random Forest for\nboth conditional excess return and volatility estimates, gives the highest Sharpe ratio of 0.73,\nwhich is a 28% increase from the buy-and-hold. An investor who reward-risk times with\nmachine learning gains about 2 percentage points on return per year relative to passively\ninvesting, while decreasing the risk.",
    "chunk_index": 16,
    "start_char": 42490,
    "end_char": 45657,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "Random Forest Volatility\n12.08\n13.54\n0.68\n26\n\nAll the active strategies outperform the buy-and-hold on a risk-adjusted basis for the\nout-of-sample period. Reward-risk timing with Random Forest, using Random Forest for\nboth conditional excess return and volatility estimates, gives the highest Sharpe ratio of 0.73,\nwhich is a 28% increase from the buy-and-hold. An investor who reward-risk times with\nmachine learning gains about 2 percentage points on return per year relative to passively\ninvesting, while decreasing the risk.\nTo quantify the economic relevance of our results and facilitate comparison, we consider\nthe perspective of the power-utility investor. Table 3 contains the average monthly realized\nutilities, certainty-equivalent (CE) yields computed as the inverse utility function of the\naverage realized utility, and terminal wealths for di\ufb00erent risk aversion coe\ufb03cients.\nFor a risk-aversion coe\ufb03cient \u00af\u03b3 = 4, the certainty-equivalent (CE) yield for the Random\nForest combined models is the highest at 9.17%. The optimal Elastic Net and Linear and\nbase strategies also give CE yields markedly greater than the market with 8.63%, 8.27%, and\n6.77%, respectively. The average monthly utility is also 33.9% greater for Random Forest\nreward-risk timing than the buy-and-hold. For a smaller risk aversion \u00af\u03b3 = 6, the utility\nincrease is 61.1%. In perspective, Campbell and Thompson (2008) estimate that the utility\ngain of timing expected returns is 35% of lifetime utility. Reward-risk timing can generate\nlarger gains relative to solely focusing on the reward or risk factors.\nNext, we run a series of time-series regression of the strategies on each other and the\nmarket index,\nfa\nt+1 = \u03b1 + \u03b2fb\nt+1 + \u03f5t+1,\n(15)\nwhere ft+1 are the monthly excess returns. A positive intercept implies that the strategy a\nincreases Sharpe ratios relative to strategy b. When this test is applied to systematic factors\n(e.g., the market portfolio) that summarize pricing information for a wide cross-section of\nassets and strategies, a positive alpha implies that our portfolio-allocation strategy expands\nthe mean-variance frontier.\nTable 4 reports results from running regressions of the machine learning reward-risk\n27\n\nTable 3: Average Realized Utilities\nIn this table, the average monthly realized utilities, annual CE yields, and terminal wealths for each\nstrategy are shown under risk aversion coe\ufb03cients 4 and 6 for the 1989 to 2019 out-of-sample period.\nPower Utility Investor\n\u00af\u03b3 = 4\nStrategy\nUtility\nCE yield\nTerminal Wealth\nMkt\n0.0056\n0.0677\n22.8993\nBase\n0.0067\n0.0813\n27.8025\nLinear Model Optimal\n0.0068\n0.0827\n47.2390\nLinear Model Returns\n0.0067\n0.0816\n39.2116\nLinear Model Volatility\n0.0070\n0.0856\n31.9458\nElastic Net Optimal\n0.0071\n0.0863\n50.3243\nElastic Net Returns\n0.0069\n0.0845\n40.5950\nElastic Net Volatility\n0.0070\n0.0850\n31.7764\nRandom Forest Optimal\n0.0075\n0.0917\n45.5343\nRandom Forest Returns\n0.0073\n0.0887\n37.1885\nRandom Forest Volatility\n0.0068\n0.083\n31.2777\n\u00af\u03b3 = 6\nMkt\n0.0036\n0.0437\n22.8993\nBase\n0.0055\n0.0670\n19.6713\nLinear Model Optimal\n0.0057\n0.0690\n35.8601\nLinear Model Returns\n0.0053\n0.0651\n29.7270\nLinear Model Volatility\n0.0056\n0.0682\n18.3737\nElastic Net Optimal\n0.0059\n0.0715\n36.6955\nElastic Net Returns\n0.0056\n0.0684\n30.6198\nElastic Net Volatility\n0.0056\n0.0678\n18.1856\nRandom Forest Optimal\n0.0058\n0.0714\n24.6255\nRandom Forest Returns\n0.0059\n0.0723\n25.3119\nRandom Forest Volatility\n0.0056\n0.0678\n18.6655\n28",
    "chunk_index": 17,
    "start_char": 45129,
    "end_char": 48565,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "Net Returns\n0.0069\n0.0845\n40.5950\nElastic Net Volatility\n0.0070\n0.0850\n31.7764\nRandom Forest Optimal\n0.0075\n0.0917\n45.5343\nRandom Forest Returns\n0.0073\n0.0887\n37.1885\nRandom Forest Volatility\n0.0068\n0.083\n31.2777\n\u00af\u03b3 = 6\nMkt\n0.0036\n0.0437\n22.8993\nBase\n0.0055\n0.0670\n19.6713\nLinear Model Optimal\n0.0057\n0.0690\n35.8601\nLinear Model Returns\n0.0053\n0.0651\n29.7270\nLinear Model Volatility\n0.0056\n0.0682\n18.3737\nElastic Net Optimal\n0.0059\n0.0715\n36.6955\nElastic Net Returns\n0.0056\n0.0684\n30.6198\nElastic Net Volatility\n0.0056\n0.0678\n18.1856\nRandom Forest Optimal\n0.0058\n0.0714\n24.6255\nRandom Forest Returns\n0.0059\n0.0723\n25.3119\nRandom Forest Volatility\n0.0056\n0.0678\n18.6655\n28\n\nTable 4: Strategy Alphas\nIn this table, we run time-series regressions of each strategy on the market and on one another\nf a\nt+1 = \u03b1 + \u03b2f b\nt+1 + \u03f5t+1. The data are monthly and the sample period is 1989 to 2019. Standard\nerrors are in parentheses and are adjusted for heteroskedasticity (White, 1980). The alphas and\nerrors are annualized in percent per year by multiplying monthly values by 12.\nUnivariate Regressions\nfa\nfb\nBeta (\u03b2)\nAlpha (\u03b1)\nR2\nNobs\nRandom Forest Optimal\nMkt\n0.57\n(0.03)\n3.37\n(1.42)\n0.76\n372\nRandom Forest Optimal\nBase\n1.04\n(0.03)\n1.44\n(0.94)\n0.89\n372\nRandom Forest Optimal\nElastic Net Optimal\n0.82\n(0.03)\n1.45\n(1.15)\n0.84\n372\nRandom Forest Optimal\nLinear Model Optimal\n0.8\n(0.03)\n1.75\n(1.2)\n0.83\n372\nElastic Net Optimal\nMkt\n0.64\n(0.04)\n3.16\n(1.64)\n0.75\n372\nElastic Net Optimal\nBase\n1.08\n(0.05)\n1.71\n(1.54)\n0.76\n372\nElastic Net Optimal\nLinear Model Optimal\n0.98\n(0.01)\n0.35\n(0.3)\n0.99\n372\nLinear Model Optimal\nMkt\n0.65\n(0.04)\n2.82\n(1.63)\n0.76\n372\nLinear Model Optimal\nBase\n1.09\n(0.05)\n1.4\n(1.59)\n0.76\n372\nBase\nMkt\n0.49\n(0.03)\n2.65\n(1.5)\n0.67\n372\n29\n\ntiming strategies on the market index and the other strategies. The intercepts (Jensen\u2019s \u03b1\u2019s)\n(Jensen, 1968) are positive and statistically signi\ufb01cant in all cases, except for the base. The\nmachine learning strategy has an annualized alpha of 3.37% and a beta of only 0.57. The\nmachine learning strategy over the base, linear model, and Elastic Net reward-risk timing\nhas annualized alphas of 1.44%, 1.75%, and 1.45%, respectively. For the comparisons, the\nalphas earned from using Elastic Net, linear model, and unconditional mean and recent\nreturn variance to forecast the excess return and volatility are smaller at 3.16% 2.82%, and\n2.65%, respectively.\nWe also conduct formal tests of marketing timing including the (HM) (Henriksson and\nMerton, 1981) and Tre-Mauzy (TM) (Treynor and Mauzy, 1966) tests. The HM test adds a\nsecond term to the model, the up-market excess return. It measures the alpha that cannot\nbe replicated by a mix of options and the market index.\nfa\nt+1 = \u03b1 + \u03b2fb\nt+1 + \u03b3 max(0, fb\nt+1) + \u03f5t+1,\n(16)\nwhere \u03b3 measures the degree of market-timing ability. In the case strategy b is the market\nindex, a positive \u03b3 would demonstrate market timing ability. The TM test has the additional\nsquared excess market return term, for which the coe\ufb03cient re\ufb02ects the convexity achieved\nby exposure to the market.",
    "chunk_index": 18,
    "start_char": 47894,
    "end_char": 50959,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "It measures the alpha that cannot\nbe replicated by a mix of options and the market index.\nfa\nt+1 = \u03b1 + \u03b2fb\nt+1 + \u03b3 max(0, fb\nt+1) + \u03f5t+1,\n(16)\nwhere \u03b3 measures the degree of market-timing ability. In the case strategy b is the market\nindex, a positive \u03b3 would demonstrate market timing ability. The TM test has the additional\nsquared excess market return term, for which the coe\ufb03cient re\ufb02ects the convexity achieved\nby exposure to the market.\nfa\nt+1 = \u03b1 + \u03b2fb\nt+1 + \u03b3(fb\nt+1)2 + \u03f5t+1,\n(17)\nIn Table 5 are the results from running the above regressions for the various strategies.\nThe Random Forest optimal strategy has statistically signi\ufb01cant coe\ufb03cients for both the TM\nand HM tests. Elastic Net optimal and return-only strategies have statistically signi\ufb01cant\ncoe\ufb03cients for the TM test but not the HM test. Unsurprisingly, the linear model and base\nstrategies do not have large positive coe\ufb03cients.\nThe next \ufb01nding is that our strategies survive transaction costs, given in Table 6.\nSpeci\ufb01cally, we evaluate our portfolio allocation strategy for the reward-risk timing portfolios\n30\n\nTable 5: Tests of Out-of-Sample Timing Ability\nIn this table, we perform marketing timing statistical tests on various strategies. The HM test:\nf a\nt+1 = \u03b1 + \u03b2f b\nt+1 + \u03b3 max(0, f b\nt+1) + \u03f5t+1 and the TM test: f a\nt+1 = \u03b1 + \u03b2f b\nt+1 + \u03b3(f b\nt+1)2 + \u03f5t+1 are\nrun and the gamma coe\ufb03cients are given. Heteroskedasticity-corrected t-statistics are in parentheses.\nThe data are monthly and the sample period is 1989 to 2019.\nStrategy\nTM\nHM\nBase\n0.004\n(1.17)\n0.031\n(0.36)\nLinear Model Optimal\n0.006\n(1.53)\n0.096\n(1.06)\nLinear Model Returns\n0.007\n(1.74)\n0.101\n(1.08)\nLinear Model Volatility\n0.005\n(1.36)\n0.056\n(0.71)\nElastic Net Optimal\n0.008\n(1.86)\n0.114\n(1.24)\nElastic Net Returns\n0.008\n(2.02)\n0.115\n(1.24)\nElastic Net Volatility\n0.005\n(1.37)\n0.056\n(0.71)\nRandom Forest Optimal\n0.008\n(2.33)\n0.134\n(1.67)\nRandom Forest Returns\n0.009\n(2.25)\n0.128\n(1.49)\nRandom Forest Volatility\n0.004\n(1.27)\n0.041\n(0.52)\n31\n\nTable 6: Transaction Costs of Machine Learning Portfolio Allocation\nIn this table, we evaluate our reward-risk timing strategies for the market when including transaction\ncosts. Lower leverage limits reduce trading activity. Speci\ufb01cally, we consider restricting risk exposure\nto be between 0 and 1 (i.e., no leverage) or 1.5. The alphas are reported with these assumptions.\nFollowing Moreira and Muir (2017), the 1bp cost comes from Fleming et al. (2003), the 10bps is from\nFrazzini, Israel, and Moskowitz (2015) when trading approximately 1% of daily volume, and the next\ncolumn adds an additional 4bps to cover for transaction costs increasing in high-volatility episodes.\nThe last column backs out the implied trading costs in basis points needed to drive the alphas to\nzero in each of the cases.\n\u03b1 After Trading Costs\nWeight\n|\u2206w|\nE[R]\n\u03b1\n1bps\n10bps\n14bps\nBreak Even\n\u00af\u03b3 = 4\nRandom Forest Optimal 1.5\n0.21\n13.42\n3.37\n3.34\n3.11\n3.01\n132.16\nElastic Net Optimal 1.5\n0.28\n14.01\n3.16\n3.12",
    "chunk_index": 19,
    "start_char": 50517,
    "end_char": 53491,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "1% of daily volume, and the next\ncolumn adds an additional 4bps to cover for transaction costs increasing in high-volatility episodes.\nThe last column backs out the implied trading costs in basis points needed to drive the alphas to\nzero in each of the cases.\n\u03b1 After Trading Costs\nWeight\n|\u2206w|\nE[R]\n\u03b1\n1bps\n10bps\n14bps\nBreak Even\n\u00af\u03b3 = 4\nRandom Forest Optimal 1.5\n0.21\n13.42\n3.37\n3.34\n3.11\n3.01\n132.16\nElastic Net Optimal 1.5\n0.28\n14.01\n3.16\n3.12\n2.82\n2.68\n93.46\nLinear Model Optimal 1.5\n0.28\n13.84\n2.82\n2.78\n2.47\n2.33\n81.70\nBase 1.5\n0.29\n11.63\n2.65\n2.62\n2.31\n2.17\n77.94\nRandom Forest Optimal 1\n0.11\n10.75\n1.96\n1.95\n1.83\n1.78\n148.80\nElastic Net Optimal 1\n0.14\n11.01\n2.00\n1.98\n1.83\n1.77\n120.6\nLinear Model Optimal 1\n0.14\n10.99\n1.88\n1.86\n1.70\n1.63\n106.94\nBase 1\n0.15\n9.63\n1.68\n1.66\n1.51\n1.44\n95.94\n\u00af\u03b3 = 6\nRandom Forest Optimal 1.5\n0.24\n11.00\n2.83\n2.8\n2.54\n2.42\n97.06\nElastic Net Optimal 1.5\n0.33\n12.55\n3.42\n3.38\n3.01\n2.85\n84.28\nLinear Model Optimal 1.5\n0.34\n12.50\n3.22\n3.18\n2.81\n2.64\n77.60\nBase 1.5\n0.33\n10.22\n2.68\n2.64\n2.28\n2.12\n67.90\nRandom Forest Optimal 1\n0.14\n9.90\n2.25\n2.23\n2.08\n2.01\n131.16\nElastic Net Optimal 1\n0.18\n10.29\n2.10\n2.08\n1.88\n1.79\n92.46\nLinear Model Optimal 1\n0.19\n10.18\n1.88\n1.86\n1.65\n1.56\n80.71\nBase 1\n0.19\n8.70\n1.77\n1.74\n1.54\n1.45\n75.34\nwhen accounting for empirically realistic transaction costs as in (Moreira and Muir, 2017).\nStrategies that capture reward-risk timing but reduce trading activity include capping the\nstrategy\u2019s leverage at 1 compared to the case with a weight limit of 1.5. These leverage\nlimits reduce trading and hence total transaction costs. We report the average absolute\nchange in monthly weights, expected return, and Jensen\u2019s alpha of each strategy before\ntransaction costs. The next columns contain the alphas when including various transaction\ncost assumptions. Finally, the last column derives the implied trading costs in basis points\nsuch that the alphas are zero in each of the cases.\nThe results indicate that machine learning reward-risk timing survives transactions\n32\n\ncosts, even with high volatility episodes where such fees rise. Overall, the annualized alpha\nof the reward-risk timing portfolio allocation strategy decreases slightly, but is still very\nlarge. Reward-risk timing with machine learning does not require extreme leverage or drastic\nportfolio rebalancing to be pro\ufb01table.\nThe empirical results overall indicate a signi\ufb01cant advantage in using machine learning\nfor portfolio allocation. With only standard predictor variables, reward-risk timing with\nmachine learning models o\ufb00ers economically substantial improvements in risk-adjusted returns\n(28% increase in Sharpe ratio). Statistically signi\ufb01cant positive alphas of 3.4% are found as\na result of the superior forecasting ability of machine learning. Finally, realistic trading costs\nare applied to gain further insight on real-life applicability, showing alphas remain large.\n5\nConclusion\nMachine learning portfolio allocation o\ufb00ers large risk-adjusted returns and utility gains\nand is feasible to implement in real-time.",
    "chunk_index": 20,
    "start_char": 53047,
    "end_char": 56094,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "substantial improvements in risk-adjusted returns\n(28% increase in Sharpe ratio). Statistically signi\ufb01cant positive alphas of 3.4% are found as\na result of the superior forecasting ability of machine learning. Finally, realistic trading costs\nare applied to gain further insight on real-life applicability, showing alphas remain large.\n5\nConclusion\nMachine learning portfolio allocation o\ufb00ers large risk-adjusted returns and utility gains\nand is feasible to implement in real-time. We perform both return- and volatility-timing, or\nreward-risk timing, with and without machine learning, showcasing the relative advantage\nthe machine learning models Random Forest and Elastic Net can provide. Furthermore,\nour strategy\u2019s performance is informative about the alpha generation process for actively\nmanaged portfolios.\nAt the same time, there are possibilities for improvements. Other machine learning\nmethods like deep neural networks may allow trading some interpretability for performance\ngains. Using predictors beyond lagged payout yields and risk-free rates may also be bene\ufb01cial.\nAdditionally, this strategy on daily or weekly data may have the bene\ufb01t of catching sharp\ndrops in the market. Since one of our goals here was to show that machine learning has an\nadvantage in \ufb01nance and portfolio allocation outside the context of big data, the results with\nstandard variables are promising.\n33\n\nReferences\n[1] Henrique BM., Sobreiro VA, Kimura H. 2019. Literature review: Machine learning\ntechniques applied to \ufb01nancial market prediction. Expert Systems With Applications\n124:226-51.\n[2] Gu S, Kelly BT, Xiu D. 2020. Empirical Asset Pricing via Machine Learning. The Review\nof Financial Studies 33(5):2223\u201373.\n[3] Goyal A, Welch I. 2008. A Comprehensive Look at The Empirical Performance of Equity\nPremium Prediction. The Review of Financial Studies 21(4):1455-508.\n[4] Boudoukh J, Michaely R, Richardson M. 2007. On the Importance of Measuring Payout\nYield: Implications for Empirical Asset Pricing. The Journal of Finance 62:877-915.\n[5] Grinold RC, Kahn RN. 1999. Active Portfolio Management: A Quantitative Approach\nfor Producing Superior Returns and Selecting Superior Returns and Controlling Risk.\nMcGraw-Hill Library of Investment and Finance.\n[6] Merton R. 1981. On Market Timing and Investment Performance. I. An Equilibrium\nTheory of Value for Market Forecasts. The Journal of Business 54(3):363-406.\n[7] Campbell JY, Thompson SB. 2008. Predicting Excess Stock Returns Out of Sample: Can\nAnything Beat the Historical Average?. The Review of Financial Studies 21(4):1509-31.\n[8] Moreira A, Muir T. 2017.\nVolatility-Managed Portfolios.\nThe Journal of Finance\n69(2):1611-44.\n[9] Kandel S, Stambaugh RF. 1996. On the Predictability of Stock Returns: An Asset-\nAllocation Perspective. The Journal of Finance 51(2):385-424.\n[10] Johannes M, Korteweg A, Polson N. 2014. Sequential Learning, Predictability, and\nOptimal Portfolio Returns. The Journal of Finance 69(2):611-644.\n34\n\n[11] Fleming J, Kirby C, Ostdiek B. 2001. The Economic Value of Volatility Timing. The\nJournal of Finance 56:329-52.\n[12] Moreira A, Muir T. 2019. Should Long-Term Investors Time Volatility?. The Journal\nof Financial Economics 131(3):507-27.\n[13] Liu F, Tang X, Zhou G. 2019. Volatility-Managed Portfolio: Does It Really Work?, The\nJournal of Portfolio Management 46(1):38-51.\n[14] Marquering W, Verbeek M. 2004. The Economic Value of Predicting Stock Index\nReturns and Volatility, Journal of Financial and Quantitative Analysis 39(2):407-29.",
    "chunk_index": 21,
    "start_char": 55613,
    "end_char": 59139,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "The Journal of Finance 69(2):611-644.\n34\n\n[11] Fleming J, Kirby C, Ostdiek B. 2001. The Economic Value of Volatility Timing. The\nJournal of Finance 56:329-52.\n[12] Moreira A, Muir T. 2019. Should Long-Term Investors Time Volatility?. The Journal\nof Financial Economics 131(3):507-27.\n[13] Liu F, Tang X, Zhou G. 2019. Volatility-Managed Portfolio: Does It Really Work?, The\nJournal of Portfolio Management 46(1):38-51.\n[14] Marquering W, Verbeek M. 2004. The Economic Value of Predicting Stock Index\nReturns and Volatility, Journal of Financial and Quantitative Analysis 39(2):407-29.\n[15] Kirby C, Ostdiek B. 2012. It\u2019s All in the Timing: Simple Active Portfolio Strategies\nthat Outperform Naive Diversi\ufb01cation. Journal of Financial and Quantitative Analysis\n47(2):437\u201367.\n[16] Nystrup P, Hansen BW, Madsen H, Lindstr\u00f6m E. 2016. Detecting change points in VIX\nand S&P 500: A new approach to dynamic asset allocation. J Asset Manag 17:361-74.\n[17] Bailey D, Borwein J, De Prado MZ, Zhu QJ. 2017. The probability of backtest over\ufb01tting.\nJournal of Computational Finance 20:39\u201369.\n[18] Zou H, Hastie T. 2005. Regularization and variable selection via the elastic net. J. R.\nStatist. Soc. B 67:301\u201320.\n[19] Breiman L. 2001. Random Forests. Machine learning 45:5\u201332.\n[20] Hastie T, Friedman J, Tibshirani R. 2017. The Elements of Statistical Learning 2nd ed.,\nSpringer.\n[21] Liaw A, Wiener M. 2002. Classi\ufb01cation and Regression by randomForest. R News\n2:18-22.\n35\n\n[22] Johannes M, Polson N, Stroud J. 2004. Sequential optimal portfolio performance: Market\nand volatility. Working Paper, Columbia University, University of Pennsylvania, and\nUniversity of Chicago.\n[23] Fama E, French KR. 1988b. Dividend yields and expected stock returns. Journal of\nFinancial Economics 222:3-25.\n[24] Engle R. 1982. Autoregressive conditional heteroskedasticity with estimates of the\nvariance of U.K. in\ufb02ation. Econometrica 50:987-1008.\n[25] Bollerslev T, 1986. Generalized Autoregressive Conditional Heteroskedasticity. Journal\nof Econometrics 31:307-27.\n[26] Matteson DS. and Ruppert D. 2015. Statistics and Data Analysis for Financial Engi-\nneering, 2nd Ed.\n[27] Boudoukh J, Richardson M, Whitelaw RF. 2006. The Myth of Long-Horizon Predictabil-\nity. Review of Financial Studies 21(4):1577-605.\n[28] Shapley LS. 1953. A value for n-person games. Contributions to the Theory of Games\n28(2):307\u201317.\n[29] Lundberg SM, Lee SI. 2017. A Uni\ufb01ed Approach to Interpreting Model Predictions.\nAdvances in Neural Information Processing Systems 21(4):4765\u201374.\n[30] Jensen MC. 1968. The Performance of Mutual Funds in the Period 1945-1964. The\nJournal of Finance 23(2):389-416.\n[31] Henriksson RD, Merton R. 1981. On Market Timing and Investment Performance. II.\nStatistical Procedures for Evaluating Forecasting Skills, In The Journal of Business 54\nNo. 4, 513-33.\n[32] Treynor JL, Mauzy K. 1966. Can Mutual Funds Outguess the Market?. Harvard\nBusiness Review 44:347-68.\n36\n\n[33] White H. 1980. A heteroskedasticity-consistent covariance matrix estimator and a direct\ntest for heteroskedasticity. Econometrica 48:817-38.\n[34] Fleming J, Kirby C, Ostdiek B, 2003. The economic value of volatility timing using\n\u201crealized\u201d volatility. Journal of Financial Economics 67:473\u2013509.\n[35] Frazzini A, Israel R, Moskowitz T. 2015. Trading costs of asset pricing anomalies.\nWorking paper, AQR Capital Management.\n[36] Samuelson PA. 1969. Lifetime Portfolio Selection by Dynamic Stochastic Programming.",
    "chunk_index": 22,
    "start_char": 58555,
    "end_char": 62015,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "Outguess the Market?. Harvard\nBusiness Review 44:347-68.\n36\n\n[33] White H. 1980. A heteroskedasticity-consistent covariance matrix estimator and a direct\ntest for heteroskedasticity. Econometrica 48:817-38.\n[34] Fleming J, Kirby C, Ostdiek B, 2003. The economic value of volatility timing using\n\u201crealized\u201d volatility. Journal of Financial Economics 67:473\u2013509.\n[35] Frazzini A, Israel R, Moskowitz T. 2015. Trading costs of asset pricing anomalies.\nWorking paper, AQR Capital Management.\n[36] Samuelson PA. 1969. Lifetime Portfolio Selection by Dynamic Stochastic Programming.\nReview of Economics and Statistics 51:239-46.\n[37] Brandt M. 1999. Estimating Portfolio and Consumption Choice: A Conditional Euler\nEquations Approach. The Journal of Finance 54(5):1609\u201345.\n[38] Gron A, J\u00f8rgensen BN, Polson NG. 2011. Optimal portfolio choice and stochastic\nvolatility. Applied Stochastic Models in Business and Industry 28(1):1-15.\n[39] Breiman L, Friedman JH, Olshen RA, Stone CJ. 1984. Classi\ufb01cation and regression\ntrees. CRC press.\n[40] Murphy K. 2012. Machine Learning - a Probabilistic Perspective. MIT Press.\n37\n\nAppendix\nA\nOptimal weights\nSamuelson (1969) showed the optimal investment fraction in the risky asset to maximize\nthe expected utility of wealth is given by:\nw\u2217\nt = \u00b5 \u2212rf\nt\n\u03b3\u03c32 .\n(18)\nIt is well known that the investment opportunities are not constant throughout time. There-\nfore, consider the following model where the market expected return and volatility change\naccording to two non-linear functions of lagged predictor variables and volatilities.\nrt = \u00b5t + \u03c3t \u00b7 zt\n(19)\n\u00b5t = gt (\u20d7xt\u22121) + \u03f5t\n(20)\nlog(\u03c32\nt ) = ht (\u20d7vt\u22121) + st,\n(21)\nwhere \u20d7xt\u22121 is the vector of predictor variables for the excess return model, \u20d7vt\u22121 is the vector\nfor the volatility model, zt, \u03f5t, and st are potentially correlated normal random variables with\nmean zero, E[zt|zt\u22121] = E[zt], E[\u03f5t|\u03f5t\u22121] = E[\u03f5t], and E[st|st\u22121] = E[st]. Functions gt and ht\nare unknown and to be estimated. In certain stylized cases, there exist closed-form solutions\nto multi-period investment problems when variables at the current time are unknown. As\nJohannes et al. (2004) point out, however, for an analytical solution, expected returns can\nbe unknown only if the current volatility is known, for instance, by the quadratic variation\nprocess. Because both future returns and volatility are predicted, to solve the optimal\nportfolio problem, we follow the existing literature and simplify the allocation problem by\nconsidering a single-period problem:\nJ(Ft\u22121) = max\nwt E[U(Wt)|Ft\u22121] = max\nwt\nZ\nU(Wt)P(rt|Ft\u22121)drt,\n(22)\n38\n\nwhere P(rt|Ft\u22121) is the predictive distribution of future returns and Ft\u22121 is the information\nset known at time t \u22121. This is similar to the approach taken in Kandel and Stambaugh\n(1996) and Johannes et al. (2014).\nThe di\ufb00erence between single and multi-period problems is that in the latter, hedging\ndemands arise from changes in variables determining the attractiveness of future investment\nopportunities. Brandt (1999) showed that hedging demands are typically very small terms in\nthe optimal weight. Additionally, portfolio choice will be myopic if the investor has power\nutility and returns are IID.",
    "chunk_index": 23,
    "start_char": 61439,
    "end_char": 64636,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "approach taken in Kandel and Stambaugh\n(1996) and Johannes et al. (2014).\nThe di\ufb00erence between single and multi-period problems is that in the latter, hedging\ndemands arise from changes in variables determining the attractiveness of future investment\nopportunities. Brandt (1999) showed that hedging demands are typically very small terms in\nthe optimal weight. Additionally, portfolio choice will be myopic if the investor has power\nutility and returns are IID.\nTo derive the optimal portfolio weight, let us assume that U(\u00b7) is twice di\ufb00erentiable,\nmonotonically increasing, and concave (which is the case for the power utility investor). Then\nby Eq. 3, the optimal portfolio is given by the \ufb01rst order condition\nE[U\n\u2032(Wt)(Rt \u2212Rf\nt )|Ft\u22121] = 0,\n(23)\nwhere Rt denotes exp(rt) \u22121, Rf\nt is exp(rf\nt ) \u22121, and the expectation is taken over the\npredictive distribution of future returns. By the de\ufb01nition of covariance and Eq. 23,\ncov[U\n\u2032(Wt), Rt \u2212Rf\nt |Ft\u22121] + E[U\u2032(Wt)|Ft\u22121]E[Rt \u2212Rf\nt |Ft\u22121] = 0,\n(24)\nTo separate the e\ufb00ects of risk and return on utility, realize that Rt has a stochastic volatility\nmixture distribution (Gron et al., 2011). In this case, a generalization of Stein\u2019s lemma (see\nAppendix B) allows us to re-write the covariance term as\ncov[U\n\u2032(Wt), Rt \u2212Rf\nt |Ft\u22121] = EQ[U\n\u2032\u2032(Wt)|Ft\u22121]cov[Wt, Rt|Ft\u22121]\n= wtEQ[U\n\u2032\u2032(Wt)|Ft\u22121]var[Rt|Ft\u22121],\n(25)\nwhere Q represents the size-biased volatility-adjusted distribution. Solving for the optimal\nweight,\nw\u2217\nt = E[Rt \u2212Rf\nt |Ft\u22121]\n\u00af\u03b3 \u00b7 var[Rt|Ft\u22121] ,\n(26)\n39\n\nwhere \u00af\u03b3 = \u2212E[U\n\u2032(Wt)|Ft\u22121]/EQ[U\n\u2032\u2032(Wt)|Ft\u22121]. This provides a justi\ufb01cation for using a\nconditional mean-variance rule.\nAs a \ufb01nal case, consider constant-mean returns and time-varying volatility:\nrt = \u00b5 + \u03c3t \u00b7 zt\n(27)\nlog(\u03c32\nt ) = ht (\u20d7vt\u22121) + st\n(28)\nStarting from Eq. 24, using the fact that E[Rt \u2212Rf\nt |Ft\u22121] = E[Rt \u2212Rf\nt ], and applying the\nsame logic, the optimal weight is given by\nw\u2217\nt =\nE[Rt \u2212Rf\nt ]\n\u00af\u03b3 \u00b7 var[Rt|Ft\u22121].\n(29)\nThe two functions gt(Ft\u22121) = Rt \u2212Rf\nt and ht(Ft\u22121) = log(\u03c32\nt ) give the expected excess\nreturn and variance, respectively, at time t given the information set Ft\u22121 at the previous\ntime. In this paper, we learn gt and ht with the machine learning algorithm Random Forest\ndiscussed in Section 3.3.\nB\nStein\u2019s lemma for stochastic volatility\nLet X be a random variable with a stochastic volatility so that X|\u03c3 is distributed\nN(\u00b5, V 2\u03c3) and \u03c3 has density p(\u03c3) that is non-negative only for \u03c3 \u22650. Let g(X) be the\ndi\ufb00erentiable function of X such that E[|g(X)|] < \u221e. Suppose that 0 < E[\u03c3] < \u221e. If\n(X, Y |\u03c3) are bivariate Normal random variables then\ncov[g(X), Y ] = EQ[g\u2032(X)]cov[X, Y ],\n(B.30)\nwhere EQ is the expectation taken under the measure induced by size-biasing q(\u03c3) =\n\u03c3p(\u03c3)/E[\u03c3]. For a proof see Gron et al. (2011).\n40\n\nC\nDecision tree algorithms\nAlgorithm C1 details how to build a regression tree in a Random Forest and is a greedy\nalgorithm (Breiman et al., 1984).",
    "chunk_index": 24,
    "start_char": 64173,
    "end_char": 67087,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "di\ufb00erentiable function of X such that E[|g(X)|] < \u221e. Suppose that 0 < E[\u03c3] < \u221e. If\n(X, Y |\u03c3) are bivariate Normal random variables then\ncov[g(X), Y ] = EQ[g\u2032(X)]cov[X, Y ],\n(B.30)\nwhere EQ is the expectation taken under the measure induced by size-biasing q(\u03c3) =\n\u03c3p(\u03c3)/E[\u03c3]. For a proof see Gron et al. (2011).\n40\n\nC\nDecision tree algorithms\nAlgorithm C1 details how to build a regression tree in a Random Forest and is a greedy\nalgorithm (Breiman et al., 1984). We refer to the recursive version in (Murphy, 2012).\nAlgorithm C1: Regression Tree\nInitialize stump node, N1(0). Nk(d) is the kth node at depth d. S denotes the data,\nand C is the set of unique labels.\nfunction \ufb01tTree(Nk(d), S, d)\n1. The prediction of the Nk(d) node is the average value of its observations,\n1\n|Nk(d)|\nP\ni\u2208Nk(d) yi\n2. De\ufb01ne the cost function as the sum of squared di\ufb00erences from the mean:\ncost({xi, yi}) = P\ni\u2208{xi,yi}(yi \u2212\u00afy)2, where \u00afy =\n1\n|{xi,yi}|\nP\ni\u2208{xi,yi} yi\nis the mean of the response variable in the speci\ufb01ed set of data.\n3. Select the optimal split:\n(j\u2217, t\u2217) = arg minj\u2208{1,..,m} mint\u2208Tj(cost({xi, yi : xij \u2264t}) + cost({xi, yi : xij > t})).\nSleft = {xi, yi : xij \u2264t}, Sright = {xi, yi : xij > t}.\n4. if notworthSplitting(d, cost, Sleft,Sright) then\nreturn Nk(d)\nelse\nUpdate the nodes:\nN1(d + 1) = \ufb01tTree(Nk(d), Sleft, d + 1)\nN2(d + 1) = \ufb01tTree(Nk(d), Sright, d + 1)\nreturn Nk(d)\nend\nResult: The regression tree model f(\u20d7x) = PD\nm=1 wm1{\u20d7x \u2208Sm}, where\nwm =\n1\n|Sm|\nP\ni\u2208Sm yi and D is the number of regions\n41\n\nThe function notworthSplitting(d, cost, Sleft,Sright) contains stopping heuristics to prevent\nover\ufb01tting. In our case, the function value is true if the fraction of examples in either Sleft or\nSright is less than smin, the minimum fraction of observations in a node for a split determined\nby the user\u2019s parameter optimization, or if the number of terminal nodes D is equal to kmax,\nthe maximum number of terminal nodes. An important note is that the Smin threshold is\napplied to the current node. For instance, a node can contain 5 observations out of 100 in\nthe data even if Smin = 0.9, but any further splits from that node will not be made since\n5/100 < 0.9.\nFor the reward Random Forest model, which estimates the excess return, the values we\nset for smin, kmax, the number of trees, and the number of variables to select from at each\nsplit (m) are 0.95, 2, 500, and 4, respectively. For the volatility Random Forest model, the\nrespective values are 0.01, 12, 500, and 4.\n42",
    "chunk_index": 25,
    "start_char": 66625,
    "end_char": 69102,
    "paper_title": "Machine Learning Portfolio Allocation",
    "paper_category": "q-fin.PM",
    "paper_filename": "Machine_Learning_Portfolio_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Machine_Learning_Portfolio_Allocation.pdf"
  },
  {
    "text": "arXiv:1701.05016v1 [q-fin.PM] 18 Jan 2017\nSubmitted paper\n1\nMean-Reverting Portfolio Design with Budget\nConstraint\nZiping Zhao, Student Member, IEEE, and Daniel P. Palomar, Fellow, IEEE\nAbstract\u2014This paper considers the mean-reverting portfolio\ndesign problem arising from statistical arbitrage in the \ufb01nan-\ncial markets. We \ufb01rst propose a general problem formulation\naimed at \ufb01nding a portfolio of underlying component assets\nby optimizing a mean-reversion criterion characterizing the\nmean-reversion strength, taking into consideration the variance\nof the portfolio and an investment budget constraint. Then\nseveral speci\ufb01c problems are considered based on the general\nformulation, and ef\ufb01cient algorithms are proposed. Numerical\nresults on both synthetic and market data show that our proposed\nmean-reverting portfolio design methods can generate consistent\npro\ufb01ts and outperform the traditional design methods and the\nbenchmark methods in the literature.\nIndex Terms\u2014Portfolio optimization, mean-reversion, cointe-\ngration, pairs trading, statistical arbitrage, algorithmic trading,\nquantitative trading.\nI. INTRODUCTION\nP\nAIRS trading [2]\u2013[6] is a well-known trading strategy\nthat was pioneered by scientists Gerry Bamberger and\nDavid Shaw, and the quantitative trading group led by Nunzio\nTartaglia at Morgan Stanley in the mid 1980s. As indicated by\nthe name, it is an investment strategy that focuses on a pair of\nassets at the same time. Investors or arbitrageurs embracing\nthis strategy do not need to forecast the absolute price of every\nsingle asset in one pair, which by nature is hard to assess, but\nonly the relative price of this pair. As a contrarian investment\nstrategy, in order to arbitrage from the market, investors need\nto buy the under-priced asset and short-sell the over-priced\none. Then pro\ufb01ts are locked in after trading positions are\nunwound when the relative mispricing corrects itself in the\nfuture.\nMore generally, pairs trading with only two trading assets\nfalls into the umbrella of statistical arbitrage [7], [8], where\nthe underlying trading basket in general consists of three or\nmore assets. Since pro\ufb01ts from such arbitrage strategies do\nnot depend on the movements and conditions of the general\n\ufb01nancial markets, statistical arbitrage is referred to as a kind\nof market neutral strategies [9], [10]. Nowadays, statistical\narbitrage is widely used by institutional investors, hedge fund\ncompanies, and many individual investors in the \ufb01nancial\nmarkets.\nIn [11], [12], the authors \ufb01rst came up with the concept\nof cointegration to describe the linear stationary and hence\nmean-reverting relationship of underlying nonstationary time\nThe authors are with the Department of Electronic and Computer Engi-\nneering, The Hong Kong University of Science and Technology (HKUST),\nClear Water Bay, Kowloon, Hong Kong (e-mail: ziping.zhao@connect.ust.hk;\npalomar@ust.hk).\nPart of the results in this paper were preliminary presented at [1].\nseries which are named to be cointegrated. Later, the cointe-\ngrated vector autoregressive model [13]\u2013[17] was proposed to\ndescribe the cointegration relations. Empirical and technical\nanalyses [18]\u2013[21] show that cointegration can be used to get\nstatistical arbitrage opportunities and such relations really exist\nin \ufb01nancial markets. Taking the prices of common stocks for\nexample, it is generally known that a stock price is observed\nand modeled as a nonstationary random walk process that\ncan be hard to predict ef\ufb01ciently.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3495,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "Part of the results in this paper were preliminary presented at [1].\nseries which are named to be cointegrated. Later, the cointe-\ngrated vector autoregressive model [13]\u2013[17] was proposed to\ndescribe the cointegration relations. Empirical and technical\nanalyses [18]\u2013[21] show that cointegration can be used to get\nstatistical arbitrage opportunities and such relations really exist\nin \ufb01nancial markets. Taking the prices of common stocks for\nexample, it is generally known that a stock price is observed\nand modeled as a nonstationary random walk process that\ncan be hard to predict ef\ufb01ciently. However, companies in\nthe same \ufb01nancial sector or industry usually share similar\nfundamental characteristics, then their stock prices may move\nin company with each other under the same trend, based\non which cointegration relations can be established. Two\nexamples are the stock prices of the two American famous\nconsumer staple companies Coca-Cola and PepsiCo and those\nof the two energy companies Ensco and Noble Corporation.\nSome examples for other \ufb01nancial assets, to name a few, are\nthe future contract prices of E-mini S&P 500 and E-mini Dow,\nthe ETF prices of SPDR S&P 500 and SPDR DJIA, the US\ndollar foreign exchange rates for different countries, and the\nswap rates for US interest rates of different maturities.\nMean-reversion is a classic indicator of predictability in\n\ufb01nancial markets and used to obtain arbitrage opportunities.\nAssets in a cointegration relation can be used to form a\nportfolio or basket and traded based upon their stationary\nmean-reversion property. We call such a designed portfolio or\nbasket of underlying assets a mean-reverting portfolio (MRP)\nor sometimes a long-short portfolio which is also called a\n\u201cspread\u201d. An asset whose price shows naturally stationarity\nis a spread as well. The pro\ufb01ts of statistical arbitrage come\ndirectly from trading on the mean-reversion of the spread\naround the long-run equilibrium. MRPs in practice are usually\nconstructed using heuristic or statistical methods. Traditional\nstatistical cointegration estimation methods are Engle-Granger\nordinary least squares (OLS) method [12] and Johansen\nmodel-based method [14]. In practice, inherent correlations\nmay exist among different MRPs. However, when having mul-\ntiple MRPs, they are commonly traded separately with their\npossible connections neglected. So a natural and interesting\nquestion is whether we can design an optimized MRP based\non the underlying spreads that could outperform every single\none. In this paper, this issue is clearly addressed.\nDesigning one MRP by choosing proportions of various\nassets in general is a portfolio optimization or asset allocation\nproblem [22]. Portfolio optimization today is considered to\nbe an important part in portfolio management as well as in\nalgorithmic trading. The seminal paper [23] by Markowitz in\n1952 laid on the foundations of what is now popularly re-\n\nSubmitted paper\n2\nferred to as mean-variance portfolio optimization and modern\nportfolio theory. Given a collection of \ufb01nancial assets, the\ntraditional mean-variance portfolio design problem is aimed\nat \ufb01nding a tradeoff between the expected return and the risk\nmeasured by the variance.",
    "chunk_index": 1,
    "start_char": 2899,
    "end_char": 6112,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "in portfolio management as well as in\nalgorithmic trading. The seminal paper [23] by Markowitz in\n1952 laid on the foundations of what is now popularly re-\n\nSubmitted paper\n2\nferred to as mean-variance portfolio optimization and modern\nportfolio theory. Given a collection of \ufb01nancial assets, the\ntraditional mean-variance portfolio design problem is aimed\nat \ufb01nding a tradeoff between the expected return and the risk\nmeasured by the variance. Different from the requirements for\nmean-variance portfolio design, in order to design a mean-\nreverting portfolio, there are two main factors to consider:\ni) the designed MRP should exhibit a strong mean-reversion\nindicating that it should have frequent mean-crossing points\nand hence bring in trading opportunities, and ii) the designed\nMRP should exhibit suf\ufb01cient but controlled variance so\nthat each trade can provide enough pro\ufb01t while controlling\nthe probability that the believed mean-reversion equilibrium\nbreaks down could be reduced.\nIn [24], the author \ufb01rst proposed to design an MRP by op-\ntimizing a criterion characterizing the mean-reversion strength\nwhich is a model-free method. Later, authors in [25] realized\nthat solving the problem in [24] could result in a portfolio\nwith very low variance, then the variance control was taken\ninto consideration and also new criteria to characterize the\nmean-reversion property were proposed for the MRP design\nproblem. In [24], [25], semide\ufb01nite programming (SDP) re-\nlaxation methods were used to solve the nonconvex problem\nformulations; however, these methods are very computation-\nally costly in general. Besides that, the design methods in [24],\n[25] were all carried out by imposing an \u21132-norm constraint on\nthe portfolio weights. This constraint brings mathematically\nconvenience to the optimization problem, but its practical\nsigni\ufb01cance in \ufb01nancial applications is dubious since the \u21132-\nnorm is not meaningful in a \ufb01nancial context. In this paper,\nwe propose to use investment budget constraints in the design\nproblems.\nThe contributions of this paper can be summarized as\nfollows. First, a general problem formulation for MRP de-\nsign problem is proposed based on which several speci\ufb01c\nproblem formulations are elaborated by considering different\nmean-reversion criteria. Second, Two classes of commonly\nused investment budget constraints on portfolio weights are\nconsidered, namely, dollar neutral constraint and net budget\nconstraint. Third, ef\ufb01cient algorithms are proposed for the\nproposed problem formulations, it is shown that some prob-\nlems after reformulations can be tackled readily by solving\nthe well-known generalized eigenvalue problem (GEVP) and\nthe generalized trust region subproblem (GTRS). The other\nproblems can be easily solved based on the majorization-\nminimization (MM) framework by solving a sequence of\nGEVPs and GTRSs, which are named iteratively reweighted\ngeneralized eigenvalue problem (IRGEVP) and iteratively\nreweighted generalized trust region subproblem (IRGTRS),\nrespectively. An extension for IRGEVP with closed-form so-\nlution in every iteration named EIRGEVP (extended IRGEVP)\nis also proposed.\nThe remaining sections of this paper are organized as\nfollows. In Section II, we introduce the design of mean-\nreverting portfolios. In Section III, we give out some mean-\nreversion criteria for an MRP, and a general formulation for the\nMRP design problem is proposed together with two commonly\nused investment budget constraint.",
    "chunk_index": 2,
    "start_char": 5668,
    "end_char": 9143,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "sequence of\nGEVPs and GTRSs, which are named iteratively reweighted\ngeneralized eigenvalue problem (IRGEVP) and iteratively\nreweighted generalized trust region subproblem (IRGTRS),\nrespectively. An extension for IRGEVP with closed-form so-\nlution in every iteration named EIRGEVP (extended IRGEVP)\nis also proposed.\nThe remaining sections of this paper are organized as\nfollows. In Section II, we introduce the design of mean-\nreverting portfolios. In Section III, we give out some mean-\nreversion criteria for an MRP, and a general formulation for the\nMRP design problem is proposed together with two commonly\nused investment budget constraint. Section IV develops the\nsolving methods for GEVP and GTRS. The MM framework\nand MM-based solving algorithms are elaborated in Section\nV. The performance of the proposed algorithms are evaluated\nnumerically in Section VI and, \ufb01nally, the concluding remarks\nare drawn in Section VII.\nNotation: Boldface upper case letters denote matrices, bold-\nface lower case letters denote column vectors, and italics\ndenote scalars. The notation 1 and I denote an all-one column\nvector and an identity matrix with proper size, respectively. R\ndenotes the real \ufb01eld with R+ denoting positive real numbers\nand RN denoting the N-dimensional real vector space. N\ndenotes the natural \ufb01eld. Z denotes the integer circle with Z+\ndenoting positive integer numbers. SK denotes the K \u00d7 K-\ndimensional symmetric matrices. The superscripts (\u00b7)T and\n(\u00b7)\u22121 denote the matrix transpose and inverse operator, respec-\ntively. Due to the commutation of the inverse and the transpose\nfor nonsingular matrices, the superscript (\u00b7)\u2212T denotes the\nmatrix inverse and transpose operator. xi,j denotes the (ith,\njth) element of matrix X and xi denotes the ith element of\nvector x. Tr (\u00b7) denotes the trace of a matrix. vec (\u00b7) denotes\nthe vectorization of a matrix, i.e., vec(X) is a column vector\nconsisting of all the columns of X stacked. \u2297denotes the\nKronecker product of two matrices.\nII. MEAN-REVERTING PORTFOLIO (MRP)\nFor a \ufb01nancial asset, e.g., a common stock, a future contract,\nan ETF, or a portfolio of them, its price at time index or\nholding period t \u2208Z+ is denoted by pt \u2208R+, and the cor-\nresponding logarithmic price or log-price yt \u2208R is computed\nas yt = log (pt), where log (\u00b7) is the natural logarithm.\nIf we consider a collection of M\nassets in a bas-\nket, their log-prices can be accordingly denoted by yt =\n[y1,t, y2,t, . . . , yM,t]T \u2208RM. Based on this basket, an MRP\nis accordingly de\ufb01ned by the portfolio weight or hedge ratio\nws = [ws,1, ws,2, . . . , ws,M]T\n\u2208RM and its (log-price)\nspread st is de\ufb01ned as st = wT\ns yt = PM\nm=1 ws,mym,t. Vector\nws indicates the market value proportion invested on the\nunderlying asset1. For m = 1, 2, . . ., M, ws,m > 0, ws,m < 0,\nand ws,m = 0 mean a long position (i.e., the asset is bought),\na short position (i.e., the asset is short-sold, or, more plainly,\nborrowed and sold), and no position, respectively.",
    "chunk_index": 3,
    "start_char": 8498,
    "end_char": 11474,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "ws,2, . . . , ws,M]T\n\u2208RM and its (log-price)\nspread st is de\ufb01ned as st = wT\ns yt = PM\nm=1 ws,mym,t. Vector\nws indicates the market value proportion invested on the\nunderlying asset1. For m = 1, 2, . . ., M, ws,m > 0, ws,m < 0,\nand ws,m = 0 mean a long position (i.e., the asset is bought),\na short position (i.e., the asset is short-sold, or, more plainly,\nborrowed and sold), and no position, respectively.\nIn Figure 1, the spread of a designed MRP together with\nthe log-prices of the two underlying assets is given. It is\nworth noting that an MRP can be interpreted as a synthesized\nstationary asset. The spread accordingly means its log-price\nwhich could be easier to predict and to make pro\ufb01ts from in\ncomparison with the underlying component assets in this MRP.\nSuppose there exist N MRPs with their spreads denoted\nby st\n=\n[s1,t, s2,t, . . . , sN,t]T\n\u2208RN. Different spreads\nmay possess different mean-reversion and variance properties\nin nature. Our objective is to design an MRP to combine\nsuch spreads into an improved overall spread with better\nproperties. In particular, we denote the portfolio by w =\n[w1, w2, . . . , wN]T \u2208RN, where w denotes the market value\n1If the spread is designed based on asset price pt instead of the log-price,\nws indicates the asset amount proportion measured in shares.\n\nSubmitted paper\n3\nLog-prices\n-2\n0\n2\n4\n6\ny1\ny2\nTime index\n0\n20\n40\n60\n80\n100 120 140 160 180 200 220 240 260 280 300\nSpread\n-1\n0\n1\ny1-0.8y2\nFig. 1.\nAn illustrative example of log-prices of two assets and a designed\nspread.\non the underlying spread. The resulting overall spread is then\ngiven by\nzt = wT st =\nN\nX\nn=1\nwnsn,t.\n(1)\nIII. MRP DESIGN PROBLEM FORMULATIONS\nTraditional portfolio design problems are based on the No-\nbel prize-winning Markowitz portfolio theory [23], [26]\u2013[28].\nThey aim at \ufb01nding a desired trade-off between return and risk,\nthe latter being measured traditionally by the variance or, in a\nmore sophisticated way, by value-at-risk and conditional value-\nat-risk. The recently proposed risk-parity portfolios [29]\u2013[31]\ncan also be categorized into this design problem.\nFor the mean-reverting portfolio, we can formulate the\ndesign problem by optimizing some mean-reversion criterion\nquantifying the mean-reversion strength of the spread zt, while\ncontrolling its variance and imposing an investment budget\nconstraint.\nA. Mean-Reversion Criteria\nIn this section, we introduce several mean-reversion criteria\nthat can characterize the mean-reversion strength of the de-\nsigned spread zt. We start by de\ufb01ning the ith order (lag-i)\nautocovariance matrix for a stochastic process st as\nMi\n=\nCov (st, st+i)\n=\nE\nh\n(st \u2212E [st]) (st+i \u2212E [st+i])T i\n,\n(2)\nwhere i \u2208N. Speci\ufb01cally, when i = 0, M0 stands for the\n(positive de\ufb01nite) covariance matrix of yt.\nSince for any random process st, we can get its centered\ncounterpart \u02dcst as \u02dcst = st \u2212E [st], in the following, we will\nuse st to denote its centered form \u02dcst.",
    "chunk_index": 4,
    "start_char": 11067,
    "end_char": 14004,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "that can characterize the mean-reversion strength of the de-\nsigned spread zt. We start by de\ufb01ning the ith order (lag-i)\nautocovariance matrix for a stochastic process st as\nMi\n=\nCov (st, st+i)\n=\nE\nh\n(st \u2212E [st]) (st+i \u2212E [st+i])T i\n,\n(2)\nwhere i \u2208N. Speci\ufb01cally, when i = 0, M0 stands for the\n(positive de\ufb01nite) covariance matrix of yt.\nSince for any random process st, we can get its centered\ncounterpart \u02dcst as \u02dcst = st \u2212E [st], in the following, we will\nuse st to denote its centered form \u02dcst.\n1) Predictability Statistics prez (w): Consider a centered\nunivariate stationary autoregressive process written as follows:\nzt = \u02c6zt\u22121 + \u01ebt,\n(3)\nwhere \u02c6zt\u22121 is the prediction of zt based on the information\nup to time t \u22121, and \u01ebt denotes a white noise independent\nfrom \u02c6zt\u22121. The predictability statistics [32] is de\ufb01ned as\nprez = \u03c32\n\u02c6z\n\u03c32z\n,\n(4)\nwhere \u03c32\nz\n=\nE\n\u0002\nz2\nt\n\u0003\nand \u03c32\n\u02c6z\n=\nE\n\u0002\n\u02c6z2\nt\u22121\n\u0003\n. If we de\ufb01ne\n\u03c32\n\u01eb = E\n\u0002\n\u01eb2\nt\n\u0003\n, then from (4), we can have \u03c32\nz = \u03c32\n\u02c6z + \u03c32\n\u01eb in the\ndenominator. When prez is small, the variance of \u01ebt dominates\nthat of \u02c6zt\u22121, and zt behaves like a white noise; when prez is\nlarge, the variance of \u02c6zt\u22121 dominates that of \u01ebt, and zt can be\nwell predicted by \u02c6zt\u22121. The predictability statistics is usually\nused to measure how close a random process is to a white\nnoise.\nUnder this criterion, in order to design a spread zt as close\nas possible to a white noise process, we need to minimize prez.\nFor a spread zt = wT st, we assume the spread st follows a\ncentered vector autoregressive model of order 1 (VAR(1)) as\nfollows:\nst = Ast\u22121 + et,\n(5)\nwhere A is the autoregressive coef\ufb01cient and et denotes a\nwhite noise independent from st\u22121. We can get A from the\nautocorrelation matrices as A = M1M\u22121\n0 . Multiplying (5) by\nw and further de\ufb01ning \u02c6zt\u22121 = wT Ast\u22121 and \u01ebt = wT et,\nwe can get \u03c32\nz = wT M0w, and \u03c32\n\u02c6z = wT Tw, where T =\nAM0AT = M1M\u22121\n0 MT\n1 . High order models VAR(p), with\np > 1, can be trivially reformulated into VAR(1) with proper\nreparametrization [33]. Then the estimator of predictability\nstatistics for zt is computed as\nprez (w) = wT Tw\nwT M0w.\n(6)\n2) Portmanteau Statistics porz (p, w): The portmanteau\nstatistics of order p [34] for a centered univariate stationary\nprocess zt is de\ufb01ned as\nporz (p) = T\np\nX\ni=1\n\u03c12\ni ,\n(7)\nwhere \u03c1i is the ith order autocorrelation (autocorrelation for\nlag i) of zt de\ufb01ned as \u03c1i = E[ztzt+i]\nE[z2\nt ]\n. The portmanteau statistics\nis used to test whether a random process is close to a white\nnoise.",
    "chunk_index": 5,
    "start_char": 13507,
    "end_char": 15990,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "2) Portmanteau Statistics porz (p, w): The portmanteau\nstatistics of order p [34] for a centered univariate stationary\nprocess zt is de\ufb01ned as\nporz (p) = T\np\nX\ni=1\n\u03c12\ni ,\n(7)\nwhere \u03c1i is the ith order autocorrelation (autocorrelation for\nlag i) of zt de\ufb01ned as \u03c1i = E[ztzt+i]\nE[z2\nt ]\n. The portmanteau statistics\nis used to test whether a random process is close to a white\nnoise. From the above de\ufb01nition, we have porz (p) \u22650 and\nthe minimum of porz (p) is attained by a white noise process,\ni.e., the portmanteau statistics for a white noise process is 0\nfor any p.\nUnder this criterion, in order to get a spread zt close to\na white noise process, we need to minimize porz (p) for a\nprespeci\ufb01ed order p. For an MRP zt = wT st, the ith order\nautocorrelation is given by\n\u03c1i = E [ztzt+i]\nE [z2\nt ]\n= wT E\n\u0002\nstsT\nt+i\n\u0003\nw\nwT E\n\u0002\nstsT\nt\n\u0003\nw = wT Miw\nwT M0w.\n(8)\nThen we can get the expression for porz (p, w) as\nporz (p, w) = T\np\nX\ni=1\n\u0012 wT Miw\nwT M0w\n\u00132\n.\n(9)\n\nSubmitted paper\n4\n3) Crossing Statistics croz (w) and Penalized Crossing\nStatistics pcroz (p, w): Crossing statistics (zero-crossing rate)\nof a centered univariate stationary process zt is de\ufb01ned as\ncroz =\n1\nT \u22121E\n\" T\nX\nt=2\n1{ztzt\u22121\u22640}\n#\n,\n(10)\nwhere 1E (zt) is the indicator function de\ufb01ned as 1E (zt) =\n(\n1,\nif zt \u2208E\n0,\nif zt /\u2208E , and the event here is E = {ztzt\u22121 \u22640}.\nCrossing statistics is used to test the probability that a sta-\ntionary process crosses its mean per unit of time and it is\neasy to notice that croz \u2208[0, 1]. According to [35], [36], for a\ncentered stationary Gaussian process zt, we have the following\nrelationship:\ncroz = 1\n\u03c0 arccos(\u03c11) .\n(11)\nRemark 1. As a special case, if zt is a centered stationary\nAR(1),\nzt = \u03c6zt\u22121 + \u01ebt,\n(12)\nwhere |\u03c6| < 1 and \u01ebt is a Gaussian white noise, then \u03c6 = \u03c11\nand accordingly the crossing statistics is croz = 1\n\u03c0 arccos(\u03c6).\nUsing this criterion, in order to get a spread zt having many\nzero-crossings, instead of directly maximizing croz, we can\nminimize \u03c11. For a spread zt = wT st, we can try to minimize\nthe \ufb01rst order autocorrelation of zt given in (8). In [25], besides\nminimizing the \ufb01rst order autocorrelation, it is also proposed\nto ensure the absolute autocorrelations of high orders |\u03c1i|s\n(i = 2, . . . , p) are small at the same time which can result in\ngood performance. In this paper, we also adopt this criterion\nand call it penalized crossing statistics of order p de\ufb01ned by\npcroz (p, w) = wT M1w\nwT M0w + \u03b7\np\nX\ni=2\n\u0012 wT Miw\nwT M0w\n\u00132\n,\n(13)\nwhere \u03b7 is a positive prespeci\ufb01ed penalization factor.",
    "chunk_index": 6,
    "start_char": 15609,
    "end_char": 18137,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "In [25], besides\nminimizing the \ufb01rst order autocorrelation, it is also proposed\nto ensure the absolute autocorrelations of high orders |\u03c1i|s\n(i = 2, . . . , p) are small at the same time which can result in\ngood performance. In this paper, we also adopt this criterion\nand call it penalized crossing statistics of order p de\ufb01ned by\npcroz (p, w) = wT M1w\nwT M0w + \u03b7\np\nX\ni=2\n\u0012 wT Miw\nwT M0w\n\u00132\n,\n(13)\nwhere \u03b7 is a positive prespeci\ufb01ed penalization factor.\nB. General MRP Design Problem Formulation\nThe MRP design problem is formulated as the optimization\nof a mean-reversion criterion denoted in general as Fz (w),\nwhich can be taken to be any of the criteria mentioned before.\nThis uni\ufb01ed criterion can be written into a compact form as\nFz (w) = \u03be wT Hw\nwT M0w + \u03b6\n\u0010\nwT M1w\nwT M0w\n\u00112\n+ \u03b7 Pp\ni=2\n\u0010\nwT Miw\nwT M0w\n\u00112\n,\n(14)\nwhich particularizes to i) prez (w), when \u03be = 1, H = T, and\n\u03b6 = \u03b7 = 0; ii) porz (p, w), when \u03be = 0, and \u03b6 = \u03b7 = 1; iii)\ncroz (w), when \u03be = 1, H = M1, and \u03b6 = \u03b7 = 0; and iv)\npcroz (p, w), when \u03be = 1, H = M1, \u03b6 = 0, and \u03b7 > 0. The\nmatrices Mis in (14) are assumed symmetric without loss of\ngenerality since they can be symmetrized.\nThe variance of the spread should also be controlled to\na certain level which can be represented as Var\n\u0002\nwT st\n\u0003\n=\nwT M0w = \u03bd. Due to this variance constraint, the denom-\ninators of Fz (w) can be removed. Denoting the portfolio\ninvestment budget constraint by W, the general MRP design\nproblem can be formulated as follows:\nmininize\nw\n\u03bewT Hw + \u03b6\n\u0000wT M1w\n\u00012 + \u03b7 Pp\ni=2\n\u0000wT Miw\n\u00012\nsubject to\nwT M0w = \u03bd\nw \u2208W,\n(15)\nwhere the objective function is denoted by fz (w) in the\nfollowing. The problem in (15) is a nonconvex problem due to\nthe nonconvexity of the objective function and the constraint\nset.\nC. Investment Budget Constraint W\nIn portfolio optimization, constraints are usually imposed\nto represent the speci\ufb01c investment guidelines. In this paper,\nwe use W to denote it and we focus on two types of budget\nconstraints: dollar neutral constraint and net budget constraint.\nDollar neutral constraint, denoted by W0, means the net\ninvestment or net portfolio position is zero; in other words, all\nthe long positions are \ufb01nanced by the short positions, com-\nmonly termed self-\ufb01nancing.2 It is represented mathematically\nby\nW0 =\n\b\n1T w = 0\n \n.\n(16)\nNet budget constraint, denoted by W1, means the net\ninvestment or net portfolio position is nonzero and equal to the\ncurrent budget which is normalized to one.3 It is represented\nmathematically by\nW1 =\n\b\n1T w = 1\n \n.",
    "chunk_index": 7,
    "start_char": 17684,
    "end_char": 20200,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "or net portfolio position is zero; in other words, all\nthe long positions are \ufb01nanced by the short positions, com-\nmonly termed self-\ufb01nancing.2 It is represented mathematically\nby\nW0 =\n\b\n1T w = 0\n \n.\n(16)\nNet budget constraint, denoted by W1, means the net\ninvestment or net portfolio position is nonzero and equal to the\ncurrent budget which is normalized to one.3 It is represented\nmathematically by\nW1 =\n\b\n1T w = 1\n \n.\n(17)\nIt is worth noting that, for two trading spreads de\ufb01ned by\nwT yt and \u2212wT yt, they are naturally the same, because in\nstatistical arbitrage the actual investment not only depends on\nw, which de\ufb01nes a spread, but also on whether a long or short\nposition is taken on this spread in the trading.\nIV. PROBLEM SOLVING ALGORITHMS VIA GEVP AND\nGTRS ALGORITHMS\nIn this section, solving methods for the MRP design prob-\nlem formulations using prez (w) and croz (w) (i.e., (15) with\n\u03b6 = \u03b7 = 0) are introduced.\nA. GEVP - Solving Algorithm for MRP Design Using\nprez (w) and croz (w) with w \u2208W0\nFor notational simplicity, we denote the matrices T in\nprez (w) and M1 in croz (w) by matrix H in general and\nrecast the problem as follows:\nminimize\nw\nwT Hw\nsubject to\nwT M0w = \u03bd\n1T w = 0,\n(18)\n2Dollar neutral constraint generally cannot be satis\ufb01ed by the traditional\ndesign methods, like methods in [12] and [14], and the methods in [25].\n3The net portfolio position can be positive or negative under net budget\nconstraint. Since the problem formulation in (15) is invariant to the sign of\nw, only the case that budget is normalized to positive 1 is considered.\n\nSubmitted paper\n5\nwhere \u03bd is a positive constant. The above problem is equivalent\nto the following nonconvex quadratically constrained quadratic\nprogramming (QCQP) [37] formulation:\nminimize\nw\nwT Hw\nsubject to\nwT M0w = \u03bd\nwT Ew = 0,\n(19)\nwhere E = 11T . By using the matrix lifting technique, i.e.,\nde\ufb01ning W = wwT , the above problem can be solved by the\nfollowing convex SDP relaxation problem:\nminimize\nW\nTr (HW)\nsubject to\nTr (M0W) = \u03bd\nTr (EW) = 0\nW \u2ab00.\n(20)\nThe following theorem gives a useful relationship between the\nnumber of variables and the number of equality constraints.\nTheorem 2 ( [38, Theorem 3.2]). Given a separable SDP as\nfollows:\nminimize\nX1,...,XL\nPL\nl=1 Tr (AlXl)\nsubject to\nPL\nl=1 Tr (BmlXl) = bm, m = 1, . . . , M\nXl \u2ab00, l = 1, . . . , L.\n(21)\nSuppose that the separable SDP are strictly feasible. Then, the\nproblem has always an optimal solution (X\u22c6\n1, . . . , X\u22c6\nL) such\nthat\nL\nX\nl=1\n[rank (X\u22c6\nl )]2 \u2264M.\nObserve that if there is only one variable X, that is to say,\nL = 1, we can get rank (X\u22c6) \u2264\n\u221a\nM.",
    "chunk_index": 8,
    "start_char": 19779,
    "end_char": 22379,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "m = 1, . . . , M\nXl \u2ab00, l = 1, . . . , L.\n(21)\nSuppose that the separable SDP are strictly feasible. Then, the\nproblem has always an optimal solution (X\u22c6\n1, . . . , X\u22c6\nL) such\nthat\nL\nX\nl=1\n[rank (X\u22c6\nl )]2 \u2264M.\nObserve that if there is only one variable X, that is to say,\nL = 1, we can get rank (X\u22c6) \u2264\n\u221a\nM. Further, if the number of\nconstraints M \u22643, a rank-1 solution can always be attainable.\nLemma 3. The nonconvex problem in (18) or (19) has no\nduality gap.\nProof: This lemma directly follows from Theorem 2 and\nthe equivalence of problems (18) and (19).\nIn other words, by solving the convex SDP in (20), there\nalways exists a rank-1 solution for W which is the solution for\nthe original problem (18), however, in practice, to \ufb01nd such a\nsolution, rank reduction methods [39] should be applied which\ncould be computationally expensive.\nAs an alternative to the SDP procedure mentioned above,\nwe \ufb01nd the problem in (18) can be ef\ufb01ciently solved as a\ngeneralized eigenvalue problem (GEVP) [40] by reformula-\ntion. Considering w = Fx, where F is the kernel that spans\nthe null space of 1T , i.e., 1T F = 0, and also required to be\nsemi-unitary, i.e., FT F = I, we can de\ufb01ne N = FT HF and\nN0 = FT M0F which is positive de\ufb01nite, then the problem\n(18) is equivalent to the following problem:\nminimize\nx\nxT Nx\nsubject to\nxT N0x = \u03bd,\n(22)\nwhich is still a nonconvex QCQP. However, this problem\nbecomes the classical GEVP problem and can be easily dealt\nwith using tailored algorithms. Here, we will apply the steepest\ndescent algorithm [41] to solve it. The procedure to solve\nproblem (18) is summarized in Algorithm 1.\nAlgorithm 1 GEVP - Algorithm for MRP design problems\nusing prez (w) and croz (w) with w \u2208W0.\nRequire: N, N0, and \u03bd > 0.\n1: Set k = 0, and choose x(k) \u2208\n\b\nx : xT N0x = \u03bd\n \n;\n2: repeat\n3:\nCompute R\n\u0000x(k)\u0001\n= x(k)T Nx(k)/x(k)T N0x(k);\n4:\nCompute d(k) = Nx(k) \u2212R\n\u0000x(k)\u0001\nN0x(k);\n5:\nx\n=\nx(k) + \u03c4d(k) with \u03c4\nchosen to minimize\nR\n\u0000x(k) + \u03c4d(k)\u0001\n;\n6:\nx(k+1) = \u221a\u03bdx/\np\nxT N0x;\n7:\nk = k + 1;\n8: until convergence\nB. GTRS - Solving Algorithm for MRP Design Using prez (w)\nand croz (w) with w \u2208W1\nAs before, for generality, we denote matrices T in prez (w)\nand M1 in croz (w) as H. Then the problems can be rewritten\nas\nminimize\nw\nwT Hw\nsubject to\nwT M0w = \u03bd\n1T w = 1,\n(23)\nwhere \u03bd is a positive constant. As before, rewriting the\nconstraint 1T w = 1 as wT Ew = 1 (since the problem is\ninvariant with respect to a sign change in w) and using the\nmatrix lifting",
    "chunk_index": 9,
    "start_char": 22074,
    "end_char": 24535,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "w \u2208W1\nAs before, for generality, we denote matrices T in prez (w)\nand M1 in croz (w) as H. Then the problems can be rewritten\nas\nminimize\nw\nwT Hw\nsubject to\nwT M0w = \u03bd\n1T w = 1,\n(23)\nwhere \u03bd is a positive constant. As before, rewriting the\nconstraint 1T w = 1 as wT Ew = 1 (since the problem is\ninvariant with respect to a sign change in w) and using the\nmatrix lifting technique, the problem in (23) can be solved by\nthe following convex SDP problem:\nminimize\nW\nTr (HW)\nsubject to\nTr (M0W) = \u03bd\nTr (EW) = 1\nW \u2ab00.\n(24)\nLike before, the nonconvex problem in (23) has no duality\ngap. Besides the above SDP method, here we introduce\nan ef\ufb01cient solving approach by reformulating (23) into a\ngeneralized trust region subproblem (GTRS) [42]. Considering\nw = w0 + Fx where w0 is any vector satisfying 1T w0 = 1\nand F is the kernel of 1T satisfying 1T F = 0 and a semi-\nunitary matrix satisfying FT F = I. Let us de\ufb01ne N = FT HF,\np = FT Hw0, b = wT\n0 Hw0, N0 = FT M0F which is positive\nde\ufb01nite, p0 = FT M0w0, and b0 = wT\n0 M0w0, then the\nproblem in (23) is equivalent to the following problem:\nminimize\nx\nxT Nx + 2pTx + b\nsubject to\nxT N0x + 2pT\n0 x + b0 = \u03bd,\n(25)\nwhich is a nonconvex QCQP and QCQPs of this type are\nspecially named GTRSs. Such problems are usually nonconvex\nbut possess necessary and suf\ufb01cient optimality conditions\nbased on which ef\ufb01cient solving methods can be derived. We\n\ufb01rst introduce the following useful theorem.\n\nSubmitted paper\n6\nTheorem 4 ( [42, Theorem 3.2]). Consider the following\nQCQP:\nminimize\nx\nq (x) \u225cxT Ax + 2aT x + a\nsubject to\nc (x) \u225cxT Bx + 2bT x + b = 0.\n(26)\nAssume that the constraint set c (x) is nonempty and that\n\u22072c (x) = 2B \u0338= 0. A vector x\u22c6is a global minimizer of\nthe problem (26) together with a multiplier \u03be\u22c6if and only if\nthe following conditions are satis\ufb01ed:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u2207q (x\u22c6) + \u03be\u22c6\u2207c (x\u22c6) = 0\nc (x\u22c6) = 0\n\u22072q (x\u22c6) + \u03be\u22c6\u22072c (x\u22c6) \u2ab00,\nand the interval set de\ufb01ned by\nI = {\u03be | A + \u03beB \u227b0}\nis not empty.\nAccording to Theorem 4, the optimality conditions for the\nprimal and dual variables (x\u22c6, \u03be\u22c6) of problem (25) are given\nas follows:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n(N + \u03be\u22c6N0) x\u22c6+ p + \u03be\u22c6p0 = 0\nx\u22c6T N0x\u22c6+ 2pT\n0 x\u22c6+ b0 \u2212\u03bd = 0\nN + \u03be\u22c6N0 \u2ab00.\n(27)\nWe assume N + \u03beN0 \u227b04, then we can see that the optimal\nsolution is given by\nx (\u03be) = \u2212(N + \u03beN0)\u22121 (p + \u03bep0) ,\n(28)\nand \u03be is the unique solution of the following equation with\nde\ufb01nition on the",
    "chunk_index": 10,
    "start_char": 24166,
    "end_char": 26516,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "variables (x\u22c6, \u03be\u22c6) of problem (25) are given\nas follows:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n(N + \u03be\u22c6N0) x\u22c6+ p + \u03be\u22c6p0 = 0\nx\u22c6T N0x\u22c6+ 2pT\n0 x\u22c6+ b0 \u2212\u03bd = 0\nN + \u03be\u22c6N0 \u2ab00.\n(27)\nWe assume N + \u03beN0 \u227b04, then we can see that the optimal\nsolution is given by\nx (\u03be) = \u2212(N + \u03beN0)\u22121 (p + \u03bep0) ,\n(28)\nand \u03be is the unique solution of the following equation with\nde\ufb01nition on the interval I:\n\u03c6 (\u03be) = 0, \u03be \u2208I,\n(29)\nwhere the function \u03c6 (\u03be) is de\ufb01ned by\n\u03c6 (\u03be) = x (\u03be)T N0x (\u03be) + 2pT\n0 x (\u03be) + b0 \u2212\u03bd,\n(30)\nand the interval I consists of all \u03be for which N + \u03beN0 \u227b0,\nwhich implies that\nI = (\u2212\u03bbmin (N, N0) , \u221e) ,\n(31)\nwhere \u03bbmin (N, N0) is the minimum generalized eigenvalue\nof matrix pair (N, N0).\nTheorem 5 ( [42, Theorem 5.2]). Assume I is not empty, then\nthe function \u03c6 (\u03be) is strictly decreasing on I unless x (\u03be) is\nconstant on I.\nIn practice, the case x (\u03be) is constant on I cannot hap-\npen. So from Theorem (5), we know when \u03c6 (\u03be) is strictly\ndecreasing on I, then a simple line search algorithm like\nbisection algorithm can be used to \ufb01nd the optimal \u03be over I.\nThe algorithm for problem (23) is summarized in Algorithm\n2.\n4The limiting case N + \u03beN0 being singular (i.e., \u03be = \u2212\u03bbmin (N, N0))\ncan be treated separately. The assumption here is reasonable since the case\nwhen \u03be = \u2212\u03bbmin (N, N0) is very rare to occur theoretically and practically.\nAlgorithm 2 GTRS - Algorithm for MRP design problems\nusing prez (w) and croz (w) with w \u2208W1.\nRequire: N, N0, p, p0, b0, \u03bbmin (N, N0), and \u03bd > 0.\n1: Set k = 0, and choose \u03be(k) \u2208(\u2212(N, N0) , \u221e);\n2: repeat\n3:\nCompute \u03c6\n\u0000\u03be(k)\u0001\naccording to (30);\n4:\nUpdate \u03be(k+1) according to the value of \u03c6\n\u0000\u03be(k)\u0001\nby a\nline search algorithm;\n5:\nk = k + 1;\n6: until convergence\n7: Compute x according to (28).\nV. PROBLEM SOLVING ALGORITHMS VIA\nMAJORIZATION-MINIMIZATION METHOD\nIn\nthis\nsection,\nwe\n\ufb01rst\ndiscuss\nthe\nmajorization-\nminimization or minorization-maximization (MM) method\nbrie\ufb02y, and then solving algorithms for the MRP design\nproblem formulations using porz (p, w) (i.e., (15) with \u03be = 0\nand \u03b6 = \u03b7 = 1) and pcroz (p, w) (i.e., (15) with \u03be = 1,\nH = M1, \u03b6 = 0 and \u03b7 > 0) are derived based on the MM\nframework and the GEVP and GTRS algorithms mentioned in\nthe previous section.\nA. The MM Method\nThe MM method [43]\u2013[45] refers to the majorization-\nminimization or minorization-maximization which is a gen-\neralization of the well-known expectation-maximization (EM)\nalgorithm. The idea behind MM is that instead of dealing with\nthe original optimization problem which could be dif\ufb01cult to\ntackle directly, it solves a series of simple surrogate subprob-\nlems.",
    "chunk_index": 11,
    "start_char": 26182,
    "end_char": 28726,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "= 0 and \u03b7 > 0) are derived based on the MM\nframework and the GEVP and GTRS algorithms mentioned in\nthe previous section.\nA. The MM Method\nThe MM method [43]\u2013[45] refers to the majorization-\nminimization or minorization-maximization which is a gen-\neralization of the well-known expectation-maximization (EM)\nalgorithm. The idea behind MM is that instead of dealing with\nthe original optimization problem which could be dif\ufb01cult to\ntackle directly, it solves a series of simple surrogate subprob-\nlems.\nSuppose the optimization problem is\nminimize\nx\nf (x)\nsubject to\nx \u2208X,\n(32)\nwhere the constraint set X \u2286RN. In general, there is no\nassumption about the convexity and differentiability on f (x).\nThe MM method aims to solve this problem by optimizing a\nsequence of surrogate functions that majorize the objective\nfunction f (x) over the set X. More speci\ufb01cally, starting\nfrom an initial feasible point x(0), the algorithm produces a\nsequence\n\b\nx(k) \naccording to the following update rule:\nx(k+1) \u2208arg min\nx\u2208X u\n\u0010\nx, x(k)\u0011\n,\n(33)\nwhere x(k) is the point generated by the update rule at the\nkth iteration and the surrogate function u\n\u0000x, x(k)\u0001\nis the\ncorresponding majorizing function of f (x) at point x(k). A\nsurrogate function is called a majorizing function of f (x) at\npoint x(k) if it satis\ufb01es the following properties:\nu\n\u0000x, x(k)\u0001\n\u2265f (x) ,\n\u2200x \u2208X,\nu\n\u0000x(k), x(k)\u0001\n= f\n\u0000x(k)\u0001\n.\n(34)\nThat is to say, the surrogate function u\n\u0000x, x(k)\u0001\nshould be an\nupper bound of the original function f (x) over X and coincide\nwith f (x) at point x(k). Although the de\ufb01nition of u\n\u0000x, x(k)\u0001\n\nSubmitted paper\n7\ngives us a great deal of \ufb02exibility for choosing it, in practice,\nthe surrogate function u\n\u0000x, x(k)\u0001\nmust be properly chosen so\nas to make the iterative update in (33) easy to compute while\nmaintaining a fast convergence over the iterations.\nThe MM method iteratively runs until some convergence\ncriterion is met. Under this MM method, the objective function\nvalue is decreased monotonically in every iteration, i.e.,\nf\n\u0010\nx(k+1)\u0011\n\u2264u\n\u0010\nx(k+1), x(k)\u0011\n\u2264u\n\u0010\nx(k), x(k)\u0011\n= f\n\u0010\nx(k)\u0011\n.\n(35)\nThe \ufb01rst inequality and the third equality follow from the\n\ufb01rst and second properties of the majorizing function in (34)\nrespectively and the second inequality follows from (33).\nB. IRGEVP and IRGTRS - Solving Algorithms for MRP De-\nsign Using porz (p, w) and pcroz (p, w)\nWe rewrite the problems using porz (p, w) and pcroz (p, w)\nin the general formulation as follows:\nminimize\nw\n\u03bewT M1w + \u03b6\n\u0000wT M1w\n\u00012\n+\u03b7 Pp\ni=2\n\u0000wT Miw\n\u00012\nsubject to\nwT M0w = \u03bd\nw \u2208W,\n(36)\nwhere the speci\ufb01c portfolio weight constraints are implicitly\nreplaced by W.\nTo\nsolve\nthe\nproblems\nin\n(36)\nvia\nmajorization-\nminimization, the key step is to \ufb01nd a majorizing function\nof the objective function such that the majorized subproblem\nis easy to solve.",
    "chunk_index": 12,
    "start_char": 28225,
    "end_char": 31027,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "w) and pcroz (p, w)\nin the general formulation as follows:\nminimize\nw\n\u03bewT M1w + \u03b6\n\u0000wT M1w\n\u00012\n+\u03b7 Pp\ni=2\n\u0000wT Miw\n\u00012\nsubject to\nwT M0w = \u03bd\nw \u2208W,\n(36)\nwhere the speci\ufb01c portfolio weight constraints are implicitly\nreplaced by W.\nTo\nsolve\nthe\nproblems\nin\n(36)\nvia\nmajorization-\nminimization, the key step is to \ufb01nd a majorizing function\nof the objective function such that the majorized subproblem\nis easy to solve. Observe that the objective function is\nquartic in w. The following mathematical manipulations are\nnecessary. We \ufb01rst compute the Cholesky decomposition of\nM0 which is M0 = LLT , where L is a lower triangular\nwith positive diagonal elements. Let us de\ufb01ne \u00afw = LT w,\n\u00af\nMi = L\u22121MiL\u2212T , and \u00af\nW = \u00afw \u00afwT . The portfolio weight\nset W is mapped to\n\u00af\nW under the linear transformation L.\nThen problem (36) can be written as\nminimize\n\u00afw, \u00af\nW\n\u03beTr\n\u0000 \u00afM1 \u00af\nW\n\u0001\n+ \u03b6\n\u0000Tr\n\u0000 \u00afM1 \u00af\nW\n\u0001\u00012\n+\u03b7 Pp\ni=1\n\u0000Tr\n\u0000 \u00af\nMi \u00af\nW\n\u0001\u00012\nsubject to\n\u00af\nW = \u00afw \u00afwT\n\u00afwT \u00afw = \u03bd\n\u00afw \u2208\u00af\nW.\n(37)\nSince Tr\n\u0000 \u00af\nMi \u00af\nW\n\u0001\n= vec\n\u0000 \u00af\nMi\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n(recall the Mis are\nassumed symmetric), problem (36) can be reformulated as\nminimize\n\u00afw, \u00af\nW\n\u03bevec\n\u0000 \u00af\nM1\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n+ vec\n\u0000 \u00af\nW\n\u0001T \u00afMvec\n\u0000 \u00af\nW\n\u0001\nsubject to\n\u00af\nW = \u00afw \u00afwT\n\u00afwT \u00afw = \u03bd\n\u00afw \u2208\u00af\nW,\n(38)\nwhere in the objective function\n\u00afM = \u03b6vec\n\u0000 \u00af\nM1\n\u0001\nvec\n\u0000 \u00afM1\n\u0001T + \u03b7 Pp\ni=2 vec\n\u0000 \u00afMi\n\u0001\nvec\n\u0000 \u00afMi\n\u0001T .\n(39)\nSpeci\ufb01cally, we can have the expressions for portmanteau\nstatistics porz (p, w) (i.e., \u03b6 = 1 and \u03b7 = 1) and penalized\ncrossing statistics pcroz (p, w) (i.e., \u03b6 = 0 and \u03b7 > 0) as\nfollows:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u00af\nMporz\n=\nPp\ni=1 vec\n\u0000 \u00afMi\n\u0001\nvec\n\u0000 \u00afMi\n\u0001T\n=\n(L \u2297L)\u22121 Pp\ni=1 vec(Mi) \u00b7\nvec(Mi)T (L \u2297L)\u2212T\n\u00af\nMpcroz\n=\n\u03b7 Pp\ni=2 vec\n\u0000 \u00af\nMi\n\u0001\nvec\n\u0000 \u00af\nMi\n\u0001T\n=\n\u03b7 (L \u2297L)\u22121 Pp\ni=2 vec (Mi) \u00b7\nvec (Mi)T (L \u2297L)\u2212T .\n(40)\nNow, the objective function in (38) is a quadratic function of\n\u00af\nW, however, this problem is still hard to solve due to the rank-\n1 constraint \u00af\nW = \u00afw \u00afwT . We then consider the application of\nthe MM trick on this problem (38) based on the following\nsimple result.\nLemma 6 ( [41, Lemma 1]). Let A \u2208SK and B \u2208SK such\nthat B \u2ab0A. Then for any point x0 \u2208RK, the quadratic\nfunction xT Ax is majorized by xT Bx + 2xT\n0 (A \u2212B) x +\nxT\n0 (B \u2212A) x0 at x0.",
    "chunk_index": 13,
    "start_char": 30618,
    "end_char": 32796,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "function of\n\u00af\nW, however, this problem is still hard to solve due to the rank-\n1 constraint \u00af\nW = \u00afw \u00afwT . We then consider the application of\nthe MM trick on this problem (38) based on the following\nsimple result.\nLemma 6 ( [41, Lemma 1]). Let A \u2208SK and B \u2208SK such\nthat B \u2ab0A. Then for any point x0 \u2208RK, the quadratic\nfunction xT Ax is majorized by xT Bx + 2xT\n0 (A \u2212B) x +\nxT\n0 (B \u2212A) x0 at x0.\nAccording to Lemma 6, given \u00af\nW(k) at the kth iteration, we\nknow the second part in the objective function of problem (38)\nis majorized by the following majorizing function at \u00af\nW(k):\nu1\n\u0000 \u00af\nW, \u00af\nW(k)\u0001\n=\n\u03c8\n\u0000 \u00af\nM\n\u0001\nvec\n\u0000 \u00af\nW\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n+2vec\n\u0000 \u00af\nW(k)\u0001T \u0000 \u00af\nM \u2212\u03c8\n\u0000 \u00af\nM\n\u0001\nI\n\u0001\nvec\n\u0000 \u00af\nW\n\u0001\n+vec\n\u0000 \u00af\nW(k)\u0001T \u0000\u03c8\n\u0000 \u00afM\n\u0001\nI \u2212\u00afM\n\u0001\nvec\n\u0000 \u00af\nW(k)\u0001\n,\n(41)\nwhere \u03c8\n\u0000 \u00af\nM\n\u0001\nis a scalar number depending on \u00af\nM and satisfy-\ning \u03c8\n\u0000 \u00af\nM\n\u0001\nI \u2ab0\u00afM. Since the \ufb01rst term vec\n\u0000 \u00af\nW\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n=\n\u0000 \u00afwT \u00afw\n\u00012 = \u03bd2 and the last term only depends on \u00af\nW(k), they\nare just two constants.\nOn the choice of \u03c8\n\u0000 \u00afM\n\u0001\n, according to Lemma 6, it is\nobvious to see that \u03c8\n\u0000 \u00af\nM\n\u0001\ncan be easily chosen to be\n\u03bbmax\n\u0000 \u00afM\n\u0001\n=\n\r\r \u00af\nM\n\r\r\n2. In the implementation of the algorithm,\nalthough\n\r\r \u00af\nM\n\r\r\n2 only needs to be computed once for the\nwhole algorithm, it is still not computationally easy to get.\nIn view of this, we introduce the following lemma to obtain\nmore possibilities for \u03c8\n\u0000 \u00af\nM\n\u0001\nwhich could be relatively easy\nto compute.\nLemma 7 ( [40]). For any matrix B \u2208RP \u00d7Q, the following\ninequalities about \u2225B\u22252 hold:\n\u2225B\u22252\n\u2264\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2225B\u2225F =\nqPP\ni=1\nPQ\nj=1 |bij|2\n\u221a\nP \u2225B\u2225\u221e=\n\u221a\nP maxi=1,...,P\nPQ\nj=1 |bij|\n\u221aQ \u2225B\u22251 = \u221aQ maxj=1,...,Q\nPP\ni=1 |bij|\n\u221aPQ \u2225B\u2225max = \u221aPQ maxi=1,...,P maxj=1,...,Q |bij|\np\n\u2225B\u2225\u221e\u2225B\u22251\n=\nr\u0010\nmaxi=1,...,P\nPQ\nj=1 |bij|\n\u0011 \u0010\nmaxj=1,...,Q\nPP\ni=1 |bij|\n\u0011\n.\n\nSubmitted paper\n8\nAccording to the above relations, \u03c8\n\u0000 \u00af\nM\n\u0001\ncan be chosen\nto be any number is larger than\n\r\r \u00afM\n\r\r\n2 but much easier to\ncompute.\nAfter ignoring the constants in (41), the majorized problem\nof problem (38) is given by\nminimize\n\u00afw, \u00af\nW\n\u03bevec\n\u0000 \u00af\nM1\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n+2vec\n\u0000 \u00af\nW(k)\u0001T \u0000 \u00af\nM \u2212\u03c8\n\u0000 \u00af\nM\n\u0001\nI\n\u0001\nvec\n\u0000 \u00af\nW\n\u0001\nsubject to\n\u00af",
    "chunk_index": 14,
    "start_char": 32401,
    "end_char": 34514,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "paper\n8\nAccording to the above relations, \u03c8\n\u0000 \u00af\nM\n\u0001\ncan be chosen\nto be any number is larger than\n\r\r \u00afM\n\r\r\n2 but much easier to\ncompute.\nAfter ignoring the constants in (41), the majorized problem\nof problem (38) is given by\nminimize\n\u00afw, \u00af\nW\n\u03bevec\n\u0000 \u00af\nM1\n\u0001T vec\n\u0000 \u00af\nW\n\u0001\n+2vec\n\u0000 \u00af\nW(k)\u0001T \u0000 \u00af\nM \u2212\u03c8\n\u0000 \u00af\nM\n\u0001\nI\n\u0001\nvec\n\u0000 \u00af\nW\n\u0001\nsubject to\n\u00af\nW = \u00afw \u00afwT\n\u00afwT \u00afw = \u03bd\n\u00afw \u2208\u00af\nW,\n(42)\nwhich can be further written as\nminimize\n\u00afw, \u00af\nW\n\u03beTr\n\u0000 \u00afM1 \u00af\nW\n\u0001\n+ 2\u03b6Tr\n\u0000 \u00afM1 \u00af\nW(k)\u0001\nTr\n\u0000 \u00af\nM1 \u00af\nW\n\u0001\n+2\u03b7 Pp\ni=2 Tr\n\u0000 \u00af\nMi \u00af\nW(k)\u0001\nTr\n\u0000 \u00af\nMi \u00af\nW\n\u0001\n\u22122\u03c8\n\u0000 \u00af\nM\n\u0001\nTr\n\u0000 \u00af\nW(k) \u00af\nW\n\u0001\nsubject to\n\u00af\nW = \u00afw \u00afwT\n\u00afwT \u00afw = \u03bd\n\u00afw \u2208\u00af\nW.\n(43)\nBy changing \u00af\nW back to \u00afw, problem (43) becomes\nminimize\n\u00afw\n\u00afwT \u00afH(k) \u00afw\nsubject to\n\u00afwT \u00afw = \u03bd\n\u00afw \u2208\u00af\nW,\n(44)\nwhere\nin\nthe\nobjective\nfunction,\n\u00afH(k)\nis\nde\ufb01ned\nin\nthis\nway\n\u00afH(k)\n=\n\u03be \u00af\nM1 + 2\u03b6\n\u0000 \u00afw(k)T \u00afM1 \u00afw(k)\u0001 \u00afM1 +\n2\u03b7 Pp\ni=2\n\u0000\u00afw(k)T \u00af\nMi \u00afw(k)\u0001 \u00af\nMi \u22122\u03c8\n\u0000 \u00af\nM\n\u0001\n\u00afw(k) \u00afw(k)T . Finally,\nwe can undo the change of variable \u00afw = LT w, obtaining\nminimize\nw\nwT H(k)w\nsubject to\nwT M0w = \u03bd\nw \u2208W,\n(45)\nwhere in the objective function\nH(k)\n=\n\u03beM1 + 2\u03b6\n\u0000w(k)T M1w(k)\u0001\nM1\n+2\u03b7 Pp\ni=2\n\u0000w(k)T Miw(k)\u0001\nMi\n\u22122\u03c8\n\u0000 \u00af\nM\n\u0001\nM0w(k)w(k)T M0.\n(46)\nMore speci\ufb01cally, for portmanteau statistics porz (p, w) (i.e.,\n\u03be = 0, \u03b6 = 1 and \u03b7 = 1) and penalized crossing statistics\npcroz (p, w) (i.e., \u03be = 1, \u03b6 = 0 and \u03b7 > 0), we have the\nfollowing expressions:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nH(k)\nporz\n=\n2 Pp\ni=1\n\u0000w(k)T Miw(k)\u0001\nMi\n\u22122\u03c8\n\u0000 \u00af\nM\n\u0001\nM0w(k)w(k)T M0,\nH(k)\npcroz\n=\nM1 + 2\u03b7 Pp\ni=2\n\u0000w(k)T Miw(k)\u0001\nMi\n\u22122\u03c8\n\u0000 \u00af\nM\n\u0001\nM0w(k)w(k)T M0.\n(47)\nFinally, in the majorization problems (44) and (45), the\nobjective functions become quadratic in the variable rather\nthan quartic in the variable as in the original problem (36).\nDepending on the speci\ufb01c form of W, problem (45) is either\nthe GEVP or GTRS problems discussed in the previous\nsections. So, in order to handle the original problem (36)\ndirectly which could be dif\ufb01cult, we just need to iteratively\nsolve a sequence of GEVPs or GTRSs. We call these MM-\nbased algorithms iteratively reweighted GEVP (IRGEVP) and\niteratively reweighted GTRS (IRGTRS) respectively which are\nsummarized in Algorithm 3.\nAlgorithm 3 IRGEVP and IRGTRS - Algorithms for MRP\ndesign problems using porz (p, w) and pcroz (p, w).\nRequire: p, Mi with i = 1, . . . , p, and \u03bd > 0.",
    "chunk_index": 15,
    "start_char": 34183,
    "end_char": 36429,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "order to handle the original problem (36)\ndirectly which could be dif\ufb01cult, we just need to iteratively\nsolve a sequence of GEVPs or GTRSs. We call these MM-\nbased algorithms iteratively reweighted GEVP (IRGEVP) and\niteratively reweighted GTRS (IRGTRS) respectively which are\nsummarized in Algorithm 3.\nAlgorithm 3 IRGEVP and IRGTRS - Algorithms for MRP\ndesign problems using porz (p, w) and pcroz (p, w).\nRequire: p, Mi with i = 1, . . . , p, and \u03bd > 0.\n1: Set k = 0, and choose initial value w(k) \u2208W;\n2: Compute \u00af\nM according to (39) and \u03c8\n\u0000 \u00af\nM\n\u0001\n;\n3: repeat\n4:\nCompute H(k) according to (46);\n5:\nUpdate w(k+1) by solving the GEVP in (22) or the\nGTRS in (25);\n6:\nk = k + 1;\n7: until convergence\nC. EIRGEVP - An Extended Algorithm for IRGEVP\nIn the MM-based algorithms mentioned above, it would be\nmuch desirable if we could get a closed-form solution for the\nsubproblems in every iteration. In fact, for IRGEVPs, applying\nthe MM trick once again, a closed-form solution is attainable\nat every iteration. To illustrate this, we rewrite the subproblem\n(44) of IRGEVP again as follows:\nminimize\nw\nwT H(k)w\nsubject to\nwT M0w = \u03bd\n1T w = 0.\n(48)\nConsidering the trick used to eliminate the linear constraint\nto get problem (22), we can get the following equivalent\nformulation:\nminimize\nx\nxT N(k)x\nsubject to\nxT N0x = \u03bd,\n(49)\nwhere F and N0 are de\ufb01ned as before; N(k) = FT H(k)F.\nConsidering the Cholesky decomposition N0 = RRT with\nR to be a lower triangular with positive diagonal elements,\nwe can have the variable transformation \u00afx = RT x. Then the\nproblem (48) becomes\nminimize\n\u00afx\n\u00afxT \u00afN(k)\u00afx\nsubject to\n\u00afxT \u00afx = \u03bd,\n(50)\nwhere \u00afN(k) = R\u22121N(k)R\u2212T .\nApplying Lemma 6 again, the objective function of problem\n(50) is majorized by the following majorizing function at \u00afx(k):\nu2\n\u0000\u00afx, \u00afx(k)\u0001\n=\n\u03c8\n\u0000 \u00afN(k)\u0001\n\u00afxT \u00afx\n+2\n\u0002\u0000 \u00afN(k) \u2212\u03c8\n\u0000 \u00afN(k)\u0001\nI\n\u0001 \u00afx(k)\u0003T \u00afx\n+\u00afx(k)T \u0002\n\u03c8\n\u0000 \u00afN(k)\u0001\nI \u2212\u00afN(k)\u0003 \u00afx(k),\n(51)\nwhere \u03c8\n\u0000 \u00afN(k)\u0001\ncan be chosen using the results from Lemma\n7. The \ufb01rst and last parts are just two constants. Note that\nalthough in the derivation we have applied the MM scheme\n\nSubmitted paper\n9\ntwice, it can be viewed as a direct majorization for the\nobjective of the original problem at w(k). The following\nlemma summarizes the overall majorizing function.\nLemma 8. For problem (36) with w \u2208W0, the majorization\nin (41) together with (51) can be shown to be a majorization\nfor the objective function of the original problem at w(k) over\nthe constraint set by the following function:\nu2\n\u0000w, w(k)\u0001\n=\n2\n\u0002\u0000H(k) \u2212\u03c8\n\u0000R\u22121FT H(k)FR\u2212T \u0001\nM0\n\u0001\nw(k)\u0003T w\n+2\u03c8\n\u0000R\u22121FT H(k)FR\u2212T\u0001\n\u03bd \u2212w(k)T H(k)w(k).\n(52)\nwhere the last two terms are constants.\nProof: See Appendix A.",
    "chunk_index": 16,
    "start_char": 35975,
    "end_char": 38618,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "problem (36) with w \u2208W0, the majorization\nin (41) together with (51) can be shown to be a majorization\nfor the objective function of the original problem at w(k) over\nthe constraint set by the following function:\nu2\n\u0000w, w(k)\u0001\n=\n2\n\u0002\u0000H(k) \u2212\u03c8\n\u0000R\u22121FT H(k)FR\u2212T \u0001\nM0\n\u0001\nw(k)\u0003T w\n+2\u03c8\n\u0000R\u22121FT H(k)FR\u2212T\u0001\n\u03bd \u2212w(k)T H(k)w(k).\n(52)\nwhere the last two terms are constants.\nProof: See Appendix A.\nThen, the majorized problem of (50) becomes\nminimize\n\u00afx\ne(k)T \u00afx\nsubject to\n\u00afxT \u00afx = \u03bd,\n(53)\nwhere e(k) = 2\n\u0000 \u00afN(k) \u2212\u03c8\n\u0000 \u00afN(k)\u0001\nI\n\u0001 \u00afx(k) for the majorization\nin (51). By Cauchy-Schwartz inequality, we have eT \u00afx \u2265\n\u2212\u2225e\u22252 \u2225\u00afx\u22252 = \u2212\u03bd \u2225e\u22252, and the equality holds only when\n\u00afx and e are aligned in the opposite direction. Considering\nthe constraint, we can get the optimal solution of (53) as\n\u00afx = \u2212\u221a\u03bd\ne\n\u2225e\u22252 . We call this algorithm extended IRGEVP\n(EIRGEVP) which is summarized in Algorithm 4.\nAlgorithm 4 EIRGEVP - An extended algorithm for IRGEVP.\nRequire: p, Mi with i = 1, . . . , p, and \u03bd > 0.\n1: Set k = 0, and choose initial value w(k) \u2208W;\n2: Compute \u00afM and \u03c8\n\u0000 \u00af\nM\n\u0001\n;\n3: repeat\n4:\nCompute \u00afN(k) and \u03c8\n\u0000 \u00afN(k)\u0001\n;\n5:\nUpdate w(k+1) with a closed-form solution;\n6:\nk = k + 1;\n7: until convergence\nVI. NUMERICAL EXPERIMENTS\nA statistical arbitrage strategy involves several steps of\nwhich the MRP design is a central one. Here, we divide the\nwhole strategy into four sequential steps, namely: assets pool\nconstruction, MRP design, unit-root test, and mean-reversion\ntrading. In the \ufb01rst step, we select a collection of possibly\ncointegrated asset candidates to construct an asset pool, on\nwhich we will not elaborate in this paper. In the second step,\nbased on the candidate assets from the asset pool, MRPs are\ndesigned using either traditional design methods like Engle-\nGranger OLS method [12] and Johansen method [13] or the\nproposed methods in this paper. In the third step, unit-root\ntest procedures like Augmented Dickey-Fuller test [46] and\nPhillips-Perron test [47] are applied to test the stationarity or\nmean-reversion property of the designed MRPs. In the fourth\nstep, MRPs passing the unit-root tests will be traded based on\na designed mean-reversion trading strategy.\nIn this section, we \ufb01rst illustrate a mean-reversion trading\nstrategy and based on that the performance of our proposed\nMRP design methods in Sections IV and V using both\nsynthetic data and real market data are shown accordingly.\nA. Mean-Reversion Trading Design\nIn this paper, we use a simple trading strategy where\nthe trading signals, i.e., to buy, to sell, or simply to hold,\nare designed based on simple event triggers. Mean-reversion\ntrading is carried out on the designed spread zt which is\ntested to be unit-root stationary. A trading position (either a\nlong position denoted by 1 or a short position denoted by\n\u22121) denotes a state for investment and it",
    "chunk_index": 17,
    "start_char": 38239,
    "end_char": 41065,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "real market data are shown accordingly.\nA. Mean-Reversion Trading Design\nIn this paper, we use a simple trading strategy where\nthe trading signals, i.e., to buy, to sell, or simply to hold,\nare designed based on simple event triggers. Mean-reversion\ntrading is carried out on the designed spread zt which is\ntested to be unit-root stationary. A trading position (either a\nlong position denoted by 1 or a short position denoted by\n\u22121) denotes a state for investment and it is opened when\nthe spread zt is away from its long-run equilibrium \u00b5z by\na prede\ufb01ned trading threshold \u2206and closed (denoted by 0)\nwhen zt crosses its equilibrium \u00b5z. (A common variation is\nto close the position after the spread crossed the equilibrium\nby more than another threshold \u2206\u2032.) The time period from\nposition opening to position closing is de\ufb01ned as a trading\nperiod.\nIn order to get a standard trading rule, we introduce a\nstandardization technique by de\ufb01ning z \u2212score which is a\nnormalized spread as follows:\n\u02dczt = zt \u2212\u00b5z\n\u03c3z\n,\n(54)\nwhere \u00b5z and \u03c3z are the mean and the standard deviation of the\nspread zt and computed over an in-sample look-back period\nin practice. For \u02dczt, it follows that E [\u02dczt] = 0 and Std [\u02dczt] = 1.\nThen, we can de\ufb01ne \u2206= d \u00d7 \u03c3z, for some value of d (e.g.,\nd = 1).\nIn a trading stage, based on the trading position and ob-\nserved (normalized) spread value at holding period t, we can\nget the trading actions at the next consecutive holding period\nt + 1. The mean-reversion trading strategy is summarized in\nTable I and a simple trading example based on this strategy is\nillustrated in Figure 2.\nSpread \u02dczt\n-1.5\u03c3z\n-\u03c3z\n-0.5\u03c3z\n0\n0.5\u03c3z\n\u03c3z\nTrading time\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\nTrading positions\n-1\n0\n1\nFig. 2. A simple example for mean-reversion trading strategy design (trading\nthreshold \u2206= \u03c3z).\n\nSubmitted paper\n10\nTABLE I\nTRADING POSITIONS, NORMALIZED SPREAD, AND TRADING ACTIONS OF A MEAN-REVERSION TRADING STRATEGY\nTrading Position at t\nNormalized Spread \u02dczt\nAction(s) Taken within Holding Period t + 1\nTrading Position at t + 1\n1\n+d \u2264\u02dczt\nClose the long pos. & Open a short pos.\n-1\n0 \u2264\u02dczt < +d\nClose the long pos.\n0\n\u02dczt < 0\nNo action\n1\n0\n+d \u2264\u02dczt\nOpen a short pos.\n-1\n\u2212d < \u02dczt < +d\nNo action\n0\n\u02dczt \u2264\u2212d\nOpen a long pos.\n1\n-1\n0 < \u02dczt\nNo action\n-1\n\u2212d < \u02dczt \u22640\nClose the short pos.\n0\n\u02dczt \u2264\u2212d\nClose the short pos. & Open a long pos.\n1\nB. Performance Metrics\nAfter an MRP is constructed, we need to de\ufb01ne the relation\nbetween the designed MRP with the underlying \ufb01nancial\nassets. Recall that the spread for the designed MRP is zt =\nwT st, where st = [s1,t, s2,t, .",
    "chunk_index": 18,
    "start_char": 40594,
    "end_char": 43213,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "\u2212d < \u02dczt < +d\nNo action\n0\n\u02dczt \u2264\u2212d\nOpen a long pos.\n1\n-1\n0 < \u02dczt\nNo action\n-1\n\u2212d < \u02dczt \u22640\nClose the short pos.\n0\n\u02dczt \u2264\u2212d\nClose the short pos. & Open a long pos.\n1\nB. Performance Metrics\nAfter an MRP is constructed, we need to de\ufb01ne the relation\nbetween the designed MRP with the underlying \ufb01nancial\nassets. Recall that the spread for the designed MRP is zt =\nwT st, where st = [s1,t, s2,t, . . . , sN,t]T with sn,t = wT\nsnyt\nfor n = 1, 2, . . ., N. By de\ufb01ning st = WT\ns yt with Ws =\n[ws1, ws2, . . . , wsN ], we get the spread zt = wT\np yt, where\nwp = Wsw denotes the portfolio weight directly de\ufb01ned on\nthe underlying assets.\nBased on the mean-reversion trading strategy introduced\nbefore and the MRP de\ufb01ned by wp here, we employ the\nfollowing performance metrics in the numerical experiments.\n1) Portfolio Return Measures: In the following, we \ufb01rst\ngive the return de\ufb01nition for one single asset, and after that,\nseveral different return measures for an MRP are talked about.\nFor one single asset, the return or cumulative return at time\nt for \u03c4 holding periods is de\ufb01ned as rt (\u03c4) = pt\u2212pt\u2212\u03c4\npt\u2212\u03c4\n, where\n\u03c4 in the parentheses denotes the period length and is usually\nomitted when the length is one. Here, the return rt (\u03c4) as a rate\nof return is used to measure the aggregate amount of pro\ufb01ts or\nlosses (in percentage) of an investment strategy on one asset\nover a time period \u03c4.\na) Pro\ufb01t and Loss (P&L): The pro\ufb01t and loss (P&L)\nmeasures the amount of pro\ufb01ts or losses (in units of dollars)\nof an investment on the portfolio for some holding periods.\nWithin one trading period, if a long position is opened on\nan MRP at time to and closed at time tc, then the multi-period\nP&L of this MRP at time t (to \u2264t \u2264tc) accumulated from\nto is computed as P&Lt (\u03c4) = wT\np rt (\u03c4) = wT\np rt (t \u2212to),\nwhere \u03c4 = t \u2212to denotes the length of the holding periods,\nand rt (\u03c4) = [r1,t (\u03c4) , r2,t (\u03c4) , . . . , rM,t (\u03c4)]T is the return\nvector. More generally, the cumulative P&L of this MRP at\ntime t for \u03c4 (0 \u2264\u03c4 \u2264t \u2212to) holding periods is de\ufb01ned as\nP&Lt (\u03c4) = wT\np rt (t \u2212to) \u2212wT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to) ,\n(55)\nwhere we de\ufb01ne rt (0) = 0. Then we have the single-period\nP&L (e.g., daily P&L, monthly P&L) denoted by P&Lt at\ntime t (i.e., \u03c4 = 1) is computed as\nP&Lt = wT\np rt (t \u2212to) \u2212wT\np rt\u22121 (t \u22121 \u2212to) .\n(56)\nLikewise, within one trading period, if a short position is\nopened on this MRP, then multi-period P&L is P&Lt (\u03c4) =\nwT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to) \u2212wT\np rt (t \u2212to) and the single-period\nP&L is P&Lt = wT\np rt\u22121 (t \u22121 \u2212to) \u2212wT\np rt (t \u2212to).",
    "chunk_index": 19,
    "start_char": 42823,
    "end_char": 45339,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "rt (t \u2212to) \u2212wT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to) ,\n(55)\nwhere we de\ufb01ne rt (0) = 0. Then we have the single-period\nP&L (e.g., daily P&L, monthly P&L) denoted by P&Lt at\ntime t (i.e., \u03c4 = 1) is computed as\nP&Lt = wT\np rt (t \u2212to) \u2212wT\np rt\u22121 (t \u22121 \u2212to) .\n(56)\nLikewise, within one trading period, if a short position is\nopened on this MRP, then multi-period P&L is P&Lt (\u03c4) =\nwT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to) \u2212wT\np rt (t \u2212to) and the single-period\nP&L is P&Lt = wT\np rt\u22121 (t \u22121 \u2212to) \u2212wT\np rt (t \u2212to). About\nthe portfolio P&L calculation within the trading periods, we\nhave the following lemma.\nLemma 9 (P&L Calculation for Mean-Reversion Trading).\nWithin one trading period, if the price change of every asset\nin an MRP is small enough, then the P&L in (55) can be\napproximately calculated by the change of the log-price spread\nzt. Speci\ufb01cally,\n1) for a long position opened on the MRP, P&Lt (\u03c4) \u2248\nzt \u2212zt\u2212\u03c4; and\n2) for a short position opened on the MRP, P&Lt (\u03c4) \u2248\nzt\u2212\u03c4 \u2212zt.\nProof: See Appendix B.\nThis lemma reveals the philosophy of the MRP design and\nalso the mean-reversion trading by showing the connection\nbetween the log-price spread value and the portfolio return.\nSince there is no trading conduct between two trading\nperiods, the P&L measures (both the multi-period P&L and\nsingle-period P&L) are simply de\ufb01ned to be 0.\nb) Cumulative P&L: In order to measure the cumulative\nreturn performance for an MRP, we de\ufb01ne the cumulative P&L\nin one trading from time t1 to t2 as\nCum. P&L(t1, t2) = Pt2\nt=t1 P&Lt.\n(57)\nc) Return on Investment (ROI): Since different MRPs\nmay have different leverage properties due to wp, we introduce\nanother portfolio return measure (rate of return) called return\non investment (ROI).\nWithin one trading period, the ROI at time t (to \u2264t \u2264tc)\nis de\ufb01ned to be the single-period P&L at time t normalized\nby the gross investment deployed which is \u2225wp\u22251 (that is the\ngross investment exposure to the market including the long\nposition investment and the short position investment) written\nas\nROIt = P&Lt/ \u2225wp\u22251 .\n(58)\nLike the P&L measures, between two trading periods, ROIt\nis de\ufb01ned to be 0.\n2) Sharpe Ratio (SR): The Sharpe ratio (SR) [48] is a\nmeasure for calculating risk-adjusted return. It describes how\nmuch excess return one can receive for the extra volatility\n(square root of variance).\nHere, the Sharpe ratio of ROI (or, equivalently, Sharpe ratio\nof P&L) for a trading stage from time t1 to t2 is de\ufb01ned as\nfollows:\n\nSubmitted paper\n11\nLog-prices\n2.0\n4.0\n6.0\ny1\ny2\ny3\ny4\ny5\ny6\ns1\n-0.4\n-0.2\n 0.0\n 0.2\n 0.4\ns2\n-0.2\n 0.0\n 0.2\ns3\n-0.5\n 0.0\n 0.5\n 1.0\ns4\n-1.0\n 0.0\n 1.0\nTime index\n0\n22\n44\n66\n88\n110\n132\n154\n176\n198\n220\n242\n264\ns5\n-0.5\n 0.0\n 0.5\nFig. 3.",
    "chunk_index": 20,
    "start_char": 44871,
    "end_char": 47528,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "Here, the Sharpe ratio of ROI (or, equivalently, Sharpe ratio\nof P&L) for a trading stage from time t1 to t2 is de\ufb01ned as\nfollows:\n\nSubmitted paper\n11\nLog-prices\n2.0\n4.0\n6.0\ny1\ny2\ny3\ny4\ny5\ny6\ns1\n-0.4\n-0.2\n 0.0\n 0.2\n 0.4\ns2\n-0.2\n 0.0\n 0.2\ns3\n-0.5\n 0.0\n 0.5\n 1.0\ns4\n-1.0\n 0.0\n 1.0\nTime index\n0\n22\n44\n66\n88\n110\n132\n154\n176\n198\n220\n242\n264\ns5\n-0.5\n 0.0\n 0.5\nFig. 3.\nLog-prices and \ufb01ve estimated spreads. (The sample length for in-\nsample training is chosen to be 5 \u00d7 12 \u00d7 22, and the sample length for\nout-of-sample trading is 12 \u00d7 22.)\nSRROI (t1, t2) = \u00b5ROI\n\u03c3ROI\n,\n(59)\nwhere\n\u00b5ROI\n=\n1\nt2\u2212t1\nPt2\nt=t1 ROIt\nand\n\u03c3ROI\n=\nh\n1\nt2\u2212t1\nPt2\nt=t1 (ROIt \u2212\u00b5ROI)2i1/2\n.\nIn\nthe\ncomputation\nof the Sharpe ratio, we set the risk-free return to 0, in which\ncase it reduces to the information ratio.\nC. Synthetic Data Experiments\nFor synthetic data experiments, we generate the sample\npath of log-prices for M \ufb01nancial assets using a multivariate\ncointegrated systems [33], [49], where there are r long-run\ncointegration relations and M \u2212r common trends. We divide\nthe sample path into two stages: in-sample training stage and\nout-of-sample backtesting or trading stage. All the parameters\nlike spread equilibrium \u00b5z, trading threshold \u2206, and portfolio\nweight w are decided in the training stage. The out-of-sample\nperformance of our design methods are tested in the trading\nstage.\nIn the synthetic experiments, we set M = 6 and r = 5 and\nonly show the performance of the MRP design methods under\nnet budget constraint W1. We estimate N = 5 spreads using\nthe generated sample path. Based on these 5 spreads, an MRP\nis designed as zt = wT st. The simulated log-prices and the\nspreads for the trading stage are shown in Figure 3.\nThe performance of the MRP designed using our proposed\nmethods are compared with those of one underlying spread and\nthe method in [25] based on pcroz (5, w) and prez (w), which\nare shown in Figure 4 and Figure 5. From our simulations, we\ncan conclude that our designed MRPs do generate consistent\npositive pro\ufb01ts. And simulation results also show that our\ndesigned portfolios can outperform the underlying spreads and\nthe MRPs designed using methods in [25] with higher Sharpe\nratios of ROIs and higher cumulative P&Ls.\nSpreads\n-0.5\n0\n0.5\n1\nMRP-pcro (prop.)\nSpread s3\nROI\n0\n0.1\n0.2\nMRP-pcro (prop.) - SR=0.61401\nROI\n0\n0.1\nSpread s3 - SR=0.54688\nTime index\n0\n22\n44\n66\n88\n110\n132\n154\n176\n198\n220\n242\n264\nCum. P&L\n0\n20\n40\nMRP-pcro (prop.)\nSpread s3\nFig. 4. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-pre\n(prop.) with one underlying spread denoted as Spread s3.",
    "chunk_index": 21,
    "start_char": 47167,
    "end_char": 49816,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "ROI\n0\n0.1\nSpread s3 - SR=0.54688\nTime index\n0\n22\n44\n66\n88\n110\n132\n154\n176\n198\n220\n242\n264\nCum. P&L\n0\n20\n40\nMRP-pcro (prop.)\nSpread s3\nFig. 4. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-pre\n(prop.) with one underlying spread denoted as Spread s3.\nSpreads\n-1\n0\n1\nMRP-pre (prop.)\nMRP-pre (exist.)\nROI\n0\n0.1\nMRP-pre (prop.) - SR=0.61263\nROI\n0\n0.1\nMRP-pre (exis.) - SR=0.58533\nTime index\n0\n22\n44\n66\n88\n110\n132\n154\n176\n198\n220\n242\n264\nCum. P&L\n0\n20\n40\n60\nMRP-pre (prop.)\nMRP-pre (exist.)\nFig. 5. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-pcro\n(prop.) and one existing benchmark method in [25] denoted as MRP-pcro\n(exist.).\nD. Market Data Experiments\nWe also test our methods using real market data from\nthe Standard & Poor\u2019s 500 (S&P 500) Index, which is\nusually considered as one of the best representatives for\nthe U.S. stock markets. The data are retrieved from Yahoo!\nFinance5 and adjusted daily closing stock prices are em-\nployed. We \ufb01rst choose stock candidates which are possibly\ncointegrated to form stock asset pools. One stock pool is\n{APA, AXP, CAT, COF, FCX, IBM, MMM}, where the stocks\nare denoted by their ticker symbols. Three spreads are con-\nstructed from this pool. Then MRP design methods are em-\nployed and unit-root tests are used to test their tradability. The\nlog-prices of the stocks and the log-prices for the three spreads\n5http://\ufb01nance.yahoo.com\n\nSubmitted paper\n12\nLog-prices\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nAPA\nAXP\nCAT\nCOF\nFCX\nIBM\nMMM\ns1\n5.2\n5.4\n5.6\ns2\n5.0\n5.2\n5.4\n2012\n2013\n2014\ns3\n5.5\n6.0\nFig. 6.\nLog-prices for {APA, AXP, CAT, COF, FCX, IBM, MMM} and\nthree spreads s1, s2, and s3.\nare shown in Figure 6.\nBased on the mean-reversion trading framework mentioned\nbefore, one trading experiment is carried out from February\n1st, 2012 to June 30th, 2014. In Figure 7, we compare the\nperformance of our designed MRP with the underlying spread\ns1. The log-prices for the designed spreads, and the out-of-\nsample performance like ROIs, Sharpe ratios of ROIs, and\ncumulative P&Ls are reported. It is shown that using our\nmethod, the designed MRP can achieve a higher Sharpe ratio\nand a better \ufb01nal cumulative return.\nSpreads\n-0.2\n0\n0.2\nMRP-cro (prop.)\nSpread s1\nROI\n-0.02\n0\n0.02\nMRP-cro (prop.) - SR=0.11375\nROI\n0\n0.02\nSpread s1 - SR=0.037232\n2012\n2013\n2014\nCum. P&L\n0\n0.5\n1\nMRP-cro (prop.)\nSpread s1\nFig. 7. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-cro\n(prop.) with one underlying spread denoted as Spread s1.\nWe also compare our proposed design method with the\nmethod in [25] using porz (5, w) in Figure 8.",
    "chunk_index": 22,
    "start_char": 49484,
    "end_char": 52267,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "SR=0.11375\nROI\n0\n0.02\nSpread s1 - SR=0.037232\n2012\n2013\n2014\nCum. P&L\n0\n0.5\n1\nMRP-cro (prop.)\nSpread s1\nFig. 7. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-cro\n(prop.) with one underlying spread denoted as Spread s1.\nWe also compare our proposed design method with the\nmethod in [25] using porz (5, w) in Figure 8. We can see that\nour method can outperform the benchmark method in terms\nof Sharpe ratio and the return performance.\nVII. CONCLUSIONS\nThe mean-reverting portfolio design problem arising from\nstatistical arbitrage has been considered in this paper. We\nSpreads\n-0.5\n0\n0.5\nMRP-por (prop.)\nMRP-por (exist.)\nROI\n-0.02\n0\n0.02\nMRP-por (prop.) - SR=0.073177\nROI\n-0.02\n0\n0.02\nMRP-por (exist.) - SR=0.049078\n2012\n2013\n2014\nCum. P&L\n0\n1\n2\nMRP-por (prop.)\nMRP-por (exist.)\nFig. 8. Comparisons of ROIs, Sharpe ratios of ROIs, and cumulative P&Ls\nbetween the MRP designed using our proposed method denoted as MRP-\npor (prop.) and one existing benchmark method in [25] denoted as MRP-por\n(exist.).\nhave formulated the MRP design problem in a general form\nby optimizing a mean-reversion criterion characterizing the\nmean-reversion strength of the portfolio and, at the same time,\ntaking into consideration the variance of the portfolio and\nan investment budget constraint. Several speci\ufb01c optimization\nproblems have been proposed based on the general design\nidea. Ef\ufb01cient algorithms have been derived to solve the design\nproblems. Numerical results show that our proposed methods\nare able to generate consistent positive pro\ufb01ts and signi\ufb01cantly\noutperform the the design methods in literature.\nAPPENDIX A\nPROOF FOR LEMMA 8\nFor problem (36) with w \u2208W0, the majorizing function in\nthe \ufb01rst majorization step (41) is denoted by u1\n\u0000w, w(k)\u0001\n, and\nthe majorizing function in the second step (51) is denoted by\nu2\n\u0000w, w(k)\u0001\n. From the majorization properties in (34), then\nwe can have have this relationship: fz (w) \u2264u1\n\u0000w, w(k)\u0001\n\u2264\nu2\n\u0000w, w(k)\u0001\n.\nThen we can get the overall majorization in w for the\nobjective function of the original problem at w(k) over the\nconstraint set by the following function:\nu2\n\u0000w, w(k)\u0001\n(in \u00afx)\n=\n2\n\u0002\u0000 \u00afN(k) \u2212\u03c8\n\u0000 \u00afN(k)\u0001\nI\n\u0001 \u00afx(k)\u0003T \u00afx\n+2\u03c8\n\u0000 \u00afN(k)\u0001\n\u03bd \u2212\u00afx(k)T \u00afN(k)\u00afx(k)\n(in x)\n=\n2\n\u0002\u0000N(k) \u2212\u03c8\n\u0000R\u22121N(k)R\u2212T \u0001\nN0\n\u0001\nx(k)\u0003T x\n+2\u03c8\n\u0000R\u22121N(k)R\u2212T \u0001\n\u03bd \u2212x(k)T N(k)x(k)\n(in w)\n=\n2\n\u0002\u0000H(k) \u2212\u03c8\n\u0000R\u22121FT H(k)FR\u2212T \u0001\nM0\n\u0001\nw(k)\u0003T w\n+2\u03c8\n\u0000R\u22121FT H(k)FR\u2212T \u0001\n\u03bd \u2212w(k)T H(k)w(k),\nwhere the last two terms in every step of the derivations\nare constants since they are independent of the optimization\nvariables.\n\nSubmitted paper\n13\nAPPENDIX B\nPROOF FOR LEMMA 9\nSince the spread is de\ufb01ned as zt = wT\np yt, then the multi-\nperiod P&L at time t for \u03c4 holding periods is given by\nP&Lt (\u03c4) =\nwT\np rt (t \u2212to) \u2212wT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to)\n=\nPM\nm=1 wp,mrm,t (t \u2212to)\n\u2212PM\nm=1 wp,mrm,t\u2212\u03c4",
    "chunk_index": 23,
    "start_char": 51867,
    "end_char": 54711,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "H(k)w(k),\nwhere the last two terms in every step of the derivations\nare constants since they are independent of the optimization\nvariables.\n\nSubmitted paper\n13\nAPPENDIX B\nPROOF FOR LEMMA 9\nSince the spread is de\ufb01ned as zt = wT\np yt, then the multi-\nperiod P&L at time t for \u03c4 holding periods is given by\nP&Lt (\u03c4) =\nwT\np rt (t \u2212to) \u2212wT\np rt\u2212\u03c4 (t \u2212\u03c4 \u2212to)\n=\nPM\nm=1 wp,mrm,t (t \u2212to)\n\u2212PM\nm=1 wp,mrm,t\u2212\u03c4 (t \u2212\u03c4 \u2212to)\n=\nPM\nm=1 wp,m\n\u0010\npm,t\npm,to \u22121\n\u0011\n\u2212PM\nm=1 wp,m\n\u0010\npm,t\u2212\u03c4\npm,to \u22121\n\u0011\n\u2248\nPM\nm=1 wp,m [log (pm,t) \u2212log (pm,to)]\n\u2212PM\nm=1 wp,m [log (pm,t\u2212\u03c4) \u2212log (pm,to)]\n=\nPM\nm=1 wp,m log (pm,t) \u2212wp,m log (pm,t\u2212\u03c4)\n=\nPM\nm=1 wp,mym,t \u2212PN\nm=1 wp,mym,t\u2212\u03c4\n=\nwT\np yt \u2212wT\np yt\u2212\u03c4\n=\nzt \u2212zt\u2212\u03c4.\nThe\napproximation\nin\nthe\nfourth\nstep\nfollows\nfrom\nlog (1 + x) \u2248x when x \u21920, where log (\u00b7) denotes the\nnatural logarithm. Similarly, for a short position on the MRP,\nthe calculation of P&Lt (\u03c4) is given by zt\u2212\u03c4 \u2212zt.\nREFERENCES\n[1] Z. Zhao and D. P. Palomar, \u201cMean-reverting portfolio design via\nmajorization-minimization method,\u201d arXiv preprint arXiv:1611.08393,\n2016.\n[2] G. Vidyamurthy, Pairs Trading: quantitative methods and analysis.\nJohn Wiley & Sons, 2004, vol. 217.\n[3] D. S. Ehrman, The handbook of pairs trading: Strategies using equities,\noptions, and futures.\nJohn Wiley & Sons, 2006, vol. 240.\n[4] R. Bookstaber, A Demon of Our Own Design: Markets, Hedge Funds,\nand the Perils of Financial Innovation.\nJohn Wiley & Sons, 2007.\n[5] W. Goetzmann, K. G. Rouwenhorst et al., \u201cPairs trading: Performance\nof a relative value arbitrage rule,\u201d Yale School of Management, Tech.\nRep., 1998.\n[6] E. Gatev, W. N. Goetzmann, and K. G. Rouwenhorst, \u201cPairs trading:\nPerformance of a relative-value arbitrage rule,\u201d Review of Financial\nStudies, vol. 19, no. 3, pp. 797\u2013827, 2006.\n[7] A. Pole, Statistical arbitrage: algorithmic trading insights and tech-\nniques.\nJohn Wiley & Sons, 2011, vol. 411.\n[8] S. F. LeRoy and J. Werner, Principles of \ufb01nancial economics.\nCam-\nbridge University Press, 2014.\n[9] B. I. Jacobs and K. N. Levy, Market Neutral Strategies.\nJohn Wiley\n& Sons, 2005, vol. 112.\n[10] J. G. Nicholas, Market Neutral Investing: Long/Short Hedge Fund\nStrategies.\nBloomberg Press, 2000.\n[11] C. W. Granger, \u201cCointegrated variables and error correction models,\u201d\nunpublished USCD Discussion Paper 83-13a, Tech. Rep., 1983.\n[12] R. F. Engle and C. W. Granger, \u201cCo-integration and error correction:\nrepresentation, estimation, and testing,\u201d Econometrica: Journal of the\nEconometric Society, pp. 251\u2013276, 1987.\n[13] S. Johansen, \u201cStatistical analysis of cointegration vectors,\u201d Journal of\neconomic dynamics and control, vol. 12, no. 2, pp. 231\u2013254, 1988.\n[14] \u2014\u2014, \u201cEstimation and hypothesis testing of cointegration vectors in\ngaussian vector autoregressive models,\u201d Econometrica: Journal of the\nEconometric Society, pp. 1551\u20131580, 1991.\n[15] R. Larsson and S. Johansen, \u201cLikelihood-based inference in cointegrated\nvector autoregressive models,\u201d 1997.\n[16] S. Johansen, \u201cModelling of cointegration in the vector autoregressive\nmodel,\u201d Economic modelling, vol. 17, no. 3, pp. 359\u2013373, 2000.",
    "chunk_index": 24,
    "start_char": 54314,
    "end_char": 57374,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "pp. 251\u2013276, 1987.\n[13] S. Johansen, \u201cStatistical analysis of cointegration vectors,\u201d Journal of\neconomic dynamics and control, vol. 12, no. 2, pp. 231\u2013254, 1988.\n[14] \u2014\u2014, \u201cEstimation and hypothesis testing of cointegration vectors in\ngaussian vector autoregressive models,\u201d Econometrica: Journal of the\nEconometric Society, pp. 1551\u20131580, 1991.\n[15] R. Larsson and S. Johansen, \u201cLikelihood-based inference in cointegrated\nvector autoregressive models,\u201d 1997.\n[16] S. Johansen, \u201cModelling of cointegration in the vector autoregressive\nmodel,\u201d Economic modelling, vol. 17, no. 3, pp. 359\u2013373, 2000.\n[17] \u2014\u2014, Likelihood-Based Inference in Cointegrated Vector Autoregressive\nModels, ser. OUP Catalogue.\nOxford university press, May 1995, no.\n9780198774501.\n[18] M. Avellaneda and J.-H. Lee, \u201cStatistical arbitrage in the US equities\nmarket,\u201d Quantitative Finance, vol. 10, no. 7, pp. 761\u2013782, 2010.\n[19] C. L. Dunis, G. Giorgioni, J. Laws, and J. Rudy, \u201cStatistical arbitrage\nand high-frequency data with an application to Eurostoxx 50 equities,\u201d\nLiverpool Business School, Working paper, 2010.\n[20] J. F. Caldeira and G. V. Moura, \u201cSelection of a portfolio of pairs based\non cointegration: The brazilian case,\u201d Federal University of Rio Grande\ndo Sul, Federal University of Santa Catarina, Brazil, 2012.\n[21] S. Drakos, \u201cStatistical arbitrage in S&P500,\u201d Journal of Mathematical\nFinance, vol. 6, no. 01, p. 166, 2016.\n[22] J. L. Farrell and W. J. Reinhart, Portfolio management: theory and\napplication.\nMcGraw-Hill, 1997.\n[23] H. M. Markowitz, \u201cPortfolio selection,\u201d The Journal of Finance, vol. 7,\nno. 1, pp. 77\u201391, 1952.\n[24] A. d\u2019Aspremont, \u201cIdentifying small mean-reverting portfolios,\u201d Quanti-\ntative Finance, vol. 11, no. 3, pp. 351\u2013364, 2011.\n[25] M. Cuturi and A. d\u2019Aspremont, \u201cMean reversion with a variance\nthreshold,\u201d in Proceedings of the 30th International Conference on\nMachine Learning (ICML-13), 2013, pp. 271\u2013279.\n[26] H. M. Markowitz, \u201cThe optimization of a quadratic function subject to\nlinear constraints,\u201d Naval research logistics Quarterly, vol. 3, no. 1-2,\npp. 111\u2013133, 1956.\n[27] W. F. Sharpe, \u201cCapital asset prices: A theory of market equilibrium\nunder conditions of risk,\u201d The journal of \ufb01nance, vol. 19, no. 3, pp.\n425\u2013442, 1964.\n[28] H. M. Markowitz, Portfolio selection: ef\ufb01cient diversi\ufb01cation of invest-\nments.\nYale university press, 1968, vol. 16.\n[29] E. Qian, \u201cRisk parity and diversi\ufb01cation,\u201d Journal of Investing, vol. 20,\nno. 1, p. 119, 2011.\n[30] D. B. Chaves, J. C. Hsu, F. Li, and O. Shakernia, \u201cRisk parity portfolio\nvs. other asset allocation heuristic portfolios,\u201d Journal of Investing,\nvol. 20, no. 1, pp. 108\u2013118, 2011.\n[31] Y. Feng and D. P. Palomar, \u201cSCRIP: Successive convex optimization\nmethods for risk parity portfolio design,\u201d IEEE Transactions on Signal\nProcessing, vol. 63, no. 19, pp. 5285\u20135300, 2015.\n[32] G. E. Box and G. C. Tiao, \u201cA canonical analysis of multiple time series,\u201d\nBiometrika, vol. 64, no. 2, pp. 355\u2013365, 1977.\n[33] H. L\u00fctkepohl, New introduction to multiple time series analysis.\nSpringer, 2007.\n[34] G. E. Box and D. A. Pierce, \u201cDistribution of residual autocorrelations in\nautoregressive-integrated moving average time series models,\u201d Journal\nof the American statistical Association, vol. 65, no. 332, pp. 1509\u20131526,\n1970.\n[35] N. D.",
    "chunk_index": 25,
    "start_char": 56777,
    "end_char": 60083,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "19, pp. 5285\u20135300, 2015.\n[32] G. E. Box and G. C. Tiao, \u201cA canonical analysis of multiple time series,\u201d\nBiometrika, vol. 64, no. 2, pp. 355\u2013365, 1977.\n[33] H. L\u00fctkepohl, New introduction to multiple time series analysis.\nSpringer, 2007.\n[34] G. E. Box and D. A. Pierce, \u201cDistribution of residual autocorrelations in\nautoregressive-integrated moving average time series models,\u201d Journal\nof the American statistical Association, vol. 65, no. 332, pp. 1509\u20131526,\n1970.\n[35] N. D. Ylvisaker, \u201cThe expected number of zeros of a stationary gaussian\nprocess,\u201d The Annals of Mathematical Statistics, vol. 36, no. 3, pp. 1043\u2013\n1046, 1965.\n[36] B. Kedem and S. Yakowitz, Time series analysis by higher order\ncrossings.\nIEEE press Piscataway, NJ, 1994.\n[37] S. Boyd and L. Vandenberghe, Convex optimization.\nCambridge\nuniversity press, 2004.\n[38] Y. Huang and D. P. Palomar, \u201cRank-constrained separable semide\ufb01nite\nprogramming with applications to optimal beamforming,\u201d IEEE Trans-\nactions on Signal Processing, vol. 58, no. 2, pp. 664\u2013678, 2010.\n[39] \u2014\u2014, \u201cRandomized algorithms for optimal solutions of double-sided\nqcqp with applications in signal processing,\u201d IEEE Transactions on\nSignal Processing, vol. 62, no. 5, pp. 1093\u20131108, 2014.\n[40] R. A. Horn and C. R. Johnson, Matrix analysis.\nCambridge university\npress, 2012.\n[41] J. Song, P. Babu, and D. P. Palomar, \u201cOptimization methods for design-\ning sequences with low autocorrelation sidelobes,\u201d IEEE Transactions\non Signal Processing, vol. 63, no. 15, pp. 3998\u20134009, 2015.\n[42] J. J. Mor\u00e9, \u201cGeneralizations of the trust region problem,\u201d Optimization\nmethods and Software, vol. 2, no. 3-4, pp. 189\u2013209, 1993.\n[43] D. R. Hunter and K. Lange, \u201cA tutorial on MM algorithms,\u201d The\nAmerican Statistician, vol. 58, no. 1, pp. 30\u201337, 2004.\n[44] M. Razaviyayn, M. Hong, and Z.-Q. Luo, \u201cA uni\ufb01ed convergence\nanalysis of block successive minimization methods for nonsmooth\noptimization,\u201d SIAM Journal on Optimization, vol. 23, no. 2, pp. 1126\u2013\n1153, 2013.\n[45] Y. Sun, P. Babu, and D. P. Palomar, \u201cMajorization-minimization algo-\nrithms in signal processing, communications, and machine learning,\u201d\nIEEE Transactions on Signal Processing, vol. PP, no. 99, p. 1, 2016.\n[46] D. A. Dickey and W. A. Fuller, \u201cDistribution of the estimators for\nautoregressive time series with a unit root,\u201d Journal of the American\nstatistical association, vol. 74, no. 366a, pp. 427\u2013431, 1979.\n[47] P. C. Phillips and P. Perron, \u201cTesting for a unit root in time series\nregression,\u201d Biometrika, vol. 75, no. 2, pp. 335\u2013346, 1988.\n[48] W. F. Sharpe, \u201cThe sharpe ratio,\u201d The journal of portfolio management,\nvol. 21, no. 1, pp. 49\u201358, 1994.\n\nSubmitted paper\n14\n[49] R. S. Tsay, Analysis of \ufb01nancial time series. John Wiley & Sons, 2005,\nvol. 543.",
    "chunk_index": 26,
    "start_char": 59607,
    "end_char": 62363,
    "paper_title": "Mean-Reverting Portfolio Design with Budget Constr",
    "paper_category": "q-fin.PM",
    "paper_filename": "Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Mean-Reverting_Portfolio_Design_with_Budget_Constr.pdf"
  },
  {
    "text": "arXiv:0904.1903v1 [q-fin.PM] 13 Apr 2009\nMINIMIZING THE EXPECTED MARKET TIME TO REACH A CERTAIN\nWEALTH LEVEL\nCONSTANTINOS KARDARAS AND ECKHARD PLATEN\nAbstract. In a \ufb01nancial market model, we consider variations of the problem of minimizing the\nexpected time to upcross a certain wealth level. For exponential L\u00b4evy markets, we show the asymp-\ntotic optimality of the growth-optimal portfolio for the above problem and obtain tight bounds for\nthe value function for any wealth level. In an It\u02c6o market, we employ the concept of market time,\nwhich is a clock that runs according to the underlying market growth. We show the optimality of\nthe growth-optimal portfolio for minimizing the expected market time to reach any wealth level.\nThis reveals a general de\ufb01nition of market time which can be useful from an investor\u2019s point of view.\nWe utilize this last de\ufb01nition to extend the previous results in a general semimartingale setting.\n1. Introduction\nThe problem of quickly reaching certain goals in wealth management is one of the most fun-\ndamental tasks in the theory and practice of \ufb01nance. However, making this idea mathematically\nprecise has been a challenge. In particular, this would require a quanti\ufb01cation of what is meant by\nachieving goals \u201cquickly\u201d in a model-independent manner, or, even better, coming endogenously\nfrom the description of the market as is perceived by its participants. Such a mathematically precise\ndescription of the \ufb02ow of time, as well as the corresponding optimal investment strategy, is clearly\nvaluable. If a robust, model-independent answer to the previous questions can be given, it would\ngo a long way towards a better understanding of the problem, as its statement should provide a\ndeep inside into key quantitative characteristics of the market. Our aim in this paper is to present\na way of addressing the aforementioned issues.\nWe proceed with a more thorough description of the problem. Imagine an investor holding some\nminute capital-in-hand, aiming to reach as quickly as possible a substantial wealth level by opti-\nmally choosing an investment opportunity in an active market. No matter what the mathematical\nformalization of the objective is, as long as it reasonably describes the above informal setting, in-\ntuition suggests that the investor should pick an aggressive strategy that provides ample wealth\ngrowth.\nThe most famous wealth-optimizing strategy that could potentially achieve this is the\ngrowth-optimal strategy, which is sometimes also called Kelly strategy, as the latter was introduced\nin [16]. Therefore, the portfolio generated by the growth-optimal strategy is a strong candidate for\nsolving the aforementioned problem, at least in an approximate sense. This last point is augmented\nby the long line of research on the importance and optimality properties of the growth optimal\nportfolio; we mention for example the very incomplete list: [17], [1], [3], [18], [7], [12]. Note also\nthat minimizing expected time to reach a wealth level is not the only interesting objective that one\nDate: October 30, 2018.\n2000 Mathematics Subject Classi\ufb01cation. 60H99, 60G44, 91B28, 91B70.\nKey words and phrases. Num\u00b4eraire portfolio, growth-optimal portfolio, market time, upcrossing, overshoot, expo-\nnential L\u00b4evy markets, It\u02c6o markets, semimartingale markets.\n1",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3317,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "in an approximate sense. This last point is augmented\nby the long line of research on the importance and optimality properties of the growth optimal\nportfolio; we mention for example the very incomplete list: [17], [1], [3], [18], [7], [12]. Note also\nthat minimizing expected time to reach a wealth level is not the only interesting objective that one\nDate: October 30, 2018.\n2000 Mathematics Subject Classi\ufb01cation. 60H99, 60G44, 91B28, 91B70.\nKey words and phrases. Num\u00b4eraire portfolio, growth-optimal portfolio, market time, upcrossing, overshoot, expo-\nnential L\u00b4evy markets, It\u02c6o markets, semimartingale markets.\n1\n\ncan seek. For example, maximizing the probability that a wealth level will be reached before some\nfuture time is also interesting; in this respect, see [6], [9].\nHere, we shall identify a variant of the \u201cquickest goal reach\u201d problem for continuous-time models\nwhere the growth-optimal portfolio is indeed the best. The problem we consider then is that of\nminimizing the expected market time that it will take to reach a certain wealth level. Market time\nwill be de\ufb01ned as a natural time scale which runs fast when the compensation for taking risk in the\nmarket is high and vice-versa. In a market with continuous asset prices, this will be achieved by\nsetting the slope of the market time equal to half the squared risk premium. In this case, it equals\nthe growth rate of the corresponding growth-optimal portfolio, which leads to the interpretation of\nmarket time as integrated maximum growth rate.\nThe \ufb01rst attempt to minimize the expected upcrossing time in a discrete-time gambling-system\nmodel was described in [5], where indeed the near optimal wealth process was found to be character-\nized by Kelly\u2019s growth-optimal strategy. Models of gambling systems, as considered in [5], could be\ninterpreted as discrete-time \ufb01nancial markets where the log-asset-price processes are random walks\nwith a \ufb01nite number of possible values for the increment of each step. The natural continuous-time\ngeneralization of the above setting is to consider exponential L\u00b4evy markets, i.e., markets where\nthe log-asset-price processes have independent and stationary increments. For these markets, we\nestablish here the exact analogues of the results in [5].\nA continuous-time problem in the context of a Black-Scholes market was treated in [11], and\nthen as an application of a more abstract problem in [10], using essentially methods of dynamic\nprogramming. In this case, the num\u00b4eraire portfolio of the market, which was introduced in [17]\nand is also called the growth-optimal portfolio as it is generated by the analogue of Kelly\u2019s growth-\noptimal strategy, is truly optimal for minimizing the expected calendar time to reach any wealth\nlevel. Unfortunately, the moment that one considers more complex It\u02c6o-process models, for example\nones that are modelling feedback e\ufb00ects, as the leverage e\ufb00ect in [4], the growth-optimal portfolio\nis no longer optimal for the problem of minimizing expected calendar time for upcrossing a certain\nwealth level. In fact, for general non-Markovian models there does not seem to be any hope in\nidentifying what the optimal strategy and wealth process are when minimizing expected calendar\ntime.",
    "chunk_index": 1,
    "start_char": 2697,
    "end_char": 5936,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "for minimizing the expected calendar time to reach any wealth\nlevel. Unfortunately, the moment that one considers more complex It\u02c6o-process models, for example\nones that are modelling feedback e\ufb00ects, as the leverage e\ufb00ect in [4], the growth-optimal portfolio\nis no longer optimal for the problem of minimizing expected calendar time for upcrossing a certain\nwealth level. In fact, for general non-Markovian models there does not seem to be any hope in\nidentifying what the optimal strategy and wealth process are when minimizing expected calendar\ntime. We note however that for Markovian models one can still characterize the optimal strategy\nand portfolio in terms of a Hamilton-Jacobi-Bellman equation, which will most likely then have to\nbe solved numerically.\nWe introduce in this paper a market clock which does not count time according to the natural\ncalendar \ufb02ow, but rather according to the overall market growth. Under the objective that one\nminimizes expected market time, we show here that the solution again yields the growth-optimal\nportfolio as nearly optimal. There is a slight problem that results in the non-optimality of the\ngrowth-optimal portfolio, if for \ufb01nite wealth levels some overshoot is possible over the targeted\nwealth level at the time of the upcrossing. If there is no overshoot, which happens in particular in\nmodels with continuous asset prices, then the growth-optimal portfolio is indeed optimal. In [2],\nthe author considers a rami\ufb01cation of the problem by o\ufb00ering a rebate for the overshoot that results\nin the growth-optimal portfolio being again optimal. Of course, we could do this even in the most\ngeneral case. Since this rebate inclusion is somewhat arbitrary, we shall refrain from using it in our\nown analysis.\n2\n\nThe optimality of the growth-optimal portfolio for minimizing expected time according to a clock\ncounting time according to the overall market growth sounds a bit like a tautological statement.\nHowever, we shall make a conscious e\ufb00ort to convey that the concept of market time is very natural,\nby taking a stepwise approach in the model generality that we consider. The exponential L\u00b4evy\nprocess case is considered \ufb01rst. There, the market-time \ufb02ow coincides with the calendar-time \ufb02ow\nup to a multiplicative constant, since the model coe\ufb03cients remain constant through time. As soon\nas the model coe\ufb03cients are allowed to randomly change, one can regard the passage of time in\nterms of the opportunities for pro\ufb01t that are available. We \ufb01rst discuss this in the realm of markets\nwhere asset-prices are modeled via It\u02c6o processes, where the arguments are more intuitive. As soon\nas the natural candidate for the market time is understood, we proceed to discuss the results in the\nvery general semimartingale model.\nThe results presented in this work are generalizations of the constant-coe\ufb03cient result in [11]. The\nuse of martingale methods and a natural de\ufb01nition of market time that we utilize make the proof\nof our claims more transparent and widens the scope and validity of the corresponding statements.\nThe structure of the paper is as follows.",
    "chunk_index": 2,
    "start_char": 5383,
    "end_char": 8493,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "as the natural candidate for the market time is understood, we proceed to discuss the results in the\nvery general semimartingale model.\nThe results presented in this work are generalizations of the constant-coe\ufb03cient result in [11]. The\nuse of martingale methods and a natural de\ufb01nition of market time that we utilize make the proof\nof our claims more transparent and widens the scope and validity of the corresponding statements.\nThe structure of the paper is as follows. In Section 2 we introduce the general \ufb01nancial market\nmodel, we de\ufb01ne the problem of minimizing expected market time and present the standing as-\nsumptions, which are basically the existence of the num\u00b4eraire portfolio. In Section 3 we specialize\nin the case of exponential L\u00b4evy market models, where market time and calendar time coincide up to\na multiplicative constant. Our \ufb01rst main result gives tight bounds for the near-optimal performance\nof the growth-optimal portfolio for any wealth level, that also result in its asymptotic optimality\nfor increasing wealth levels. In Section 4 we use It\u02c6o processes to model the market. After some\ndiscussion on the concept of market time, our second main result shows also here the optimality of\nthe growth-optimal portfolio. In Section 5, the concept of market time in a general semimartingale\nsetting is introduced and a general result that covers all previous cases is presented. Finally, Section\n6 contains the proofs of the results in the previous sections.\n2. Description of the Problem\nIn the following general remarks we \ufb01x some notation that will be used throughout.\nBy R+ we shall denote the positive real line, Rd the d-dimensional Euclidean space, and N the set\nof natural numbers {1, 2, . . .}. Superscripts will be used to indicate coordinates, both for vectors\nand for processes; for example z \u2208Rd is written z = (z1, . . . , zd). On Rd, \u27e8\u00b7, \u00b7\u27e9will denote the\nusual inner product: \u27e8y, z\u27e9:= Pd\ni=1 yizi for y and z in Rd. Also | \u00b7 | will denote the usual norm:\n|z| :=\np\n\u27e8z, z\u27e9for z \u2208Rd.\nOn R+ equipped with the Borel \u03c3-\ufb01eld B(R+), Leb will denote the Lebesgue measure.\nAll stochastic processes appearing in the sequel are de\ufb01ned on a \ufb01ltered probability space\n(\u2126, F, F, P). Here, P is a probability on (\u2126, F), where F is a \u03c3-algebra that will make all in-\nvolved random variables measurable. The \ufb01ltration F = (Ft)t\u2208R+ is assumed to satisfy the usual\nhypotheses of right-continuity and saturation by P-null sets. It will be assumed throughout that\nF0 is trivial modulo P.\nFor a c`adl`ag (right continuous with left limits) stochastic process X = (Xt)t\u2208R+, de\ufb01ne Xt\u2212:=\nlims\u2191t Xs for t > 0 and X0\u2212:= 0. The process X\u2212will denote this last left-continuous version of\nX and \u2206X := X \u2212X\u2212will be the jump process of X.\n3\n\n2.1. Assets and wealth processes. The d-dimensional semimartingale S = (S1, . . .",
    "chunk_index": 3,
    "start_char": 8021,
    "end_char": 10850,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "will be assumed throughout that\nF0 is trivial modulo P.\nFor a c`adl`ag (right continuous with left limits) stochastic process X = (Xt)t\u2208R+, de\ufb01ne Xt\u2212:=\nlims\u2191t Xs for t > 0 and X0\u2212:= 0. The process X\u2212will denote this last left-continuous version of\nX and \u2206X := X \u2212X\u2212will be the jump process of X.\n3\n\n2.1. Assets and wealth processes. The d-dimensional semimartingale S = (S1, . . . , Sd) will be\ndenoting the discounted, with respect to the savings account, price process of d \ufb01nancial assets.\nStarting with initial capital x \u2208R+, and investing according to some predictable and S-integrable\nstrategy \u03d1, an investor\u2019s discounted total wealth process is given by\n(2.1)\nXx,\u03d1 := x +\nZ \u00b7\n0\n\u27e8\u03d1t, dSt\u27e9.\nRe\ufb02ecting the investor\u2019s ability only to hold a portfolio of nonnegative total tradeable wealth,\nwe then de\ufb01ne the set of all nonnegative wealth processes starting from initial capital x \u2208R+:\nX(x) :=\nn\nXx,\u03d1 as in (2.1)\n\f\f\f \u03d1 is predictable and S-integrable, and Xx,\u03d1 \u22650\no\n.\nIt is straightforward that X(x) = xX(1) and that x \u2208X(x) for all x \u2208R+. We also set X :=\nS\nx\u2208R+ X(x).\n2.2. The problem. We shall be concerned with the problem of quickly reaching a wealth level\n\u2113starting from capital x. This, of course, is nontrivial only when x < \u2113, which will be tacitly\nassumed throughout. The challenge is now to rigorously de\ufb01ne what is meant by \u201cquickly\u201d. Take\nO = (Ot)t\u2208R+ to be an increasing and adapted process such that, P-a.s., O0 = 0 and O\u221e= +\u221e. O\nwill be representing some kind of internal clock of the market, which we shall call market time. In\nthe following sections we shall be more precise on choosing O, guided by what we shall learn when\nidentifying the consequences of applying the growth-optimal strategy.\nFor any c`adl`ag process X and \u2113\u2208R+, de\ufb01ne the \ufb01rst upcrossing market time of X at level \u2113:\n(2.2)\nT (X; \u2113) := inf {Ot \u2208R+ | Xt \u2265\u2113} .\nOf course, if \u2113\u2264x then T (X; \u2113) = 0 for all X \u2208X(x). With the aforementioned inputs, de\ufb01ne for\nall x < \u2113the value function\n(2.3)\nv(x; \u2113) :=\ninf\nX\u2208X(x) E [T (X; \u2113)] .\nOur aims in this work are to:\n\u2022 identify a natural de\ufb01nition for the market time O;\n\u2022 obtain an explicit formula, or at least some useful tight bounds, for the value function v(x; \u2113)\nof (2.3); and\n\u2022 \ufb01nd the optimal, or perhaps near optimal, portfolio for the above problem.\n2.3. Standing assumptions. In order to make headway with the problem described in \u00a72.2, we\nshall make two natural and indispensable assumptions regarding the \ufb01nancial market that will be\nin force throughout.\nAssumptions 2.1. In our \ufb01nancial market model, we assume the following:\n(1) There exists b\nX \u2208X(1) such that X/ b\nX is a supermartingale for all X \u2208X.\n(2) For every \u2113\u2208R+, there exists X \u2208X(1), possibly depending on \u2113, such that, P-a.s.,\nT (X;",
    "chunk_index": 4,
    "start_char": 10470,
    "end_char": 13208,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "portfolio for the above problem.\n2.3. Standing assumptions. In order to make headway with the problem described in \u00a72.2, we\nshall make two natural and indispensable assumptions regarding the \ufb01nancial market that will be\nin force throughout.\nAssumptions 2.1. In our \ufb01nancial market model, we assume the following:\n(1) There exists b\nX \u2208X(1) such that X/ b\nX is a supermartingale for all X \u2208X.\n(2) For every \u2113\u2208R+, there exists X \u2208X(1), possibly depending on \u2113, such that, P-a.s.,\nT (X; \u2113) < +\u221e.\nA process b\nX with the properties described in Assumption 2.1(1) is unique and is called the\nnum\u00b4eraire portfolio. Existence of the num\u00b4eraire portfolio is a minimal assumption for the viability\nof the \ufb01nancial market. It is essentially equivalent to the boundedness in probability of the set\n4\n\n{XT | X \u2208X(1)} of all possible discounted wealths starting from unit capital and observed at any\ntime T \u2208R+. We refer the interested reader to [7], [12] and [15] for more information in this\ndirection. We shall frequently refer to the num\u00b4eraire portfolio as the growth-optimal portfolio, as\nthe two notions coincide.\nAssumption 2.1(2) constitutes what has been coined a \u201cfavorable game\u201d in [5] and it is necessary\nin order for the problem described in (2.3) to have \ufb01nite value and therefore to be well-posed. Under\nAssumption 2.1(2), and in view of the property X(x) = xX(1) for x \u2208R+, it is obvious that for all\nx \u2208R+ and \u2113\u2208R+, there exists X \u2208X(x) such that P [T (X; \u2113) < +\u221e] = 1.\nActually, if Assumption 2.1(1) is in force, Assumption 2.1(2) has a convenient equivalent.\nProposition 2.2. Under Assumption 2.1(1), Assumption 2.1(2) is equivalent to:\n(2\u2032) limt\u2192+\u221eb\nXt = +\u221e, P-a.s.\nThis last result enables one to check easily the validity of Assumptions 2.1 by looking only at\nthe num\u00b4eraire portfolio. In each of the speci\ufb01c cases we shall consider in the sequel, equivalent\ncharacterizations of Assumptions 2.1 will be given in terms of the model under consideration.\n3. Exponential L\u00b4evy Markets\n3.1. The set-up. For this section we assume that the discounted asset-price processes satisfy\ndSi\nt = Si\nt\u2212dRi\nt for t \u2208R+, where, for all i = 1, . . . , d, Ri is a L\u00b4evy process on (\u2126, F, F, P). Each Ri\nfor i = 1, . . . , d is the total returns process associated to Si.\nIn order to make sure that the asset-price processes remain nonnegative, it is necessary and\nsu\ufb03cient that \u2206Ri \u2265\u22121 for all i = 1, . . . , d. We shall actually impose a further restriction on the\nstructure of the jumps of the returns processes, also bounding them from above. This is mostly\ndone in order to obtain later in Theorem 3.3 a statement which parallels the result in [5]. For the\nasymptotic result that will be presented in \u00a76.5 this bounded-jump assumption will be dropped.\nAssumption 3.1. For all i = 1, . . .",
    "chunk_index": 5,
    "start_char": 12725,
    "end_char": 15509,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "\u2265\u22121 for all i = 1, . . . , d. We shall actually impose a further restriction on the\nstructure of the jumps of the returns processes, also bounding them from above. This is mostly\ndone in order to obtain later in Theorem 3.3 a statement which parallels the result in [5]. For the\nasymptotic result that will be presented in \u00a76.5 this bounded-jump assumption will be dropped.\nAssumption 3.1. For all i = 1, . . . , d we have \u22121 \u2264\u2206Ri \u2264\u03ba, for some \u03ba \u2208R+.\nDenote by R the d-dimensional L\u00b4evy process (R1, . . . , Rd). In view of the boundedness of the\njumps of R, as stated in Assumption 3.1 above, we can write\n(3.1)\nRT = aT + \u03c3WT +\nZ\n[0,T]\u00d7Rd z (\u00b5( dz, dt) \u2212\u03bd( dz) dt)\nfor all T \u2208R+. In view of Assumption 3.1, the elements in the above representation satisfy:\n\u2022 a \u2208Rd.\n\u2022 \u03c3 is a (d \u00d7 m)-matrix, where m \u2208N.\n\u2022 W is a standard m-dimensional Brownian motion on (\u2126, F, F, P).\n\u2022 \u00b5 is the jump measure of R, i.e., the random counting measure on R+ \u00d7 Rd de\ufb01ned via\n\u00b5([0, T] \u00d7 E) := P\n0\u2264t\u2264T IE\\{0}(\u2206Rt) for T \u2208R+ and E \u2286Rd.\n\u2022 \u03bd, the compensator of \u00b5, is a L\u00b4evy measure on (Rd, B(Rd)), where B(Rd) is the Borel\n\u03c3-\ufb01eld on Rd. More precisely, \u03bd is a measure with \u03bd[{0}] = 0, \u03bd\n\u0002\nRd \\ [\u22121, \u03ba]\n\u0003\n= 0 and\nR\nRd |x|2\u03bd[ dx] < +\u221e.\nFor more information on L\u00b4evy processes one can check for example [19].\nDe\ufb01ne the (d\u00d7d) matrix c := \u03c3\u03c3\u22a4, where \u201c\u22a4\u201d denotes matrix transposition. The triplet (a, c, \u03bd)\nwill play a crucial role in the discussion below.\n5\n\nIn the notation of (2.1), let Xx,\u03d1 \u2208X(x). The nonnegativity requirement Xx,\u03d1 \u22650 is equivalent\nto \u2206Xx,\u03d1 \u2265Xx,\u03d1\n\u2212, or further to \u27e8\u03d1, \u2206S\u27e9\u2265Xx,\u03d1\n\u2212. Since \u2206Si = Si\n\u2212\u2206Ri for each i = 1, . . . , d, and\nrecalling that \u03bd is the L\u00b4evy measure of R, we conclude that Xx,\u03d1 \u22650 if and only if\n\u0000\u03d1i\nt(\u03c9)Si\nt\u2212(\u03c9)\n\u0001\ni=1,...,d \u2208Xx,\u03d1\nt\u2212(\u03c9)C, for all (\u03c9, t) \u2208\u2126\u00d7 R+,\nwhere C is the set of natural constraints de\ufb01ned via\nC :=\nn\n\u03b7 \u2208Rd \f\f\f \u03bd\n\u0002\nz \u2208Rd | \u27e8\u03b7, z\u27e9< \u22121\n\u0003\n= 0\no\n.\nIt is easy to see that C is convex; it is also closed, as follows from Fatou\u2019s lemma.\n3.2. Growth rate. For any \u03c0 \u2208C, de\ufb01ne\n(3.2)\ng(\u03c0) := \u27e8\u03c0, a\u27e9\u22121\n2 \u27e8\u03c0, c\u03c0\u27e9\u2212\nZ\nRd [\u27e8\u03c0, z\u27e9\u2212log(1 + \u27e8\u03c0, z\u27e9)] \u03bd[ dz].\nFor \u03c0 \u2208C, g(\u03c0) is the drift rate of the logarithm of the wealth process X \u2208X(1) that satis\ufb01es\ndXt = Xt\u2212\u27e8\u03c0, dRt\u27e9= Xt\u2212d \u27e8\u03c0, Rt\u27e9for all t \u2208R+; for this reason, g(\u03c0) is also called the growth\nrate of the last wealth process.\nDe\ufb01ne g\u2217:= sup\u03c0\u2208C g(\u03c0) to be the maximum growth rate.",
    "chunk_index": 6,
    "start_char": 15099,
    "end_char": 17446,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "For any \u03c0 \u2208C, de\ufb01ne\n(3.2)\ng(\u03c0) := \u27e8\u03c0, a\u27e9\u22121\n2 \u27e8\u03c0, c\u03c0\u27e9\u2212\nZ\nRd [\u27e8\u03c0, z\u27e9\u2212log(1 + \u27e8\u03c0, z\u27e9)] \u03bd[ dz].\nFor \u03c0 \u2208C, g(\u03c0) is the drift rate of the logarithm of the wealth process X \u2208X(1) that satis\ufb01es\ndXt = Xt\u2212\u27e8\u03c0, dRt\u27e9= Xt\u2212d \u27e8\u03c0, Rt\u27e9for all t \u2208R+; for this reason, g(\u03c0) is also called the growth\nrate of the last wealth process.\nDe\ufb01ne g\u2217:= sup\u03c0\u2208C g(\u03c0) to be the maximum growth rate.\nSince 0 \u2208C, we certainly have\ng\u2217\u2265g(0) = 0. Actually, under the bounded-jump Assumption 3.1, the standing Assumptions 2.1\nare equivalent to 0 < g\u2217< \u221e. In order to achieve this last claim, we shall connect the viability of\nthe market with the concept of immediate arbitrage opportunities, as will be now introduced.\n3.3. Market viability. De\ufb01ne the set I of immediate arbitrage opportunities to consist of all\nvectors \u03be \u2208Rd such that c\u03be = 0, \u03bd\n\u0002\nz \u2208Rd | \u27e8\u03be, z\u27e9< 0\n\u0003\n= 0 and \u27e8\u03be, a\u27e9\u22650, and where further at\nleast one of \u03bd\n\u0002\nz \u2208Rd | \u27e8\u03be, z\u27e9> 0\n\u0003\n> 0 or \u27e8\u03be, a\u27e9> 0 holds. As part of the next result, we get that\nthe previously-described exponential L\u00b4evy market is viable if and only if the intersection of I with\nthe recession cone of C, de\ufb01ned as \u02c7C := T\nu>0 uC, is empty.\nProposition 3.2. Assumptions 2.1 are equivalent to requiring both I \u2229\u02c7C = \u2205and g\u2217> 0.\nSuppose now that the above is true, as well as that Assumption 3.1 is in force. Then, g\u2217< \u221e\nand there exists \u03c1 \u2208C such that g(\u03c1) = g\u2217. Furthermore, the num\u00b4eraire portfolio b\nX satis\ufb01es the\ndynamics d b\nXt = b\nXt\u2212\u27e8\u03c1, dRt\u27e9= b\nXt\u2212d \u27e8\u03c1, Rt\u27e9. In other words, for T \u2208R+,\n(3.3)\nlog\n\u0000 b\nXT\n\u0001\n= \u27e8\u03c1, RT \u27e9\u22121\n2 \u27e8\u03c1, c\u03c1\u27e9T \u2212\nX\n0\u2264t\u2264T\n(\u27e8\u03c1, \u2206Rt\u27e9\u2212log (1 + \u27e8\u03c1, \u2206Rt\u27e9)) .\nInstead of using the general Assumptions 2.1 in this section, we shall use the equivalent conditions\nI \u2229\u02c7C = \u2205and g\u2217> 0. We also note that the vector \u03c1 \u2208C in the statement of Proposition 3.2\nthat leads to the num\u00b4eraire portfolio is essentially unique, modulo any degeneracies that might be\npresent in the market and lead to non-zero portfolios having zero returns.\n3.4. The main result. Since L\u00b4evy processes have stationary and independent increments, the\nnatural candidate for market time is to consider calendar time up to a multiplicative constant\n\u03b3 > 0, i.e., to set Ot = \u03b3t for t \u2208R+. In Theorem 3.3 below, we shall actually choose \u03b3 = g\u2217. This\nturns out to be the appropriate choice of market velocity that re\ufb02ects a universal characteristic\nof the market and will result in the bounds (3.4) for the optimal upcrossing time in Theorem 3.3\nbelow not to depend on the actual model under consideration.\n6",
    "chunk_index": 7,
    "start_char": 17080,
    "end_char": 19560,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "\u03b3t for t \u2208R+. In Theorem 3.3 below, we shall actually choose \u03b3 = g\u2217. This\nturns out to be the appropriate choice of market velocity that re\ufb02ects a universal characteristic\nof the market and will result in the bounds (3.4) for the optimal upcrossing time in Theorem 3.3\nbelow not to depend on the actual model under consideration.\n6\n\nTheorem 3.3. We work under Assumption 3.1, and also assume that I\u2229\u02c7C = \u2205and g\u2217> 0. De\ufb01ne\nthe \ufb01nite nonnegative constant \u03b1 := inf\n\b\n\u03b2 \u2208R+ | \u03bd\n\u0002\nz \u2208Rd | \u27e8\u03c1, z\u27e9> \u03b2\n\u0003\n= 0\n \n. Let the market time\nO be de\ufb01ned via Ot = g\u2217t for all t \u2208R+. With b\nX(x) := x b\nX, we have the inequalities:\n(3.4)\nlog\n\u0012 \u2113\nx\n\u0013\n\u2264v(x; \u2113) \u2264E\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n\u2264log\n\u0012 \u2113\nx\n\u0013\n+ log(1 + \u03b1).\nActually, Theorem 3.3 is an instance of a more general statement that will be presented in Section\n5. We note that the bounds (3.4) are in complete accordance with the discrete-time result in [5]\nand that the nonnegative constant log(1 + \u03b1) does not involve x or \u2113.\nRemark 3.4. Under a mild condition, namely that the marginal one-dimensional distributions of\nlog( b\nX) are non-lattice, the overshoot of log( b\nX) over the level log(\u2113) actually has a limiting distri-\nbution as \u2113\u2192\u221ethat is supported on [0, log(1 + \u03b1)]. In that case,\nlim\n\u2113\u2192\u221e\n\u0012\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n\u2212log\n\u0012 \u2113\nx\n\u0013\u0013\nexists and is exactly equal to the mean of that limiting distribution.\n3.5. True optimality. There is a special case when the growth-optimal portfolio is indeed optimal\nfor all levels \u2113, which covers in particular the Black-Scholes market result in [11]. The following\nresult directly stems out of the statement of Theorem 3.3.\nCorollary 3.5. Suppose that the num\u00b4eraire portfolio b\nX of (3.3) has no positive jumps: \u27e8\u03c1, \u2206R\u27e9\u2264\n0. Then,\nv(x; \u2113) = log\n\u0012 \u2113\nx\n\u0013\n= E\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n.\nFor an easy example where the last equality occurs, consider in (3.1) the case where d = 1, \u03ba = 0\nand a = a1 > 0. This is a reasonable model where the excess rate of return is strictly positive and\nonly negative jumps are present in the dynamics of the discounted asset-price process.\n3.6. Asymptotic optimality without the bounded-jump assumption. Theorem 3.3 gives\nthe asymptotic (for large \u2113) optimality of the growth-optimal portfolio, since, by (3.4),\n(3.5)\nlim\n\u2113\u2192\u221e\nv(x; \u2113)\nlog(\u2113)\n= 1 =\nlim\n\u2113\u2192\u221e\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\nlog(\u2113)\n.\nThe validity of the asymptotic optimality in (3.5) goes well-beyond the bounded-jump Assump-\ntion 3.1, as we shall describe now. For the total returns process R = (R1, . . . , Rd), we can write the\ncanonical representation (3.1) if and only if the L\u00b4evy measure \u03bd is such that\nR\nRd\n\u0000|x| \u2227|x|2\u0001\n\u03bd[ dx] <\n+\u221e.",
    "chunk_index": 8,
    "start_char": 19229,
    "end_char": 21813,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "\u2113) optimality of the growth-optimal portfolio, since, by (3.4),\n(3.5)\nlim\n\u2113\u2192\u221e\nv(x; \u2113)\nlog(\u2113)\n= 1 =\nlim\n\u2113\u2192\u221e\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\nlog(\u2113)\n.\nThe validity of the asymptotic optimality in (3.5) goes well-beyond the bounded-jump Assump-\ntion 3.1, as we shall describe now. For the total returns process R = (R1, . . . , Rd), we can write the\ncanonical representation (3.1) if and only if the L\u00b4evy measure \u03bd is such that\nR\nRd\n\u0000|x| \u2227|x|2\u0001\n\u03bd[ dx] <\n+\u221e. In that case, the de\ufb01nition in (3.2) of the growth rate is still the same, even without the\nvalidity of Assumption 3.1. We then have the following result.\nProposition 3.6. Suppose that the canonical representation (3.1) is valid. Then, if I \u2229\u02c7C = \u2205and\ng\u2217> 0 hold, we have g\u2217< \u221eand that there exists \u03c1 \u2208C such that g(\u03c1) = g\u2217. One can then de\ufb01ne\nthe growth-optimal portfolio b\nX using (3.3). De\ufb01ning O via Ot = g\u2217t, and with b\nX(x) := x b\nX, the\nasymptotics (3.5) hold.\n7\n\n4. It\u02c6o Markets and Market Time\nAs already mentioned in the Introduction, the growth-optimal portfolio is not optimal for the\nproblem of minimizing the expected calendar time to reach a wealth level when considering models\nwhere the coe\ufb03cients may change randomly through time. If the objective is somewhat altered\ninto minimizing expected market time, as we shall de\ufb01ne below, then the growth-optimal portfolio\nis indeed optimal. It is our belief that the notion of market time, as it naturally emerges in our\npaper, has a very clear and natural interpretation and makes deep sense, and is therefore worth\nstudying beyond the context of the questions raised.\nTo keep the technical details simple, in this section we assume that S is an It\u02c6o process. Later,\nin Section 5, we shall see how to relax this assumption to more complex models and still keep the\nmain result holding.\n4.1. The set-up. The dynamics of the discounted asset-prices are:\n(4.1)\ndSi\nt = Si\nt\n\uf8eb\n\uf8edai\nt dt +\nm\nX\nj=1\n\u03c3ij\nt dW j\nt\n\uf8f6\n\uf8f8,\nfor each i = 1, . . . , d and t \u2208R+. Here a = (ai)i=1,...,d is the predictable d-dimensional process of\nexcess appreciation rates, \u03c3 = (\u03c3ij)i=1,...,d, j=1,...,m is a predictable (d \u00d7 m)-matrix-valued process of\nvolatilities and W = (W j)j=1,...,m is a standard m-dimensional Brownian motion on (\u2126, F, F, P).\nWe let c := \u03c3\u03c3\u22a4denote the (d \u00d7 d)-matrix-valued process of local covariances.\n4.2. Assumptions. The general Assumptions 2.1 have a well-described equivalent for the It\u02c6o mar-\nket we are considering.\nProposition 4.1. Assumptions 2.1 are equivalent to the following:\n(1) There exists a d-dimensional predictable process \u03c1 such that, (P \u2297Leb)-a.e., c\u03c1 = a. (In\nthat case, \u03c1 = c\u2020a where c\u2020 is the Moore-Penrose pseudo-inverse of c.)\n(2)\nR T\n0 |\u03bbt|2 dt < \u221efor all T \u2208R+, where \u03bb := \u03c3\u22a4c\u2020a is the m-dimensional risk premium\nprocess. (Then, |\u03bb|2 =",
    "chunk_index": 9,
    "start_char": 21370,
    "end_char": 24133,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "d)-matrix-valued process of local covariances.\n4.2. Assumptions. The general Assumptions 2.1 have a well-described equivalent for the It\u02c6o mar-\nket we are considering.\nProposition 4.1. Assumptions 2.1 are equivalent to the following:\n(1) There exists a d-dimensional predictable process \u03c1 such that, (P \u2297Leb)-a.e., c\u03c1 = a. (In\nthat case, \u03c1 = c\u2020a where c\u2020 is the Moore-Penrose pseudo-inverse of c.)\n(2)\nR T\n0 |\u03bbt|2 dt < \u221efor all T \u2208R+, where \u03bb := \u03c3\u22a4c\u2020a is the m-dimensional risk premium\nprocess. (Then, |\u03bb|2 =\n\na, c\u2020a\n\u000b\n= \u27e8\u03c1, c\u03c1\u27e9.)\n(3)\nR \u221e\n0 |\u03bbt|2 dt = \u221e, P-a.s.\nIn this case, it follows that the logarithm of the num\u00b4eraire portfolio b\nX is given by\n(4.2)\nlog( b\nX) = 1\n2\nZ \u00b7\n0\n|\u03bbt|2 dt +\nZ \u00b7\n0\n\u03bbt dWt.\nIt follows from (4.2) that g\u2217\nt := (1/2)|\u03bbt|2 equals the maximum growth rate at time t \u2208R+ in\nthe given It\u02c6o market.\nAs we did in the case of exponential L\u00b4evy markets, we shall use statements (1), (2) and (3) of\nProposition 4.1 in place of the general Assumptions 2.1 in what follows.\n4.3. Market time. With the above notation de\ufb01ne now, similar to the previous section, the market\ntime process O = (Ot)t\u2208R+ by setting it equal to the integral over the maximum growth rate, i.e.,\nOt :=\nZ t\n0\ng\u2217\ns ds = 1\n2\nZ t\n0\n|\u03bbs|2 ds\n8\n\nfor t \u2208R+. Observe that, under the validity of statements (1), (2) and (3) of Proposition 4.1, we\nhave P[O\u221e= \u221e] = 1 as follows from Proposition 4.1(3). As explained in \u00a72.2, for given x < \u2113, our\naim is to \ufb01nd the wealth process X \u2208X(x) that minimizes E [T (X; \u2113)].\nWe brie\ufb02y explain why the problem of minimizing expected market time to reach a wealth level\nusing such a random clock and not calendar time, is natural and worth studying. Consider for\nsimplicity the one-asset case d = 1. Then, at any time t \u2208R+, |\u03bbt|2 = |at/\u03c3t|2 is the \u201csquared\nsignal to noise ratio\u201d of the asset-price process or more precisely the squared risk premium. When\nthis quantity is small, the opportunities for making pro\ufb01ts over those obtainable from the savings\naccount are rather small; on the other hand, when |\u03bbt|2 is large, at time t \u2208R+ an investor has a lot\nof opportunities to use the favorable fact that the premium for taking risk is high. Stalling to reach\nthe wealth level \u2113when opportunities are favorable should be punished more severely, especially for\nfund managers, and this is exactly what the market time O does. From an economic point of view,\nmarket time simply conforms with the underlying growth of the market.\n4.4. The main result. We are ready to present the solution to the optimization problem of \u00a72.2,\nboth giving an expression for the value function v and showing again that the growth-optimal\nportfolio is optimal.\nTheorem 4.2.",
    "chunk_index": 10,
    "start_char": 23625,
    "end_char": 26291,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "should be punished more severely, especially for\nfund managers, and this is exactly what the market time O does. From an economic point of view,\nmarket time simply conforms with the underlying growth of the market.\n4.4. The main result. We are ready to present the solution to the optimization problem of \u00a72.2,\nboth giving an expression for the value function v and showing again that the growth-optimal\nportfolio is optimal.\nTheorem 4.2. Under the validity of statements (1), (2) and (3) of Proposition 4.1 for an It\u02c6o\nmarket, and with b\nX(x) := x b\nX \u2208X(x), for x < \u2113we have:\nv(x; \u2113) = log\n\u0012 \u2113\nx\n\u0013\n= E\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n.\nOnce again, this last result is a special case of Theorem 5.3 that will be presented in the next\nsection.\n5. Market Time in General Semimartingale Markets\nThe purpose of this section is to give a wide-encompassing de\ufb01nition of market time for semi-\nmartingale \ufb01nancial markets and to present a general result on the expected market time to reach\na given wealth level, of which both Theorem 3.3 and Theorem 4.2 are special cases. We are now in\nthe very general market model described in Section 2.\n5.1. Market time. Guided by the discussions and results in both the exponential L\u00b4evy market\ncase of Section 3 and the It\u02c6o market case of Section 4, it makes sense to de\ufb01ne market time as the\nunderlying optimal growth of the market, i.e., the drift part of the logarithm of the growth-optimal\nportfolio. We shall have to make minimal assumptions for market time to be well-de\ufb01ned; namely,\nthat the drift part of the logarithm of the growth-optimal portfolio does exist.\nThe following result, which is a re\ufb01ned version of Proposition 2.2, ensures that the discussions\nthat follow make sense.\nProposition 5.1. Under the validity of Assumption 2.1(1), further assume that the logarithm of\nthe num\u00b4eraire portfolio b\nX is a special semimartingale and write log( b\nX) = O + M for its canonical\ndecomposition, where O is a predictable nondecreasing process and M is a local martingale. Then,\nAssumption 2.1(2) is equivalent to:\n(2\u2032\u2032) limt\u2192+\u221eOt = +\u221e, P-a.s.\n9\n\nThe following slightly strengthened version of Assumptions 2.1 will enable us to state our general\nresult in Theorem 5.3.\nAssumptions 5.2. With Assumptions 2.1 in force, we further postulate that the logarithm of the\nnum\u00b4eraire portfolio b\nX is a special semimartingale.\nUnder Assumptions 5.2, we can write log( b\nX) = O + M, where O is a predictable nondecreasing\nprocess and M is a local martingale. We then de\ufb01ne market time to be the nondecreasing predictable\nprocess O. According to Proposition 5.1, we have, P-a.s., O0 = 0 and O\u221e= \u221e. This makes O a\nbona \ufb01de clock.\n5.2. A general result. In what follows, \u03b1 will denote a nonnegative, possibly in\ufb01nite-valued\nrandom variable such that\n(5.1)\n\u2206b\nX\nb\nX\u2212\n\u2264\u03b1.",
    "chunk_index": 11,
    "start_char": 25853,
    "end_char": 28639,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "O + M, where O is a predictable nondecreasing\nprocess and M is a local martingale. We then de\ufb01ne market time to be the nondecreasing predictable\nprocess O. According to Proposition 5.1, we have, P-a.s., O0 = 0 and O\u221e= \u221e. This makes O a\nbona \ufb01de clock.\n5.2. A general result. In what follows, \u03b1 will denote a nonnegative, possibly in\ufb01nite-valued\nrandom variable such that\n(5.1)\n\u2206b\nX\nb\nX\u2212\n\u2264\u03b1.\nOf course, \u03b1 can be chosen in a minimal way as \u03b1 := supt\u2208R+(\u2206b\nXt/ b\nXt\u2212).\nTheorem 5.3. Let Assumption 5.2 be in force. With the above de\ufb01nition of the market time O\nand a random variable \u03b1 satisfying (5.1), we have\n(5.2)\nlog\n\u0012 \u2113\nx\n\u0013\n\u2264v(x; \u2113) \u2264E\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n\u2264log\n\u0012 \u2113\nx\n\u0013\n+ E [log(1 + \u03b1)]\nIt is straightforward that Theorem 5.3 covers both Theorem 3.3 and Theorem 4.2 as special cases.\nFor Theorem 3.3, \u03b1 is the constant de\ufb01ned in its statement, while for Theorem 4.2 we have \u03b1 = 0.\nDividing the inequalities (5.2) with log(\u2113) throughout, we get the following corollary of Theorem\n5.3.\nCorollary 5.4. In the setting of Theorem 5.3, suppose that E[log(1 + \u03b1)] < \u221e. Then,\nlim\n\u2113\u2192\u221e\nv(x; \u2113)\nlog(\u2113)\n= 1 =\nlim\n\u2113\u2192\u221e\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\nlog(\u2113)\n.\nThis last result shows that, under some integrability condition on the possible size of the jumps of\nthe logarithm of the growth-optimal portfolio, the problem of possible overshoots vanishes asymp-\ntotically when considering increasing wealth levels \u2113.\n6. Proofs\nBefore we embark on proving all the results of the previous sections, we de\ufb01ne, in accordance to\n(2.2), for any c`adl`ag process X and \u2113\u2208R+,\n\u03c4(X; \u2113) := inf {t \u2208R+ | Xt \u2265\u2113} .\nto be the \ufb01rst upcrossing calendar time of X at level \u2113. It is clear that \u03c4(X; \u2113) is a stopping time\nand that O\u03c4(X;\u2113) = T (X; \u2113) for all c`adl`ag processes X and \u2113\u2208R+.\n10\n\n6.1. Proof of Proposition 2.2. Recall that the clock O satis\ufb01es, P[O\u221e= \u221e] = 1. Therefore, for\nany X \u2208X and \u2113\u2208R+, P[\u03c4(X; \u2113) < \u221e] = 1 is equivalent to P[T (X; \u2113) < \u221e] = 1.\nCondition (2\u2032) of Proposition 2.2 obviously implies Assumption 2.1(2). Conversely, assume that\nAssumptions 2.1 are in force. For any n \u2208N, pick X \u2208X(1) such that, P[\u03c4 n < \u221e] = 1, where\n\u03c4 n := \u03c4(X; n). Since X/ b\nX is a nonnegative supermartingale, the optional sampling theorem (see\nfor example \u00a71.3.C of [13]) gives:\n1 \u2265E\n\u0014X\u03c4 n\nb\nX\u03c4 n\n\u0015\n\u2265nE\n\u0014 1\nb\nX\u03c4 n\n\u0015\n.\nIt follows that (1/ b\nX\u03c4 n)n\u2208N converges to zero in probability. As 1/ b\nX is a nonnegative supermartin-\ngale, this implies that limt\u2192\u221e(1/ b\nXt) = 0, P-a.s., which establishes the result.\n6.2. Proof of Proposition 5.1.",
    "chunk_index": 12,
    "start_char": 28249,
    "end_char": 30738,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "a nonnegative supermartingale, the optional sampling theorem (see\nfor example \u00a71.3.C of [13]) gives:\n1 \u2265E\n\u0014X\u03c4 n\nb\nX\u03c4 n\n\u0015\n\u2265nE\n\u0014 1\nb\nX\u03c4 n\n\u0015\n.\nIt follows that (1/ b\nX\u03c4 n)n\u2208N converges to zero in probability. As 1/ b\nX is a nonnegative supermartin-\ngale, this implies that limt\u2192\u221e(1/ b\nXt) = 0, P-a.s., which establishes the result.\n6.2. Proof of Proposition 5.1. Under the assumption that the num\u00b4eraire portfolio b\nX is a special\nsemimartingale with canonical decomposition b\nX = O + M, the event equality\nn\nlim\nt\u2192\u221e\nb\nXt = +\u221e\no\n=\nn\nlim\nt\u2192\u221eOt = +\u221e\no\n,\nwhich is to be understood in a modulo P sense, is a consequence of Proposition 3.21 in [12]. Then,\nthe result of Proposition 5.1 readily follows in view of Proposition 2.2.\n6.3. Proof of Proposition 3.2. The fact that I \u2229\u02c7C = \u2205is equivalent to the existence of \u03c1 \u2208C\nsuch that g(\u03c1) = g\u2217< \u221e, as well as that b\nX as de\ufb01ned in (3.3) is the num\u00b4eraire portfolio is a\nconsequence of Lemma 4.1 in [14], as soon as one also uses the bounded-jump Assumption 3.1.\nNow, it is straightforward to check that g\u2217= 0 is equivalent to b\nX being a positive local martingale,\nin which case we have that, P-a.s., limt\u2192\u221eb\nXt < \u221e. On the other hand, if g\u2217> 0 then the L\u00b4evy\nprocess log( b\nX) is integrable and has strictly positive drift g\u2217; therefore, P-a.s., limt\u2192\u221eb\nXt = \u221e. In\nview of Proposition 2.2, the result follows.\n6.4. Proof of Proposition 4.1. The fact that (1) and (2) of Proposition 4.1 are equivalent to the\nexistence of the num\u00b4eraire portfolio b\nX, as well as that b\nX given by (4.2), is a special case of Theorem\n3.15 in [12] \u2014 see also [8]. Under the validity of (1) and (2) of Proposition 4.1, it is straightforward\nto see that (3) of Proposition 4.1 is equivalent to limt\u2192\u221eb\nXt = \u221e. Using Proposition 2.2, the result\nfollows.\n6.5. Proof of Theorem 5.3. Let bL(x) := log( b\nX(x)). Observe that, since \u2206b\nX \u2264\u03b1 b\nX\u2212,\n(6.1)\n\u2206bL(x) = log\n \n1 + \u2206b\nX\nb\nX\u2212\n!\n\u2264log(1 + \u03b1).\nWrite bL(x) = log(x) + O + M, where M is a local martingale. Let (\u03c4 n)n\u2208N be a localizing sequence\nfor M. The estimate (6.1) gives, for all n \u2208N,\nlog(x) + E\nh\nO\u03c4 n\u2227\u03c4( b\nX(x);\u2113)\ni\n= E\nh\nbL\u03c4 n\u2227\u03c4( b\nX(x);\u2113)(x)\ni\n\u2264log(\u2113) + E[log(1 + \u03b1)].\nLetting now n tend to in\ufb01nity and using the monotone convergence theorem, we get\n(6.2)\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n\u2264log(\u2113/x) + E[log(1 + \u03b1)].\n11\n\nTake now any X \u2208X(x). If P[T (X, \u2113) = \u221e] > 0, we have E [T (X, \u2113)] = \u221eand log(\u2113/x) \u2264\nE [T (X, \u2113)] is trivial.\nIt remains to consider the case P[T (X, \u2113) < \u221e] = 1, or equivalently\nP[\u03c4 (X, \u2113) < \u221e] = 1.",
    "chunk_index": 13,
    "start_char": 30380,
    "end_char": 32863,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "bL\u03c4 n\u2227\u03c4( b\nX(x);\u2113)(x)\ni\n\u2264log(\u2113) + E[log(1 + \u03b1)].\nLetting now n tend to in\ufb01nity and using the monotone convergence theorem, we get\n(6.2)\nE\n\u0002\nT ( b\nX(x); \u2113)\n\u0003\n\u2264log(\u2113/x) + E[log(1 + \u03b1)].\n11\n\nTake now any X \u2208X(x). If P[T (X, \u2113) = \u221e] > 0, we have E [T (X, \u2113)] = \u221eand log(\u2113/x) \u2264\nE [T (X, \u2113)] is trivial.\nIt remains to consider the case P[T (X, \u2113) < \u221e] = 1, or equivalently\nP[\u03c4 (X, \u2113) < \u221e] = 1.\nFor all \u01eb \u2208(0, 1), de\ufb01ne X\u01eb := (1\u2212\u01eb)X+\u01ebx. Then, X\u01eb \u2208X(x) and \u03c4 (X\u01eb, \u01ebx + (1 \u2212\u01eb)\u2113) = \u03c4 (X, \u2113).\nThe drift part of the process L\u01eb := log (X\u01eb) is bounded above by O. Therefore,\nL\u01eb \u2264log(x) + O + M\u01eb\nfor some local martingale M\u01eb. Let (\u03c4 \u01eb,n)n\u2208N be a localizing sequence for M\u01eb. Since the stopped\nprocess M\u01eb\n\u03c4(X,\u2113)\u2227\u03c4 \u01eb,n\u2227\u00b7 is a martingale, we have that\nE\nh\nL\u01eb\n\u03c4(X,\u2113)\u2227\u03c4 \u01eb,n\ni\n\u2264log(x) + E\n\u0002\nO\u03c4(X,\u2113)\u2227\u03c4 \u01eb,n\n\u0003\n= log(x) + E [T (X, \u2113) \u2227O\u03c4 \u01eb,n] .\nNow, L\u01eb is uniformly bounded from below by log(\u01ebx). Furthermore, \u2191limn\u2192\u221eO\u03c4 n = \u221eholds in\na P-a.s. sense. Therefore, applications of Fatou\u2019s Lemma and the monotone convergence theorem\nwill give\nlog(\u2113) + log(1 \u2212\u01eb) \u2264E\nh\nL\u01eb\n\u03c4(X,\u2113)\ni\n\u2264\nlim inf\nn\u2192\u221eE\nh\nL\u01eb\n\u03c4(X,\u2113)\u2227\u03c4 n\ni\n\u2264\nlog(x) + lim inf\nn\u2192\u221eE [T (X, \u2113) \u2227O\u03c4 n]\n=\nlog(x) + E [T (X, \u2113)] .\nSending now \u01eb to zero, we also get log(\u2113/x) \u2264E [T (X, \u2113)] for all X \u2208X(x) that satisfy P[T (X, \u2113) <\n\u221e] = 1. This, coupled with (6.2), \ufb01nishes the proof.\n6.6. Proof of Proposition 3.6. The existence of \u03c1 \u2208C such that g(\u03c1) = g\u2217< \u221efollows from\nLemma 4.1 in [14] in view of I \u2229\u02c7C \u0338= \u2205. Note that the \ufb01niteness of g\u2217is straightforward from the\nde\ufb01ning equation (3.2) for g.\nCall bL := log( b\nX). For each n \u2208N, let\nbLn := bL \u2212\nX\nt\u2264\u00b7\n(\u2206bLt)I{\u2206bLt>n}.\nThen, bLn is a L\u00b4evy process and we can write\nbLn\nt = gnt + Mn\nt\nfor all t \u2208R+, where Mn is a L\u00b4evy martingale and \u2191limn\u2192\u221egn = g\u2217> 0. Then,\nE[T ( b\nX(x); \u2113)] = g\u2217E[\u03c4( b\nX(x); \u2113)] \u2264g\u2217E\nh\n\u03c4\n\u0010\nbLn(x); log(\u2113)\n\u0011i\n\u2264g\u2217\ngn\n\u0012\nlog\n\u0012 \u2113\nx\n\u0013\n+ log(1 + n)\n\u0013\n,\nholds for all n \u2208N such that gn > 0, where the last inequality follows along the same lines of the\nproof of (6.2). It then follows that\nlim sup\n\u2113\u2192\u221e\nE[T ( b\nX(x); \u2113)]\nlog(\u2113)\n\u2264g\u2217\ngn\nholds for all n \u2208N such that gn > 0. Since \u2191limn\u2192\u221egn = g\u2217> 0, sending n to in\ufb01nity in the last\ninequality we get\nlim sup\n\u2113\u2192\u221e\nE[T ( b\nX(x); \u2113)]\nlog(\u2113)\n\u22641.\n12",
    "chunk_index": 14,
    "start_char": 32476,
    "end_char": 34651,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "the last inequality follows along the same lines of the\nproof of (6.2). It then follows that\nlim sup\n\u2113\u2192\u221e\nE[T ( b\nX(x); \u2113)]\nlog(\u2113)\n\u2264g\u2217\ngn\nholds for all n \u2208N such that gn > 0. Since \u2191limn\u2192\u221egn = g\u2217> 0, sending n to in\ufb01nity in the last\ninequality we get\nlim sup\n\u2113\u2192\u221e\nE[T ( b\nX(x); \u2113)]\nlog(\u2113)\n\u22641.\n12\n\nOf course, in view of the bounds (5.2) of Theorem 5.3, we always have\n1 = lim\n\u2113\u2192\u221e\nv(x; \u2113)\nlog(\u2113) \u2264lim inf\n\u2113\u2192\u221e\nE[T ( b\nX(x); \u2113)]\nlog(\u2113)\n,\nwhich completes the proof.\nReferences\n[1] P. H. Algoet and T. M. Cover, Asymptotic optimality and asymptotic equipartition properties of log-optimum\ninvestment, Ann. Probab., 16 (1988), pp. 876\u2013898.\n[2] D. C. Aucamp, An investment strategy with overshoot rebates which minimizes the time to attain a speci\ufb01ed\ngoal, Management Sci., 23 (1976/77), pp. 1234\u20131241.\n[3] D. Becherer, The numeraire portfolio for unbounded semimartingales, Finance Stoch., 5 (2001), pp. 327\u2013341.\n[4] F. Black, Studies of stock price volatility changes, in Proceedings of the 1976 Meetings of the Business and\nEconomics Statistics Section, American Statistical Association, Berkeley, Calif., 1976, pp. 177\u2013181.\n[5] L. Breiman, Optimal gambling systems for favorable games, in Proc. 4th Berkeley Sympos. Math. Statist. and\nProb., Vol. I, Univ. California Press, Berkeley, Calif., 1961, pp. 65\u201378.\n[6] S. Browne, Reaching goals by a deadline: digital options and continuous-time active portfolio management,\nAdv. in Appl. Probab., 31 (1999), pp. 551\u2013577.\n[7] M. M. Christensen and K. Larsen, No arbitrage and the growth optimal portfolio, Stoch. Anal. Appl., 25\n(2007), pp. 255\u2013280.\n[8] F. Delbaen and W. Schachermayer, The existence of absolutely continuous local martingale measures, Ann.\nAppl. Probab., 5 (1995), pp. 926\u2013945.\n[9] H. F\u00a8ollmer and P. Leukert, Quantile hedging, Finance Stoch., 3 (1999), pp. 251\u2013273.\n[10] D. Heath, S. Orey, V. Pestien, and W. Sudderth, Minimizing or maximizing the expected time to reach\nzero, SIAM J. Control Optim., 25 (1987), pp. 195\u2013205.\n[11] D. Heath and W. Sudderth, Continuous-time portfolio management: Minimizing the expected time to reach\na goal. Unpublished manuscript, 1984.\n[12] I. Karatzas and C. Kardaras, The num\u00b4eraire portfolio in semimartingale \ufb01nancial models, Finance Stoch.,\n11 (2007), pp. 447\u2013493.\n[13] I. Karatzas and S. E. Shreve, Brownian motion and stochastic calculus, vol. 113 of Graduate Texts in\nMathematics, Springer-Verlag, New York, second ed., 1991.\n[14] C. Kardaras, No-Free-Lunch equivalences for exponential L\u00b4evy models. To appear in Mathematical Finance,\n2007.\n[15] C. Kardaras and E. Platen, On the semimartingale property of discounted asset-price processes in \ufb01nancial\nmodeling. submitted for publication, 2008.\n[16] J. L. Kelly, Jr., A new interpretation of information rate, Bell. System Tech. J., 35 (1956), pp. 917\u2013926.\n[17] J. B. J. Long, The num\u00b4eraire portfolio, Journal of Financial Economics, 26 (1990), pp. 29\u201369.\n[18] E. Platen, A benchmark approach to \ufb01nance, Math. Finance, 16 (2006), pp. 131\u2013151.\n[19] K.-I. Sato, L\u00b4evy processes and in\ufb01nitely divisible distributions, vol. 68 of Cambridge Studies in Advanced\nMathematics, Cambridge University Press, Cambridge, 1999.",
    "chunk_index": 15,
    "start_char": 34358,
    "end_char": 37522,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "in \ufb01nancial\nmodeling. submitted for publication, 2008.\n[16] J. L. Kelly, Jr., A new interpretation of information rate, Bell. System Tech. J., 35 (1956), pp. 917\u2013926.\n[17] J. B. J. Long, The num\u00b4eraire portfolio, Journal of Financial Economics, 26 (1990), pp. 29\u201369.\n[18] E. Platen, A benchmark approach to \ufb01nance, Math. Finance, 16 (2006), pp. 131\u2013151.\n[19] K.-I. Sato, L\u00b4evy processes and in\ufb01nitely divisible distributions, vol. 68 of Cambridge Studies in Advanced\nMathematics, Cambridge University Press, Cambridge, 1999.\nConstantinos Kardaras, Mathematics and Statistics Department, Boston University, 111 Cumming-\nton Street, Boston, MA 02215, USA.\nE-mail address: kardaras@bu.edu\nEckhard Platen, School of Finance and Economics & Department of Mathematical Sciences, Uni-\nversity of Technology, Sydney, P.O. Box 123, Broadway, NSW 2007, Australia.\nE-mail address: eckhard.platen@uts.edu.au\n13",
    "chunk_index": 16,
    "start_char": 36998,
    "end_char": 37896,
    "paper_title": "Minimizing the expected market time to reach a cer",
    "paper_category": "q-fin.PM",
    "paper_filename": "Minimizing_the_expected_market_time_to_reach_a_cer.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Minimizing_the_expected_market_time_to_reach_a_cer.pdf"
  },
  {
    "text": "NON-LINEAR SHRINKAGE OF THE PRICE RETURN COVARIANCE\nMATRIX IS FAR FROM OPTIMAL FOR PORTFOLIO OPTIMISATION\nChristian Bongiorno & Damien Challet\nUniversit\u00e9 Paris-Saclay, CentraleSup\u00e9lec\nLaboratoire de Math\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes\n91192 Gif-sur-Yvette, France\nOctober 14, 2022\nABSTRACT\nPortfolio optimization requires sophisticated covariance estimators that are able to \ufb01lter out estimation\nnoise. Non-linear shrinkage is a popular estimator based on how the Oracle eigenvalues can be\ncomputed using only data from the calibration window. Contrary to common belief, NLS is not\noptimal for portfolio optimization because it does not minimize the right cost function when the asset\ndependence structure is non-stationary. We instead derive the optimal target. Using historical data,\nwe quantify by how much NLS can be improved. Our \ufb01ndings reopen the question of how to build\nthe covariance matrix estimator for portfolio optimization in realistic conditions.\n1\nIntroduction\nCovariance \ufb01ltering is essential in multivariate Finance [1] and in all scienti\ufb01c \ufb01elds (see [2] for a recent review). A very\npopular family of estimators \ufb01lters the covariance matrix by only changing its eigenvalues while keeping its eigenvectors\nuntouched. They are known as Rotationally Invariant Estimators (RIEs). The best known RIE is linear shrinkage [3].\nMore recently, several methods of non-linear shrinkage (NLS) have been introduced [4, 5, 6, 7]. NLS asymptotically\nconverges to the Oracle estimator, which minimizes the Frobenius (element-wise square) distance between the \ufb01ltered\nand the true population covariance matrix. The asymptotic limit corresponds to in\ufb01nitely large data matrices at \ufb01xed\naspect ratio (the number of lines divided by the number of columns). Two important conditions must be met to achieve\nthis convergence: the dependence structure between the time series must be constant, and the price return distribution\nmust not be too heavy-tailed [4, 5, 6, 7].\nBecause NLS is provably a best estimator is some sense, it is also widely used for portfolio optimization ([8, 9, 10, 11,\n12, 13, 14] among hundreds of references), including in the much cited the state-of-the-art combination of dynamic\nconditional covariance and NLS [15, 16]. This results from the implicit belief that Frobenius-optimal eigenvalues are\nalso optimal for portfolio optimization, or equivalently that the Oracle eigenvalues inevitably provide the best (or almost\nthe best) eigenvalues for portfolio optimization. There is however no proof that it is actually the case.\nOur main hypothesis is that the covariance matrix eigenvalues that are best for global minimum variance minimization\ndo not coincide with the Oracle eigenvalues and thus that non-linear shrinkage is not optimal in general. A conceptually\nsimple way to demonstrate this hypothesis is to compute the RIE eigenvalues that do yield the optimal Global Minimum\nVariance portfolio and show that this RIE outperforms the Oracle. Such an estimator is obtained solving a quadratic\nprogramming problem, which admits an optimal solution different from the Oracle.\nWe investigate with real \ufb01nancial data where this discrepancy comes from and reversely in which conditions the NLS is\na good approximation to the optimal eigenvalues.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3300,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "to demonstrate this hypothesis is to compute the RIE eigenvalues that do yield the optimal Global Minimum\nVariance portfolio and show that this RIE outperforms the Oracle. Such an estimator is obtained solving a quadratic\nprogramming problem, which admits an optimal solution different from the Oracle.\nWe investigate with real \ufb01nancial data where this discrepancy comes from and reversely in which conditions the NLS is\na good approximation to the optimal eigenvalues. We found that only when both calibration and test windows are very\nlarge with respect to the number of stocks and when the covariance coef\ufb01cients between calibration and test differs only\ndue to sample size noise (in other words, both sample covariance matrices have the same expected covariance matrix),\nthen the two estimators share similar performances.\narXiv:2112.07521v2 [q-fin.PM] 13 Oct 2022\n\nA PREPRINT - OCTOBER 14, 2022\n2\nProblem Statement\nConsider n price return time series split into a calibration time window of length \u03b4in and a test time windows of length\n\u03b4out, and let us denote the in-sample (empirical) covariance matrix by \u03a3in \u2208Rn\u00d7n and the out-of-sample (realized)\ncovariance matrix by \u03a3out \u2208Rn\u00d7n respectively. According to the spectral theorem, the in-sample covariance matrix\ncan be decomposed into a sum of terms involving its eigenvalues \u03bb = (\u03bbk) \u2208Rn and their associated eigenvectors\ncomponents vi = (vik) \u2208Rn as\n\u03a3in\nij =\nn\nX\nk=1\n\u03bbin\nk vin\nikvin\njk.\n(1)\nAn RIE estimator \u039e(\u03bb\u2217) uses \ufb01ltered eigenvalues, which yields the spectral decomposition\n\u039e(\u03bb\u2217)ij =\nn\nX\nk=1\n\u03bb\u2217\nkvin\nikvin\njk,\n(2)\nwhere \u03bb\u2217are a set of \ufb01ltered eigenvalues obtained with some procedure and vin are still the eigenvectors of \u03a3in.\nWe aim to compare the \ufb01ltered eigenvalues from NLS and from the ones that are actually optimal for portfolio\noptimization.\n2.1\nOracle Eigenvalues\nThe Oracle eigenvalues are obtained from the out-of-sample covariance matrix from\n\u03bbOracle\nk\n=\nn\nX\ni=1\nn\nX\nj=1\nvin\nik\u03a3out\nij vin\njk,\n(3)\nwhere vin are the in-sample eigenvectors.\nSuch an eigenvalue correction provably produces an estimator that minimizes the Frobenius norm [4, 7]\n||\u039e(\u03bbOracle) \u2212\u03a3out||F =\nn\nX\ni=1\nn\nX\nj=1\n\u0002\n\u039e(\u03bbOracle)ij \u2212\u03a3out\nij\n\u00032 .\n(4)\n2.2\nOptimal RIEs for GMV Portfolios\nThe simplest portfolio optimization problem (and the most relevant one to assess covariance \ufb01ltering methods) is Global\nMinimum Variance (GMV) portfolios. We denote the fraction of capital assigned to each possible asset i = 1, \u00b7 \u00b7 \u00b7 , n\nby w \u2208Rn. GMV portfolios aim to minimize the realized portfolio variance \u03a3out at \ufb01xed net leverage P\ni wi = 1.\nMathematically, the problem can be written as the function of the weights as\nmin\nw\nn\nX\ni=1\nn\nX\nj=1\nwi\u03a3out\nij wj\nwith\nN\nX\nk=1\nwk = 1.\n(5)\nThis problem is readily solved if the future is known: the optimal GMV weights are given by\nwopt =\n(\u03a3out)\u22121e\ne\u2032(\u03a3out)\u22121e,\n(6)\nwhere e = 1, \u00b7 \u00b7 \u00b7 , 1 is a n-dimensional vector of ones.",
    "chunk_index": 1,
    "start_char": 2831,
    "end_char": 5726,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "\ufb01xed net leverage P\ni wi = 1.\nMathematically, the problem can be written as the function of the weights as\nmin\nw\nn\nX\ni=1\nn\nX\nj=1\nwi\u03a3out\nij wj\nwith\nN\nX\nk=1\nwk = 1.\n(5)\nThis problem is readily solved if the future is known: the optimal GMV weights are given by\nwopt =\n(\u03a3out)\u22121e\ne\u2032(\u03a3out)\u22121e,\n(6)\nwhere e = 1, \u00b7 \u00b7 \u00b7 , 1 is a n-dimensional vector of ones.\nThe main contribution of this paper is to show how not optimal NLS is for GMV portfolio optimization in a practical\ncontext. To this end, we compute the GMV-optimal RIE, which constrains the weights w to be written as a function of\n\u039e(\u03bb\u2217) instead of \u03a3out, the optimization variables being \u039e\u2019s eigenvalues. The optimal weights are now\nwk =\nPn\nj=1 \u039e\u22121\nkj\nPn\ni=1\nPn\nj=1 \u039e\u22121\nij\n,\n(7)\nwhere \u039e\u22121 is the inverse covariance matrix RIE whose spectral decomposition is\n\u039e\u22121\nij =\nn\nX\nk=1\n1\n\u03bb\u2217\nk\nvin\nikvin\njk.\n(8)\n2\n\nA PREPRINT - OCTOBER 14, 2022\nIt is important to point out that the denominator of 7 is a normalization factor which ensures that the sum of the weights\nequals one. Equation 7 is thus equivalent to\n(\nwk = Pn\nj=1 \u039e\u22121\nkj ;\nk = 1, \u00b7 \u00b7 \u00b7 , n\nPn\nk=1 wk = 1.\n(9)\nAnother important point is that the optimal GMV portfolio obtained from 9 and 8 does not depend on the scale of the\neigenvalues \u03bb\u2217\nk (or, equivalently, the average volatility). Thus, the only constraints we must impose on the eigenvalues\nis their non-negativity\n\u03b6k := 1\n\u03bb\u2217\nk\n\u22650; for k = 1, \u00b7 \u00b7 \u00b7 , n.\n(10)\nFinally, the full QP problem is expressed as\nminw,\u039e\u22121,\u03b6\nPn\ni=1 wi\u03a3out\nij wj\nsubject to\nwk = Pn\nj=1 \u039e\u22121\nkj ;\nk = 1, \u00b7 \u00b7 \u00b7 , n\nPn\nk=1 wk = 1\n\u039e\u22121\nij = Pn\nk=1 \u03b6kvin\nikvin\njk;i, j = 1, \u00b7 \u00b7 \u00b7 , n\n\u03b6k \u22650;\nk = 1, \u00b7 \u00b7 \u00b7 , n.\n(11)\nEq. 11 de\ufb01nes a convex Quadratic Programming problem, which can be solved by numerical methods. In this QP\nproblem formulation the variables w and \u039e\u22121 are slack variables that will be identi\ufb01ed by the optimization algorithm.\nIn total, the QP has n(n + 5)/2 variables: n for w, n(n + 1)/2 for \u039e\u22121, as the inverse covariance is symmetric, and n\nfor \u03b6 which are of interest here. The number of constraints are (n2 + 5n + 2)/2. The resulting optimal \u03b6k can be then\nnormalized to have a set of eigenvalues whose sum equals expected volatility.\nThe procedure described above does not guarantee an ordered sequence of optimal eigenvalues. Let us therefore add\nn \u22121 ordering constraints to Eq. 11, which yields\nminw,\u039e\u22121,\u03b6\nPn\ni=1 wi\u03a3out\nij wj\nsubject to\nwk = Pn\nj=1 \u039e\u22121\nkj ;",
    "chunk_index": 2,
    "start_char": 5376,
    "end_char": 7778,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "of interest here. The number of constraints are (n2 + 5n + 2)/2. The resulting optimal \u03b6k can be then\nnormalized to have a set of eigenvalues whose sum equals expected volatility.\nThe procedure described above does not guarantee an ordered sequence of optimal eigenvalues. Let us therefore add\nn \u22121 ordering constraints to Eq. 11, which yields\nminw,\u039e\u22121,\u03b6\nPn\ni=1 wi\u03a3out\nij wj\nsubject to\nwk = Pn\nj=1 \u039e\u22121\nkj ;\nk = 1, \u00b7 \u00b7 \u00b7 , n\nPn\nk=1 wk = 1\n\u039e\u22121\nij = Pn\nk=1 \u03b6kvin\nikvin\njk; i, j = 1, \u00b7 \u00b7 \u00b7 , n\n\u03b6k \u22650;\nk = 1, \u00b7 \u00b7 \u00b7 , n\n\u03b6k \u2265\u03b6k\u22121;\nk = 2, \u00b7 \u00b7 \u00b7 , n.\n(12)\nThese constraints necessarily imply a less optimal solution with respect to the unsorted case. A Python implementation\nof both QP problems is available from [17].\n3\nResults: real Global Minimum Variance portfolios\nWe apply both estimators to a data set of adjusted daily returns of the most capitalized US equities spanning the\n1995-2017 period.\nThe experiments are carried out in the following way. We randomly select two contiguous time intervals [t \u2212\u03b4in, t[ and\n[t, t + \u03b4out[ from the whole period, remove all the stocks which have more than 20% of missing values or zero returns,\nand discard any two stocks with an in-sample correlation larger than 0.95. From the remaining assets, we randomly\nselect n = 50 stocks and we compute \u03a3in and \u03a3out from the two intervals respectively.\nFrom the eigenvector basis of \u03a3in and \u03a3out we compute the optimal eigenvalues (Eqs (11)), the sorted optimal eigenvalue\nand the Oracle ones (Eqs (12)) and the Oracle eigenvalues, which optimize the Frobenius norm. We stress that NLS\naims to approximate the Oracle eigenvalues, thus produces slightly worse results [18]. For that reason, we only include\nthe Oracle eigenvalues in our study. Finally, we compute the out-of-sample (realized volatility) of GMV portfolios for\neach RIE.\nIn the upper panels of Fig. 1, we show the average annualized volatility over 10,000 portfolios with randomly chosen\nassets at random times. The performance ranking is always the same one: applying Oracle correction is always better\nthan using only the past (\u03a3in, and the weights from the QP problem are always better than the Oracle RIE.\nOne notices a large gap between the Optimal and the Optimal sorted weights; when \u03b4out < n + 1, this gap comes from\nthe fact that when \u03b4out < n + 1, the out-of-sample covariance matrix is not positively-de\ufb01ned. This means that there\n3\n\nA PREPRINT - OCTOBER 14, 2022\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 20, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 200, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 2000, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.",
    "chunk_index": 3,
    "start_char": 7372,
    "end_char": 10145,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "out-of-sample covariance matrix is not positively-de\ufb01ned. This means that there\n3\n\nA PREPRINT - OCTOBER 14, 2022\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 20, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 200, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 2000, n = 50\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 20, n = 50 (stationarized)\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 200, n = 50 (stationarized)\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\n102\n103\nout\n0.00\n0.05\n0.10\n0.15\n0.20\nvolatility [annalized]\nin = 2000, n = 50 (stationarized)\nin\nOracle\nOptimal (sort)\nOptimal\nout P.D.\nFigure 1: Realized volatility as a function of the out-of-sample period length. The upper panels display results from the\noriginal data, while the lower ones use stationarized data. The vertical black dotted lines refer to the transition point\nn = \u03b4out to a positive-de\ufb01ned out-of-sample covariance matrix. Matrix inversion of \u03a3in for \u03b4out < n + 1 is obtained\nwith a pseudo-inverse [19]. Averages over 10,000 portfolios of n = 50 random assets in random periods from daily\ndata (large capitalization US equities).\nare d = n + 1 \u2212\u03b4out null eigenvalues which imply an eigenspace of dimension d from which every portfolio will\nhave null variance. This is a purely mechanical effect that cannot be exploited from in-sample data only. This gap\ndisappears when the monotonicity of the \ufb01ltered eigenvalues is imposed. Interestingly, when \u03b4out > n + 1 the difference\nof performance between all the methods remains approximately constant until \u03b4out reaches very large values (1000 days,\ni.e., about 4 years of data). The phenomenon occurs when the in-sample window size \u03b4in increases from 200 to 2000.\nThis effect comes from the non-stationarity of the dependence structure in the \ufb01nancial data we use. As a consequence,\nvery large calibration or test periods produce out-of-date eigenvector bases.\nTo con\ufb01rm this hypothesis, we stationarize the in-sample and out-of-sample time-series. The idea is simply to shuf\ufb02e\nthe in-sample and out-of-sample days of each subperiod [t \u2212\u03b4in, t + \u03b4out[, in such a way that both the in-sample and\nout-of-sample holds a similar proportion of past and future days, which yields the same expected covariance matrix in\nboth periods. In the lower panels of Fig. 1 we show that increasing both \u03b4in and \u03b4out on two stationarized time-series\nreduces substantially the bias. This comes from the fact that in these conditions, the in-sample and out-of-sample\neigenvector bases tend to be very similar. If either of the two time-window lengths are reduced, the similarity between\nthe two eigenvector bases decreases.\n4\nConclusion\nCovariance \ufb01ltering for portfolio optimization, while having progressed much in the last decades, still needs fundamental\nimprovements.",
    "chunk_index": 4,
    "start_char": 9676,
    "end_char": 12732,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "covariance matrix in\nboth periods. In the lower panels of Fig. 1 we show that increasing both \u03b4in and \u03b4out on two stationarized time-series\nreduces substantially the bias. This comes from the fact that in these conditions, the in-sample and out-of-sample\neigenvector bases tend to be very similar. If either of the two time-window lengths are reduced, the similarity between\nthe two eigenvector bases decreases.\n4\nConclusion\nCovariance \ufb01ltering for portfolio optimization, while having progressed much in the last decades, still needs fundamental\nimprovements. The problem lies in the nonstationary nature of dependence in \ufb01nancial markets, which con\ufb02icts with\none of the main assumptions of the optimal stationary RIE, and which implies that the Frobenius distance is not the right\ncost function for the optimal RIE in a non-stationary world. The correct optimal RIE, derived in this work, currently\ndoes not have any asymptotical estimator that use only in-sample data, and thus, \ufb01nding the optimal covariance cleaning\nscheme for equity markets is still an open question.\nAny improvement will mechanically improve on the state-of-the art DCC+NLS scheme [15]. The simplest route is to\nkeep improving RIEs for which many exact asymptotic results are known [2]. For example, we recently showed that a\nlong-term average approach of the Oracle eigenvalues outperforms the Oracle eigenvalues in systems with nonstationary\ndependencies such as US and Hong Kong equity markets [18].\nWe \ufb01nally note that a RIE does not \ufb01lter the noise in the eigenvectors which contain useful additional but noisy\nstructures. A way to \ufb01lter the latter is provided for example by ans\u00e4tze such as hierarchical clustering [20] or\nprobabilistic hierarchical clustering [21, 22], which outperform the optimal stationary RIE for GMV portfolios when\n4\n\nA PREPRINT - OCTOBER 14, 2022\n\u03b4in < 2n and DCC+NLS when the asset universe keeps changing, as it is the case in real life. An alternative route is to\ntrain a deep neural network to learn to predict both the eigenvalues and the eigenvectors.\nAcknowledgments\nThis work was performed using HPC resources from the \u201cM\u00e9socentre\u201d computing center of CentraleSup\u00e9lec and \u00c9cole\nNormale Sup\u00e9rieure Paris-Saclay supported by CNRS and R\u00e9gion \u00cele-de-France.\nFunding\nThis publication stems from a partnership between CentraleSup\u00e9lec and BNP Paribas.\nReferences\n[1] Richard O Michaud. The Markowitz optimization enigma: Is \u201coptimized \u201d optimal? Financial Analysts Journal,\n45(1):31\u201342, 1989.\n[2] Jo\u00ebl Bun, Jean-Philippe Bouchaud, and Marc Potters. Cleaning large correlation matrices: tools from random\nmatrix theory. Physics Reports, 666:1\u2013109, 2017.\n[3] Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance matrices. Journal\nof multivariate analysis, 88(2):365\u2013411, 2004.\n[4] Olivier Ledoit and Sandrine P\u00e9ch\u00e9. Eigenvectors of some large sample covariance matrix ensembles. Probability\nTheory and Related Fields, 151(1):233\u2013264, 2011.\n[5] Olivier Ledoit, Michael Wolf, et al. Nonlinear shrinkage estimation of large-dimensional covariance matrices.\nThe Annals of Statistics, 40(2):1024\u20131060, 2012.\n[6] Daniel Bartz. Cross-validation based nonlinear shrinkage. arXiv preprint arXiv:1611.00798, 2016.\n[7] Jo\u00ebl Bun, Romain Allez, Jean-Philippe Bouchaud, and Marc Potters. Rotational invariant estimator for general\nnoisy matrices. IEEE Transactions on Information Theory, 62(12):7475\u20137490, 2016.",
    "chunk_index": 5,
    "start_char": 12172,
    "end_char": 15615,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "analysis, 88(2):365\u2013411, 2004.\n[4] Olivier Ledoit and Sandrine P\u00e9ch\u00e9. Eigenvectors of some large sample covariance matrix ensembles. Probability\nTheory and Related Fields, 151(1):233\u2013264, 2011.\n[5] Olivier Ledoit, Michael Wolf, et al. Nonlinear shrinkage estimation of large-dimensional covariance matrices.\nThe Annals of Statistics, 40(2):1024\u20131060, 2012.\n[6] Daniel Bartz. Cross-validation based nonlinear shrinkage. arXiv preprint arXiv:1611.00798, 2016.\n[7] Jo\u00ebl Bun, Romain Allez, Jean-Philippe Bouchaud, and Marc Potters. Rotational invariant estimator for general\nnoisy matrices. IEEE Transactions on Information Theory, 62(12):7475\u20137490, 2016.\n[8] Taras Bodnar, Nestor Parolya, and Wolfgang Schmid. Estimation of the global minimum variance portfolio in\nhigh dimensions. European Journal of Operational Research, 266(1):371\u2013390, 2018.\n[9] Yi Ding, Yingying Li, and Xinghua Zheng. High dimensional minimum variance portfolio estimation under\nstatistical factor models. Journal of Econometrics, 222(1, Part B):502\u2013515, 2021. Annals Issue:Financial\nEconometrics in the Age of the Digital Economy.\n[10] Francisco Rubio, Xavier Mestre, and Daniel P Palomar. Performance analysis and optimal selection of large\nminimum variance portfolios under estimation risk. IEEE Journal of Selected Topics in Signal Processing,\n6(4):337\u2013350, 2012.\n[11] Liusha Yang, Romain Couillet, and Matthew R McKay. A robust statistics approach to minimum variance portfolio\noptimization. IEEE Transactions on Signal Processing, 63(24):6684\u20136697, 2015.\n[12] Liusha Yang, Romain Couillet, and Matthew R McKay. Minimum variance portfolio optimization with robust\nshrinkage covariance estimation. In 2014 48th Asilomar Conference on Signals, Systems and Computers, pages\n1326\u20131330. IEEE, 2014.\n[13] Ruili Sun, Tiefeng Ma, Shuangzhe Liu, and Milind Sathye. Improved covariance matrix estimation for portfolio\nrisk measurement: A review. Journal of Risk and Financial Management, 12(1), 2019.\n[14] Zhao Zhao, Olivier Ledoit, and Hui Jiang. Risk reduction and ef\ufb01ciency increase in large portfolios: Gross-\nexposure constraints and shrinkage of the covariance matrix. Journal of Financial Econometrics, 2021.\n[15] Robert F. Engle, Olivier Ledoit, and Michael Wolf. Large dynamic covariance matrices. Journal of Business &\nEconomic Statistics, 37(2):363\u2013375, 2019.\n[16] Gianluca De Nard, Robert F Engle, Olivier Ledoit, and Michael Wolf. Large dynamic covariance matrices:\nEnhancements based on intraday data. Journal of Banking & Finance, 138:106426, 2022.\n[17] Christian Bongiorno. Optimal RIE, 2021. python3.5 package version 1.0.\n[18] Christian Bongiorno, Damien Challet, and Gr\u00e9goire Loeper. Cleaning the covariance matrix of strongly nonsta-\ntionary systems with time-independent eigenvalues. arXiv preprint arXiv:2111.13109, 2021.\n5\n\nA PREPRINT - OCTOBER 14, 2022\n[19] Ester Pantaleo, Michele Tumminello, Fabrizio Lillo, and Rosario N Mantegna. When do improved covariance\nmatrix estimators enhance portfolio optimization? an empirical comparative study of nine estimators. Quantitative\nFinance, 11(7):1067\u20131080, 2011.\n[20] Michele Tumminello, Fabrizio Lillo, and Rosario N Mantegna. Hierarchically nested factor model from multivari-\nate data. EPL (Europhysics Letters), 78(3):30006, 2007.\n[21] Christian Bongiorno and Damien Challet. Covariance matrix \ufb01ltering with bootstrapped hierarchies. PloS one,\n16(1):e0245092, 2021.\n[22] Christian Bongiorno and Damien Challet. Reactive global minimum variance portfolios with k\u2212bahc covariance\ncleaning. arXiv preprint arXiv:2005.08703, 2020.\n6",
    "chunk_index": 6,
    "start_char": 14964,
    "end_char": 18530,
    "paper_title": "Non-linear shrinkage of the price return covarianc",
    "paper_category": "q-fin.PM",
    "paper_filename": "Non-linear_shrinkage_of_the_price_return_covarianc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Non-linear_shrinkage_of_the_price_return_covarianc.pdf"
  },
  {
    "text": "Optimal Dynamic Strategies on Gaussian Returns\nNick Firoozye and Adriano Koshiyama\nJune 5, 2019\nDepartment of Computer Science\nUniversity College London\nn.firoozye@ucl.ac.uk, a.koshiyama@cs.ucl.ac.uk\nJune 5, 2019\nAbstract\nDynamic trading strategies, in the spirit of trend-following or mean-reversion, rep-\nresent an only partly understood but lucrative and pervasive area of modern \ufb01nance.\nAssuming Gaussian returns and Gaussian dynamic weights or signals, (e.g., linear \ufb01l-\nters of past returns, such as simple moving averages, exponential weighted moving\naverages, forecasts from ARIMA models), we are able to derive closed-form expressions\nfor the \ufb01rst four moments of the strategy\u2019s returns, in terms of correlations between\nthe random signals and unknown future returns. By allowing for randomness in the\nasset-allocation and modelling the interaction of strategy weights with returns, we\ndemonstrate that positive skewness and excess kurtosis are essential components of all\npositive Sharpe dynamic strategies, which is generally observed empirically; demon-\nstrate that total least squares (TLS) or orthogonal least squares is more appropriate\nthan OLS for maximizing the Sharpe ratio, while canonical correlation analysis (CCA)\nis similarly appropriate for the multi-asset case; derive standard errors on Sharpe ra-\ntios which are tighter than the commonly used standard errors from Lo; and derive\nstandard errors on the skewness and kurtosis of strategies, apparently new results. We\ndemonstrate these results are applicable asymptotically for a wide range of stationary\ntime-series.\nKeywords: Algorithmic Trading, Dynamic Strategies, over-\ufb01tting, Quantitative\nFinance, Signal Processing\nMSC Numbers: 60G10, 62E15, 62P05, 62F99, 91G70, 91G80\nJEL Classi\ufb01cations: C13, C58, C61, G11, G19\n1\nIntroduction\nCTAs (Commodity Trading Advisors) or managed-future accounts are a subset of asset man-\nagers with over $341bn of assets under management [Barclay Hedge, 2017] as of Q2 2017.\nThe predominant strategy which CTAs employ is trend-following. Meanwhile, bank struc-\nturing desks have devised a variety of risk-premia or styles strategies (including momentum,\nmean-reversion, carry, value, etc) which have been estimated to correspond to between\napproximately $150bn [Miller, 2016] to $200bn [Allenbridge IS, 2014] assets under manage-\nment. Responsible for over 80% of trade volume in equities and a large (but undocumented\namount due to the OTC nature) of the FX market, [Credit Suisse, 2017], high-frequency\ntrading \ufb01rms (HFTs) and e-trading desks in investment banks are known to make use of\n1\narXiv:1906.01427v1 [q-fin.PM] 31 May 2019\n\nmany strategies which are e\ufb00ectively short-term mean-reversion strategies. In spite of the\nrelatively large industry undergoing recent signi\ufb01cant growth, a careful analysis of the sta-\ntistical properties of strategies, including their optimisation, has only been undertaken in\nrelatively limited contexts.\nFigure 1: SocGen Trend Followers Index: daily returns and monthly returns pro\ufb01les\nThe corresponding statistics for the SG Trend index area in the table below and except\nfor some noise show that skewness and excess kurtosis are laregly positive for CTAs.\nTable 1: Soc Gen Trend Index, Daily and Monthly Statistics\nDaily\nMonthly\nAnn Avg Return (%)\n5.695\n5.752\nVolatily (%)\n13.283\n14.088\nSharpe Ratio\n0.429\n0.408\nSkewness\n-0.448\n0.186\nExc Kurtosis\n3.845\n0.807\nAlgorithmic trading strategies we consider are time-series strategies,",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3480,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "SocGen Trend Followers Index: daily returns and monthly returns pro\ufb01les\nThe corresponding statistics for the SG Trend index area in the table below and except\nfor some noise show that skewness and excess kurtosis are laregly positive for CTAs.\nTable 1: Soc Gen Trend Index, Daily and Monthly Statistics\nDaily\nMonthly\nAnn Avg Return (%)\n5.695\n5.752\nVolatily (%)\n13.283\n14.088\nSharpe Ratio\n0.429\n0.408\nSkewness\n-0.448\n0.186\nExc Kurtosis\n3.845\n0.807\nAlgorithmic trading strategies we consider are time-series strategies, often divided into\nmean-reverting or reversal strategies, trend-following or momentum strategies, and value\nstrategies (also sometimes known as mean-reversion).1. Each such time-series related strat-\n1Other common strategies include carry and short-gamma or short-vol. Unlike mean-reversion, momen-\ntum, and value, these do not rely on the speci\ufb01cs of the auto-correlation function\n2\n\negy is a form of signal processing. In more standard signal processing, the major interest\nis in the de-noised or smoothed signals and their properties. In algorithmic trading, the\ninterest is instead in the relationship between statistics like the moving average or some\nother form of smoothed historic returns (unfortunately, usually termed the signal) and the\nunknown future returns. We show that when we consider both to be random variables, it is\nactually the interaction between these so-called signals and future returns which determines\nthe strategy\u2019s behaviour.\nEquities, and in particular SPX is known to mean-revert over short horizons (e.g., shorter\nthan 1m, typically on the order of 5-10 days) and trend only over longer horizons (i.e.,\n3m-18m), and mean-revert again over even longer horizons (i.e., 2y-5y) as has been well-\nestablished by the quant equities literature following on the study of [Jegadeesh and Titman, 1993]\nand the work of [Fama and French, 1992]. This distinct form of behaviour, with reversals on\nsmall-scale, trend on an intermediate and reversion on a long scale, is frequently observed\nacross a large number of asset classes and strategies can be designed to take advantage of\nthe behaviour of asset-prices across each time-scale.\nOur initial goal is to \ufb01nd a signal, Xt usually a linear function of historic log-(excess)\nreturns {Rt} which can be used as a dynamic weight for allocating to the underlying asset\non a regular basis. We assume log-price Pt = Pt\n1 Rk. Examples of commonly used signals\nfor macro-traders (CTAs, and other trend followers) include:\n\u2022 Simple Moving Average (SMA):\nXt = 1\nT\nT\nX\n1\nRt\u2212k\n\u2022 Exponentially-Weighted Moving Average (EWMA):\nXt = c(\u03bb)\n\u221e\nX\nk=1\n\u03bbkRt\u2212k\n\u2022 Holt-Winters (HW, or double exponential smoothing) with or without seasonals, Damped\nHW\n\u2022 Di\ufb00erence between current price and moving average:2\nXt = Pt\u22121 \u22121\nT\nT\nX\n1\nPt\u2212k\n\u2022 Forecasts from ARMA(p, q) models:\nXt = \u03c61Rt\u22121 + ... + \u03c6pRt\u2212p + \u03b81\u03b5t\u22121 + ... + \u03b8q\u03b5t\u2212q\n\u2022 Di\ufb00erences between SMAs:\nXt = 1\nT1\nT1\nX\n1\nPt\u2212k \u22121\nT2\nT2\nX\n1\nPt\u2212j\n2We note that if we replace P by log(P) and Rt = log(Pt) \u2212log(Pt\u22121), this \ufb01lter amounts to Xt =\nP T \u2212k\nT\nRt\u2212k, i.e., a triangular \ufb01lter on returns, which bears some similarity to EWMA on returns.\n3",
    "chunk_index": 1,
    "start_char": 2963,
    "end_char": 6111,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "or without seasonals, Damped\nHW\n\u2022 Di\ufb00erence between current price and moving average:2\nXt = Pt\u22121 \u22121\nT\nT\nX\n1\nPt\u2212k\n\u2022 Forecasts from ARMA(p, q) models:\nXt = \u03c61Rt\u22121 + ... + \u03c6pRt\u2212p + \u03b81\u03b5t\u22121 + ... + \u03b8q\u03b5t\u2212q\n\u2022 Di\ufb00erences between SMAs:\nXt = 1\nT1\nT1\nX\n1\nPt\u2212k \u22121\nT2\nT2\nX\n1\nPt\u2212j\n2We note that if we replace P by log(P) and Rt = log(Pt) \u2212log(Pt\u22121), this \ufb01lter amounts to Xt =\nP T \u2212k\nT\nRt\u2212k, i.e., a triangular \ufb01lter on returns, which bears some similarity to EWMA on returns.\n3\n\n\u2022 Di\ufb00erences between EWMAs:\nXt = c(\u03bb1)\nX\n\u03bbk\n1Rt\u2212k \u2212c(\u03bb2)\nX\n\u03bbk\n2Rt\u2212k\nand variations using volatility or variance weighting such as z-scores (SMAs or EWMAs\nweighted by a simple or weighted standard deviation, see [Harvey et al., 2018]), and trans-\nformations of each of the signals listed above (e.g. allocations depending on sigmoids of\nmoving averages, reverse sigmoids, Winsorised signals, etc.). Other signals commonly used\nin equity algorithmic trading include economic and corporate releases, and sentiment as\nderived from unstructured datasets such as news releases.\nThe returns from algorithmic trading strategies are well documented (see, e.g., [Asness-Moskowitz-Pedersen, 2013\n[Baltas and Kosowski, 2013], [Hurst-Ooi-Pedersen, 2017] and [Lemp\u00e9ri\u00e8re et al., 2014]). Al-\nthough many methods have been used to derive signals by practitioners, (see, e.g., [Bruder et al, 2011]\nfor a compendium), many of these methods are equally good (or bad) and it makes little\npractical di\ufb00erence whether one uses ARMA, EWMA or SMA as the starting point for a\nstrategy design (see e.g., [Levine and Pedersen, 2015]). In this paper, we only touch on nor-\nmalised signals (e.g., z-scores) and strategy returns, leaving their discussion for a subsequent\nstudy. We meanwhile note that the spirit of this paper\u2019s results carry through for the case\nof normalized signals and strategy returns.\nFrequently, exponential smoothers have been the e\ufb00ective best models in various eco-\nnomic forecasting competitions (see, e.g., the results of the \ufb01rst three M-competitions\n[Makridakis, 2000]), showing perhaps that their simplicity bestows a certain robustness,\nand their original intuition was sound even if the statistical foundation took a signi\ufb01cant\ntime to catch up. In fact, EWMA and HW can both be justi\ufb01ed as state-space models\n(see [Hyndman et al., 2008]), and this formulation brings with it a host of bene\ufb01ts from\nmere intellectual satisfaction to statistical hypothesis tests, change-point tests, and a metric\nfor goodness-of-\ufb01t. Exponential smoothing with multiplicative or additive seasonals and\ndampened weighted slopes are used to successfully forecast a signi\ufb01cant number of economic\ntime-series (e.g., inventories, employment, monetary aggregates). EWMA (and the related\n(S)MA), and HW remain some of the most commonly used \ufb01ltering methods for CTAs and\nHFT shops.\nIn the case of returns which are normal with \ufb01xed autocorrelation function (ACF), i.e.,\nthose which are covariance stationary, signals created from linear combinations of historic\nreturns are indeed normal random variables which are jointly normal with returns.",
    "chunk_index": 2,
    "start_char": 5647,
    "end_char": 8735,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "forecast a signi\ufb01cant number of economic\ntime-series (e.g., inventories, employment, monetary aggregates). EWMA (and the related\n(S)MA), and HW remain some of the most commonly used \ufb01ltering methods for CTAs and\nHFT shops.\nIn the case of returns which are normal with \ufb01xed autocorrelation function (ACF), i.e.,\nthose which are covariance stationary, signals created from linear combinations of historic\nreturns are indeed normal random variables which are jointly normal with returns. External\ndatasets (e.g., unstructured data, corporate releases), are less likely to contain normally\ndistributed variables although there is an argument for asymptotic normality. Irrespective,\nour approach is to assume normality of both returns and signals as a starting point for\nfurther analysis.\nWhile there is signi\ufb01cant need for further study, there have nonetheless been a number of\nempirical and theoretical results of note in this area. Fung and Hsieh were the \ufb01rst to look\nat the empirical properties of momentum strategies [Fung and Hsieh, 1997], noting (without\nany theoretical foundation) the resemblance of strategy returns to straddle pay-o\ufb00s.3 Potters\nand Bouchaud [Potters and Bouchaud, 2005] studied the signi\ufb01cant positive skewness of\ntrend-following returns, showing that for successful strategies, the median pro\ufb01tability of\ntrades is negative. The empirical returns of dynamic strategies are far from normal, and\ncommon values for skewness and kurtosis for single strategies can have skewness in the range\nof [1.3, 1.7] and kurtosis in the range [8.8, 15.3] respectively (see [Ho\ufb00man-Kaminski, 2016]).\n3Or as they claimed, the returns of trend following resemble those of an extremely exotic option (which\nis not actually traded), daily-traded \u201clook-back straddles.\u201d\n4\n\nBruder and Gaussel [Bruder and Gaussel, 2011] and [Hamdan et al., 2016] (see Appendix\n2 for a superlative use of SDE-bassed methods for analyzing a wide variety of dynamic\nstrategies) used SDEs to study the power-option like behaviour of pay-o\ufb00s. Martin and\nZou considered general but IID discrete time distributions (see [Martin-Zou, 2012] and\n[Martin-Bana, 2012]) to study the term-structure of skewness over various horizons and\nthe e\ufb00ects of certain non-linear transforms on the term structure of return distributions.\nMore reBcently, Bouchaud et al [Bouchaud et al, 2016] considered more general discrete-\ntime distributions to study the convexity of pay-o\ufb00s, and the e\ufb00ective dependence of returns\non long-term vs short-term variance. Other studies have focused predominantly on the em-\npirical behaviour of returns, the relationship to macro-\ufb01nancial conditions, the persistence\nof trend-following returns, and the bene\ufb01ts from their inclusion into broader portfolios.\nIn the larger portion of the theoretical studies, the assumptions have been minimal in or-\nder to consider more general return distributions. Due to their generality, the derived results\nare somewhat more restrictive. Rather than opting for the most general, we choose more\nspeci\ufb01c distributional assumptions, in the hope that we can obtain broader, possibly more\npractical results. Aside from this current study, the authors have extended this work fur-\nther to consider the endemic problem of over-\ufb01tting (see [Koshiyama and Firoozye, 2018]),\nproposing total least squares with covariance penalties as a means of model-selection, show-\ning their outperformance to standard methods, using OLS with AIC.",
    "chunk_index": 3,
    "start_char": 8251,
    "end_char": 11706,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "consider more general return distributions. Due to their generality, the derived results\nare somewhat more restrictive. Rather than opting for the most general, we choose more\nspeci\ufb01c distributional assumptions, in the hope that we can obtain broader, possibly more\npractical results. Aside from this current study, the authors have extended this work fur-\nther to consider the endemic problem of over-\ufb01tting (see [Koshiyama and Firoozye, 2018]),\nproposing total least squares with covariance penalties as a means of model-selection, show-\ning their outperformance to standard methods, using OLS with AIC.\nIn this paper, we consider underlying assets with stationary Gaussian returns and a\n\ufb01xed auto-correlation function (i.e., they are a discrete Gaussian process). While we make\nno defence of the realism of using normal returns, we \ufb01nd that normality can be exploited\nin order to ensure we understand how the returns of linear and non-linear strategies should\nwork in theory and to further the understanding of the interaction between properties of\nreturns and of the signals as a basis for the development and analysis of dynamic strategies\nin practice.\nGiven a purely-random mean-zero covariance-stationary discrete-time Gaussian process\nfor returns, the signals listed above, whether a EWMA or an ARMA forecast, can be ex-\npressed as convolution \ufb01lters of past returns, i.e., our signal Xt can be expressed as\nXt =\nX\nk\u22651\n\u03c6(k)Rt\u2212k\nThis is an example of a time-invariant linear \ufb01lter of a Gaussian process. If we restrict\nour attention to those \ufb01lters for which are square summable, P\u221e\n1 \u03c6(k)2 < \u221e, then it is\nwell-known that the resulting \ufb01ltered series is also Gaussian and jointly Gaussian with Rt.\nOur underlying premise is that the important distributions to consider for the analysis of\ndynamic strategies is a product of Gaussians (rather than a single Gaussian as would usually\napply in asymptotic analysis of asset returns). This product measure can be justi\ufb01ed on\nmany levels and we discuss large sample approximations in the appendix.\nThe resulting measure which determines the success of the strategy is the correlation\nbetween the returns and the signals, a measure which, in the context of measuring an active\nmanager\u2019s skill is known as the information coe\ufb03cient or IC as given in the Fundamental\nLaw of Active Management detailed in [Grinold and Kahn, 1999]. While there is a large\nbody of literature on the IC and its relationship to information ratios, (see for example\n[Lee, 2000] for formulas similar to equation (5)), the derivations, resulting formulae and\nconclusions di\ufb00er signi\ufb01cantly.\nWe should also mention the work on random matrix theory by Potters and Bouchaud\n([Bouchaud and Potters, 2009]), which touches on many of the topics we consider in this\npaper. In particular their analysis of returns as products of Gaussians or t-distributions\n5\n\nis very lose to our own. While many of the emphases are once again di\ufb00erent to ours, we\nbelieve the general area of Random Matrix Theory to be a fruitful approach to trading\nstrategies.",
    "chunk_index": 4,
    "start_char": 11101,
    "end_char": 14163,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "on random matrix theory by Potters and Bouchaud\n([Bouchaud and Potters, 2009]), which touches on many of the topics we consider in this\npaper. In particular their analysis of returns as products of Gaussians or t-distributions\n5\n\nis very lose to our own. While many of the emphases are once again di\ufb00erent to ours, we\nbelieve the general area of Random Matrix Theory to be a fruitful approach to trading\nstrategies.\nThe primary tool we use to derive results is Isserlis\u2019 theorem [Isserlis, 1918] or Wick\u2019s\ntheorem (as it is known in the context of particle physics [Wick, 1950]). This relates products\nand powers of multivariate normal random variables to their means and covariances. Wick\u2019s\ntheorem has been applied in areas from particle physics, to quantum \ufb01eld theory to stock\nreturns and there are some recent e\ufb00orts to extend to non-Gaussian distributions (see, e.g.,\n[Michalowicz et al., 2011] for Gaussian-mixture and [Kan, 2008] for products of quadratic\nforms and elliptic distributions), and it has been applied to continuous processes via the\ncentral limit theorem (see [Parczewski, 2014]). We have used these theorems in the context\nof dynamic (algorithmic) trading strategies to \ufb01nd expressions for the \ufb01rst four moments of\nstrategy returns in closed-form. While it is not necessarily the aim of all scienti\ufb01c studies\nof trading strategies to \ufb01nd closed-form expressions, the ease with which we can describe\nstrategy returns makes this direction relatively appealing and allows for a number of future\nextensions.\nThe paper is divided into sections on one asset, considered over a single period. With a\nnormal signal, we will show there is a universal bound on the one-period Sharpe ratio, skew-\nness and kurtosis. We explain the role of total or orthogonal least squares as an alternative\nto OLS for strategy optimisation. We look at the corresponding re\ufb01nements to measures\nof Sharpe ratio standard error for these dynamic strategies, improving on the large-sample\ntheory based standard errors in more common use. We also introduce standard errors on\nskewness and kurtosis, which are distinct from those for Gaussian returns and present some\nbasic results about multiple assets and diversi\ufb01cation. Finally, we discuss the role of product\nmeasures, more pertinent to the study of dynamic strategies than simple Gaussian measures.\nIn the appendices, we present closed-form solutions to Sharpe ratios in the case of non-zero\nmeans. We also discuss extensions to our optimisations in the presence of transaction costs.\nWe touch on the extension to multiple periods as well. As we mentioned, further extensions\nto over-\ufb01tting by the use of covariance penalties (akin to Mallow\u2019s Cp or AIC/BIC) have\nbeen presented separately in [Koshiyama and Firoozye, 2018].\n2\nSingle period linear strategies\nWe consider the (log) returns of a single asset, Rt \u223cN (0, \u03c32\nR) returns with auto-covariance\nfunction at lag k, \u03b3(k) = E[RtRt\u2212k], together with corresponding auto-correlation function\n(ACF), c(k) = \u03b3(k)/\u03b3(0) at lag k.",
    "chunk_index": 5,
    "start_char": 13748,
    "end_char": 16770,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "use of covariance penalties (akin to Mallow\u2019s Cp or AIC/BIC) have\nbeen presented separately in [Koshiyama and Firoozye, 2018].\n2\nSingle period linear strategies\nWe consider the (log) returns of a single asset, Rt \u223cN (0, \u03c32\nR) returns with auto-covariance\nfunction at lag k, \u03b3(k) = E[RtRt\u2212k], together with corresponding auto-correlation function\n(ACF), c(k) = \u03b3(k)/\u03b3(0) at lag k.\nOur main aim is to work with strategies based on linear portfolio weights (or signals)\nXt = \u03a3\u221e\n1 akRt\u2212k for coe\ufb03cients ak generating the corresponding dynamic strategy returns\nSt = Xt \u00b7 Rt (here, and always, the signal, Xt is assumed to only have appropriately lagged\ninformation). Example strategy weights include exponentially weighted moving averages\nak \u221d\u03bbk, simple moving averages ak = 1\nT 1[1,...,T ], forecasts from ARMA models, etc. Most\nimportantly, the portfolio weights X are normal and jointly normal with returns R.\nIn\nAppendix B, we show that for a wide set of signals discussed in the Introduction, when\napplied to Gaussian returns, the signal and returns are jointly Gaussian.\nWe restrict our attention to return distributions over a single period. In the case of\nmany momentum strategies, this period can be one day, if not longer. For higher-frequency\nintra-day strategies, this period can be much shorter. The pertinent concern is that the\nhorizon (i.e., one period) is the same horizon over which the rebalancing of strategy weights\n6\n\nis done. If weights are rebalanced every \ufb01ve minutes, then the single period should be \ufb01ve\nminutes. This is a necessary assumption in order to ensure the joint normality of (as yet\nindeterminate) signals and future returns. Moreover, this assumption will give some context\nto our results, which imply a maximal Sharpe ratio, maximal skewness and maximal kurtosis\nfor dynamic linear strategies.\nWe are interested in characterizing the moments of the strategy\u2019s unconditional returns,\nthe corresponding standard errors on estimated quantities, and means of optimising various\nnon-dimensional measures of returns such as the Sharpe ratio via the use of non-linear\ntransformations of signals. Our goal is to look at unconditional properties of the strategy. It\nis important to avoid foresight in strategy design and this directly impacts the conditional\nproperties of strategies (e.g., conditional densities involve conditioning on the currently\nobserved signal to determine properties of the returns, which are just Gaussian). In the\ncontext of our study, we are concerned with one-period ahead returns of the unconditional\nreturns distribution of our strategy, where both the signals and the returns are unobserved,\nand the resulting distributions (in our case, the product of two normals) are much richer\nand more realistic \u2013 for the interested reader, we have added a more detailed discussion of\nour framework in Appendix G.\n2.1\nProperties of linear strategies\nGiven the joint normality of the signal and the returns, we can explicitly characterise the one-\nperiod strategy returns (see [Cui et al., 2016]). To allow for greater extendibility, we prefer\nto only consider the moments of the resulting distributions.",
    "chunk_index": 6,
    "start_char": 16391,
    "end_char": 19540,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "our case, the product of two normals) are much richer\nand more realistic \u2013 for the interested reader, we have added a more detailed discussion of\nour framework in Appendix G.\n2.1\nProperties of linear strategies\nGiven the joint normality of the signal and the returns, we can explicitly characterise the one-\nperiod strategy returns (see [Cui et al., 2016]). To allow for greater extendibility, we prefer\nto only consider the moments of the resulting distributions. These can be characterized\neasily using Isserlis\u2019 theorem [Isserlis, 1918], which gives all moments for any multivariate\nnormal random variable in terms of the mean and variance. We also refer to [Haldane, 1942]\nwho meticulously produces both non-central and central moments for powers and products\nof Gaussians. While this is a routine application of Isserlis\u2019 theorem, the algebra can be\ntedious, so we quote the results.\nTheorem 2.1 (Isserlis (1918)). If X \u223cN (0, \u03a3),then\nE[X1X2 \u00b7 \u00b7 \u00b7 X2n] =\n2n\nX\ni=1\nY\ni\u0338=j\nE[XiXj]\nand\nE[X1X2 \u00b7 \u00b7 \u00b7 X2n\u22121] = 0\nwhere the P Q is over all the (2n)!/(2nn!) unique partitions of X1, X2, . . . X2n into pairs\nXiXj.\nHaldane\u2019s paper quotes a large number of moment-based results for various powers of\neach normal. We quote the relevant results.\nTheorem 2.2 (Haldane (1942)). If x, y \u223cN (0, 1) with correlation \u03c1 then\nE[xy] =\n\u03c1\nE[x2y2] =\n1 + 2\u03c12\nE[x3y3] =\n3\u03c1(3 + 2\u03c12)\nE[x4y4] =\n3(3 + 24\u03c12 + 8\u03c14)\n7\n\nand thus the central moments of xy are\n\u00b51 =\n\u03c1\n(1)\n\u00b52 =\n1 + \u03c12\n(2)\n\u00b53 =\n2\u03c1(3 + \u03c12)\n(3)\n\u00b54 =\n3(3 + 14\u03c12 + 3\u03c14)\n(4)\nFrom these one period moments, (and a simple scaling argument giving the dependence\non \u03c3(x) and \u03c3(y)) we can characterise Sharpe ratio, skewness, etc., and can also de\ufb01ne\nobjective functions in order to determine some sense of optimality for a given strategy.\nTheorem 2.3 (Linear Gaussian). For single asset returns and a one period strategy, Rt \u223c\nN (0, \u03c32\nR) and Xt \u223cN (0, \u03c32\nX) jointly normal with correlation \u03c1, the Sharpe ratio is given\nby\nSR =\n\u03c1\np\n1 + \u03c12 ,\n(5)\nthe skewness is given as\n\u03b33 = 2\u03c1(3 + \u03c12)\n(1 + \u03c12)\n3\n2 ,\n(6)\nand the kurtosis is given by\n\u03b34 = 3(3 + 14\u03c12 + 3\u03c14)\n(1 + \u03c12)2\n(7)\nIn the appendix, we extend equations (5) and (6) to the case of non-zero means.\nProof. A simple application of Theorem 2.2 give us the following \ufb01rst two moments for our\nstrategy St = Xt \u00b7 Rt: \u00b51 = E[St] = E[X \u00b7 R] = \u03c3X\u03c3R\u03c1. and \u00b52 = V ar[St] = \u03c32\nX\u03c32\nR(\u03c12 + 1)\n. Thus we can derive the following results for the Sharpe ratio,\nSharpe =\n\u00b51\n\u00b51/2\n2\n=\n\u03c3X\u03c3R\u03c1\n\u03c3X\u03c3R\u221a\n\u03c12+1\n=\n\u03c1\n\u221a\n\u03c12+1\nMoreover, we can see that the skewness,\n\u03b33 =\n\u00b53\n\u00b53/2\n2\n=\n2\u03c1(3+\u03c12)\n(1+\u03c12)3/2\nFinally, the kurtosis is given by\n\u03b34 =\n\u00b54\n\u00b52\n2\n=\n3(3+14\u03c12+3\u03c14)\n(1+\u03c12)2\n8",
    "chunk_index": 7,
    "start_char": 19076,
    "end_char": 21696,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "Theorem 2.2 give us the following \ufb01rst two moments for our\nstrategy St = Xt \u00b7 Rt: \u00b51 = E[St] = E[X \u00b7 R] = \u03c3X\u03c3R\u03c1. and \u00b52 = V ar[St] = \u03c32\nX\u03c32\nR(\u03c12 + 1)\n. Thus we can derive the following results for the Sharpe ratio,\nSharpe =\n\u00b51\n\u00b51/2\n2\n=\n\u03c3X\u03c3R\u03c1\n\u03c3X\u03c3R\u221a\n\u03c12+1\n=\n\u03c1\n\u221a\n\u03c12+1\nMoreover, we can see that the skewness,\n\u03b33 =\n\u00b53\n\u00b53/2\n2\n=\n2\u03c1(3+\u03c12)\n(1+\u03c12)3/2\nFinally, the kurtosis is given by\n\u03b34 =\n\u00b54\n\u00b52\n2\n=\n3(3+14\u03c12+3\u03c14)\n(1+\u03c12)2\n8\n\nIf we restrict our attention to positive correlations, all three dimensionless statistics are\nmonotonically increasing in \u03c1. Consequently, strategies that maximize one of these statistics\nwill maximize the others, although the impact of correlation upon Sharpe ratio, skewness\nand kurtosis is di\ufb00erent. We illustrate the cross-dependencies in the following charts, de-\npicting the relationships between the variables.\nIn \ufb01gure 2, the shaded blue histograms\ncorrespond to correlation ranges ({[\u22121, \u22120.5], [\u22120.5, 0], [0, 0.5], [0.5, 1]}). We note that a\nuniform distribution in correlations maps into a higher likelihood of extreme Sharpe ratios\nand an even higher likelihood of extreme skewness and kurtosis.\nFigure 2:\nCorrelation,\nSharpe ratio,\nSkewness,\nand Kurtosis pairwise re-\nlationship.\nA uniform\ndistribution\nin\ncorrelation\nis\nbucketed\ninto\nfour\nranges\n{[\u22121, \u22120.5], [\u22120.5, 0], [0, 0.5], [0.5, 1]} as depicted in the bar charts in shades of blue. After\ntransforming the correlation into SR, \u03b33 and \u03b34 the frequencies are no longer uniform.\nSkewness ranges in [\u221223/2, 23/2] \u2248[\u22122.8, 2.8]. Unlike the Sharpe ratio, Skewness\u2019 de-\npendence on correlation tends to \ufb02atten, so to achieve 90% peak skewness, one needs only\nachieve a 0.60 correlation, while for a 90% peak Sharpe, one needs a correlation of 0.85.\nKurtosis is an even function and varies from a minimal value of 9 to a maximum of 15. In\npractice, correlations will largely be close to zero and the resulting skewness and kurtosis\nsigni\ufb01cantly smaller than the maximal values.\nAlthough we analyse the moments of the strategy St = XtRt, the full product density is\nactually known in closed form (see appendix A, [Cui et al., 2016] and [Nadarajah-Pog\u00e1ny, 2016]).\nIt is clear that the distribution of the strategy is leptokurtic even when it is not predictive\n(when the correlation is exactly zero, the strategy has a kurtosis of 9). In the limit as \u03c1 \u21921,\nthe strategy\u2019s density approaches that of a non-central \u03c72, an e\ufb00ective best-case density\nwhen considering the design of optimal linear dynamic strategies.\n9\n\nAn optimised strategy with su\ufb03cient lags (and a means of ensuring parsimony) may\nbe able to capture both mean-reversion and trend and result in yet higher correlations.\nAnnualised Sharpe ratios of between 0.5-1.5 are most common (i.e., correlations of between\n3% to 9%) for single asset strategies in this relatively low-frequency regime.\n2.2\nOptimisation: Maximal Correlation, Total least squares\nMany algorithmic traders will explain how problematic strategy optimisation is, given the\nendless concerns of over-\ufb01tting, etc.",
    "chunk_index": 8,
    "start_char": 21284,
    "end_char": 24297,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "strategies.\n9\n\nAn optimised strategy with su\ufb03cient lags (and a means of ensuring parsimony) may\nbe able to capture both mean-reversion and trend and result in yet higher correlations.\nAnnualised Sharpe ratios of between 0.5-1.5 are most common (i.e., correlations of between\n3% to 9%) for single asset strategies in this relatively low-frequency regime.\n2.2\nOptimisation: Maximal Correlation, Total least squares\nMany algorithmic traders will explain how problematic strategy optimisation is, given the\nendless concerns of over-\ufb01tting, etc. Although these are a concern, the na\u00efve use of strategies\nwhich are merely pulled out of thin air is equally problematic, where there is no explicit use\nof optimisation (and, in its place more eye-balling strategies or targeting Sharpe ratios rather\nloosely, e\ufb00ectively a somewhat loose mental optimisation exercise). Practical considerations\nabound and real-world returns are neither Gaussian nor stationary. We argue irrespectively\nthat using optimisation and a well-speci\ufb01ed utility function as a starting point is a means of\npreventing strategies from being just untested heuristics. Unlike most discretionary traders\u2019\nheuristics (or rules of thumb) which have their place as a means of dealing with uncertainty\n(see for example [Gigerenzer and Todd, 1999]), heuristic quantitative trading strategies run\nthe risk of being entirely arbitrary, or are subject to a large number of human biases, in\nmarked contrast to the monniker quantitative investment strategies.\nFigure 3: EWMA Strategy Sharpe Ratio vs \u03b1, MSE and correlation for S&P 500\nreversal strategies\nWhere optimisation is used, the most common optimisation method is to minimize the\nmean-squared error (MSE) of the forecast. Our results show that rather than to minimize\nthe L2 norm between our signal and the forecast returns (or to maximize the likelihood), if\nthe objective is to maximize the Sharpe ratio, we must maximize the correlation.\nWe can see in \ufb01gures 3 and 4, a depiction of \ufb01ts of strategies applied to S&P 500 using\nEWMA and HW \ufb01lters for a variety of parameters. The relationship between MSE and\nSharpe ratio is not monotone in MSE for the EWMA \ufb01lter as we see in \ufb01gure 3, while it is\nmuch closer to being linear in the case of the relationship between correlation and Sharpe.\nFor the case of HW (with two parameters), in \ufb01gure 4 any given MSE can lead to a non-\nunique Sharpe ratio, sometimes with a very broad range, leading us to conclude that the\noptimization is poorly posed. The relationship of correlation to Sharpe is obviously closer\nto being linear, with higher correlations almost always leading to higher Sharpe ratios.\nIn the case of a one-dimensional forecasting problem with (unconstrained) linear signals,\noptimizing the correlation amounts to using what is known as total least squares regression\n10\n\nFigure 4: Holt-Winters Strategy Sharpe Ratio vs MSE and correlation for S&P\n500 Reversal Strategies\n(TLS) or orthogonal distance regression, a form of principal components regression (see, e.g.,\n[Golub and Van Loan, 1980] and [Markovsky and Van Hu\ufb00ell, 2007]). In the multivariate\ncase, it would be more closely related to canonical correlation analysis (CCA).",
    "chunk_index": 9,
    "start_char": 23757,
    "end_char": 26956,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "ratios.\nIn the case of a one-dimensional forecasting problem with (unconstrained) linear signals,\noptimizing the correlation amounts to using what is known as total least squares regression\n10\n\nFigure 4: Holt-Winters Strategy Sharpe Ratio vs MSE and correlation for S&P\n500 Reversal Strategies\n(TLS) or orthogonal distance regression, a form of principal components regression (see, e.g.,\n[Golub and Van Loan, 1980] and [Markovsky and Van Hu\ufb00ell, 2007]). In the multivariate\ncase, it would be more closely related to canonical correlation analysis (CCA).\nUnlike OLS, where the dependent variable is assumed to be measured with error and\nthe independent variables are assumed to be measured without error, in total least squares\nregression, both dependent and independent variables are assumed to be measured with\nerror, and the objective function compensates for this by minimizing the sum squared of\northogonal distances to the \ufb01tted hyperplane. This is a simple form of errors-in-variables\n(EIV) regression and has been studied since the late 1870s, and is most closely related to\nprincipal components analysis. For k regressors, the TLS \ufb01t will produce weights which are\northogonal to the \ufb01rst k \u22121 principal components.\nSo, if we consider the signal X = Z\u03b2 to be a linear combination of features, with Z \u2208Rk\na k-dimensional feature space, then we note that\n\u02c6\u03b2OLS = (Z\u2032Z)\u22121Z\u2032R\nbut\n\u02c6\u03b2T LS = (Z\u2032Z \u2212\u03c32\nk+1I)\u22121Z\u2032R\nwhere \u03c3k+1 is the smallest singular value for the T \u00d7 (k + 1) dimensional matrix \u02dcX = [R, Z]\n(i.e., the concatenation of the features and the returns, see, e.g., [Rahman and Yu, 1987]4).It\nis well known that, for the case of OLS, the smooth or hat matrix \u02c6R = MR is given by\nM OLS = Z(Z\u2032Z)\u22121Z\u2032\nwith tr(M OLS) = k, the number of features. In contrast,\nM T LS = Z(Z\u2032Z \u2212\u03c32\nk+1I)\u22121Z\u2032\nand e\ufb00ectively has a greater number of degrees of freedom than that of OLS, i.e.,\ntr(M T LS) \u2265tr(M OLS)\n4A more common method for extracting TLS estimates is via a PCA of the concatenation matrix \u02dc\nX,\nwhere \u02c6\u03b2T LS is chosen to cancel the least signi\ufb01cant principal component\n11\n\nwith equality only when there is complete collinearity5 For this reason, many people see\nTLS as an anti-regularisation method and may result in less-stable response to outliers\n(see for example, [Zhang, 2017]).\nConsequently, there is extensive study of regularised\nTLS, typically using a weighted ridge-regression (or Tikhonov) penalty (see discussion in\n[Zhang, 2017] for more detail on this large body of research).\nThe stability of TLS in\nout-of-sample performance is an issue we broach in our study of over-\ufb01tting penalties (see\n[Koshiyama and Firoozye, 2018]).\nWhile maximizing correlation rather than minimizing the MSE seems a very minor\nchange in objective function, the formulas di\ufb00er from those of standard OLS. The end\nresult is a linear \ufb01t which takes into account the errors in the underlying conditioning in-\nformation. We believe that it should be of relatively little consequence when the features\nare appropriately normalized, as is the case",
    "chunk_index": 10,
    "start_char": 26402,
    "end_char": 29432,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "performance is an issue we broach in our study of over-\ufb01tting penalties (see\n[Koshiyama and Firoozye, 2018]).\nWhile maximizing correlation rather than minimizing the MSE seems a very minor\nchange in objective function, the formulas di\ufb00er from those of standard OLS. The end\nresult is a linear \ufb01t which takes into account the errors in the underlying conditioning in-\nformation. We believe that it should be of relatively little consequence when the features\nare appropriately normalized, as is the case for univariate time-series estimation, although\nsome authors have suggested that optimising TLS is not appropriate for prediction (see,\ne.g., [Fuller, 2009] section 1.6.3). When we seek to maximize the Sharpe ratio of a strategy,\nthe objective should not be prediction, but rather optimal weight choice.\n2.3\nMaximal Sharpe ratios, Maximal Skewness, Minimal Kurtosis\nSurprisingly, there appears to be a maximal Sharpe ratio for linear strategies. In the case\nof normal signals and normal returns, the maximal Sharpe ratio is that of a non-central \u03c72\ndistribution and the resulting maximal statistics are\nSRmax =\n\u221a\n2\n2 \u22480.707\n\u03b3max\n3\n=\n2\n\u221a\n2 \u22482.828\n\u03b3max\n4\n=\n15.000\nWhile the estimate for the Sharpe ratio may seem surprisingly low, we comment that\nthese are for a single period, for one single rebalancing. For a daily rebalanced strategy, if\nwe na\u00efvely annualize the Sharpe ratio (by a factor of\n\u221a\n252), we get a maximal Sharpe of\napproximately SRmax \u224811.225, a level generally well beyond what is attained in practice.\nThe statistics, \u03b3max\n3\nand \u03b3max\n4\ndo not scale when annualized, but are still large irrespective\nof the time horizon.\nWe note that our assumption of normality could easily be relaxed by considering non-\nlinear transforms of the signals X with the end-result that the maximal Sharpe Ratio bounds\nare relaxed. While this is beyond the scope of the current paper, we note that it is easy to\nshow that simple non-linear strategies, going long one unit if the signal is above a threshold\nk and short one unit if it below \u2212k, i.e., fk(X) = 1X>k \u22121X<k can be shown to have\narbitrarily large Sharpe Ratios, depending on the choice of threshold, k. The probability of\ninitiating such an arbitrarily high Sharpe ratio trade likewise decreases to being negligible.\n5In this case, it is also known that tr(M) = tr(L) where L = (Z\u2032Z \u2212\u03c32\nk+1I)\u22121Z\u2032Z and we know that\nthe singular values of \u03c3(L) = {\u03bb2\ni /(\u03bb2\ni \u2212\u03c32\nk+1)} where \u03bbi are the singular values of Z (or correspondingly,\n\u03bb2\ni are the singular values of Z\u2032Z), and \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbk > 0 ([Leyang, 2012]). By the Wilkinson interlacing\ntheorem, \u03bbk \u2265\u03c3k+1 \u22650 (see [Rahman and Yu, 1987]). Consequently,\ntr(MT LS) =\nX\ni\n\u03bb2\ni\n(\u03bb2\ni \u2212\u03c32\nk+1) \u2265k = tr(MOLS)\nwith equality i\ufb00\u03c32\nk+1 = 0 (i.e., when there the R2 = 100% and consequently, OLS and TLS coincide). In\nother words, tr(MT LS) \u2265tr(MOLS).\n12",
    "chunk_index": 11,
    "start_char": 28930,
    "end_char": 31775,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "Z (or correspondingly,\n\u03bb2\ni are the singular values of Z\u2032Z), and \u03bb1 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbk > 0 ([Leyang, 2012]). By the Wilkinson interlacing\ntheorem, \u03bbk \u2265\u03c3k+1 \u22650 (see [Rahman and Yu, 1987]). Consequently,\ntr(MT LS) =\nX\ni\n\u03bb2\ni\n(\u03bb2\ni \u2212\u03c32\nk+1) \u2265k = tr(MOLS)\nwith equality i\ufb00\u03c32\nk+1 = 0 (i.e., when there the R2 = 100% and consequently, OLS and TLS coincide). In\nother words, tr(MT LS) \u2265tr(MOLS).\n12\n\nThus, stationary returns with a small non-zero autocorrelation can lead to violations of\nHansen-Jagannathan (or good deal bounds).\nNoticeable as well from these formulas is that, while Sharpe and skewness may change\nsign, kurtosis is always bounded below and takes a minimum value of 9 (i.e., an excess kur-\ntosis of 6). Normality of the resulting strategy returns is not a good underlying assumption,\nsince the theoretical value of the Jarque-Bera test would be, at\nJB(n)\n=\nn \u2212k + 1\n6\n(\u03b32\n3 + (\u03b34 \u22123)2\n6\n)\n\u2265\n(n \u2212k + 1)\n6\n(36\n4 )\n=\n1.5(n \u2212k + 1)\nand this is asymptotically \u03c72(2) (i.e., rejection of normality at a 0.99 con\ufb01dence interval of\nJB > 9.210). Theoretically, we would need a relatively small sample to be able to reject\nnormality.\n3\nRe\ufb01ned Standard Errors\nGiven that we have closed-form estimates of a number of relevant statistics for dynamic\nlinear strategies, it makes sense to consider the e\ufb00ects of estimation error upon quantities\nsuch as the Sharpe ratio. Many analysts and traders who consider dynamic strategies in\npractice will consider altering the strategies on an ongoing basis, and are typically in a\nquandary over whether the observed change in Sharpe ratio or skewness, when they make\nchanges to their strategies, are in fact statistically signi\ufb01cant.\n3.1\nStandard Errors for Sharpe Ratios\nWhile there are formulas for standard errors for Sharpe ratios of generic assets, these are\nnot speci\ufb01c to Sharpe ratios generated by dynamic trading strategies, and as a consequence,\nthere is some possibility of re\ufb01ning them.\nWe refer to [Pav, 2016] for an exhaustive overview of the mechanics of Sharpe ratios,\nand in particular, Section 1.4, quoting many of the known results about standard errors.\nSpeci\ufb01cally, we look to [Lo, 2002] for large-sample estimates of standard errors for Sharpe\nratios of generic assets, given the asymptotic normality of returns. For a sample of size N\nand IID returns, he obtains the large-sample distribution,\nc\nSR \u223cN\n\u0000SR, stderr2\nLo\n\u0001\n,\nso a standard error, stderrLo =\nq\n(1 + 1\n2 SR2)/T which he suggests should be approximated\nusing standard error\nq\n(1 + 1\n2 c\nSR\n2)/T.\nWhile Lo\u2019s estimates may be appropriate for generic assets, for Sharpe ratios derived from\ndynamic strategies, we have a somewhat more re\ufb01ned characterisation of the variability of\nthe estimated Sharpe ratios. With correlated Gaussian signals and returns, we derive the\nfollowing result\nCorollary 3.1 (Stderrs). For returns Rt \u223cN (0, \u03c32\nR) and signal Xt \u223cN (0, \u03c32\nX) with\n13",
    "chunk_index": 12,
    "start_char": 31392,
    "end_char": 34274,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "1\n2 SR2)/T which he suggests should be approximated\nusing standard error\nq\n(1 + 1\n2 c\nSR\n2)/T.\nWhile Lo\u2019s estimates may be appropriate for generic assets, for Sharpe ratios derived from\ndynamic strategies, we have a somewhat more re\ufb01ned characterisation of the variability of\nthe estimated Sharpe ratios. With correlated Gaussian signals and returns, we derive the\nfollowing result\nCorollary 3.1 (Stderrs). For returns Rt \u223cN (0, \u03c32\nR) and signal Xt \u223cN (0, \u03c32\nX) with\n13\n\ncorrelation \u03c1, and sample size T, the standard errors are given by\nstderrimplied\n=\n1\n(\u02c6\u03c12+1)3/2\nq\n1\u2212\u02c6\u03c12\nT \u22122\n(8)\n\u2248(1 \u2212c\nSR\n2)\nq\n1\u22122 c\nSR\n2\nT \u22122\n(9)\nfor |c\nSR| <\n\u221a\n2/2.\nProof. As is well known, for a bivariate Gaussian process of sample size T, the distribution\nfor the sample (Pearson) correlation is given by\n\u02c6\u03c1 \u223cf\u03c1(\u02c6\u03c1) = (T \u22122)(1 \u2212\u03c12)(T \u22121)/2(1 \u2212\u02c6\u03c12)(T \u22124)/2\n\u03c0\nZ \u221e\n0\ndw\n(cosh(w) \u2212\u03c1\u02c6\u03c1)T \u22121\n(10)\nThe standard errors which approximate those in equation (10) for \u02c6\u03c1 are\nstderr\u03c1 =\nr\n1 \u2212\u02c6\u03c12\nT \u22122\n(attributed to Sheppard, and used by Pearson, see, e.g., [Hald, 2008]). Taken together with\nthe results of Theorem 2.3, we apply the delta method to \ufb01nd that the resulting standard\nerrors for our plug-in estimate for the Sharpe ratio, c\nSR =\n\u02c6\u03c1\n\u221a\n\u02c6\u03c12+1 is given by\nstderrimplied\n=\n\u2202c\nSR\n\u2202\u02c6\u03c1 \u00b7 stderr\u03c1\n=\n1\n(\u02c6\u03c12 + 1)3/2\nr\n1 \u2212\u02c6\u03c12\nT \u22122 .\nwhich gives us equation (8). If we solve for \u02c6\u03c1 in terms of c\nSR, we are able to derive equation\n(9).\nWe note that in spite of the fact that Lo\u2019s standard errors are very near our estimates\nfor large sample size, the entire sampling distribution from our estimates are much more\nconcentrated than the N (0, stderr2\nLo), potentially leading to tighter con\ufb01dence intervals at\nthe 99% or higher con\ufb01dence levels. We can see that the tail of the distribution given by Lo\nis much fatter than ours, in \ufb01gure (6).\nMertens gives a re\ufb01nement of Lo\u2019s result ([Mertens, 2002]) by including adjustments for\nskewness and excess kurtosis:\nstderr2\nMertens =\n\u00001 + 1\n2\n\u02c6\nSR\n2 \u2212\u03b33 \u00b7 \u02c6\nSR + \u03b34 \u22123\n4\n\u00b7 \u02c6\nSR\n2\u0001\n.\n(11)\nIf we use our plug-in estimates for skewness and excess kurtosis (i.e., coming from equations\n(6 and 7)) into equation (11) we are able to \ufb01nd a modestly tighter estimate of the standard\nerror than Lo. For most smaller amplitude correlations, this estimate comes very close to our\nestimate of standard error (see \ufb01gure (7)) and for small N and low correlations, Lo\u2019s standard\nerrors are in fact tighter. For large correlations, our standard errors are signi\ufb01cantly tighter.\nFor large sample sizes, there is little di\ufb00erence between them. Using our estimates for \u03b33\nand \u03b34, Mertens\u2019 approximation is always tighter than Lo\u2019s; in particular for correlations\n|\u03c1| < 0.5, Mertens\u2019 approximation appears almost identical to our own.",
    "chunk_index": 13,
    "start_char": 33805,
    "end_char": 36525,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "amplitude correlations, this estimate comes very close to our\nestimate of standard error (see \ufb01gure (7)) and for small N and low correlations, Lo\u2019s standard\nerrors are in fact tighter. For large correlations, our standard errors are signi\ufb01cantly tighter.\nFor large sample sizes, there is little di\ufb00erence between them. Using our estimates for \u03b33\nand \u03b34, Mertens\u2019 approximation is always tighter than Lo\u2019s; in particular for correlations\n|\u03c1| < 0.5, Mertens\u2019 approximation appears almost identical to our own. Irrespective, we\nargue in section 5 that our standard errors are more appropriate for dynamic strategies if\nthere is any signi\ufb01cant di\ufb00erence between the measures.\n14\n\nFigure 5: Sharpe ratio and Con\ufb01dence Interval Comparisons, based on di\ufb00erent\nsample sizes.\nWe note that the implied con\ufb01dence intervals are within Lo\u2019s, although\nprimarily for larger predictive power.\nFigure 6: Sharpe ratios full distribution While the 95th percentile shows close agreement\nbetween Lo\u2019s large-sample standard errors and implied standard errors, the distribution of\nimplied is far more fat-tailed.\n3.2\nStandard Errors for Higher Moments\nUsing exactly the same procedure, we can easily derive standard errors for both skewness\nand kurtosis. In terms of classical con\ufb01dence intervals, we consider [Joanes and Gill, 1998]\nand [Cram\u00e9r, 1946] which apply to Gaussian (and non-Gaussian distributions), noting that\n[Lo, 2002] is a broader result on the large-sample limits of Sharpe Ratios. We are concerned\nwith Pearson skewness and kurtosis, i.e.,\n\u03b33\n= \u00b53\nm u3/2\n2\n\u03b34\n= \u00b54\n\u00b52\n2\nalthough it is not hard to consider other de\ufb01nitions of skewness and kurtosis using unbiased\nestimators of the moments as are given in [Joanes and Gill, 1998], in this case originally\nfrom [Cram\u00e9r, 1946]. Given these de\ufb01nitions, under the assumption of normality for the\nunderlying returns (or correspondingly, using large-sample limits) where the sample size is\nT, standard errors are given as\nstderr\u03b33\n=\nq\n6(T \u22122)\n(T +1)(T +3)\nstderr\u03b34\n=\nq\n24T (T \u22122)(T \u22123)\n(T +1)2(T +3)(T +5)\nIn the case of dynamic strategies, using our assumption of normal signal and normal\nreturns, we are able to derive the following:\n15\n\nFigure 7: Standard errors based on di\ufb00erent sample sizes and formulas. Ignoring\nparameter uncertainty, Merten\u2019s adjustment to Lo\u2019s standard errors improves standard errors\nto be nearly as tight as implied. In practice, parameter uncertainty hurts the performance.\nCorollary 3.2 (Higher moment standard errors). For returns Rt \u223cN (0, \u03c32\nR) and signal\nXt \u223cN (0, \u03c32\nX) with correlation \u03c1, and sample size T, the standard errors are given by6\nstderr\u03b33\n= \u22126(\u02c6\u03c12\u22121)\n(\u02c6\u03c12+1)5/2 \u00b7\nq\n1\u2212\u02c6\u03c12\nT \u22122\nand\nstderr\u03b34\n= \u221248\u02c6\u03c1(\u02c6\u03c12\u22121)\n(\u02c6\u03c12+1)3\n\u00b7\nq\n1\u2212\u02c6\u03c12\nT \u22122\nfor |\u02c6\u03c1| < 1.\nWe rely on the delta-method, recognizing that stderr\u03b3k = \u2202\u03b3k/\u2202\u03c1 \u00b7 stderr\u03c1 for k = 3, 4.\nGiven the following easily calculated derivatives:\n\u2202\u03b33\n\u2202\u03c1\n= \u22126(\u03c12\u22121)\n(\u03c12+1)5/2\n(12)\n\u2202\u03b34\n\u2202\u03c1\n= \u221248\u03c1(\u03c12\u22121)\n(\u03c12+1)3\n(13)\nAs we can tell from the formulas in corollary (3.2), the derived standard errors for both\nskewness and kurtosis collapse to zero when \u03c1 = 1.",
    "chunk_index": 14,
    "start_char": 36018,
    "end_char": 39093,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "\u00b7\nq\n1\u2212\u02c6\u03c12\nT \u22122\nand\nstderr\u03b34\n= \u221248\u02c6\u03c1(\u02c6\u03c12\u22121)\n(\u02c6\u03c12+1)3\n\u00b7\nq\n1\u2212\u02c6\u03c12\nT \u22122\nfor |\u02c6\u03c1| < 1.\nWe rely on the delta-method, recognizing that stderr\u03b3k = \u2202\u03b3k/\u2202\u03c1 \u00b7 stderr\u03c1 for k = 3, 4.\nGiven the following easily calculated derivatives:\n\u2202\u03b33\n\u2202\u03c1\n= \u22126(\u03c12\u22121)\n(\u03c12+1)5/2\n(12)\n\u2202\u03b34\n\u2202\u03c1\n= \u221248\u03c1(\u03c12\u22121)\n(\u03c12+1)3\n(13)\nAs we can tell from the formulas in corollary (3.2), the derived standard errors for both\nskewness and kurtosis collapse to zero when \u03c1 = 1.\nWhile we can solve for \u03c1 in terms of \u03b3k for k = 3, 4, the formulas are not easy to present\n(especially for kurtosis) and we believe that the statement, in terms of correlation is easier\nto use.\nWe note that, unlike the argument for using our re\ufb01ned standard errors over those\npresented in [Lo, 2002], the rationale for using the skewness and kurtosis standard errors\npresented in equations (12) is that returns are, for most practical purposes, not close to nor-\nmal, and the product of two normals is more relevant for dynamic strategies. We elaborate\non this in Section 5.\n4\nMultiple assets\nWe consider whether there is a diversi\ufb01cation bene\ufb01t from adding more independent bets to\nour portfolio, and to what extent we can bene\ufb01t from this. For context we note that port-\nfolios of dynamic strategies can behave very di\ufb00erently from single strategies. For instance,\n6 While \u03c1 can be expressed in terms of either \u03b33 or \u03b34 to eliminate \u03c1 from these expressions, unlike the\ncase of the standard errors of the Sharpe ratio, the expressions are too complicated to be that useful.\n16\n\nFigure 8: Standard errors for skewness for di\ufb00erent sample sizes, implied vs\nGaussian Implied standard errors, especially for skewness are generally larger than those\nfor normal distributions. We argue that the implied standard errors are more appropriate\nfor dynamic strategies.\nFigure 9: Standard errors for kurtosis for di\ufb00erent sample sizes, implied vs Gaus-\nsian Implied kurtosis standard errors are sometimes larger and sometimes tighter than the\nGaussian case. We argue that the implied standard errors are more appropriate for dynamic\nstrategies.\n17\n\nHo\ufb00man-Kaminski have noted ([Ho\ufb00man-Kaminski, 2016]) that while single strategies can\nhave skewness ranging from around [1.3, 1.7] and kurtosis from [8.8, 15.3], portfolio skewness\ncan be as low as 0.1.\nWe \ufb01rst consider N indepedent returns as an N-vector, Rt \u223cN (0, \u03c32I), assumed to\nhave the same variance. We devise signals Xt \u223cN (0, \u03b32I). The inner-product Xt \u00b7 Rt has\na density \u03c8 whose moment generating function is given by [Simons, 2006]:\nMN(t) = (1 \u22122t\u03c3\u03b3\u03c1 \u2212\u03c32\u03b32t2(1 \u2212\u03c12))\u2212N/2.\nFrom this we can easily derive four moments:\n\u00b51, =\nN\u03c3\u03b3\u03c1\n\u00b52 =\nN\u03c32\u03b32((N + 1)\u03c12 + 1)\n\u00b53 =\nN(N + 2)\u03c33\u03b33\u03c1((N + 1)\u03c12 + 3)\n\u00b54 =\n\u03c34\u03b34\u0010\n(N + 6)(N + 4)(N + 2)N\u03c14 + 3(N + 2)N(1 \u2212\u03c12)2 +\n6(N + f4)(N + 2)N\u03c12(1 \u2212\u03c12)\n\u0011\nThis leads to centralized moments\n\u03c32 = N(\u03c12 + 1)\nand\n\u00b5c\n3 = 2N\u03c1(\u03c12 + 3)\nFrom",
    "chunk_index": 15,
    "start_char": 38667,
    "end_char": 41496,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "function is given by [Simons, 2006]:\nMN(t) = (1 \u22122t\u03c3\u03b3\u03c1 \u2212\u03c32\u03b32t2(1 \u2212\u03c12))\u2212N/2.\nFrom this we can easily derive four moments:\n\u00b51, =\nN\u03c3\u03b3\u03c1\n\u00b52 =\nN\u03c32\u03b32((N + 1)\u03c12 + 1)\n\u00b53 =\nN(N + 2)\u03c33\u03b33\u03c1((N + 1)\u03c12 + 3)\n\u00b54 =\n\u03c34\u03b34\u0010\n(N + 6)(N + 4)(N + 2)N\u03c14 + 3(N + 2)N(1 \u2212\u03c12)2 +\n6(N + f4)(N + 2)N\u03c12(1 \u2212\u03c12)\n\u0011\nThis leads to centralized moments\n\u03c32 = N(\u03c12 + 1)\nand\n\u00b5c\n3 = 2N\u03c1(\u03c12 + 3)\nFrom these we derive the Sharpe ratio:\nSR =\n\u221a\nN\u03c1\np\n\u03c12 + 1\nMaximizing the SR over \u03c1 leads to\n\u221a\nN\n\u221a\n2\n2\n, clearly showing the bene\ufb01t of diversi\ufb01cation\nwhen measuring the Sharpe ratio.\nThe skewness is\n\u03b33 =\n1\n\u221a\nN\n2\u03c1(\u03c12 + 3)\n(\u03c12 + 1)3/2\nand if we consider maximal Sharpe, the corresponding skewness is\n\u03b3max\n3\n=\n8N\n(2N)3/2 = 2\n\u221a\n2\n\u221a\nN\nwill show reductions on the order of 1/\n\u221a\nN in the total number of (orthogonal) assets. This\nis as expected from large diverse portfolios. In the limit, simple application of central limit\ntheory should give us asymptotic normality. E\ufb00ectively, introducing more purely orthogonal\nassets will increase Sharpe ratios, but decreases the (relatively desirable) positive skewness.\nIf we have multiple possibly correlated assets and multiple, possibly correlated signals, we\nassert that an optimal strategy would be to perform canonical correlation analysis (CCA),\n18\n\n7 resulting in a set of decorrelated strategies (using a and combination of signals to weight a\nportfolio of assets). The resulting strategies are decorrelated but with unequal returns and\nvariances. Many results of this section would apply after scaling the portfolio returns. The\nend-result could easily be optimized using simple mean-variance analysis (reweighting the\nreturns on the independent strategies). We leave the details for another study.\nWhile our optimizer is unlikely to be in use among CTAs, it is still notable that widely\ndiversi\ufb01ed CTAs (irrespective of underlying asset correlations) appear to have decent Sharpe\nratios but relatively lower positive skewness, much in line with the discussion of this section.\nOur simple results here about the \ufb01nal Sharpe ratio and skewness of course depend on\nindependence of the underlying assets and of course the signals themselves, which must only\nbe correlated with their respective asset returns. While this is a not an altogether natural\nsetting, it is suggestive of the gains that can be made in introducing purely orthogonal sources\nof risk, or perhaps in orthogonalizing (or attempting to) asset returns prior to forming\nsignals, later recombining into a portfolio, and that this may lead to far more desirable\nproperties of portfolios than \ufb01nding strategies on multiple non-orthogonalized assets.\n5\nGaussian Returns vs Products of Gaussians Returns\nWhile we believe that the assumption of Gaussian returns (and Gaussian signal) is a simpli\ufb01-\ncation, we also believe this is far more realistic than the assumption of Gaussian returns for a\ndynamic strategy.",
    "chunk_index": 16,
    "start_char": 41141,
    "end_char": 44007,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "forming\nsignals, later recombining into a portfolio, and that this may lead to far more desirable\nproperties of portfolios than \ufb01nding strategies on multiple non-orthogonalized assets.\n5\nGaussian Returns vs Products of Gaussians Returns\nWhile we believe that the assumption of Gaussian returns (and Gaussian signal) is a simpli\ufb01-\ncation, we also believe this is far more realistic than the assumption of Gaussian returns for a\ndynamic strategy. Throughout this paper we consider Gaussian (log) returns R \u223cN (0, \u03c32\nR)\nand Gaussian signal X \u223cN (0, \u03c32\nX) which together are jointly Gaussian, and together form\ncomponents of the dynamic strategy St = XtRt, whose properties we study.\nTo be clear, our signal is not considered to have foresight and is fully known as of time\nt, while the return Rt is from t to t + \u03b4t. All expectations calculated are unconditional, or,\ncan be thought of as conditioned on t0 < t < t + \u03b4t. Consequently, each element, the signal\nand the return will be random variables.\nWere we to consider expectations conditional on t, then the resulting strategy returns St\nwould be trivially Gaussian. In the unconditional case, the resulting returns are far more\ninteresting and relevant.\nCTA returns are known to generally be positively skewed and highly kurtotic over the\nrelevant horizons we are concerned with (i.e., daily, weekly, monthly), as has been noted\nby [Potters and Bouchaud, 2005], [Ho\ufb00man-Kaminski, 2016] and others. If we measure far\n7Canonical correlation (from [Hotelling, 1936], see for example, [Rencher and Christiansen, 2012]) is de-\n\ufb01ned by \ufb01rst \ufb01nding the linear vectors w1 and v1 withe |w1| = |v1| = 1, such that \u03c1(w1 \u00b7 R, v1 \u00b7 X) is\nmaximized. The resulting correlation is the canonical correlation. The canonical variates are de\ufb01ned by\n\ufb01nding subsequent unit-vectors wk and vk such that \u03c1(wk \u00b7 R, wj \u00b7 R) = \u03b4kj, \u03c1(vk \u00b7 X, vj \u00b7 X) = \u03b4kj, and\n\u03c1(wk \u00b7 R, vk \u00b7 X) is maximized, leading to \u03c1(wk \u00b7 R, vj \u00b7 X) = rk\u03b4kj .\nThe solution is via a generalized\neigenvalue problem\n\u03a3\u22121\nRR\u03a3RX\u03a3\u22121\nXX\u03a3XRwk\n=\nr2\nkwk\n\u03a3\u22121\nXX\u03a3XR\u03a3\u22121\nRR\u03a3RXvk\n=\nr2\nkvk\nwhere \u03a3 is the partitioned correlation matrix of (R, X) and the canonical correlates wk and vk are the\neigenvectors with the same eigenvalues rk. The corresponding portfolios of canonical strategies, SCCA\nk\n\u2261\n(vk \u00b7 X)(wk \u00b7 R) each have returns and variances as characterised by equation (1 and 2) with corresponding\ncorrelations rk (i.e., with Sharpe ratios given by SR[Sk] = rk/\nq\nr2\nk + 1) and, due to their independence,\ncan easily be weighted to optimize the portfolio Sharpe Ratio. The method of weighting the cannonical\nstrategies is of course, similar to a risk-parity portfolio, due to the independence of asset returns. We assert\nthat this method gives the maximal Sharpe ratio for the linear combination of signals and returns, although\nwe leave this proof to a subsequent paper.\n19",
    "chunk_index": 17,
    "start_char": 43563,
    "end_char": 46424,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "by SR[Sk] = rk/\nq\nr2\nk + 1) and, due to their independence,\ncan easily be weighted to optimize the portfolio Sharpe Ratio. The method of weighting the cannonical\nstrategies is of course, similar to a risk-parity portfolio, due to the independence of asset returns. We assert\nthat this method gives the maximal Sharpe ratio for the linear combination of signals and returns, although\nwe leave this proof to a subsequent paper.\n19\n\nlonger-horizon returns, asymptotic theory should show that favourable qualities like skewness\nmay disappear.\nConsequently, even though we make many comparisons to results stemming from either\nasymptotic theory (e.g., [Lo, 2002]) or using exact normality, this comparison does not, in\nfact, compare like-for-like. Clearly [Lo, 2002] is appropriate for large-samples, as is possible\nunder conditions when the central limit theorem (CLT) holds, e.g., with weak-dependence,\nsumming returns over increasingly longer horizons, or in the case of a large cross-sectional\ndimension with increasing numbers of decorrelated assets. For dynamic strategies, asymp-\ntotic normality should be expected for large numbers of decorrelated dynamic strategies as\nwell as for long-horizon (e.g., annual or longer, non-overlapping) returns for single dynamic\nstrategies.\nConsequently, we believe our standard error results are more appropriate for hypothesis\ntesting on statistics for dynamic strategies. We discuss a strategy for establishing product\nmeasures as large-sample limits in appendix A, although asymptotics are beyond the scope\nof this current study.\n6\nConclusion\nFully systematic dynamic strategies are used by a large portion of the asset management\nindustry as well as by many non-institutional participants. Meanwhile, they are only partly\nunderstood.\nMany funds and strategies (e.g., especially investment bank smart-beta or\nstyles-based products) involve investment in strategies which are not optimised in any sense.\nStrategies which are paid via index-swaps have great limits in terms of their adaptability,\nleading to often highly suboptimal end-results. While there have been some very signi\ufb01cant\nresults derived in the theoretical properties of these dynamic strategies, there is still much\nmore work left to do. Given that most academic literature in this area considers more general\ndistributions, there has not been a \ufb01rm foundation to build and extend these results.\nIt is hoped that this paper does form a foundational approach to the study of dynamic\nstrategies and how to optimize them. We make e\ufb00orts to understand their properties without\nclaiming to understand why they work (i.e., why there are stable ACFs in the \ufb01rst place).\nGiven that most asset returns returns are known to have non-trivial autocorrelations, we\ncan establish many results. In particular, we have derived a number of results merely by\napplying well-known techniques to dynamic strategies, e.g.,:\n\u2022 Strategy returns can be shown to be positively skewed and leptokurtic.\n\u2022 Sharpe ratios can be characterized, as can skewness and kurtosis.\n\u2022 The standard errors for Sharpe, skewness and kurtosis can be derived.\n\u2022 Strategies designed to optimise Sharpe ratios should be based on TLS rather than\nminimizing prediction error.\n\u2022 Gains from adding orthogonal assets/risks can be quanti\ufb01ed.",
    "chunk_index": 18,
    "start_char": 45996,
    "end_char": 49291,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "particular, we have derived a number of results merely by\napplying well-known techniques to dynamic strategies, e.g.,:\n\u2022 Strategy returns can be shown to be positively skewed and leptokurtic.\n\u2022 Sharpe ratios can be characterized, as can skewness and kurtosis.\n\u2022 The standard errors for Sharpe, skewness and kurtosis can be derived.\n\u2022 Strategies designed to optimise Sharpe ratios should be based on TLS rather than\nminimizing prediction error.\n\u2022 Gains from adding orthogonal assets/risks can be quanti\ufb01ed.\nSome of these items are empirically well-known, but others are genuinely new. Meanwhile,\nwe have extended our results to the derivation of over-\ufb01tting penalties akin to Mallow\u2019s Cp\nor AIC and can be used to do model selection and predict likely out-of-sample Sharpe ratios\nfrom in-sample \ufb01ts (see [Koshiyama and Firoozye, 2018]).\nOur study is incomplete. We believe that there is a good deal of interesting work to be\ndone in areas such as:\n20\n\n\u2022 optimal linear strategies incorporating transaction costs.\n\u2022 optimal linear strategies relaxing normality.\n\u2022 normalized linear signals (e.g., z-scores) and optimal non-linear functions of z-scores.8\n\u2022 non-linear strategies which are optimised to speci\ufb01c utility functions, possibly incor-\nporating smoothness constraints, especially when relaxing normality.\n\u2022 local optimality when relaxing stationarity.\n\u2022 good-deal bounds in the presence of auto-correlated assets with possible non-stationarity\nor structural breaks.\nWe note that our assumptions were never meant to be completely realistic: stationary\nreturns with \ufb01xed ACF and Gaussian innovations can only work in theory, not in reality.\nMany quantitative traders design strategies to overcome the challenges of dealing with real-\nworld data issues and the issues of over-\ufb01tting. We nonetheless present them as a good\nstarting point for further analysis, hoping to use this work as the basis for further exploration\nand to put the general study of dynamic strategies onto a more \ufb01rm theoretical footing.\nSome of our \ufb01ndings should be of note to practitioners. In particular, the use of OLS\nand other forecast error minimizing methods is not necessarily optimal, depending on the\nproblem at hand; total-least squares or other correlation-maximizing methods such as CCA\nmay be more e\ufb03cient. High Sharpe ratios and positive skewness are often quoted as ratio-\nnales for entering into strategies and, strategies are changed with the rationale of increasing\nthese measures. The relative signi\ufb01cance of any of these changes depends on con\ufb01dence in-\ntervals or standard errors, and we have derived these speci\ufb01cally suited for dynamic trading\nstrategies. Kurtosis is not studied as often, but as we show, all dynamic strategies should\nbe leptokurtic and this is an important attribute of these strategies. Other results, such\nas over-\ufb01tting penalties and optimal non-linear strategies, we save for later papers. With\na more solid theoretical footing as a sort of rule-of-thumb for the development, optimisa-\ntion, selection and alteration of dynamic strategies, we only hope that there can be room to\nimprove strategy design.\nAcknowledgements\nN. Firoozye would like to give his wholehearted love and appreciation to Fauziah, for hanging\non, when the paper was always almost done.",
    "chunk_index": 19,
    "start_char": 48786,
    "end_char": 52065,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "and this is an important attribute of these strategies. Other results, such\nas over-\ufb01tting penalties and optimal non-linear strategies, we save for later papers. With\na more solid theoretical footing as a sort of rule-of-thumb for the development, optimisa-\ntion, selection and alteration of dynamic strategies, we only hope that there can be room to\nimprove strategy design.\nAcknowledgements\nN. Firoozye would like to give his wholehearted love and appreciation to Fauziah, for hanging\non, when the paper was always almost done. I am hoping the wait is \ufb01nally over. Adriano\nSoares Koshiyama would like to to acknowledge the funding for its PhD studies provided by\nthe Brazilian Research Council (CNPq) through the Science Without Borders program.\nThe authors would also like to thank Brian Healy and Marco Avellaneda for the many\nsuggestions and encouragement.\nFinally, were it not for the product design method as\npractised by Nomura\u2019s QIS team, the authors would never have been inspired to pursue a\nmathematical approach to this topic.\n\u2014\u2014\u2014\u2014\u2014\u2013\n8We note that normalized signals applied to normalized returns series can be represented as the product\nof two Student t-distributions, which is also relatively well-studied [Nadrajah-Kotz, 2003, Joarder, 2007]\nand the results are qualitatively very similar to those which we have produced in this study. However, the\nmore commonly used strategy of applying normalized signals to returns, with the resulting strategies then\nvol-scaled, cannot be derived as a trivial application of well-known results\n21\n\nA\nFull distributions for single period\nIn general, for X and R having joint density \u03c8X,R(x, r), and have St = XtRt is known to\nhave the product pdf,\n\u03c8S(s) =\nZ \u221e\n\u2212\u221e\n\u03c8X,R\u0000x, s\nx\n\u0001 1\n|x|dx\n(14)\nand, in the special case where X \u223cN (0, \u03c32\nX) and R \u223cN (0, \u03c32\nR) jointly normal with\ncorrelation \u03c1 (i.e., \u03c8 being a bivariate gaussian), this results in the closed-form expression:\nps =\n1\n\u03c0\u03c3R\u03c3X\nexp\n\u0000\u03c1s\n\u03c3R\u03c3X(1 \u2212\u03c12)\n\u0001\nK0\n\u0000|s|\n\u03c3R\u03c3X(1 \u2212\u03c12)\n\u0001\n(15)\nwhere K0(\u00b7) is a modi\ufb01ed Bessel function of the 2nd kind ([Simons, 2006], p 51, eq 6.15).\nThe more general density for non-zero means, is given in [Cui et al., 2016] as an in\ufb01nite\nseries. In the special cases of independence and of correlated but zero mean, the expressions\nbecome much simpler and we choose to focus on the zero-mean case here. The density is\nunbounded at zero and has fat tails and positive skewness, becoming more pronounced with\nhigher correlation. We can see the distribution for a variety of correlations in \ufb01gure (10),\nwith the skewness becoming increasingly pronounced for higher \u03c1. In the limit as \u03c1 \u21921 the\ndistribution converges to that of the central \u03c72 distribution with one degree of freedom.\nFigure\n10:\nComplete\nproduct\ndistributions,\nfor\n\u03c1\n\u2208\n{0, 0.2, 0.4, 0.6., 0.8, 1.0}, normalised to have unit variance, so they\ncan be depicted on one plot. Note the singularity at 0, the increasing\nasymmetry, nearly truncated left-tails and marginally fatter right-tails\nwith increasing \u03c1.",
    "chunk_index": 20,
    "start_char": 51536,
    "end_char": 54530,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "correlations in \ufb01gure (10),\nwith the skewness becoming increasingly pronounced for higher \u03c1. In the limit as \u03c1 \u21921 the\ndistribution converges to that of the central \u03c72 distribution with one degree of freedom.\nFigure\n10:\nComplete\nproduct\ndistributions,\nfor\n\u03c1\n\u2208\n{0, 0.2, 0.4, 0.6., 0.8, 1.0}, normalised to have unit variance, so they\ncan be depicted on one plot. Note the singularity at 0, the increasing\nasymmetry, nearly truncated left-tails and marginally fatter right-tails\nwith increasing \u03c1.\nIn fact, K0(z) = O(e\u2212z/\u221az) for z \u2192\u221eand we can see that the tail behaviour of the\npdf in equation (15) changes quite signi\ufb01cantly from when \u03c1 = 0 and K0(z) is the only term\n22\n\nto consider, to when \u03c1 > 0, introducing an asymmetry. The Bessel function is unbounded\nat z = 0. Asymptotically, we have the following behaviour:\nps =\nO(e\u2212|s|/\np\n|s|)\nfor \u03c1 = 0, |s| \u2192\u221e\nps =\nO(e\u2212s/\np\n|s|)\nfor \u03c1 > 0, s \u2192\u221e\nps =\nO(es/\np\n|s|)\nfor \u03c1 > 0, s \u2192\u2212\u221e\nps =\nO(\u2212log |s|)\nfor |s| \u21920\nB\nConvolution Filters as Jointly Gaussian\nIf we have a purely-random mean-zero covariance-stationary discrete-time Gaussian process\nRt, we note by Wold Decomposition, that all stationary Gaussian processes can be repre-\nsented as MA(\u221e) in terms of Gaussian innovation process and coe\ufb03cients in l2, with no\ndeterministic component, i.e.,\nRt =\n\u221e\nX\nk=0\n\u03c6(k)\u03f5t\u2212k\nfor \u03f5 \u223cN(0, \u03c32), P\u221e\n0 \u03c62\nk < \u221eand \u03c6(0) = 1.\nMore speci\ufb01cally, we have\nE[Rt]\n=\n0\nV ar(Rt)\n=\n\u03c32\nR\ncorr(Rt, Rs)\n=\n\u03b3(t \u2212s)\n(i.e., with ACF \u03b3), and this would be su\ufb03cient to determine \u03c6 if we so wished.\nWe are interested in constructing signals: Xt. A standard signal we will consider is a\nconvolution signal, i.e.,\nXt =\nX\nk\u22651\n\u03c6(k)Rt\u2212k\nAll the signals mentioned in the introduction (e.g., moving average or di\ufb00erence of moving\naverages or ARMA based forecasts), can be expressed as convolutions with historic returns.\nA convolution \ufb01lter is an example of a time-invariant linear \ufb01lter. It the coe\ufb03cients \u03c6 \u2208l2\nthen it is well known that the resulting \ufb01ltered series Xt are Gaussian9. The \ufb01ltered series\nXt is also jointly Gaussian with Rt.\nE[Xt]\n=\n0\nV ar(Xt)\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)E[Rt\u2212kRt\u2212j]\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)\u03b3(k \u2212j)\u03c32\nR\n9see e.g., Gallagher, R, Stochastic Processes: Theory for Applications, 2014, (Cambridge UP: Cambridge),\nor Gallagher R, Principles of Digital Communications. MIT Open Coursework. Section 7.4.2, Theorem 7.4.1.\n23\n\n(dropping all \ufb01rst order terms because E[Rt] = 0 ) and,\ncorr(Rt, Xt)\n=\nE[P\nk\u22651 \u03c6(k)Rt\u2212k, Rt]\nstd(X)\u03c3R\n=\nP\nk\u22651 \u03c6(k)\u03b3(k)\n(P\nk,j\u22651 \u03c6(k)\u03c6(j)\u03b3(k \u2212j))1/2\ncancelling out all \u03c3R terms.\nConsequently,\nsgn(corr(Rt, Xt)) = sgn(\u03b3 \u00b7 \u03c6))\n(i.e., the sign of this in\ufb01nite inner product matters most for determining usefulness of a\ngiven convolution design).\nOf the signals mentioned in the introduction, EWMA and SMA in returns, di\ufb00erences\nof EWMAs and SMAs in returns, and forecasts from ARMA models are all examples of\nconvolution \ufb01lters with l2 coe\ufb03cients.",
    "chunk_index": 21,
    "start_char": 54036,
    "end_char": 56919,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "0 ) and,\ncorr(Rt, Xt)\n=\nE[P\nk\u22651 \u03c6(k)Rt\u2212k, Rt]\nstd(X)\u03c3R\n=\nP\nk\u22651 \u03c6(k)\u03b3(k)\n(P\nk,j\u22651 \u03c6(k)\u03c6(j)\u03b3(k \u2212j))1/2\ncancelling out all \u03c3R terms.\nConsequently,\nsgn(corr(Rt, Xt)) = sgn(\u03b3 \u00b7 \u03c6))\n(i.e., the sign of this in\ufb01nite inner product matters most for determining usefulness of a\ngiven convolution design).\nOf the signals mentioned in the introduction, EWMA and SMA in returns, di\ufb00erences\nof EWMAs and SMAs in returns, and forecasts from ARMA models are all examples of\nconvolution \ufb01lters with l2 coe\ufb03cients. Most signals constructed in levels (e.g., the di\ufb00erence\nbetween a price and its simple moving average), are not, in general, Gaussian, although a\ndi\ufb00erence between a price and one or more EWMAs may be Gaussian depending on the\ndata-generating process for the price series (i.e., for MA processes).\nOf course, a linear time-invariant \ufb01lter with l2 coe\ufb03cient is just one example of a signal\nXt which is jointly Gaussian with returns Rt. Similarly, if Zt is a set of Gaussian (exogenous)\nfeatures, then Xt = Zt\u03b2 will also be Gaussian and we will assume the Zt are jointly Gaussian\nwith Rt, meaning also Xt and Rt will be jointly Gaussian.\nC\nLimiting behaviour for convolution of stationary re-\nturns\nWe assert some asymptotic approximation results for dynamic strategies, only outlining\ntheir proof. Our claim is that this justi\ufb01es the use and analysis of product of Gaussian\ndistributions in stationary (or locally stationary) distributions. The proof itself is the direct\nconsequence of much more general work on the limits of quadratic forms by G\u00f6tze and\nTikhonov and by the Wold decomposition theorem.\nLetting \u03b7 be iid random variables with mean zero and unit variance, and letting \u03f5 be iid\nnormal random variables with zero mean and unit variance, we form the quadratic forms:\nQn =\nn\nX\nj,k=1\nan\njk\u03b7j\u03b7k and Gn =\nn\nX\nj,k=1\nan\njk\u03f5j\u03f5k.\nWe write the metric\n\u03b4n(Qn, Gn) = sup\nx |P{Qn \u2264x} \u2212P{Gn \u2264x}|.\nWe simplify the statement of Theorem 1 from [G\u00f6tze and Tikhomorov, 1999]:\nTheorem C.1 (Goetze-Tikhomirov). Let \u03b7 be IID with\nE\u03b7 = 0, E\u03b72 = 1, E|\u03b7|3 = \u03b23 < \u221e.\nThen there is a constant C such that\n\u03b4n(Qn, Gn) \u2264C\u03b22\n3\u0393n\nwhere \u0393n = max1\u2264j\u2264n\nPn\nk=1 |an\njk|.\n24\n\nOur assertion is a simple application of the results in [G\u00f6tze and Tikhomorov, 1999], (see\n[G\u00f6tze and Tikhomorov, 2002] and [G\u00f6tze et al., 2007] for further results) which applies to\nlimiting theorems of quadratic forms of random variables.\nTheorem C.2 (Products of Gaussians). Let Rt be a covariance stationary process with\nbounded 3rd moments and mean zero and its Wold decomposition given by Rt = P\u221e\ns=1 b(s)\u03b7(t\u2212\ns) with \u03b7 a white-noise process. Let the signal Xt be a convolution of the lagged returns Rt\nwith an L2 convolution kernel, \u03c6 and Xt = P\u221e\n1 \u03c6(s)Rt\u2212s. We let RN\nt\n= PN\n0 b(s)\u03b7(t \u2212s)\nand XN\nt\n= PN\n1 \u03c6(s)RN\nt\u2212s be truncated sums (only involving the \ufb01rst N terms),\nSN\nt =\n1\n\u221a\nN\nXN\nt \u00b7\n1\n\u221a\nN\nRN\nt\nbe the scaled truncated strategy returns.",
    "chunk_index": 22,
    "start_char": 56424,
    "end_char": 59319,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "a covariance stationary process with\nbounded 3rd moments and mean zero and its Wold decomposition given by Rt = P\u221e\ns=1 b(s)\u03b7(t\u2212\ns) with \u03b7 a white-noise process. Let the signal Xt be a convolution of the lagged returns Rt\nwith an L2 convolution kernel, \u03c6 and Xt = P\u221e\n1 \u03c6(s)Rt\u2212s. We let RN\nt\n= PN\n0 b(s)\u03b7(t \u2212s)\nand XN\nt\n= PN\n1 \u03c6(s)RN\nt\u2212s be truncated sums (only involving the \ufb01rst N terms),\nSN\nt =\n1\n\u221a\nN\nXN\nt \u00b7\n1\n\u221a\nN\nRN\nt\nbe the scaled truncated strategy returns.\nThen there is a pair of Gaussians \u02dcRN\nt and \u02dcXN\nt\n( \u02dcSN\nt = \u02dcXN\nt \u00b7 \u02dcRN\nt be the Gaussian strategy\nreturns)such that\n\u03b4n(SN\nt , \u02dcSN\nt ) \u21920,\nor, in other words, that the product of Gaussian approximation can be arbitrarily close to\nthe original strategy.\nWe note that the product St = XtRt is given by the quadratic form:\nSt = XtRt =< A\u03b7, \u03b7 >\nwhere A is the operator given by\nA(u, v) =\nX\ns\u2264t\u22121\n\u03c6(t \u2212s)b(s \u2212u)b(t \u2212v)\nfor u, v \u2264t.\nSN\nt\n=\n1\n\u221a\nN\nXN\nt \u00b7\n1\n\u221a\nN\nRN\nt\n=\n< AN\u03b7N, \u03b7N >\nwhere An is an n \u00d7 n matrix\nAn\nu,v = 1\nN\nX\ns\u2208[t\u2212n,t\u22121]\n\u03c6(t \u2212s)b(s \u2212u)b(t \u2212v),\nfor u, v ranging in [t \u2212n, t] and \u03b7N = {\u03b7s}s\u2208[t,t\u2212N].\nWe note that An is lower triangular with no diagonal terms (elements on the diago-\nnal correspond to instantaneously available knowledge, contemporaneous with the observed\nreturns themselves and elements in the upper triangle of the matrix correspond to direct\nforesight). Moreover, with su\ufb03cient conditions on the original series Rt (i.e., on the Wold\ncoe\ufb03cients b) and on the convolution coe\ufb03cients \u03c6, the \u0393N = max1\u2264j\u2264n\nPn\nk=1 |An\njk| can be\nshown to decay to zero.\nA direct application of the theory of quadratic forms would apply when the convolution\ncoe\ufb03cients are su\ufb03ciently well-behaved at in\ufb01nity.\n25\n\nThis is only one of the possible approaches to an asymptotic theory justifying the use of\nproducts of Gaussians.10 While asymptotic approaches are not the main point of this paper,\nit should be clear that products of Gaussians help to approximate the behaviour of a wide\narray of dynamic strategies.\nD\nNonzero means: Sharpe ratios and Skewness\nBy an abuse of notation, we de\ufb01ne SR[R] to be \u00b5R/\u03c3R and by an abuse of notation, we\nde\ufb01ne SR[X] = \u00b5X/\u03c3X (for X the signal),\nCorollary 1: If R \u223cN (\u00b5R, \u03c32\nR) and X \u223cN (\u00b5X, \u03c32\nX) then\nSR[X \u00b7 R] =\nSR[R] \u00b7 SR[X] + \u03c1\n(SR[R]2 + SR[X]2 + 2\u03c1 SR[R] \u00b7 SR[S] + \u03c12 + 1)1/2\nCorollary 2: If R \u223cN (\u00b5R, \u03c32\nR) and X \u223cN (\u00b5X, \u03c32\nX) then\n\u03b33[X \u00b7 R] =\n2\u03c1(\u03c12 + 3 + 3 SR[R]2 + 3 SR[X]2)\n(SR[R]2 + SR[X]2 + 2\u03c1 SR[R] \u00b7 SR[X] + \u03c12 + 1)3/2\nWe note the",
    "chunk_index": 23,
    "start_char": 58858,
    "end_char": 61301,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "signal),\nCorollary 1: If R \u223cN (\u00b5R, \u03c32\nR) and X \u223cN (\u00b5X, \u03c32\nX) then\nSR[X \u00b7 R] =\nSR[R] \u00b7 SR[X] + \u03c1\n(SR[R]2 + SR[X]2 + 2\u03c1 SR[R] \u00b7 SR[S] + \u03c12 + 1)1/2\nCorollary 2: If R \u223cN (\u00b5R, \u03c32\nR) and X \u223cN (\u00b5X, \u03c32\nX) then\n\u03b33[X \u00b7 R] =\n2\u03c1(\u03c12 + 3 + 3 SR[R]2 + 3 SR[X]2)\n(SR[R]2 + SR[X]2 + 2\u03c1 SR[R] \u00b7 SR[X] + \u03c12 + 1)3/2\nWe note the one period Sharpe ratio of the strategy may depend on both the interaction\nbetween the Sharpe ratios of the Signals (weights) and the Returns, in particular whether\nthey have the same sign or not, together with the sign of the correlation.\nIn fact, the\namplitude of the resulting strategy SR may be more dependent on the respective Sharpe\nratios rather than \u03c1 since after all, \u22121 \u2264\u03c1 \u22641,while SR[R] and SR[X] may individually be\nabove 1.\nE\nTransaction Costs\nThe sections above consider optimal linear strategies with no transaction costs. If we include\ntransaction costs then the formulas are not nearly as elegant, but the results may still remain\ntractable.\nMaximizing Sharpe ratios are often the result of maximizing a quadratic utility of returns,\ne.g.,\nU[XR] = E[XR] \u2212\u03b3 var[XR]\n(16)\nwhere \u03b3 is a measure of risk-aversion, sometimes called a Kelly constant. Extremals of the\nutility in equation (16) are known to coincide with maximal Sharpe ratios.\nWe only look at convolution \ufb01lter strategies, i.e., \u03c6 = (0, \u03c61, \u03c62, . . . , \u03c6K) which give a\ncorresponding signal as Xt = \u03c6 \u2217Rt = PK\n1 \u03c6kRt\u2212k. As we mentioned above, \ufb01tting \u03c6 via\nTLS instead of OLS is most appropriate in the case of no-transaction costs.\nIf we include transaction costs proportional to a constant \u03bd, rather than to maximize a\nquadratic utility in (16), we can add the extra term11, e.g.,\nU[XR] = E[XR] \u2212\u03b3 var[XR] \u2212\u03bdE[|\u2206X|]\n10Other approaches include assuming in\ufb01nitessimal Gaussian increments which are observed and \u201cstored\u201d\nand used in a convolution, then applied as a weight on a strategy which itself is held for a longer time. This\ne\ufb00ecftively results in some product of averages of returns and, obviously, when appropriately scaled can be\nshown to have a limit of a product of Gaussians.\n11Alternatively, a term such as E[|\u2206X| \u00b7 P] where P = P0 + P Rt could be added. Again, with work\nwe could equally well characterize this expectation, using properties of distributions derived from Gaussians\nand some application of Isserlis\u2019 theorem\n26\n\nGiven that\n\u2206X = \u03c61Rt\u22121 +\nK\nX\nk=1\n\u2206\u03c6kRt\u2212k \u2212\u03c6KRt\u2212K \u2261\u2206\u03c6 \u2217R\nwhere \u2206\u03c6 = (0, \u03c61, \u2206\u03c61, \u2206\u03c62, . . . , \u2206\u03c6K, \u2212\u03c6K). The r.v. is normal, \u2206X \u223cN(0, \u03c32\n\u2206X) and,\nusing the properties of folded Gaussian variables, we can characterise\nE[|\u2206X|] =\nr\n2\n\u03c0 \u03c3\u2206X\nThe entire utility then can be written as\nU[XR] = \u03c1\u03c3X\u03c3R \u2212\u03b3\u03c32\nX\u03c32\nR(1 + \u03c12) \u2212\u03bd\nr\n2\n\u03c0 \u03c3\u2206X\nOptimising this utility will be very much like a standard least-squares problem except\nthe term \u03c3\u2206X is a form of regularization.",
    "chunk_index": 24,
    "start_char": 60994,
    "end_char": 63770,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "of Isserlis\u2019 theorem\n26\n\nGiven that\n\u2206X = \u03c61Rt\u22121 +\nK\nX\nk=1\n\u2206\u03c6kRt\u2212k \u2212\u03c6KRt\u2212K \u2261\u2206\u03c6 \u2217R\nwhere \u2206\u03c6 = (0, \u03c61, \u2206\u03c61, \u2206\u03c62, . . . , \u2206\u03c6K, \u2212\u03c6K). The r.v. is normal, \u2206X \u223cN(0, \u03c32\n\u2206X) and,\nusing the properties of folded Gaussian variables, we can characterise\nE[|\u2206X|] =\nr\n2\n\u03c0 \u03c3\u2206X\nThe entire utility then can be written as\nU[XR] = \u03c1\u03c3X\u03c3R \u2212\u03b3\u03c32\nX\u03c32\nR(1 + \u03c12) \u2212\u03bd\nr\n2\n\u03c0 \u03c3\u2206X\nOptimising this utility will be very much like a standard least-squares problem except\nthe term \u03c3\u2206X is a form of regularization.\nIn fact if we let C being the ACF (Toeplitz) matrix of (Rt, . . . Rt\u2212k), i.e.,\nC =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1\nc(1)\nc(2)\n. . .\nc(k \u22121)\nc(1)\n1\nc(1)\n. . .\nc(k \u22122)\n...\n...\n...\n...\n...\nc(k \u22121)\nc(k \u22122)\nc(k \u22123)\n. . .\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nand X = \u03c6 \u2217R with \u03c6 = (\u03c61, \u03c62, . . . , \u03c6k) and let 10 = (1, 0, . . . , 0) then \u03c3X = \u03c3R\n\u221a\u03c6\u2032 \u00b7 C \u00b7 \u03c6,\n\u03c1 = \u03c6\u2032 \u00b7 C \u00b7 10 and \u03c3\u2206X = \u03c3R\np\n(\u2206\u03c6)\u2032 \u00b7 C \u00b7 (\u2206\u03c6), e\ufb00ectively penalizing changes in the \u03c6k.\nThe resulting optimisation problem thus becomes\nU[XR] = \u03c32\nR\np\n\u03c6\u2032 \u00b7 C \u00b7 \u03c6\u03c6\u2032 \u00b7 C \u00b7 10 \u2212\u03b3\u03c34\nR(\u03c6\u2032 \u00b7 C \u00b7 \u03c6)(1 + (\u03c6\u2032 \u00b7 10)2) \u2212\u03bd\nr\n2\n\u03c0 \u03c3R\np\n(\u03b4\u03c6)\u2032 \u00b7 C \u00b7 (\u2206\u03c6)\nThis \ufb01nal regularization term should ensure that the \ufb01lter weights \u03c6k do not vary too\nmuch between themselves (i.e., it is a sort of smoothness constraint analogous to those in\na Lasso or Ridge-regression, but with a slightly di\ufb00erent functional form). Unlike the case\nof an L 2 penalty as in Ridge regression or an L 1 penalty as in Lasso, this term though is\nneither linear nor quadratic.\nWe do not consider properties of the solutions of optimal trading strategies with trans-\naction costs in this paper.\nF\nMultiperiod Returns\nGiven the ease of analysis of Gaussian returns, it is straightforward to calculate moments\nof the strategy returns to any horizon. While we do not explore further implications, we\nproduce relevant formulas in this section for future elaboration.\nFor long-horizon trades we note the following ([Magnus, 1978])\nTheorem[Magnus] Let A be a symmetric matrix and R \u223cN (0, V ) with V positive\n27\n\nde\ufb01nite. De\ufb01ne p = R\u2032AR. then the expectation, variance, skewness and kurtosis of p are:\n\u00b5\n=\ntr(AV )\n\u03c32\n=\n2 tr(AV )2\n\u03b33\n=\n2\n\u221a\n2\ntr(AV )3\n(tr(AV )2)3/2\n\u03b34\n=\n12 tr(AV )4\n(tr(AV )2)2\nwhich would allow us to calculate Sharpe ratios, skewness and kurtosis to any horizon.\nContinuous analogues are feasible using functional central limit theory for Wick products\n(see [Parczewski, 2014]).\nGiven this and the various moment conditions for our Gaussian returns:\nE[Rt] =\n0\nE[R2\nt ] =\n\u03c32\nE[R3\nt ] =\n0\nE[RtRs] =\nC(t \u2212s)\u03c32\nwhere C(0) = 1 and C(\u2212k) = C(k), we can combine for characterising strategy moments.",
    "chunk_index": 25,
    "start_char": 63293,
    "end_char": 65828,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "are:\n\u00b5\n=\ntr(AV )\n\u03c32\n=\n2 tr(AV )2\n\u03b33\n=\n2\n\u221a\n2\ntr(AV )3\n(tr(AV )2)3/2\n\u03b34\n=\n12 tr(AV )4\n(tr(AV )2)2\nwhich would allow us to calculate Sharpe ratios, skewness and kurtosis to any horizon.\nContinuous analogues are feasible using functional central limit theory for Wick products\n(see [Parczewski, 2014]).\nGiven this and the various moment conditions for our Gaussian returns:\nE[Rt] =\n0\nE[R2\nt ] =\n\u03c32\nE[R3\nt ] =\n0\nE[RtRs] =\nC(t \u2212s)\u03c32\nwhere C(0) = 1 and C(\u2212k) = C(k), we can combine for characterising strategy moments.\nIf the ACF matrix \u02dcC is known with certainty, of course, then the linear \ufb01lter which\nmaximizes the correlation of signal to returns is merely given by \ufb01nding the eigenvector\ncorresponding to the smallest eigenvalue, i.e.,\nvmin = argminv\nv\u2032 \u02dcCv\n|v|2\nand normalizing the \ufb01rst coe\ufb03cient to be one, i.e., ak = \u2212v(k + 1)/v(1).\nFor longer horizons, w use the formulas given by Magnus, or equally compute the term-\nstructure by hand:\n\u00b51(T) = E\nX\nXtRt = \u03c3X\u03c3R\u03c1T\nand\nE(\nX\nXtRt)2\n=\nX X\nE[XtRtXsRs]\n=\nX X\u0000E[XtRt]E[XsRs] + E[XtXs]E[RtRs] + E[XtRs]E[XsRt])\n=\n\u03c32\nX\u03c32\nR\n\u0000T 2\u03c12 +\nX X\n(C(i \u2212j)D(i \u2212j) + \u03c1(i \u2212j)\u03c1(j \u2212i))\n\u0001\nwhere C(k) is the ACF for R and D(k) is the ACF for signal X, and \u03c1(k) = E[XtRt\u2212k] and\n\u03c1(k) = \u03c1(\u2212k) and \u03c1(0) = \u03c1 is the contemporaneous correlation.\nConsequently,\nvar(\nX\nXtRt) = \u03c32\nX\u03c32\nR\n\u00002\nT \u22121\nX\nk=1\n(T \u2212k)(C(k)D(k) + \u03c1(k)\u03c1(\u2212k)) + T(1 + \u03c12)\n\u0001\nand consequently, the Sharpe ratio to any horizon is given by\nSR(T) =\n\u03c1T 1/2\n1 + \u03c12 + 2 PT \u22121\n1\nT \u2212k\nT (C(k)D(k) + \u03c1(k)\u03c1(\u2212k))\ngiving us the term-structure of Sharpe ratios by horizon.\n28\n\nG\nSet-up details\nIf we have a purely-random mean-zero covariance-stationary discrete-time Gaussian process\nRt, we note by Wold Decomposition, that all stationary Gaussian processes can be repre-\nsented as MA(\u221e) in terms of Gaussian innovation process and coe\ufb03cients in l2, with no\ndeterministic component.\nSpeci\ufb01cally, we have\nE[Rt]\n=\n0\nV ar(Rt)\n=\n\u03c32\nR\ncorr(Rt, Rs)\n=\n\u03b3(t \u2212s)\n(i.e., with ACF \u03b3)\nThen we are interested in constructing signals: Xt. A standard signal we will consider\nis a convolution signal, i.e.,\nXt =\nX\nk\u22651\n\u03c6(k)Rt\u2212k\nThis is an example of a time-invariant linear \ufb01lter.\nIt the coe\ufb03cients \u03c6 \u2208l2 then it is\nwell known that the resulting \ufb01ltered series Xt are Gaussian1. The \ufb01ltered series Xt is also\njointly Gaussian with Rt.\nWe note that if the \u03c6(k) can be derived as the coe\ufb03cients of an ARMA model forecast,\nor they can be from a simple EWMA, as we have mentioned in the paper.\nE[Xt]\n=\n0\nV ar(Xt)\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)E[Rt\u2212kRt\u2212j]\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)\u03b3(k \u2212j)\u03c32\nR\n(dropping all \ufb01rst order terms because E[Rt] = 0 ) and,\ncorr(Rt, Xt)\n=\nE[P \u03c6(k)Rt\u2212k, Rt]\nstd(X)\u03c3R\n=\nP \u03c6(k)\u03b3(k)\n(P\nk,j\u22651 \u03c6(k)\u03c6(j)\u03b3(k \u2212j))1/2\ncancelling out all \u03c3R terms.",
    "chunk_index": 26,
    "start_char": 65317,
    "end_char": 68006,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "series Xt is also\njointly Gaussian with Rt.\nWe note that if the \u03c6(k) can be derived as the coe\ufb03cients of an ARMA model forecast,\nor they can be from a simple EWMA, as we have mentioned in the paper.\nE[Xt]\n=\n0\nV ar(Xt)\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)E[Rt\u2212kRt\u2212j]\n=\nX\nk,j\u22651\n\u03c6(k)\u03c6(j)\u03b3(k \u2212j)\u03c32\nR\n(dropping all \ufb01rst order terms because E[Rt] = 0 ) and,\ncorr(Rt, Xt)\n=\nE[P \u03c6(k)Rt\u2212k, Rt]\nstd(X)\u03c3R\n=\nP \u03c6(k)\u03b3(k)\n(P\nk,j\u22651 \u03c6(k)\u03c6(j)\u03b3(k \u2212j))1/2\ncancelling out all \u03c3R terms.\nConsequently,\nsgn(corr(Rt, Xt)) = sgn(\u03b3 \u00b7 \u03c6))\n(i.e., the sign of this in\ufb01nite inner product matters most for determining usefulness of a given\nconvolution design). A linear time-invariant \ufb01lter with l2 coe\ufb03cient is just one example of\na signal Xt which is jointly Gaussian with returns Rt. Similarly, if Zt is a set of Gaussian\n(exogenous) features, then Xt = Zt\u03b2 will also be Gaussian and we will assume the Zt are\njointly Gaussian with Rt, meaning also Xt and Rt will be jointly Gaussian.\n1see e.g., Gallagher, R, Stochastic Processes: Theory for Applications, Cambridge UP, 2014, or Gallagher\nR, MIT Open Coursework, Principles of Digital Communications, Section 7.4.2, Theorem 7.4.1.\n29\n\nReferences\n[Acar, 1990] Acar, E. Expected returns of directional forecasters. in Advanced Trading\nStrategies, ed. E Acar and S Satchell, 1990, (Butterworth-Heinemann: Oxford), 122-\n151.\n[Allenbridge IS, 2014] Allendbridge IS, Quantitative Investment Strategy Survey, 2014.\n[Asness-Moskowitz-Pedersen, 2013] Asness, C.S., Moskowitz, T.J. and Pedersen, L.H. Value\nand momentum everywhere. The J. Finance, 2013 68(3), 929-985.\n[Credit Suisse, 2017] Avramovic, A., We\u2019re All High Frequency Traders Now. Credit Suisse\nMarket Structure, Trading Strategy. 15 March 2017,https://edge.credit-suisse.\ncom/edge/Public/Bulletin/Servefile.aspx?FileID=28410&m=-1290757752\n(Ac-\ncessed 21 Aug 2017)\n[Cram\u00e9r, 1946] Cram\u00e9r, H. Mathematical Methods of Statistics. 1946 (Princeton University\nPress: Princeton).\n[Babu and Feigelson, 1992] Babu, G.J., and Feigelson, E.D. Analytical and Monte Carlo\ncomparisons of six di\ufb00erent linear least squares \ufb01ts. Communications in Statistics-\nSimulation and Computation 21(2), 1992, 533-549.\n[Baltas and Kosowski, 2013] Baltas, N. and Kosowski, R. Momentum Strategies in Futures\nMarkets and Trend-following Funds (January 5, 2013). Presented at Finance Meeting\nEUROFIDAI-AFFI Paper, Paris. December 2012. Available at SSRN: https://ssrn.\ncom/abstract=1968996 (Accessed 21 August 2017).\n[Barclay Hedge, 2017] Barclay\nHedge:\nCTA\u2019s\nAsset\nUnder\nManagement.\nAvailable\nonline\nat\nhttps://www.barclayhedge.com/research/indices/cta/Money_Under_\nManagement.html, (accessed 23 Sep 2017).\n[Bouchaud and Potters, 2009] Bouchaud, J.P., and Potters, M. Financial applications of\nrandom matrix theory: a short review. Available online at arXiv https://arxiv.org/\nabs/0910.1205 (2009), (accessed 2 Jan 2016).\n[Bouchaud et al, 2016] Bouchaud, J.P., Dau, T.L., Deremble, C., Lemp\u0155i\u00e8re, Y., Nguyen,\nT.T., Potters M. Tail Protection for Long Investors: Convexity at Work. J Inv Strat,\nDec 2017, 7(1), 61-84.\n[Bruder et al, 2011] Bruder, B. and Dao, T.L., Richard, J.C., and Roncalli, T., Trend Fil-\ntering Methods for Momentum Strategies. December 1, 2011. Available online at SSRN:\nhttps://ssrn.com/abstract=2289097, (accessed 2 July 2014).\n[Bruder and Gaussel, 2011] Bruder, B and Gaussel, N. Risk-return analysis of Dynamic\ninvestment strategies. Lyxor White paper, Issue 7 (Jun 2011), available online\nat SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465623 (ac-\ncessed 2 July 2014).",
    "chunk_index": 27,
    "start_char": 67560,
    "end_char": 71096,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "M. Tail Protection for Long Investors: Convexity at Work. J Inv Strat,\nDec 2017, 7(1), 61-84.\n[Bruder et al, 2011] Bruder, B. and Dao, T.L., Richard, J.C., and Roncalli, T., Trend Fil-\ntering Methods for Momentum Strategies. December 1, 2011. Available online at SSRN:\nhttps://ssrn.com/abstract=2289097, (accessed 2 July 2014).\n[Bruder and Gaussel, 2011] Bruder, B and Gaussel, N. Risk-return analysis of Dynamic\ninvestment strategies. Lyxor White paper, Issue 7 (Jun 2011), available online\nat SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465623 (ac-\ncessed 2 July 2014).\n[Cui et al., 2016] Cui, G, Yu, X, Iommelli, S, and Kong, L. Exact distribution for the prod-\nuct of two correlated Gaussian random variables. IEEE Sig Proc Letters, Nov 2016, 23\n(11), 1662-1666.\n30\n\n[Fama and French, 1992] Fama, E.F., and French, K.R., The Cross-Section of Expected\nStock Returns, J Finance, Jun 1992, 47(2), 427-465.\n[Fuller, 2009] Fuller, Wayne A. Measurement error models, Vol. 305, 2009,(John Wiley &\nSons: New York).\n[Fung and Hsieh, 1997] Fung and Hsieh, Empirical Characteristics of Dynamic Trading\nStrategies: The case of Hedge Funds, Rev Fin Stud, 1997, 10(2), 275-302.\n[Gigerenzer and Todd, 1999] Gigerenzer, G., Todd, P.M., and the ABC Research Group.\nSimple heuristics that make us smart 1999, (Oxford University Press: Oxford).\n[G\u00f6tze and Tikhomorov, 1999] G\u00f6tze, F., and Tikhomirov, A. Asymptotic Distribution of\nQuadratic Forms, Annals of Prob, 1999, 50, 1072-98.\n[G\u00f6tze and Tikhomorov, 2002] G\u00f6tze, F and Tikhomirov, A., Asymptotic Distribution of\nQuadratic Forms and Applications, J Theoretical Prob, 2002, 15, 423-75.\n[G\u00f6tze et al., 2007] G\u00f6tze, F and Tikhomirov, A., and Yurchenko, V. Asymptotic Expansion\nin the Central Limit Theorem for Quadratic Forms, J of Mathematical Sciences, 2007,\n147(4), 6891-6911.\n[Golub and Van Loan, 1980] Golub, G.H., and Van Loan, C.F. An analysis of the total least\nsquares problem. SIAM Journal on Numerical Analysis, 1980, 17(6), 883-893.\n[Grinold and Kahn, 1999] Grinold, R.C. and Kahn, R.N., Active Portfolio Management,\n2nd Edition, 1999, (McGraw-Hill:New York).\n[Haldane, 1942] Haldane, J.B.S., Moments of the Distributions of Powers and Products of\nNormal Variates. Biometrika, Apr 1942, 32 (3,4), 226-242.\n[Hald, 2008] Hald, A.A History of Parametric Statistical Inference from Bernoulli to Fisher,\n24 Aug 2008, (Springer Science & Business Media: Berlin), 1713-1935.\n[Hamdan et al., 2016] Hamdan, R., Pavlowsky, F., Roncalli, T., and Zheng, B. A Primer on\nAlternative Risk Premia. April 2016. Available online at SSRN: https://ssrn.com/\nabstract=2766850 (accessed 1 Dec 2016).\n[Harvey et al., 2018] Harvey, C.R. and Hoyle, E. and Korgaonkar, R. and Rattray, S. and\nSargaison, M. and Van Hemert, O. The Impact of Volatility Targeting. May 21, 2018.\nAvailable online at SSRN: https://ssrn.com/abstract=3175538. (accessed 2 July\n2018).\n[Ho\ufb00man-Kaminski, 2016] Ho\ufb00man,\nB.\nand\nKaminski,\nK.\nThe\ntaming\nof\nthe\nSkew. Campbell White Paper Series,\nJune 2016. Available online at https:\n//www.valuewalk.com/wp-content/uploads/2016/06/The_Taming_of_the_Skew_\n__Campbell__Company.pdf, (accessed 8 Feb 2018).\n[Hotelling, 1936] , Hotelling, H. Relations between two sets of variants. 1936.Biometrika,\n28, 321-377.\n[Hurst-Ooi-Pedersen, 2017] Hurst, B., Ooi, Y.H. and Pedersen, L.H. A century of evidence\non trend-following investing. (June 27, 2017). Available Online at SSRN: https://\nssrn.com/abstract=2993026. (Accessed 2 July 2017).\n31",
    "chunk_index": 28,
    "start_char": 70510,
    "end_char": 73998,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "2018.\nAvailable online at SSRN: https://ssrn.com/abstract=3175538. (accessed 2 July\n2018).\n[Ho\ufb00man-Kaminski, 2016] Ho\ufb00man,\nB.\nand\nKaminski,\nK.\nThe\ntaming\nof\nthe\nSkew. Campbell White Paper Series,\nJune 2016. Available online at https:\n//www.valuewalk.com/wp-content/uploads/2016/06/The_Taming_of_the_Skew_\n__Campbell__Company.pdf, (accessed 8 Feb 2018).\n[Hotelling, 1936] , Hotelling, H. Relations between two sets of variants. 1936.Biometrika,\n28, 321-377.\n[Hurst-Ooi-Pedersen, 2017] Hurst, B., Ooi, Y.H. and Pedersen, L.H. A century of evidence\non trend-following investing. (June 27, 2017). Available Online at SSRN: https://\nssrn.com/abstract=2993026. (Accessed 2 July 2017).\n31\n\n[Hyndman et al., 2008] Hyndman, R., Koehler, A.B., Ord, J.K., and Snyder, R.D. Fore-\ncasting with exponential smoothing: the state space approach, 2008, (Springer Science\n& Business Media: Berlin).\n[Isserlis, 1918] Isserlis, L. On a formula for the product-moment coe\ufb03cient of any order of\na normal frequency distribution in any number of variables. Biometrika 1918, 12(1-2),\n134-139.\n[Jegadeesh and Titman, 1993] Jegadeesh, N. and Titman, S. Returns to Buying Winners\nand Selling Losers: Implications for Stock Market E\ufb03ciency, J Finance, Mar 1993,\n48(1), 65-91.\n[Joanes and Gill, 1998] Joanes, D.N. and Gill, C. N., Comparing measures of sample skew-\nness and kurtosis. The Statistician, 1998, 47, 183-189.\n[Joarder, 2007] Joarder, A., On some characteristics of the bivariate t-distribution. Intl J\nModern Math, 2(2), 191-204\n[Kan, 2008] Kan, R. From moments of sum to moments of product. J Multivariate Anal,\n2008, 99(3) 542-554.\n[Ko, 2006] Ko, M. \"Functional central limit theorems for multivariate linear processes gener-\nated by dependent random vectors.\" COMMUNICATIONS-KOREAN MATHEMATI-\nCAL SOCIETY 21.4 (2006): 779.\n[Koshiyama and Firoozye, 2018] Koshiyama, A.S., and Firoozye, N. B., Avoiding Back-\ntesting Over\ufb01tting by Covariance-Penalties: An Empirical Investigation of the Ordinary\nand Total Least Squares Cases. https://arxiv.org/abs/1905.05023.\n[Lee, 2000] Lee, W. Theory and methodology of tactical asset allocation, 2000, Vol. 65. (John\nWiley & Sons: New York).\n[Lemp\u00e9ri\u00e8re et al., 2014] Lemp\u00e9ri\u00e8re,\nY.,\nDeremble,\nC.,\nSeager,\nP.,\nPotters,\nM.,\n&\nBouchaud, J. P. Two centuries of trend following. 2014. Available online at arXiv\nhttps://arxiv.org/abs/1404.3274, (accessed 17 Jun 2016).\n[Levine and Pedersen, 2015] Levine, A. and Pedersen, L.H., Which Trend Is Your Friend?\n(May 7, 2015). Financial Analysts Journal, vol. 72, no. 3 (May/June 2016). Available\nat SSRN: https://ssrn.com/abstract=2603731. (accessed 17 Jun 2015).\n[Leyang, 2012] Leyang, W. Properties of the Total Least Squares estimation. Geodesy and\nGeodynamics, 2012, 3(4), 39-46.\n[Lo, 2002] Lo, A.W. The statistics of Sharpe ratios. Fin Analysts J, 2002, 58(4), 36-52.\n[Magnus, 1978] Magnus, J.R. The moments of products of quadratic forms in normal vari-\nables. Stat Neerlandica, 1978, 32(4), 201-210.\n[Makridakis, 2000] Makridakis, S., & Hibon, M. The M3-Competition: results, conclusions\nand implications. Int J Forecasting, 2002, 16(4), 451-476. Chicago\n[Markovsky and Van Hu\ufb00ell, 2007] Markovsky, I. and Van Hu\ufb00el, S. Overview of total least-\nsquares methods. Signal Proc, 2007, 87(10), 2283-2302.\n32\n\n[Martin-Bana, 2012] Martin, R and Bana, A. Non-linear Momentum Strategies. Risk Mag-\nazine, Nov 2012, 60-65.\n[Martin-Zou, 2012] Martin, R and Zou, D. Momentum Trading: \u2019Skews me. Risk Magazine,\nAug 2012, 52-57.\n[Mertens, 2002] Mertens, E. Comments on variance of the IID estimator in Lo.",
    "chunk_index": 29,
    "start_char": 73317,
    "end_char": 76877,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "M. The M3-Competition: results, conclusions\nand implications. Int J Forecasting, 2002, 16(4), 451-476. Chicago\n[Markovsky and Van Hu\ufb00ell, 2007] Markovsky, I. and Van Hu\ufb00el, S. Overview of total least-\nsquares methods. Signal Proc, 2007, 87(10), 2283-2302.\n32\n\n[Martin-Bana, 2012] Martin, R and Bana, A. Non-linear Momentum Strategies. Risk Mag-\nazine, Nov 2012, 60-65.\n[Martin-Zou, 2012] Martin, R and Zou, D. Momentum Trading: \u2019Skews me. Risk Magazine,\nAug 2012, 52-57.\n[Mertens, 2002] Mertens, E. Comments on variance of the IID estimator in Lo. Re-\nsearch Note, 2002, Available online at http://www.elmarmertens.com/research/\ndiscussion#TOC-Correct-variance-for-estimated-Sharpe-Ratios (Accessed on 2\nJuly 2017).\n[Michalowicz et al., 2011] Michalowicz, J. V., Nichols, J. M., Bucholtz, F., & Olson, C. C.\nA general Isserlis theorem for mixed-Gaussian random variables. Stat & Prob Letters,\n2011, 81(8), 1233-1240.\n[Miller, 2016] Miller,\nM.\nEvaluating\nbank-sponsored\nrisk-premium\nstrate-\ngies,\nPensions\n&\nInvestments,\nAPRIL\n5,\n2017.\nAvailable\nonline\nat\nhttp://www.pionline.com/article/20170405/ONLINE/170409975/\nevaluating-bank-sponsored-risk-premium-strategies. (Accessed 8 May 2017).\n[Nadrajah-Kotz, 2003] Nadarajah, S. and Kotz, S. Multitude of bivariate t-distributions. J\nTheo Appl Stats, 2004 38, 527-39.\n[Nadarajah-Pog\u00e1ny, 2016] Nadarajah, S., and Pog\u00e1ny, T.K. On the distribution of the prod-\nuct of correlated normal random variables. Comptes Rendus Math, 2016, 354(2), 201-\n204.\n[Parczewski, 2014] Parczewski, Peter. A Wick functional limit theorem. Probab. Math. Stat.,\n2014 34(1), 127-145.\n[Pav, 2016] Pav, Steven E., Notes on Sharpe ratios. March 2016, Available online at https:\n//cran.r-project.org/web/packages/SharpeR/vignettes/SharpeRatio.pdf. (Ac-\ncessed 27 Jan 2017).\n[Potters and Bouchaud, 2005] Potters, M., and Bouchaud, J.P. Trend followers lose more of-\nten than they gain. 2005. Available online at arXiv https://arxiv.org/abs/physics/\n0508104. (Accessed 2 Feb 2017).\n[Rahman and Yu, 1987] Anisur Rahman, M. and Yu, K.B. Total Least Squares Estimation\nUsing Linear Prediction. IEEE Trans Acoust, Speech and Sig Proc, Oct 1987, 35(10),\n1440-54.\n[Rencher and Christiansen, 2012] Rencher, A.C., and Christensen, W.F., MEthods of Mul-\ntivarite Analysis, 3rd ed., 2012, (Wiley: New York), ch 11.\n[Simons, 2006] Simons, M. K, Probability Distributions Involving Gaussian Random Vari-\nables, 2006, (Springer Verlag: Berlin), 2006, ch 6.\n[Wick, 1950] Wick, G.C., The evaluation of the collision matrix. Phys Rev, 1950, 80(2),\n268-272.\n[Zhang, 2017] Zhang, Xian-Da, Matrix Analysis and Applications, 2017, (Cambridge UP:\nCambridge), pp. 334-5.\n33",
    "chunk_index": 30,
    "start_char": 76330,
    "end_char": 78995,
    "paper_title": "Optimal Dynamic Strategies on Gaussian Returns",
    "paper_category": "q-fin.PM",
    "paper_filename": "Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Optimal_Dynamic_Strategies_on_Gaussian_Returns.pdf"
  },
  {
    "text": "RELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL\nPORTFOLIOS\nDMITRY B. ROKHLIN\nAbstract. We consider a single-period portfolio selection problem for an investor, max-\nimizing the expected ratio of the portfolio utility and the utility of a best asset taken in\nhindsight. The decision rules are based on the history of stock returns with unknown distri-\nbution. Assuming that the utility function is Lipschitz or H\u00a8older continuous (the concavity\nis not required), we obtain high probability utility bounds under the sole assumption that\nthe returns are independent and identically distributed. These bounds depend only on the\nutility function, the number of assets and the number of observations. For concave utili-\nties similar bounds are obtained for the portfolios produced by the exponentiated gradient\nmethod. Also we use statistical experiments to study risk and generalization properties\nof empirically optimal portfolios. Herein we consider a model with one risky asset and a\ndataset, containing the stock prices from NYSE.\n1. Introduction\nWe consider a single-period portfolio selection problem, where the decision rules are based\non the history of stock returns. It is assumed that the returns are independent and identically\ndistributed, but their distribution is unknown. We represent investor\u2019s preferences by an\nexpected utility and use the sample average approximation (SAA) (see, e.g., [17]) for the\nsolution of the related expected utility maximization problem. In the terminology of the\nstatistical learning theory our main goal is to obtain high-probability bounds (generalization\nbounds or utility bounds) for the di\ufb00erence between the optimal utility value and the true\nutility of the empirically optimal portfolio (estimation error), as well as for the di\ufb00erence\nbetween the true utility and the empirical utility for such portfolio.\nLet us mention two speci\ufb01c features of the problem under consideration, which make\nsome di\ufb03culties in an application of standard results. First, some classical utility functions,\nlike the power function, are neither bounded nor globally Lipschitz. Second, most classical\nmodels, like the Black-Scholes, assume that the returns are unbounded. Similar unbounded\nproblems appear in general learning theory: see [6] and a lot of references therein. They\nrequire some additional assumptions, problem reformulations and the development of special\ntools.\nIn the present paper we pass to the relative utility maximization, where the objective\nfunction equals to the expected ratio of the utility u of some portfolio to the utility of\nthe best portfolio for the returns, which are known in hindsight. This allows to avoid any\nassumption on the returns, besides the i.i.d. hypothesis. As for u, we assume that it belongs\nto the class of positive, non-decreasing functions, satisfying the global Lipschitz or H\u00a8older\n2020 Mathematics Subject Classi\ufb01cation. 91G10, 68Q32.\nKey words and phrases. Portfolio selection; Relative utility; Statistical learning; Empirical utility; Gen-\neralization bounds.\nThe research is supported by the Russian Science Foundation, project 17-19-01038.\n1\narXiv:2006.05204v1 [q-fin.PM] 9 Jun 2020\n\n2\nDMITRY B. ROKHLIN\ncondition, and some speci\ufb01c condition, regarding its behavior at zero and in\ufb01nity. The power\nfunction satis\ufb01es these assumptions.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3317,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "class of positive, non-decreasing functions, satisfying the global Lipschitz or H\u00a8older\n2020 Mathematics Subject Classi\ufb01cation. 91G10, 68Q32.\nKey words and phrases. Portfolio selection; Relative utility; Statistical learning; Empirical utility; Gen-\neralization bounds.\nThe research is supported by the Russian Science Foundation, project 17-19-01038.\n1\narXiv:2006.05204v1 [q-fin.PM] 9 Jun 2020\n\n2\nDMITRY B. ROKHLIN\ncondition, and some speci\ufb01c condition, regarding its behavior at zero and in\ufb01nity. The power\nfunction satis\ufb01es these assumptions. For the same problem with a concave utility function\nwe study the estimation error for the portfolio produced by the stochastic version of the\nexponentiated gradient algorithm of [18].\nThe obtained utility bounds contain only those quantities, which are known for the in-\nvestor: the number of return observations; the number of stocks; constants, related to the\nutility function; and a data-dependent quantity in the case of the exponentiated gradient\nalgorithm: Theorems 1 \u2013 3.\nPassing to the relative utility certainly a\ufb00ects investor\u2019s attitude towards risk. In the case\nof one risky asset it appears, that an investor with the relative utility is more risk averse\nthan in the case of the ordinary utility. However, in the case of multiple risky assets our\nempirical results show that the situation can be the opposite. Furthermore, we present simple\nstatistical experiments demonstrating that typically it is impossible to get a reliable estimate\nof the optimal portfolio on the base of daily historical observations. A related phenomenon,\nwhich was mainly demonstrated for the risk-return modeling of investor\u2019s preferences, is\nknown as the fragility of SAA in portfolio optimization: see [1] and references therein.\nLet us mention some papers, considering single-period portfolio selection problems in the\nstatistical learning framework. In [8, 10], the authors studied the in\ufb02uence of the portfolio\nconstraints on the out-of-sample performance. The papers [10, 11] presented out-of-sample\nbounds for the loss probabilities of the portfolios, satisfying some empirical VaR- and CVaR-\ntype constraints. The regularization and cross validation methods were applied to the mean-\nvariance and mean-CVaR problems in [1]. One can also \ufb01nd in [1] several other references to\nthe works, considering the regularization methods. In [2] the authors considered an expected\nutility maximization problem with side information and applied a regularization to obtain\nout-of-sample guarantees for the certainty equivalent of the out-of-sample portfolio value.\nThe rest of the paper is organized as follows. In Section 2 we state the problem and mention\nthe consistency of the SAA method. Section 3 contains the main result of the paper: Theorem\n2, which gives upper bounds for the expected maximum of an empirical process, associated\nto the relative utility function. The Lipschitz and H\u00a8older cases are studied separately. In\nboth cases we consider the Rademacher complexity of the class of relative utility functions,\nparametrized by the portfolio weights. In the Lipshitz case this quantity is estimated by the\nTalagrand contraction lemma and the Massart lemma, in the H\u00a8older case we consider the\npacking numbers and the Dudley entropy integral. The obtained estimates directly lead to\nhigh-probability utility bounds via the concentration inequalities.",
    "chunk_index": 1,
    "start_char": 2772,
    "end_char": 6165,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "of an empirical process, associated\nto the relative utility function. The Lipschitz and H\u00a8older cases are studied separately. In\nboth cases we consider the Rademacher complexity of the class of relative utility functions,\nparametrized by the portfolio weights. In the Lipshitz case this quantity is estimated by the\nTalagrand contraction lemma and the Massart lemma, in the H\u00a8older case we consider the\npacking numbers and the Dudley entropy integral. The obtained estimates directly lead to\nhigh-probability utility bounds via the concentration inequalities. Section 4 presents similar\nbounds for the portfolios produced by the stochastic exponentiated gradient algorithm of\n[18]. Here we combine its online version with the online-to-batch conversion scheme: see\n[22].\nSections 5 and 6 deal with statistical experiments, related to the analysis of risk and\ngeneralization properties of empirically optimal portfolios. Section 5 considers the case of\none risky asset, obeying the discrete Black-Scholes model, while in Section 6 we analyze\na dataset, containing daily stock returns form NYSE. The conclusions are already brie\ufb02y\ndescribed above. Here we additionally indicate the utilized solution methods for the empirical\nutility maximization problems. In Section 5 the problem is one-dimensional, and it is solved\nsimply via the bisection method.\nIn Section 6 we propose a greedy modi\ufb01cation of the\nstochastic exponentiated gradient algorithm to solve the correspondent is multidimensional\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n3\nproblem. For logarithmic utility the results are compared with [4, 13]. The code for Sections\n5, 6 is available at https://github.com/drokhlin/Relative_utility_bounds_code.\n2. Problem formulation\nLet (s1\nk, . . . , sd\nk) be strictly positive prices of d assets (stocks) at time moments k =\n0, . . . , n + 1, and let rj\nk = sj\ni/sj\nk\u22121, j = 1, . . . , d, k = 1, . . . , n + 1 be the total daily returns\n(price relatives). At time n an investor distributes his wealth Xn = 1 between these assets\nbased on the price history (r1, . . . , rn). In other words, he selects a portfolio (\u03b31\nn, . . . , \u03b3d\nn),\nwhere \u03b3j\nn(r1, . . . , rn) \u22650 is the number of units of the asset j to be bought. So, the wealth\nwill be distributed between d assets in accordance with the fractions (or weights)\n\u03bdn =\n\u0012\u03b3j\nnsj\nn\nXn\n\u0013d\nj=1\n\u2208\u2206=\n(\nz \u22650 :\nd\nX\nj=1\nzj = 1\n)\n.\nAt time n + 1 the wealth becomes\nXn+1 = \u27e8\u03b3n, sn+1\u27e9= \u27e8\u03bdn, rn+1\u27e9.\nBy \u27e8a, b\u27e9we denote the usual scalar product in Rd.\nOur standing assumptions concern the investor utility function and the returns.\nAssumption 1. Investor\u2019s utility function u : (0, \u221e) 7\u2192(0, \u221e) is non-decreasing and con-\ntinuous.\nAssumption 2. The return vectors (r1\nk, . . . , rd\nk), k = 1, . . . , n + 1 are independent and\nidentically distributed.\nConsider the single-period optimization problem\nU(\u03bd) = Ef(\u03bd, rn+1) := Eu(\u27e8\u03bd, rn+1\u27e9)\nu\n\u0000r\u2217\nn+1\n\u0001\n\u2192max\n\u03bd\u2208\u2206,\nr\u2217\nn+1 := max\n1\u2264j\u2264d rj\nn+1.",
    "chunk_index": 2,
    "start_char": 5606,
    "end_char": 8545,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "= \u27e8\u03b3n, sn+1\u27e9= \u27e8\u03bdn, rn+1\u27e9.\nBy \u27e8a, b\u27e9we denote the usual scalar product in Rd.\nOur standing assumptions concern the investor utility function and the returns.\nAssumption 1. Investor\u2019s utility function u : (0, \u221e) 7\u2192(0, \u221e) is non-decreasing and con-\ntinuous.\nAssumption 2. The return vectors (r1\nk, . . . , rd\nk), k = 1, . . . , n + 1 are independent and\nidentically distributed.\nConsider the single-period optimization problem\nU(\u03bd) = Ef(\u03bd, rn+1) := Eu(\u27e8\u03bd, rn+1\u27e9)\nu\n\u0000r\u2217\nn+1\n\u0001\n\u2192max\n\u03bd\u2208\u2206,\nr\u2217\nn+1 := max\n1\u2264j\u2264d rj\nn+1.\n(2.1)\nThe objective function U(\u03bd) of this problem equals to the expected ratio of the utility u of\nsome portfolio \u03bd to the utility of the best portfolio taken in hindsight, that is, under the\nassumption that the values rn+1 are known. In the latter case the investor simply takes an\nasset with the largest return. Since u is non-decreasing, the relative utility f takes values in\n(0, 1]. The set \u2206is compact and the function U is continuous, as follows from the continuity\nof \u03bd 7\u2192f(\u03bd, r) and the dominated convergence theorem. Hence an optimal solution \u03bd\u2217of\n(2.1) exists.\nIt is natural to consider the empirical utility maximization problem\nbUn(\u03bd) = bfn(\u03bd, rn+1) = 1\nn\nn\nX\nk=1\nu(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\n\u2192max\n\u03bd\u2208\u2206.\n(2.2)\nClearly, this problem also has an optimal solution b\u03bdn.\nFurthermore, consider the empirical process \u03bd 7\u2192Gn(\u03bd) = bUn(\u03bd) \u2212U(\u03bd). Using the in-\nequalities\nbUn(\u03bd\u2217) \u2264bUn(b\u03bdn),\nU(b\u03bdn) \u2264U(\u03bd\u2217),\n\n4\nDMITRY B. ROKHLIN\nwe get\nU(\u03bd\u2217) \u2212U(b\u03bdn) \u2264U(\u03bd\u2217) \u2212bUn(\u03bd\u2217) + bUn(b\u03bdn) \u2212U(b\u03bdn) \u2264U(\u03bd\u2217) \u2212bUn(\u03bd\u2217) + sup\n\u03bd\u2208\u2206\nGn(\u03bd),\n(2.3)\nbUn(b\u03bdn) \u2212U(\u03bd\u2217) \u2264bUn(b\u03bdn) \u2212U(b\u03bdn) \u2264sup\n\u03bd\u2208\u2206\nGn(\u03bd).\n(2.4)\nNote, that when \u03bdn is random, by U(\u03bdn) we mean the conditional expectation:\nU(\u03bdn) = E (f(\u03bdn, rn+1)|r1, . . . , rn)).\nThis quantity can be called the \u201ctrue utility\u201d of \u03bdn by analogy to the \u201ctrue risk\u201d in machine\nlearning: see [23].\nIn learning theory the di\ufb00erence U(\u03bd\u2217) \u2212U(b\u03bdn) is called an estimation error: [23]. It\ndescribes the performance of the empirical utility maximizer b\u03bdn. The quantity bUn(b\u03bdn) can\nbe regarded as a statistical estimate of the true utility U(b\u03bdn) of b\u03bdn. This estimate is always\noptimistically biased:\nEU(b\u03bdn) \u2264U(\u03bd\u2217) = EbUn(\u03bd\u2217) \u2264EbUn(b\u03bdn).\nThe di\ufb00erence EbUn(b\u03bdn) \u2212EU(b\u03bdn) \u22650 is known as optimizer\u2019s curse: [26, 19].\nWe see that the key quantity is the supremum of the empirical process Gn.\nBy the\nstrong law of large numbers Gn(\u03bd) \u21920 a.s. for a \ufb01xed \u03bd. Moreover, since the function\n\u03bd 7\u2192u(\u27e8\u03bd, r\u27e9)/u(r\u2217) is continuous and bounded, the convergence is uniform:\nsup\n\u03bd\u2208\u2206\n|Gn(\u03bd)| \u21920 a.s.,\nn \u2192\u221e\nby [25, Theorem 7.53]. From (2.3), (2.4) we see that\nU(\u03bd\u2217) \u2264lim inf\nn\u2192\u221eU(b\u03bdn),\nlim sup\nn\u2192\u221e\nbUn(b\u03bdn) \u2264U(\u03bd\u2217).\nThe reverse inequalities U(\u03bd\u2217) \u2265U(b\u03bdn),\nlim inf\nn\u2192\u221e\nbUn(b\u03bdn) \u2265lim inf\nn\u2192\u221e\nbUn(\u03bd\u2217) = U(\u03bd\u2217)\nimply that bUn(b\u03bdn) \u2192U(\u03bd\u2217), U(b\u03bdn) \u2192U(\u03bd\u2217), n \u2192\u221ea.s. without further assumptions.\nThus, the method of empirical utility maximization is consistent: see the de\ufb01nition in [28,\nChapter 3], where the convergence in probability is considered.",
    "chunk_index": 3,
    "start_char": 8036,
    "end_char": 10965,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "continuous and bounded, the convergence is uniform:\nsup\n\u03bd\u2208\u2206\n|Gn(\u03bd)| \u21920 a.s.,\nn \u2192\u221e\nby [25, Theorem 7.53]. From (2.3), (2.4) we see that\nU(\u03bd\u2217) \u2264lim inf\nn\u2192\u221eU(b\u03bdn),\nlim sup\nn\u2192\u221e\nbUn(b\u03bdn) \u2264U(\u03bd\u2217).\nThe reverse inequalities U(\u03bd\u2217) \u2265U(b\u03bdn),\nlim inf\nn\u2192\u221e\nbUn(b\u03bdn) \u2265lim inf\nn\u2192\u221e\nbUn(\u03bd\u2217) = U(\u03bd\u2217)\nimply that bUn(b\u03bdn) \u2192U(\u03bd\u2217), U(b\u03bdn) \u2192U(\u03bd\u2217), n \u2192\u221ea.s. without further assumptions.\nThus, the method of empirical utility maximization is consistent: see the de\ufb01nition in [28,\nChapter 3], where the convergence in probability is considered. In the next section we provide\nnon-asymptotic bounds for Gn.\n3. Utility bounds\nLet us represent the supremum of the empirical process Gn in the form\nsup\n\u03bd\u2208\u2206\nGn(\u03bd) = E sup\n\u03bd\u2208\u2206\nGn(\u03bd) + sup\n\u03bd\u2208\u2206\nGn(\u03bd) \u2212E sup\n\u03bd\u2208\u2206\nGn(\u03bd).\nPut Rn = (r1, . . . , rn), \u03a6(Rn) = sup\u03bd\u2208\u2206Gn(\u03bd). We have\n|\u03a6(r1, . . . , \u02dcrk, . . . , rn) \u2212\u03a6(r1, . . . , rk, . . . , rn)| =\n\f\f\f\f\fsup\n\u03bd\n \n1\nm\nX\ni\u0338=k\nu(\u27e8\u03bd, ri\u27e9)\nu(r\u2217\ni )\n\u2212U(\u03bd) + 1\nm\nu(\u27e8\u03bd, \u02dcrk\u27e9)\nu(\u02dcr\u2217\nk)\n!\n\u2212sup\n\u03bd\n \n1\nm\nX\ni\u0338=k\nu(\u27e8\u03bd, ri\u27e9)\nu(r\u2217\ni )\n\u2212U(\u03bd) + 1\nm\nu(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\n!\f\f\f\f\f \u2264sup\n\u03bd\n\f\f\f\f\n1\nm\nu(\u27e8\u03bd, \u02dcrk\u27e9)\nu(\u02dcr\u2217\nk)\n\u22121\nm\nu(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\n\f\f\f\f \u22641\nm.\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n5\nBy the McDiarmid concentration inequality (see [20, Theorem D.8]) this bounded di\ufb00erences\nproperty implies that\nP\n\u0012\nsup\n\u03bd Gn(\u03bd) \u2212E sup\n\u03bd Gn(\u03bd) \u2265\u03b5\n\u0013\n= P(\u03a6(Rn) \u2212E\u03a6(Rn) \u2265\u03b5) \u2264e\u22122m\u03b52,\nor, equivalently,\nP\n \nsup\n\u03bd Gn(\u03bd) \u2212E sup\n\u03bd Gn(\u03bd) \u2265\nr\n1\n2n ln 1\n\u03b4\n!\n\u2264\u03b4.\n(3.1)\nFor the di\ufb00erence U(\u03bd\u2217) \u2212bUn(\u03bd\u2217) we have a similar estimate:\nP\n \nU(\u03bd\u2217) \u2212bUn(\u03bd\u2217) \u2265\nr\n1\n2n ln 1\n\u03b4\n!\n\u2264\u03b4,\n(3.2)\nwhich follows from the Hoe\ufb00ding inequality [20, Theorem D.2]: a special case of the McDi-\narmid inequality.\nNote, that to get the inequalities (3.1), (3.2) we need not impose any growth assumptions\non u. This is an advantage of the relative utility. Let us formulate the obtained result more\nexplicitly.\nTheorem 1. With probability at least 1 \u2212\u03b4 we have\nU(\u03bd\u2217) \u2212U(b\u03bdn) \u2264E sup\n\u03bd\u2208\u2206\nGn(\u03bd) +\nr\n2\nn ln 2\n\u03b4,\n(3.3)\nbUn(b\u03bdn) \u2212U(b\u03bdn) \u2264E sup\n\u03bd\u2208\u2206\nGn(\u03bd) +\nr\n1\n2n ln 1\n\u03b4.\n(3.4)\nThe distinction in constants in the right-hand sides of (3.3), (3.4) is due to the fact that\nwe applied both inequalities (3.1), (3.2) to (2.3) and only the \ufb01rst one to (2.4). In the \ufb01rst\ncase the following argumentation is used: if\nP\n \n\u03bei \u2265\nr\n1\n2n ln 1\n\u03b4\n!\n\u2264\u03b4,\ni = 1, 2,\nthen\nP\n \n\u03be1 + \u03be2 \u22652\nr\n1\n2n ln 2\n\u03b4\n!\n\u2264\n2\nX\ni=1\nP\n \n\u03bei \u2265\ns\n1\n2n ln 1\n\u03b4/2\n!\n\u2264\u03b4.\nTheorem 2 contains the main result of the paper: the upper bounds for E sup\u03bd\u2208\u2206Gn(\u03bd).\nTheorem 2. Assume that the utility function u is uniformly H\u00a8older continuous on (0, \u221e):\n|u(x) \u2212u(y)| \u2264K|x \u2212y|\u03b1\n(3.5)\nwith some \u03b1 \u2208(0, 1], K > 0. Assume further that\nA := sup\nx>0\nx\u03b1\nu(x) < \u221e.\n(3.6)",
    "chunk_index": 4,
    "start_char": 10449,
    "end_char": 13053,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "the \ufb01rst\ncase the following argumentation is used: if\nP\n \n\u03bei \u2265\nr\n1\n2n ln 1\n\u03b4\n!\n\u2264\u03b4,\ni = 1, 2,\nthen\nP\n \n\u03be1 + \u03be2 \u22652\nr\n1\n2n ln 2\n\u03b4\n!\n\u2264\n2\nX\ni=1\nP\n \n\u03bei \u2265\ns\n1\n2n ln 1\n\u03b4/2\n!\n\u2264\u03b4.\nTheorem 2 contains the main result of the paper: the upper bounds for E sup\u03bd\u2208\u2206Gn(\u03bd).\nTheorem 2. Assume that the utility function u is uniformly H\u00a8older continuous on (0, \u221e):\n|u(x) \u2212u(y)| \u2264K|x \u2212y|\u03b1\n(3.5)\nwith some \u03b1 \u2208(0, 1], K > 0. Assume further that\nA := sup\nx>0\nx\u03b1\nu(x) < \u221e.\n(3.6)\n\n6\nDMITRY B. ROKHLIN\nThen\nE sup\n\u03bd\u2208\u2206\nGn(\u03bd) \u22642AK\nr\n2 ln d\nn\n,\n\u03b1 = 1,\n(3.7)\nE sup\n\u03bd\u2208\u2206\nGn(\u03bd) \u2264CAK\nr\nd \u22121\n\u03b1n ,\n\u03b1 \u2208(0, 1),\n(3.8)\nwhere C > 0 is an absolute constant.\nProof. Let \u03b5i, i = 1, . . . , n be independent Rademacher random variables: P(\u03b5i = 1) =\nP(\u03b5i = \u22121) = 1/2, which are also independent from r1, . . . , rn.\nConsider the empirical\nRademacher complexity (see , e.g., [20])\nbR(F \u25e6Rn) = 1\nnE\n \nsup\n\u03bd\u2208\u2206\nn\nX\ni=1\n\u03b5i\nu(\u27e8\u03bd, ri\u27e9)\nu(r\u2217\ni )\n\f\f\f\fRn\n!\nof the set of functions F = {r 7\u2192u(\u27e8\u03bd, r\u27e9)/u(r\u2217) : \u03bd \u2208\u2206} with respect to the random\nsequence Rn = (r1, . . . , rn). In fact we compute the Rademacher complexity of the following\nset of n-dimensional vectors:\nF \u25e6Rn :=\n\u001a\u0012u(\u27e8\u03bd, r1\u27e9)\nu(r\u2217\n1)\n, . . . , u(\u27e8\u03bd, rn\u27e9\nu(r\u2217\nn)\n\u0013\n: \u03bd \u2208\u2206\n\u001b\n.\nFor clarity recall (see [23]) that the Rademacher complexity of a set C \u2282Rn is de\ufb01ned by\nthe formula\nbR(C) = 1\nnE sup\na\u2208C\nn\nX\ni=1\n\u03b5iai.\n(3.9)\nLet us consider the case \u03b1 = 1. The symmetrization argument ([27, Lemma 7.4]) gives\nthe bound\nE sup\n\u03bd\u2208\u2206\nGn(\u03bd) \u22642E bR(F \u25e6Rn).\n(3.10)\nFor \u03a8(x, r) = u(x)/u(r\u2217), r\u2217= max1\u2264i\u2264d ri we have\n|\u03a8(x, r) \u2212\u03a8(y, r)| \u2264\nK\nu(r\u2217)|x \u2212y|.\nLiterally following the proof of Talagrand\u2019s contraction lemma, given in [20, Lemma 5.7], we\nget the inequality\nbR(F \u25e6Rn) = 1\nnE\n \nsup\n\u03bd\u2208\u2206\nn\nX\ni=1\n\u03b5i\u03a8(\u27e8\u03bd, ri\u27e9, ri)\n\f\f\f\fRn\n!\n\u2264K\nn E\n \nsup\n\u03bd\u2208\u2206\nn\nX\ni=1\n\u03b5i\n\u27e8\u03bd, ri\u27e9\nr\u2217\ni\n\f\f\f\fRn\n!\n= K bR(H \u25e6Rn),\nH := {r 7\u2192\u27e8\u03bd, r\u27e9/r\u2217: \u03bd \u2208\u2206}.\n(3.11)\nNote, that the only di\ufb00erence with the Talagrand contraction lemma is that the Lipschitz\nconstant for x 7\u2192\u03a8(x, r) depends on r.\nThe Rademacher complexity of the set H equals to the Rademacher complexity of its\nextreme points (as follows from [23, Lemma 26.7]), corresponding to the vectors of the\nstandard basis: \u03bd \u2208{e1, . . . , ed}, ei = (\u03b4ij)d\nj=1, where \u03b4ij is Kronecker symbol. Thus,\nbR(H \u25e6Rn) = bR\n\u0012 r1\nu(r\u2217), . . . ,\nrd\nu(r\u2217)\n\u0013\n.\n(3.12)",
    "chunk_index": 5,
    "start_char": 12601,
    "end_char": 14853,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "Lipschitz\nconstant for x 7\u2192\u03a8(x, r) depends on r.\nThe Rademacher complexity of the set H equals to the Rademacher complexity of its\nextreme points (as follows from [23, Lemma 26.7]), corresponding to the vectors of the\nstandard basis: \u03bd \u2208{e1, . . . , ed}, ei = (\u03b4ij)d\nj=1, where \u03b4ij is Kronecker symbol. Thus,\nbR(H \u25e6Rn) = bR\n\u0012 r1\nu(r\u2217), . . . ,\nrd\nu(r\u2217)\n\u0013\n.\n(3.12)\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n7\nHere rj/u(r\u2217) = (rj\n1/u(r\u2217\n1), . . . , (rj\nn/u(r\u2217\nn)) \u2208Rn are the normalized trajectories of the returns,\nand the right-hand side of (3.12) is computed in accordance with (3.9). The Rademacher\ncomplexity of a \ufb01nite set of vectors can be estimated by Massart\u2019s lemma (see [20, Theorem\n3.7]). Applying this lemma to the right-hand side of (3.12), we get the inequality\nbR\n\u0012 r1\nu(r\u2217), . . . ,\nrd\nu(r\u2217)\n\u0013\n\u2264A\n\u221an\n\u221a\n2 ln d,\n(3.13)\nsince by (3.6),\n\u2225rj/u(r\u2217)\u22252 =\nv\nu\nu\nt\nn\nX\nk=1\n \nrj\nk\nu(r\u2217\nk)\n!2\n\u2264A\u221an,\nwhere \u2225a\u22252 =\npPn\ni=1 a2\ni is the l2-norm. The inequality (3.7) now follows from (3.10) \u2013\n(3.13).\nIn the case \u03b1 < 1 \ufb01rst note that for \ufb01xed Rn the process\nZn(\u03bd) = 1\nn\nn\nX\nk=1\n\u03b5k\nu(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\nis subgaussian (see [27, De\ufb01nition 5.20]) with respect to the data dependent pseudometric\n\u03c1(\u03bd, \u03bd\u2032) = 1\nn\n n\nX\nk=1\n\u0012u(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\n\u2212u(\u27e8\u03bd\u2032, rk\u27e9)\nu(r\u2217\nk)\n\u00132!1/2\n,\nde\ufb01ned on \u2206. That is,\nE\n\u0012\ne\u03bb(Zn(\u03bd)\u2212Zn(\u03bd\u2032))\n\f\f\f\fRn\n\u0013\n=\nn\nY\ni=1\nE\n\u0014\nexp\n\u0012\u03bb\nn\u03b5i\nu(\u27e8\u03bd, rk\u27e9) \u2212u(\u27e8\u03bd\u2032, rk\u27e9)\nu(r\u2217\nk)\n\u0013\f\f\f\fRn\n\u0015\n\u2264e\u03bb2\u03c12(\u03bd,\u03bd\u2032)/2.\nHere we used an elementary inequality Ee\u03bb\u03b5ia \u2264e\u03bb2a2/2: [29, Example 2.3].\nA set N \u2282\u2206is called \u03f5-dispersed if \u03c1(\u03bd, \u03bd\u2032) \u2265\u03f5 for \u03bd, \u03bd\u2032 \u2208N with \u03bd \u0338= \u03bd\u2032. Let D(\u2206, \u03c1, \u03f5)\nbe the \u03f5-packing number of (\u2206, \u03c1):\nD(\u2206, \u03c1, \u03f5) = sup{|N| : N is an \u03f5-dispersed}.\nHere |N| is the cardinality of N. The conditional expectation of the supremum of Zn is\nbounded by the Dudley entropy integral ([5, Corollary 13.2]):\nbR(F \u25e6Rn) = E\n\u0012\nsup\n\u03bd\u2208\u2206\nZn(\u03bd)|Rn\n\u0013\n\u226412\nZ d/2\n0\np\nln D(\u2206, \u03c1, \u03f5) d\u03f5,\n(3.14)\nwhere d is the diameter of \u2206.\nConditions (3.5), (3.6) imply that\n\u03c1(\u03bd, \u03bd\u2032) \u2264K\nn\n n\nX\nk=1\n|\u27e8\u03bd \u2212\u03bd\u2032, rk\u27e9|2\u03b1\nu2(r\u2217\nk)\n!1/2\n\u2264K\nn\n n\nX\nk=1\n(r\u2217\nk)2\u03b1\u2225\u03bd \u2212\u03bd\u2032\u22252\u03b1\n1\nu2(r\u2217\nk)\n!1/2\n\u2264KA\n\u221an \u2225\u03bd \u2212\u03bd\u2032\u2225\u03b1\n1,\n(3.15)\n\n8\nDMITRY B. ROKHLIN\nwhere \u2225a\u22251 = Pd\nj=1 |aj| is the the l1-norm. For the \u03f5-packing number of \u2206with the metric,\ninduced by \u2225\u00b7 \u22251, we have the inequality D(\u2206, \u2225\u00b7 \u22251, \u03f5) \u2264(5/\u03f5)d\u22121 (see [9, Proposition C.1]).\nFrom (3.15) it follows that if \u03c1(\u03bd, \u03bd\u2032) \u2265\u03f5 then\n\u2225\u03bd \u2212\u03bd\u2032\u22251 \u2265\n\u0012\u221an\u03b5\nKA\n\u00131/\u03b1\n.\nHence,\nD(\u2206, \u03c1, \u03f5) \u2264D\n \n\u2206, \u2225\u00b7 \u22251,\n\u0012\u221an\u03f5\nKA\n\u00131/\u03b1!\n\u22645d\u22121\n\u0012 KA\n\u221an\u03f5\n\u0013(d\u22121)/\u03b1\n.",
    "chunk_index": 6,
    "start_char": 14490,
    "end_char": 16915,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "\u2225\u03bd \u2212\u03bd\u2032\u2225\u03b1\n1,\n(3.15)\n\n8\nDMITRY B. ROKHLIN\nwhere \u2225a\u22251 = Pd\nj=1 |aj| is the the l1-norm. For the \u03f5-packing number of \u2206with the metric,\ninduced by \u2225\u00b7 \u22251, we have the inequality D(\u2206, \u2225\u00b7 \u22251, \u03f5) \u2264(5/\u03f5)d\u22121 (see [9, Proposition C.1]).\nFrom (3.15) it follows that if \u03c1(\u03bd, \u03bd\u2032) \u2265\u03f5 then\n\u2225\u03bd \u2212\u03bd\u2032\u22251 \u2265\n\u0012\u221an\u03b5\nKA\n\u00131/\u03b1\n.\nHence,\nD(\u2206, \u03c1, \u03f5) \u2264D\n \n\u2206, \u2225\u00b7 \u22251,\n\u0012\u221an\u03f5\nKA\n\u00131/\u03b1!\n\u22645d\u22121\n\u0012 KA\n\u221an\u03f5\n\u0013(d\u22121)/\u03b1\n.\n(3.16)\nFurthermore, by (3.15) the diameter of \u2206with respect to \u03c1 is estimated as\nd \u22642\u03b1KA\n\u221an ,\n(3.17)\nsince \u2225\u03bd \u2212\u03bd\u2032\u22251 \u2264\u2225\u03bd\u22251 +\u2225\u03bd\u2032\u22251 \u22642. Let us substitute the estimates (3.16), (3.17) into (3.14),\nand perform the change of variables z = \u221an\u03b5/(2\u03b1\u22121KA):\nbR(F \u25e6Sn) \u226412\nZ 2\u03b1\u22121KA/\u221an\n0\nv\nu\nu\ntln\n \n5d\u22121\n\u0012 KA\n\u221an\u03f5\n\u0013(d\u22121)/\u03b1!\nd\u03f5\n= 12\nr\nd \u22121\n\u03b1\nZ 2\u03b1\u22121KA/\u221an\n0\ns\nln\n\u0012\n5\u03b1 KA\n\u221an\u03f5\n\u0013\nd\u03f5\n= 12\nr\nd \u22121\n\u03b1\n2\u03b1\u22121KA\n\u221an\nZ 1\n0\nr\nln\n5\u03b1\n2\u03b1\u22121z dz \u2264C1KA\nr\nd \u22121\n\u03b1n ,\nC1 = 12\nZ 1\n0\nr\nln 5\nz dz.\nTogether with (3.10) this completes the proof (C = 2C1).\n\u25a1\nIn a most natural way condition (3.6) is satis\ufb01ed by the power utility function u(x) = x\u03b1,\n\u03b1 \u2208(0, 1]. This function also satis\ufb01es (3.5) with K = 1, as easily follows from the inequality\n([12, Appendix A, Lemma 5.1])\n(x + y)\u03b1 \u2264x\u03b1 + y\u03b1,\nx, y > 0.\nFor u(x) = x\u03b1 the problem (2.1) reduces to the optimization of the ordinary power utility\nfunction after the price normalization:\nU(\u03bd) = E\u27e8\u03bd, rn+1/r\u2217\nn+1\u27e9\u03b1.\nThe power utility is natural in one more respect: the relative utility (3.1) in this case is\nindependent of investor\u2019s wealth x:\nEu(x\u27e8\u03bd, rn+1\u27e9)\nu\n\u0000xr\u2217\nn+1\n\u0001\n= E\u27e8\u03bd, rn+1/r\u2217\nn+1\u27e9\u03b1.\nThis means that one can consider the problems (2.1), (2.2) dynamically in an online manner.\nAt each step the investor will act myopically similar to the case of the ordinary logarithmic\nutility.\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n9\nRemark 1. Under additional assumptions condition (3.6) on the utility function can be\nrelaxed. In fact we need only the upper bound for r\u2217\nk/u(r\u2217\nk). Thus, if there exists a riskless\nasset (cash) with rk = 1, then the supremum in (3.6) can be taken over [1, \u221e). Furthermore,\nif the returns are bounded, then the supremum can be taken over a \ufb01nite interval. In this\ncase usually it is enough to consider the Lipschitz case \u03b1 = 1.\nRemark 2. Theorems 1, 2 give high probability error bounds. From (2.3), (2.4) it follows\nthat\nmax{U(\u03bd\u2217) \u2212EU(b\u03bdn), E(bUn(b\u03bdn) \u2212U(b\u03bdn))} \u2264E sup\n\u03bd\u2208\u2206\nGn(\u03bd),\nThus, Theorem 2 provides also error bounds in expectation.\nRemark 3. The obtained error bounds are of order n\u22121/2. In general the main assumption,\nwhich allows to obtain O(1/n) bounds, is the strong concavity of U: [24, 21]. However, such\nassumption requires additional conditions on the returns ri, which we want to avoid in the\npresent paper.",
    "chunk_index": 7,
    "start_char": 16544,
    "end_char": 19195,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "Lipschitz case \u03b1 = 1.\nRemark 2. Theorems 1, 2 give high probability error bounds. From (2.3), (2.4) it follows\nthat\nmax{U(\u03bd\u2217) \u2212EU(b\u03bdn), E(bUn(b\u03bdn) \u2212U(b\u03bdn))} \u2264E sup\n\u03bd\u2208\u2206\nGn(\u03bd),\nThus, Theorem 2 provides also error bounds in expectation.\nRemark 3. The obtained error bounds are of order n\u22121/2. In general the main assumption,\nwhich allows to obtain O(1/n) bounds, is the strong concavity of U: [24, 21]. However, such\nassumption requires additional conditions on the returns ri, which we want to avoid in the\npresent paper.\n4. Stochastic exponentiated gradient algorithm\nIn this section we additionally assume that the utility function u is concave. Recall that\nthe subdi\ufb00erential of \u2212u at any point y \u2208(0, \u221e) is an interval:\n\u2202(\u2212u)(y) = [\u2212D\u2212u(y), \u2212D+u(y)],\nwhere D\u2212u(y) and D+u(y) are the left and right derivatives: see [16, Chap. I]. We have\nD\u2212u(y) \u2265D+u(y) \u22650, as u is non-decreasing.\nWe use the exponentiated gradient (EG) algorithm of [18] to solve the empirical utility\nmaximization problem (2.2). Consider the empirical distribution generated by the sample\n(r1, . . . , rn), and a random variable br with this distribution:\nbP(br = rk) = 1\nn,\nk = 1, . . . , n.\nPut\nrn = min\n1\u2264k\u2264n min\n1\u2264i\u2264d ri\nk,\nrn = max\n1\u2264k\u2264n max\n1\u2264i\u2264d ri\nk\nand consider the convex functions\n\u03bd 7\u2192fj(\u03bd) = 1 \u2212u(\u27e8\u03bd, brj\u27e9)\nu(br\u2217\nj)\n: \u22067\u2192[0, 1].\nFrom the description of their subdi\ufb00erentials:\n\u2202fj(\u03bd) =\n\u001a\n\u03b3\nu(br\u2217\nj)brj : \u03b3 \u2208[\u2212D\u2212u(\u27e8\u03bd, brj\u27e9), \u2212D+u(\u27e8\u03bd, brj\u27e9)]\n\u001b\nand the inequalities 0 < rn \u2264\u27e8\u03bd, brj\u27e9, j = 1, . . . , n, we see that the absolute values of the\nsubgradient components are bounded by the constant\nLn = D\u2212u(rn) \u00b7\nmax\nrn\u2264x\u2264rn\nx\nu(x) = D\u2212u(rn) \u00b7\nrn\nu(rn).\nIndeed, u(x)/x is non-increasing: [16, Proposition 1.1.4], and the subdi\ufb00erential mapping is\nmonotone:\n\u03b31 \u2264\u03b32\nwhenever\n\u03b3i \u2208\u2202(\u2212u)(yi),\n0 < y1 < y2,\n\n10\nDMITRY B. ROKHLIN\nsee [16, Theorem 4.2.1]. It follows that the functions fj are Ln-Lipschitz with respect to\nl1-norm: see [22, Lemma 2.6].\nApply the exponentiated gradient algorithm to f1, . . . , fm:\n\u03bdi\n0 = 1/d,\ni = 1, . . . , d,\n(4.1)\nai\nj = \u03bdi\nj\u22121 exp\n\u0012\n\u03b7D\u2212u(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\nbri\nj\n\u0013\n,\n\u03bdi\nj =\nai\nj\nPd\nl=1 al\nj\n,\n(4.2)\ni = 1, . . . , d, j = 1, . . . , m \u22121, where \u03b7 > 0 is a parameter. Note that,\n\u2212D\u2212u(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\nbrj \u2208\u2202fj(\u03bd).\nFor a moment assume that brj \u2208(0, \u221e)d is an arbitrary sequence. The basic problem of\nthe online convex optimization theory is to \ufb01nd a sequence \u03bd0, . . . , \u03bdm\u22121 such that \u03bdj\u22121 does\nnot depend on fj, . . . , fm and the regret\nRegretm(\u03bd) =\nm\nX\nj=1\nfj(\u03bdj\u22121) \u2212\nm\nX\nj=1\nfj(\u03bd) =\nm\nX\nj=1\nu(\u27e8\u03bd, brj\u27e9)\nu(br\u2217\nj)\n\u2212\nm\nX\nj=1\nu(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\nis small uniformly over \u03bd \u2208\u2206.",
    "chunk_index": 8,
    "start_char": 18676,
    "end_char": 21262,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "brj\u27e9)\nu(br\u2217\nj)\nbrj \u2208\u2202fj(\u03bd).\nFor a moment assume that brj \u2208(0, \u221e)d is an arbitrary sequence. The basic problem of\nthe online convex optimization theory is to \ufb01nd a sequence \u03bd0, . . . , \u03bdm\u22121 such that \u03bdj\u22121 does\nnot depend on fj, . . . , fm and the regret\nRegretm(\u03bd) =\nm\nX\nj=1\nfj(\u03bdj\u22121) \u2212\nm\nX\nj=1\nfj(\u03bd) =\nm\nX\nj=1\nu(\u27e8\u03bd, brj\u27e9)\nu(br\u2217\nj)\n\u2212\nm\nX\nj=1\nu(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\nis small uniformly over \u03bd \u2208\u2206. It is well known that the EG algorithm with \u03b7 =\nq\nln d\nm\n1\nLn\nensures the estimate\nRegretm(\u03bd) \u22642Ln\n\u221am\n\u221a\nln d,\n(4.3)\nsee [22, Corollary 2.14] (a constant is corrected).\nFor an i.i.d. random sequence brj we can apply to (4.1), (4.2) the online-to-batch con-\nversion scheme: [22, Chap. 5]. In this case it is natural to call (4.1), (4.2) the stochastic\nexponentiated gradient (SEG) algorithm. Denote by bE is the expectation with respect to\nthe empirical distribution of r1, . . . , rn. For any \ufb01xed \u03bd,\nbEu(\u27e8\u03bd, brj\u27e9)\nu(br\u2217\nj)\n= 1\nn\nn\nX\nk=1\nu(\u27e8\u03bd, rk\u27e9)\nu(r\u2217\nk)\n= bUn(\u03bd).\n(4.4)\nFurthermore, since \u03bdj\u22121 is \u03c3(br1, . . . , brj\u22121)-measurable, we have\nbEu(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\n= bEbE\n\u0012u(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\n\f\f\f\fbr1, . . . , brj\u22121\n\u0013\n= bE1\nn\nn\nX\nk=1\nu(\u27e8\u03bdj\u22121, rk\u27e9)\nu(r\u2217\nk)\n,\n1\nm\nbE\nm\nX\nj=1\nu(\u27e8\u03bdj\u22121, rj\u27e9)\nu(r\u2217\nj)\n= 1\nm\nm\nX\nj=1\nbE1\nn\nn\nX\nk=1\nu(\u27e8\u03bdj\u22121, rk\u27e9)\nu(r\u2217\nk)\n= 1\nn\nn\nX\nk=1\nbE 1\nm\nm\nX\nj=1\nu(\u27e8\u03bdj\u22121, rk\u27e9)\nu(r\u2217\nk)\n\u2264bE1\nn\nn\nX\nk=1\nu(\u27e8\u03bdm, rk\u27e9)\nu(r\u2217\nk)\n= bEbUn(\u03bdm),\n(4.5)\nwhere\n\u03bdm = 1\nm\nm\u22121\nX\nj=0\n\u03bdj.\n(4.6)\nIn these calculations r1, . . . , rn are regarded as constants. Note that \u03bdj, \u03bdm depend also on\nn, but we suppress this dependence in the notation.\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n11\nFrom (4.3) \u2013 (4.5) we get\n2Ln\nr\nln d\nm \u2265bERegretm(\u03bd)\nm\n= 1\nm\nbE\nm\nX\nj=1\n\u0012u(\u27e8\u03bd, brj\u27e9)\nu(br\u2217\nj)\n\u2212u(\u27e8\u03bdj\u22121, brj\u27e9)\nu(br\u2217\nj)\n\u0013\n\u2265bUn(\u03bd) \u2212bEbUn(\u03bdm).\nIn particular, for an empirical utility maximizer b\u03bdn,\nbUn(b\u03bdn) \u2264bEbUn(\u03bdm) + 2Ln\nr\nln d\nm \u2264bUn(\u03bdm) +\nr\n1\n2n ln 1\n\u03b4 + 2Ln\nr\nln d\nm\n(4.7)\nwith probability at least 1 \u2212\u03b4 by Hoe\ufb00ding\u2019s inequality ([20, Theorem D.2]):\nbP(bEbUn(\u03bdm) \u2212bUn(\u03bdm) \u2265\u03b5) = bP\n \n1\nn\nn\nX\nk=1\nu(\u27e8\u03bdm, rk\u27e9)\nu(r\u2217\nk)\n\u2212bE1\nn\nn\nX\nk=1\nu(\u27e8\u03bdm, rk\u27e9)\nu(r\u2217\nk)\n\u2265\u03b5\n!\n\u2264e\u22122\u03b52n\nwith \u03b5 =\nq\n1\n2n ln 1\n\u03b4.\nWe now able to provide for \u03bdm an analog of inequality (3.3):\nU(\u03bd\u2217) \u2212U(\u03bdm) = U(\u03bd\u2217) \u2212bUn(\u03bd\u2217) + bUn(\u03bd\u2217) \u2212bUn(\u03bdn) + bUn(\u03bdn) \u2212bUn(\u03bdm) + bUn(\u03bdm) \u2212U(\u03bdm)\n\u2264(U(\u03bd\u2217) \u2212bUn(\u03bd\u2217)) + (bUn(\u03bdn) \u2212bUn(\u03bdm)) + sup\n\u03bd\u2208\u2206\nGn(\u03bd).\nApplying (3.2), (4.7) and (3.1) respectively to the tree terms in the right-hand side, we get\nthe following result.",
    "chunk_index": 9,
    "start_char": 20869,
    "end_char": 23273,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "k=1\nu(\u27e8\u03bdm, rk\u27e9)\nu(r\u2217\nk)\n\u2212bE1\nn\nn\nX\nk=1\nu(\u27e8\u03bdm, rk\u27e9)\nu(r\u2217\nk)\n\u2265\u03b5\n!\n\u2264e\u22122\u03b52n\nwith \u03b5 =\nq\n1\n2n ln 1\n\u03b4.\nWe now able to provide for \u03bdm an analog of inequality (3.3):\nU(\u03bd\u2217) \u2212U(\u03bdm) = U(\u03bd\u2217) \u2212bUn(\u03bd\u2217) + bUn(\u03bd\u2217) \u2212bUn(\u03bdn) + bUn(\u03bdn) \u2212bUn(\u03bdm) + bUn(\u03bdm) \u2212U(\u03bdm)\n\u2264(U(\u03bd\u2217) \u2212bUn(\u03bd\u2217)) + (bUn(\u03bdn) \u2212bUn(\u03bdm)) + sup\n\u03bd\u2208\u2206\nGn(\u03bd).\nApplying (3.2), (4.7) and (3.1) respectively to the tree terms in the right-hand side, we get\nthe following result.\nTheorem 3. Assume that the function u is concave. Then for the average portfolio (4.6),\nproduced by the SEG algorithm (4.1), (4.2), with probability at least 1 \u22123\u03b4 the following\nestimate holds true:\nU(\u03bd\u2217) \u2212U(\u03bdm) \u2264E sup\n\u03bd\u2208\u2206\nGn(\u03bd) + 3\nr\n1\n2n ln 1\n\u03b4 + 2Ln\nr\nln d\nm .\nCertainly, the estimates of Theorem 2 still can be applied to E sup\u03bd\u2208\u2206Gn(\u03bd). Thus, Theo-\nrem 3 gives a high-probability bound for the estimation error of the stochastic exponentiated\ngradient algorithm. The value of m can be taken su\ufb03ciently large to get for the estimation\nerror of \u03bdm the bound of the same order as for the exact empirical utility maximizer b\u03bdn. The\nmentioned value of m is data dependent, since the Lipschitz constant Ln depends on the\nreturns (r1, . . . , rn). Note, that we need no new data to generate an arbitrary large sample\nbr1, . . . , brm used in the SEG algorithm.\n5. Power utility: the case of one risky asset\nConsider the case d = 2. In this section we will put upper indexes in brackets. Assume\nthat the investor can keep money in cash: r(1)\nt\n= 1, or invest in a risky asset, whose daily\nreturns are log-normal and follow the discrete-time Black-Scholes model:\nr(2)\nk\n= exp\n\u0012\u00b5 \u2212\u03c32/2\nT\n+ \u03c3\n\u221a\nT\nZk\n\u0013\n,\nk = 1, . . . , n.\n(5.1)\nHere T = 252 is the number of trading days in a year; Zk are independent standard normal\nvariables: Zk \u223cN(0, 1); n is the sample size, which we assume to be multiple of T. Put\n\n12\nDMITRY B. ROKHLIN\nTable 1. Average optimal weight \u03bd(2) of the risky asset\n\u03b1\n0.001\n0.01\n0.1\n0.2\n0.3\n0.5\n0.75\n0.9\nOrdinary power\nutility, \u03d5\n0.7380\n0.7448\n0.8188\n0.9118\n0.9775\n1\n1\n1\nRelative power\nutility, \u03c8\n0.7376\n0.7397\n0.7637\n0.7961\n0.8367\n0.9245\n0.9909\n1\n\u00b5 = 0.15, which corresponds to\nE\nTY\nk=1\nr(2)\nk\n= e\u00b5 \u22481.162\nannual expected return for the risky asset, and \u03c3 = 0.45. We have\nln r(2)\nk\n\u223cN\n\u0012\u00b5 \u2212\u03c32/2\nT\n, \u03c3\n\u221a\nT\n\u0013\n= N(1.93 \u00b7 10\u22124, 2.83 \u00b7 10\u22122).\nIn this section we assume that u(x) = x\u03b1, \u03b1 \u2208(0, 1]. The the relative empirical utility\nmaximization problem (2.2) takes the form\n\u03c8(\u03bd(2)) = 1\nn\nn\nX\nk=1\n\u27e8\u03bd, rk/r\u2217\nk\u27e9\u03b1 = 1\nn\nn\nX\nk=1\n \n1\nmax{1, r(2)\nk }\n+\nr(2)\nk\n\u22121\nmax{1, r(2)\nk }\n\u03bd(2)\n!\u03b1\n\u2192\nmax\n\u03bd(2)\u2208[0,1] .",
    "chunk_index": 10,
    "start_char": 22860,
    "end_char": 25375,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "return for the risky asset, and \u03c3 = 0.45. We have\nln r(2)\nk\n\u223cN\n\u0012\u00b5 \u2212\u03c32/2\nT\n, \u03c3\n\u221a\nT\n\u0013\n= N(1.93 \u00b7 10\u22124, 2.83 \u00b7 10\u22122).\nIn this section we assume that u(x) = x\u03b1, \u03b1 \u2208(0, 1]. The the relative empirical utility\nmaximization problem (2.2) takes the form\n\u03c8(\u03bd(2)) = 1\nn\nn\nX\nk=1\n\u27e8\u03bd, rk/r\u2217\nk\u27e9\u03b1 = 1\nn\nn\nX\nk=1\n \n1\nmax{1, r(2)\nk }\n+\nr(2)\nk\n\u22121\nmax{1, r(2)\nk }\n\u03bd(2)\n!\u03b1\n\u2192\nmax\n\u03bd(2)\u2208[0,1] .\n(5.2)\nFor comparison consider also the ordinary empirical utility:\n\u03d5(\u03bd(2)) = 1\nn\nn\nX\nk=1\n\u27e8\u03bd, rk\u27e9\u03b1 = 1\nn\nn\nX\nk=1\n\u0010\n1 + (r(2)\nk\n\u22121)\u03bd(2)\u0011\u03b1\n\u2192\nmax\n\u03bd(2)\u2208[0,1] .\n(5.3)\nFor a large n = T \u00b7 103 = 2.52 \u00b7 105 we applied to \u03d5\u2032(\u03bd2), \u03c8\u2032(\u03bd2) the bisection method\noptimize.bisect from the module scipy (Python) with the default tolerance parameter.\nThe results, averaged over 100 realizations of (r(2)\nk )n\nk=1, are presented in Table 1.\nWe see that the relative utility makes the investor more risk averse. This property can\nbe easily explained. Instead of the power utility function consider a di\ufb00erentiable increasing\nconcave function u.\nWithout loss of generality, we can assume that u(1) = 1.\nFor the\nexpected utilities, corresponding to (5.2), (5.3), we have\n\u03c8\u2032(\u03bd(2)) := \u2202U(\u03bd)\n\u2202\u03bd(2) = E\n\u0012u\u2032(1 + (r(2) \u22121)\u03bd(2))\nu(max{1, r(2)})\n(r(2) \u22121)\n\u0013\n= E\n\u0000u\u2032(1 + (r(2) \u22121)\u03bd(2))(r(2) \u22121)I{r(2)\u22641}\n\u0001\n+ E\n\u0012u\u2032(1 + (r(2) \u22121)\u03bd(2))\nu(r(2))\n(r(2) \u22121)I{r(2)>1}\n\u0013\n\u2264E\n\u0000u\u2032(1 + (r(2) \u22121)\u03bd(2))(r(2) \u22121)\n\u0001\n= \u2202eU(\u03bd)\n\u2202\u03bd(2) =: \u03d5\u2032(\u03bd(2)),\nwhere eU(\u03bd) = Eu(\u27e8\u03bd, r\u27e9) is the ordinary expected utility. The functions \u03c8\u2032, \u03d5\u2032 are decreasing.\nIt follows that the zero of \u03c8\u2032 is smaller than the zero of \u03d5\u2032 (for simplicity we assume that a\nzero is unique). A similar argumentation works for the empirical utilities.\nHowever, in the next section we will see that the discussed property is not universal. In\na model with several risky assets the optimal portfolio, corresponding to the relative power\nutility, can be more risky, than for the ordinary utility.\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n13\nNext we argue that if the price of a risky asset follows the Black-Scholes model, neither\n10 nor 100 years are enough to make any reliable conclusions concerning the optimal value\n\u03bd(\u2217,2) on the basis of daily historical prices.\nFor \u03b1 = 0.2 in the left panels of Fig. 1 we show the histograms of the optimal weight b\u03bd(2)\nn\nof\nthe risky asset for 200 realizations of daily returns (r(2)\nk )n\nk=1, where n = 252 \u00b7 10k, k = 1, 2, 3.\nTo estimate the true utility U(\u03bd) of b\u03bd we used the empirical mean bUN(\u03bd) with very large\nN = 107. The histogram of linearly transformed true utilities (U(b\u03bd)\u2212U(w0))\u00b7104, w0 = (1, 0)\nare shown in the right panels in Fig. 1.",
    "chunk_index": 11,
    "start_char": 25006,
    "end_char": 27596,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "b\u03bd(2)\nn\nof\nthe risky asset for 200 realizations of daily returns (r(2)\nk )n\nk=1, where n = 252 \u00b7 10k, k = 1, 2, 3.\nTo estimate the true utility U(\u03bd) of b\u03bd we used the empirical mean bUN(\u03bd) with very large\nN = 107. The histogram of linearly transformed true utilities (U(b\u03bd)\u2212U(w0))\u00b7104, w0 = (1, 0)\nare shown in the right panels in Fig. 1. In the same way we obtained the estimates of the\noptimal weight of the risky asset: \u03bd\u2217,2 \u22480.81, and its utility\n(U(b\u03bd\u2217) \u2212U(w0)) \u00b7 104 \u22480.42.\n(5.4)\nWe see that optimal portfolio weights very slowly concentrate near the optimal value. In\nparticular for n = 252 \u00b7 10 in most cases b\u03bd(2)\nn\nsimply takes the extreme values 0 and 1. Only\nfor n = 252 \u00b7 103 the largest peak is near the optimum. But even in this case it is blurred.\nNote, however, that the true utilities of b\u03bd(2)\nn\ndemonstrate somewhat better concentration\nnear the optimum (5.4). These conclusions are not speci\ufb01c for the relative power utility or\nfor a speci\ufb01c value of \u03b1. For for other values of \u03b1, and for the ordinary power or logarithmic\nutilities the results will be similar.\nNote that the slow concentration phenomenon (which is related to the fragility of SAA\nin portfolio optimization: [1]) does not contradict Theorems 1, 2. Roughly speaking, these\ntheorems give the estimate\nU(\u03bd\u2217) \u2212U(w0) \u2264U(b\u03bdn) \u2212U(w0) + O\n\u0012 1\n\u221an\n\u0013\nwith high probability. From (5.4) it follows that we need n at least of order 108 to get a\nnontrivial lower bound for U(b\u03bdn) \u2212U(w0).\n6. Experiments with NYSE data\nWe considered two datasets, containing daily stock returns form the New-York Stock\nExchange (NYSE):\n\u2022 NYSE1: Contains 5651 daily returns of 36 stocks for the period ending in 1984,\n\u2022 NYSE2: Contains 11178 daily returns of 19 stocks for the period ending in 2006.\nBoth datasets were taken from http://www.cs.bme.hu/~oti/portfolio/data.html. NYSE1\nis a classical dataset, considered in many papers, starting from [7] (see the references in\n[13, 14]). NYSE2 was \ufb01rst analized in [13], where the authors also proposed a simple greedy\nalgorithm for the empirical logarithmic utility maximization:\n1\nn\nn\nX\nk=1\nln\u27e8\u03bd, rk\u27e9\u2192max\n\u03bd\u2208\u2206.\nIn this paper we are interested in an application of the exponentited gradient (EG) algo-\nrithm. Note that already in [15] this algorithm was applied to the NYSE1 dataset and the\nlogarithmic utility. However, our goal here is di\ufb00erent: we want to solve the problem (2.2).\nUnfortunately we were unable to do this using the algorithm in the form (4.1), (4.2) or with\ntime-varying learning rate \u03b7 (e.g., applying the doubling trick: see [22]). So, we propose its\nmodi\ufb01cation: the greedy doubly stochastic exponentiated gradient (GDSEG) algorithm. For\nclarity we present its pseudocode for the power utility u(x) = x\u03b1.\n\n14\nDMITRY B. ROKHLIN\nFigure 1. Histograms of optimal weight b\u03bd(2)\nn\nof the risky asset (left panels)\nand of linearly transformed true utility (U(b\u03bdn) \u2212U(w0)) \u00b7 104, w0 = (1, 0)\n(right panels) for 200 realizations of daily returns (r(2)\nk )n\nk=1 for n = 252 \u00b7 10k,\nk = 1, 2, 3.",
    "chunk_index": 12,
    "start_char": 27258,
    "end_char": 30263,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "were unable to do this using the algorithm in the form (4.1), (4.2) or with\ntime-varying learning rate \u03b7 (e.g., applying the doubling trick: see [22]). So, we propose its\nmodi\ufb01cation: the greedy doubly stochastic exponentiated gradient (GDSEG) algorithm. For\nclarity we present its pseudocode for the power utility u(x) = x\u03b1.\n\n14\nDMITRY B. ROKHLIN\nFigure 1. Histograms of optimal weight b\u03bd(2)\nn\nof the risky asset (left panels)\nand of linearly transformed true utility (U(b\u03bdn) \u2212U(w0)) \u00b7 104, w0 = (1, 0)\n(right panels) for 200 realizations of daily returns (r(2)\nk )n\nk=1 for n = 252 \u00b7 10k,\nk = 1, 2, 3. The case of relative power utility with \u03b1 = 0.2.\nThe algorithm accepts either the original returns rk, or the scaled returns rk/r\u2217\nk. The \ufb01rst\ncase corresponds to the traditional power utility, the second one to the relative power utility.\nAt each point \u03bd the algorithm tries to make a step according to line 9, corresponding to\n(4.2), where the return rk and the learning rate are taken randomly by sampling k and \u03b7\nfrom the uniform distributions over {1, . . . , n} and [0, \u03b7] respectively. In fact, this is a step of\na stochastic gradient method with random learning rate. That\u2019s why we call the algorithm\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n15\nGreedy doubly stochastic exponentiated gradient algorithm (GDSEG) for the power utility\nInput: \u03b7 > 0: an upper bound for learning rate; n_attempts: an upper bound for the num-\nber of attempts to improve a current portfolio; threshold: an improvement threshold;\n{ri\nk : k \u2208{1, . . . , n}, i \u2208{1, . . . , d}}: an array of daily returns; \u03b1 \u2208(0, 1]\n1: \u03bdi := 1/d, i = 1, . . . , d\n2: if the relative utility is considered then\n3:\nri\nk := ri\nk/ maxd\nj=1(rj\nk), i = 1, . . . , d, k = 1, . . . , n\n4: end if\n5: attempt := 0\n6: while attempt \u2264n_attempts do\n7:\nChoose k \u2208{1, . . . , n} uniformly at random\n8:\nChoose \u03b7 \u2208[0, \u03b7] uniformly at random\n9:\nai := \u03bdi exp (\u03b7ri\nk/\u27e8\u03bd, rk\u27e91\u2212\u03b1) ,\nwi :=\nai\nPd\nj=1 aj ,\n10:\nattempt := attempt + 1\n11:\nif\n1\nn\nPn\nt=1\u27e8w, rt\u27e9\u03b1 \u22651\nn\nPn\nt=1\u27e8\u03bd, rt\u27e9\u03b1 + threshold then\n12:\n\u03bd := w, attempt := 0\n13:\nend if\n14: end while\nOutput: an optimal portfolio \u03bd\n\u201cdoubly stochastic\u201d. Furthermore, the step will be actually performed only if the value of the\nobjective function for the new portfolio w surpasses the current value by a threshold: line\n11. The algorithm stops if no such improvement is obtained for some prede\ufb01ned number of\nattempts: n_attempts.\nFor the logarithmic utility one should put \u03b1 = 0, and substitute in line 11 the power\nfunction by the logarithm. We do not consider the relative utility in this case.",
    "chunk_index": 13,
    "start_char": 29660,
    "end_char": 32264,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "optimal portfolio \u03bd\n\u201cdoubly stochastic\u201d. Furthermore, the step will be actually performed only if the value of the\nobjective function for the new portfolio w surpasses the current value by a threshold: line\n11. The algorithm stops if no such improvement is obtained for some prede\ufb01ned number of\nattempts: n_attempts.\nFor the logarithmic utility one should put \u03b1 = 0, and substitute in line 11 the power\nfunction by the logarithm. We do not consider the relative utility in this case.\nThe algorithm was applied to NYSE1 and NYSE2 datasets with the following parameters:\n\u03b7 = 1, n_attempts = 104, threshold = 10\u221210. The number of iterations and the results\ndepend on the seed parameter. The average number of attempts to improve the current\nportfolio for 30 runs of the algorithm was about 283 \u00b7 103 for NYSE1 and 73 \u00b7 103 for NYSE2.\nIn both cases the output portfolio \u03bd concentrates only on few stocks: 5 for NYSE1 and 3 for\nNYSE2. We drop \u03bdi with \u03bdi < 0.001 and normalize the results:\n\u03bdi :=\n\u03bdiI{\u03bdi\u22650.001}\nPd\nj=1 \u03bdjI{\u03bdj\u22650.001}\n.\nFor the logarithmic utility the results can be compared with those of [4, 13]. In Tables\n2, 3 we present minimal and maximal values for each weight, obtained in 30 runs of the\nGDSEG algorithm. The accumulated wealth Xn = Qn\nt=1\u27e8\u03bd, rt\u27e9, in fact, does not depend on\na particular output \u03bd:\nNYSE1 : X5651 \u2248250.6,\nannual return: 1.279;\nNYSE2 : X11178 \u22484100.8,\nannual return: 1.206.\nThe annual return is computed by the formula X252/n\nn\n.\nIn general the GDSEG algorithm need not be so stable. For the power utility u(x) = x\u03b1\nwe implemented the following strategy: take an output \u03bd, corresponding to the largest value\n\n16\nDMITRY B. ROKHLIN\nTable 2. Optimal weights for the logarithmic utility, NYSE1: 30 experiments\nof the GDSEG algorithm\nStock\nWeight\n[4]\nWeight\nGDSEG, [min, max]\ncomme\n0.2767\n[0.2766, 0.2770]\nespey\n0.1953\n[0.1952, 0.1956]\niroqu\n0.0927\n[0.0925, 0.0929]\nkinar\n0.2507\n[0.2506, 0.2508]\nmeico\n0.1845\n[0.1842, 0.1847]\nTable 3. Optimal weights for the logarithmic utility, NYSE2: 30 experiments\nof the GDSEG algorithm\nStock\nWeight\n[13]\nWeight\nGDSEG, [min, max]\nhp\n0.177\n[0.1771, 0.1776]\nmorris\n0.747\n[0.7468, 0.7472]\nschlum\n0.076\n[0.0753, 0.0757]\nof the empirical utility function obtained in 10 experiments. The results for NYSE2 dataset\nare presented in Table 4. In the sequel we concentrate only on NYSE2.\nTable 4. NYSE2: optimal portfolio weights, corresponding to the largest\nvalue of the empirical power utility function obtained in 10 experiments of\nthe GDSEG algorithm; the accumulated wealth Xn, n = 11178; the annual\nreturns and the annual volatilities of these portfolios\nOrdinary utility\nRelative utility\n\u03b1\nStocks\nWeights\nXn\nAnn\nret.\nAnn.\nvolat.\nWeights\nXn\nAnn.\nret.\nAnn.\nvolat.\n0.01\nhp\nmorris\nschlum\n0.1792\n0.7518\n0.0690\n4100.4\n1.206\n0.234\n0.1782\n0.7523\n0.0695\n4100.4\n1.206\n0.234\n0.1\nhp\nmorris\nschlum\n0.1762\n0.7766\n0.0473\n4091.2\n1.206\n0.237\n0.1617\n0.7882\n0.0501\n4085.7\n1.206\n0.238\n0.2\nhp\nmorris\n0.1779\n0.8221\n4035.7\n1.206\n0.245\n0.1476\n0.8524\n3999.7",
    "chunk_index": 14,
    "start_char": 31781,
    "end_char": 34774,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "GDSEG algorithm; the accumulated wealth Xn, n = 11178; the annual\nreturns and the annual volatilities of these portfolios\nOrdinary utility\nRelative utility\n\u03b1\nStocks\nWeights\nXn\nAnn\nret.\nAnn.\nvolat.\nWeights\nXn\nAnn.\nret.\nAnn.\nvolat.\n0.01\nhp\nmorris\nschlum\n0.1792\n0.7518\n0.0690\n4100.4\n1.206\n0.234\n0.1782\n0.7523\n0.0695\n4100.4\n1.206\n0.234\n0.1\nhp\nmorris\nschlum\n0.1762\n0.7766\n0.0473\n4091.2\n1.206\n0.237\n0.1617\n0.7882\n0.0501\n4085.7\n1.206\n0.238\n0.2\nhp\nmorris\n0.1779\n0.8221\n4035.7\n1.206\n0.245\n0.1476\n0.8524\n3999.7\n1.206\n0.248\n0.3\nhp\nmorris\n0.1589\n0.8411\n4016.1\n1.206\n0.247\n0.1069\n0.8931\n3912.5\n1.205\n0.253\n0.5\nhp\nmorris\n0.0972\n0.9028\n3885.4\n1.205\n0.254\n0\n1\n3496.7\n1.202\n0.270\n0.75\nmorris\n1\n3496.7\n1.202\n0.269\n1\n3496.7\n1.202\n0.270\nNote that as \u03b1 is growing, the utility maximizer concentrates more on one stock. This\ne\ufb00ect is stronger for the relative utility. Such behavior can be quali\ufb01ed as more risky: see\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n17\nthe annual volatility of portfolio returns in Table 4. This quantity is de\ufb01ned as the empirical\nstandard deviation of (\u27e8b\u03bdn, rk\u27e9)n\nk=1, multiplied by\n\u221a\n252. For the log-optimal portfolio from\nTable 3 it equals to 0.233.\nData used in the above calculations can be considered as a realization of some multidi-\nmensional stochastic process. From the example considered in Section 5 it is clear that the\nvalues of an empirical utility function can be very sensitive to such realizations. To get more\ninsight on the risk and generalization properties of empirically optimal portfolios, let us try\nto describe the stock prices by the multidimensional Black-Scholes model:\ndSi\nt = Si\nt\u00b5idt + Si\nt\nm\nX\nj=1\n\u03c3ij dW j\nt ,\ni = 1, . . . , d,\n(6.1)\nwhere (W 1, . . . , W m) is a standard Wiener process, \u00b5 is the drift vector and \u03c3 is the volatility\nmatrix. Solving the system of stochastic di\ufb00erential equations (6.1), we get\nSi\nt = Si\n0 exp\n \n\u00b5i \u22121\n2\nm\nX\nj=1\n(\u03c3ij)2\n!\nt +\nm\nX\nj=1\n\u03c3ijW j\nt\n!\n,\ni = 1, . . . , d.\nIf t = 1 corresponds to one year, then the daily log-returns should be approximated as follows\nln ri\nk =\n \n\u03b1i \u22121\n2\nm\nX\nj=1\n(\u03c3ij)2\n!\nh +\nm\nX\nj=1\n\u03c3ij(W j\nkh \u2212W j\n(k\u22121)h),\nh = 1/252,\nk = 1, . . . , n. (6.2)\nWe estimated the expectation vector and the covariance matrix\n \n\u03b1ih \u22121\n2\nm\nX\nj=1\n(\u03c3ij)2h\n!d\ni=1\n,\n m\nX\nk=1\n\u03c3ik\u03c3kjh\n!d\ni,j=1\nof (ln ri\nk)d\ni=1 for NYSE2 dataset, using the numpy module. This allows to generate the arti\ufb01-\ncial data by (6.2). For the empirically optimal portfolios from Tables 3, 4, as well as for the\nportfolio with uniform weights: w = (1/d, . . . , 1/d), d = 19, we computed some statistical\ncharacteristics of the annual accumulated wealth X252, using these data.",
    "chunk_index": 15,
    "start_char": 34274,
    "end_char": 36917,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "m\nX\nj=1\n(\u03c3ij)2h\n!d\ni=1\n,\n m\nX\nk=1\n\u03c3ik\u03c3kjh\n!d\ni,j=1\nof (ln ri\nk)d\ni=1 for NYSE2 dataset, using the numpy module. This allows to generate the arti\ufb01-\ncial data by (6.2). For the empirically optimal portfolios from Tables 3, 4, as well as for the\nportfolio with uniform weights: w = (1/d, . . . , 1/d), d = 19, we computed some statistical\ncharacteristics of the annual accumulated wealth X252, using these data. The results are col-\nlected in Table 5. This table mainly demonstrates the risk properties of empirically optimal\nportfolios. For example, as \u03b1 growth, the portfolios become more risky: their expectations\nand standard deviations increase, but medians decrease. The portfolios, corresponding to\nthe relative power utility are more risky than for the ordinary one, in contrast to the example\nin Section 5, but in accordance with Table 4: see again the annual volatility columns.\nThe considered dataset is favorable for the investor: the stock prices are growing (on\naverage). Moreover, the performance is evaluated with respect to a concrete model. However,\neven in this case the investment decisions, based on the historical data, are risky.\nFor\nexample, from Table 5 we see that for the log-optimal portfolio there is 5% chance to loose\nmore than 18% of an initial wealth within 1 year.\nNote that the means are larger than the medians. This is in line with [13], where it is\nexplained that typically Xn is less then the EXn for log-optimal portfolios. We see also that\nthe medians give good estimates for the annual returns from Table 4.\nFinally, we tried to estimate the true utility of the empirically optimal portfolios, con-\nstructed for trajectories of the Black-Scholes model. We used the same method as in Section\n5, but with the GDSEG algorithm instead of bisection. Namely, for \u03b1 = 0.2 we considered\n\n18\nDMITRY B. ROKHLIN\nTable 5. Statistical characteristics of the annual accumulated wealth X252\nfor the portfolios from Table 4 for the arti\ufb01cial data (6.2) with the param-\neters, estimated for NYSE2. Averaging was performed over 106 realizations,\ngenerated by the Black-Scholes model.\nPortfolio\nMean\nMedian\nStd.\ndeviation\n5-th\npercentile\n95-th\npercentile\nuniform\n1.165\n1.152\n0.183\n0.891\n1.487\nlog-optimal\n1.240\n1.207\n0.294\n0.820\n1.772\n\u03b1 = 0.01\nordinary\nrelative\n1.240\n1.240\n1.207\n1.207\n0.295\n0.295\n0.819\n0.819\n1.775\n1.775\n\u03b1 = 0.1\nordinary\nrelative\n1.241\n1.242\n1.207\n1.207\n0.299\n0.300\n0.815\n0.814\n1.785\n1.787\n\u03b1 = 0.2\nordinary\nrelative\n1.243\n1.244\n1.206\n1.206\n0.310\n0.314\n0.805\n0.801\n1.808\n1.815\n\u03b1 = 0.3\nordinary\nrelative\n1.244\n1.245\n1.206\n1.205\n0.312\n0.320\n0.803\n0.794\n1.812\n1.828\n\u03b1 = 0.5\nordinary\nrelative\n1.245\n1.247\n1.205\n1.202\n0.322\n0.342\n0.793\n0.771\n1.831\n1.872\n200 trajectories (r1, . . . , rn), n = 11178 generated by the Black-Scholes model (6.2) with pa-\nrameters, estimated for NYSE2 dataset. For each trajectory the empirically optimal portfolio\nwas computed by the GDSEG algorithm (we picked the best portfolio in 10 experiments).",
    "chunk_index": 16,
    "start_char": 36509,
    "end_char": 39470,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "1.815\n\u03b1 = 0.3\nordinary\nrelative\n1.244\n1.245\n1.206\n1.205\n0.312\n0.320\n0.803\n0.794\n1.812\n1.828\n\u03b1 = 0.5\nordinary\nrelative\n1.245\n1.247\n1.205\n1.202\n0.322\n0.342\n0.793\n0.771\n1.831\n1.872\n200 trajectories (r1, . . . , rn), n = 11178 generated by the Black-Scholes model (6.2) with pa-\nrameters, estimated for NYSE2 dataset. For each trajectory the empirically optimal portfolio\nwas computed by the GDSEG algorithm (we picked the best portfolio in 10 experiments).\nFor a \ufb01xed trajectory the optimal portfolio concentrated on a few number of stock (from 1\nto 4). For illustration purposes in Fig. 2(a) we show the average weight of each stock over 200\noptimal portfolios. As in Table 3, the largest average weights have the stocks with numbers\n9 (hp), 16 (morris), 18 (schlum). The next two positions occupy 12 (jnj) and 14 (merck).\nThe true utility of each portfolio was evaluated by the empirical mean, computed for a\nlarge sample: n = 107. In Fig. 2(b), similar to left panels in Fig. 1, we see a large cluster\nof very good portfolios. However, the the concentration is far from perfect. Let us mention\nalso that the median (\u22481.45) of the true utility is greater than the mean (\u22481.40).\n7. Conclusion\nIn this paper we studied generalization properties of the empirically optimal portfolios\nfor the relative utility maximization problem. We obtained high probability bounds for the\nestimation error and for the di\ufb00erence between the empirical and true utilities.\nSimilar\nbounds were obtained for the portfolios, produced by the stochastic exponentiated gradient\nalgorithm. The only assumptions, imposed on the returns is the i.i.d. hypothesis. The\nobtained bounds depend only the information available to the investor. We also performed\n\nRELATIVE UTILITY BOUNDS FOR EMPIRICALLY OPTIMAL PORTFOLIOS\n19\nFigure 2. Relative power utility with \u03b1 = 0.2. (a) Average weight of each\nstock in empirically optimal portfolio over 200 realizations of the Black-Scholes\nmodel (6.2); (b) Histogram of the evaluated true utility for the same 200\noptimal portfolios.\nsome statistical experiments, demonstrating risk and generalization properties of the empir-\nically optimal portfolios. For a multidimensional problem we proposed the greedy doubly\nstochastic exponentiated gradient (GDSEG) algorithm.\nLet us mention some topics for further study.\n\u2022 In Theorems 1 \u2013 3 we considered the case of relative utility functions. To obtain\nsimilar bounds for ordinary utilities, in general one need to analyze the tails of the\nreturn distributions. In addition, the results of [6] should be useful for analysis of\nthis problem.\n\u2022 The proposed GDSEG algorithm was enough for our purposes, but it requires large\namount of calculations. It may be interesting to study this algorithm and its im-\nprovements in more detail.\n\u2022 Using side information is an important method for the construction of successful\nportfolio strategies. The recent papers [3, 2] contain theoretical and practical ideas\nthat can be employed to study this problem in the statistical learning framework.\nReferences\n[1] G.-Y. Ban, N. El Karoui, and A.E.B. Lim. Machine learning and portfolio optimization.",
    "chunk_index": 17,
    "start_char": 39017,
    "end_char": 42151,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "was enough for our purposes, but it requires large\namount of calculations. It may be interesting to study this algorithm and its im-\nprovements in more detail.\n\u2022 Using side information is an important method for the construction of successful\nportfolio strategies. The recent papers [3, 2] contain theoretical and practical ideas\nthat can be employed to study this problem in the statistical learning framework.\nReferences\n[1] G.-Y. Ban, N. El Karoui, and A.E.B. Lim. Machine learning and portfolio optimization. Management\nScience, 64(3):1136\u20131154, 2018.\n[2] T. Bazier-Matte and E. Delage. Generalization bounds for regularized portfolio selection with market\nside information. INFOR: Information Systems and Operational Research, 58(2):374\u2013401, 2020.\n[3] D. Bertsimas and N. Kallus. From predictive to prescriptive analytics. Management Science, 66(3):1025\u2013\n1044, 2020.\n[4] A. Borodin, R. El-Yaniv, and V. Gogan. On the competitive theory and practice of portfolio selection\n(extended abstract). In G.H. Gonnet and A. Viola, editors, LATIN 2000: Theoretical Informatics, pages\n173\u2013196, Berlin, Heidelberg, 2000. Springer.\n[5] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of inde-\npendence. Oxford University Press, Oxford, 2013.\n\n20\nDMITRY B. ROKHLIN\n[6] C. Cortes, S. Greenberg, and M. Mohri. Relative deviation learning bounds and generalization with\nunbounded loss functions. Ann. Math. Artif. Intell., 85:45\u201370, 2019.\n[7] T.M. Cover. Universal portfolios. Mathematical Finance, 1(1):1\u201329, 1991.\n[8] V. DeMiguel, L. Garlappi, F.J. Nogales, and R. Uppal. A generalized approach to portfolio optimization:\nImproving performance by constraining portfolio norms. Management Science, 55(5):798\u2013812, 2009.\n[9] S. Ghosal and A. van der Vaart. Fundamentals of Nonparametric Bayesian Inference. Cambridge Uni-\nversity Press, Cambridge, 2017.\n[10] J. Gotoh and A. Takeda. On the role of norm constraints in portfolio selection. Comput. Manag. Sci.,\n8:323\u2013353, 2011.\n[11] J. Gotoh and A. Takeda. Minimizing loss probability bounds for portfolio selection. European Journal\nof Operational Research, 217(2):371 \u2013 380, 2012.\n[12] A. Gut. Probability: a graduate course. Springer, New York, 2013.\n[13] L. Gy\u00a8or\ufb01, G. Ottucs\u00b4ak, and A. Urb\u00b4an. Empirical log-optimal portfolio selections: a survey. In Machine\nlearning for \ufb01nancial engineering, pages 81\u2013118. World Scienti\ufb01c, 2012.\n[14] L. Gy\u00a8or\ufb01, G. Ottucs\u00b4ak, and H. Walk. The growth optimal investment strategy is secure, too. In G. Con-\nsigli, D. Kuhn, and P. Brandimarte, editors, Optimal Financial Decision Making under Uncertainty,\npages 201\u2013223. Springer International Publishing, Cham, 2017.\n[15] D.P. Helmbold, R.E. Schapire, Y. Singer, and M.K. Warmuth. On-line portfolio selection using multi-\nplicative updates. Mathematical Finance, 8(4):325\u2013347, 1998.\n[16] J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Convex Analysis and Minimization Algorithms. Springer-\nVerlag, Berlin, 1993.\n[17] S. Kim, R. Pasupathy, and S. G. Henderson. A guide to sample average approximation. In M.C. Fu,\neditor, Handbook of Simulation Optimization, pages 207\u2013243. Springer, New York, 2015.\n[18] J. Kivinen and M.K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.\nInformation and computation, 132(1):1\u201363, 1997.\n[19] D. Kuhn, P.M. Esfahani, V.A. Nguyen, and S. Sha\ufb01eezadeh-Abadeh. Wasserstein distributionally robust\noptimization: Theory and applications in machine learning. In INFORMS TutORials in Operations\nResearch, chapter 6, pages 130\u2013166. 2019.\n[20] M. Mohri, A. Rostamizadeh, and A.",
    "chunk_index": 18,
    "start_char": 41639,
    "end_char": 45231,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "Pasupathy, and S. G. Henderson. A guide to sample average approximation. In M.C. Fu,\neditor, Handbook of Simulation Optimization, pages 207\u2013243. Springer, New York, 2015.\n[18] J. Kivinen and M.K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.\nInformation and computation, 132(1):1\u201363, 1997.\n[19] D. Kuhn, P.M. Esfahani, V.A. Nguyen, and S. Sha\ufb01eezadeh-Abadeh. Wasserstein distributionally robust\noptimization: Theory and applications in machine learning. In INFORMS TutORials in Operations\nResearch, chapter 6, pages 130\u2013166. 2019.\n[20] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press,\nCambridge, MA, 2018.\n[21] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic\noptimization. In Int. Conf. Mach. Learn., pages 449\u2013456, 2012.\n[22] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends\u00ae in Ma-\nchine Learning, 4(2):107\u2013194, 2012.\n[23] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.\nCambridge University Press, New York, 2014.\n[24] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform conver-\ngence. Journal of Machine Learning Research, 11:2635\u20132670, 2010.\n[25] A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on Stochastic Programming: Modeling and\nTheory, Second Edition. SIAM, Philadelphia, 2014.\n[26] J.E. Smith and R.L. Winkler. The optimizer\u2019s curse: Skepticism and postdecision surprise in decision\nanalysis. Management Science, 52(3):311\u2013322, 2006.\n[27] R. van Handel. APC 550:\nProbability in high dimension. Lecture Notes. Princeton University,\nhttps://web.math.princeton.edu/ rvan/APC550.pdf, 2016.\n[28] V. Vapnik. Statistical learning theory. Wiley, New York, 1998.\n[29] M.J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press,\nCambridge, 2019.\nI.I. Vorovich Institute of Mathematics, Mechanics and Computer Sciences and Regional\nScientific and Educational Mathematical Center of Southern Federal University\nE-mail address: dbrohlin@sfedu.ru",
    "chunk_index": 19,
    "start_char": 44629,
    "end_char": 46787,
    "paper_title": "Relative utility bounds for empirically optimal po",
    "paper_category": "q-fin.PM",
    "paper_filename": "Relative_utility_bounds_for_empirically_optimal_po.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/Relative_utility_bounds_for_empirically_optimal_po.pdf"
  },
  {
    "text": "arXiv:0902.3836v1 [q-fin.PM] 23 Feb 2009\nThe E\ufb00ects of Market Properties on Portfolio Diversi\ufb01cation in\nthe Korean and Japanese Stock Markets\nCheoljun Eom,1 Jongwon Park,2 Woo-Sung Jung,3, 4 Taisei Kaizoji,5 and Yong H. Kim6\n1Division of Business Administration,\nPusan National University, Busan 609-735, Republic of Korea\u2217\n2Division of Business Administration,\nThe University of Seoul, Seoul 130-743, Republic of Korea\n3Department of Physics and Basic Science Research Institute,\nPohang University of Science and Technology,\nPohang 790-784, Republic of Korea\n4Center for Polymer Studies and Department of Physics,\nBoston University, Boston, MA 02215, USA\n5Division of Social Sciences, International Christian University, Tokyo 181-8585, Japan\n6College of Business, University of Cincinnati, OH 45221, USA\n(Dated: September 4, 2021)\nAbstract\nIn this study, we have investigated empirically the e\ufb00ects of market properties on the degree of\ndiversi\ufb01cation of investment weights among stocks in a portfolio. The weights of stocks within a\nportfolio were determined on the basis of Markowitz\u2019s portfolio theory. We identi\ufb01ed that there was\na negative relationship between the in\ufb02uence of market properties and the degree of diversi\ufb01cation\nof the weights among stocks in a portfolio. Furthermore, we noted that the random matrix theory\nmethod could control the properties of correlation matrix between stocks; this may be useful in\nimproving portfolio management for practical application.\n\u2217Electronic address: shunter@pusan.ac.kr\n1\n\nI.\nINTRODUCTION\nIn the \ufb01eld of \ufb01nance, portfolio management is a crucial study topic, both from an\nacademic and a practical perspective. Markowitz\u2019s portfolio theory has had a signi\ufb01cant\nin\ufb02uence on portfolio research, in that it proposes a quantitative method for structuring\na portfolio with a minimum (maximum) risk (return) for a given return (risk) [1]. More-\nover, investment weights for stocks in a portfolio require well-distributed diversi\ufb01cation to\ne\ufb00ectively reduce the risk of a portfolio. Diversi\ufb01cation is critical for the application of\nMarkowitz\u2019s optimum portfolio from a practical viewpoint, and the two factors that can\na\ufb00ect diversi\ufb01cation are the number of stocks in a portfolio and the level of distribution of\nthe investment weights.\nFirst, previous literature concerning the e\ufb00ects of the changes in the number of stocks\nwithin a portfolio on reducing the risk by diversi\ufb01cation have provided relatively robust em-\npirical evidences. Evans et al. previously suggested that a portfolio\u2019s risk can be reduced by\n50% or more if the number of stocks in a portfolio is 10 \u223c15, and also provided empirical ev-\nidence suggesting that the addition of more stocks into the portfolio exerts only insigni\ufb01cant\nrisk reduction e\ufb00ects (2 \u223c5%) [2]. Without question, the risk reduced by diversi\ufb01cation is\nan unsystematic risk associated with stocks [3]. Their research \ufb01ndings provide important\ncriteria for determining the number of stocks in practical portfolio management. Elton et\nal. presented an analytical equation which allows for an examination of the degree of risk\nreduction when adding new stocks to a portfolio [4]. Statman assessed the e\ufb00ects of adding\nstocks to a portfolio on reducing the risk of an existing portfolio from the standpoints of\nmarginal bene\ufb01ts obtainable from reducing the portfolio risk and the marginal costs that\noccur as the result of increasing the number of stocks in a portfolio [5].",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3460,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "is\nan unsystematic risk associated with stocks [3]. Their research \ufb01ndings provide important\ncriteria for determining the number of stocks in practical portfolio management. Elton et\nal. presented an analytical equation which allows for an examination of the degree of risk\nreduction when adding new stocks to a portfolio [4]. Statman assessed the e\ufb00ects of adding\nstocks to a portfolio on reducing the risk of an existing portfolio from the standpoints of\nmarginal bene\ufb01ts obtainable from reducing the portfolio risk and the marginal costs that\noccur as the result of increasing the number of stocks in a portfolio [5]. The study sug-\ngested that, for practical purposes, the number of stocks should be increased to 30 (for a\nborrowing investor) or 40 (for a lending investor) in order to realize the e\ufb00ects of portfolio\ndiversi\ufb01cation.\nNext, there are studies that address the level of distribution of investment weights among\nstocks in a portfolio, which is crucial to the implementation of an e\ufb00ective portfolio diversi-\n\ufb01cation. These studies attempted to determine whether the total investment is distributed\nwell among stocks within a portfolio. However, on the basis of the empirical evidence, al-\nthough an e\ufb03cient portfolio with minimized risk for a given return from traditional portfolio\n2\n\ntheory is created, the investment weights among stocks within a created portfolio are con-\ncentrated on a few stocks\u2013that is to say, they are not well distributed. Such investment\nweights of lower diversi\ufb01cation among stocks within a portfolio function as a limitation to\nthe use of the portfolio theory in practical applications.\nAccording to the random matrix theory (RMT) recently derived from the \ufb01eld of econo-\nphysics to the \ufb01eld of \ufb01nance [6, 7], observed results exist which indicate that portfolio\nmanagement can be improved by controlling the correlation matrix, as an important input\ndata in the Markowitz\u2019s optimum portfolio. Financial time series data contains noise, \ufb01nite-\nness of time series, missing data, and thin trading - these factors force the correlation matrix\ncalculated from the actual data to contain measurement errors. Therefore, the RMT can\nimprove the portfolio theory by removing the measurement error inherent to the correlation\nmatrix. According to the results obtained by combining the RMT with Markowitz\u2019s opti-\nmum portfolio [8, 9, 10], using the correlation matrix that controlled the correlation matrix\nof a past period on the basis of the RMT method, as the estimated correlation matrix of\na future period, provides a better approximation of the portfolio elicited from the actual\ncorrelation matrix of the future period than the conventional approach using a correlation\nmatrix of a past period as the estimated correlation matrix of a future investment period.\nThe results also show that the utility of the RMT improves reliability in portfolio selection,\nas well as accuracy and stability in portfolio risk evaluation.\nThe primary objective of our study is to examine empirically whether the level of dis-\ntribution of investment weights for stocks in a portfolio can be improved by controlling the\ndivere properties included in the correlation matrix.",
    "chunk_index": 1,
    "start_char": 2840,
    "end_char": 6038,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "the conventional approach using a correlation\nmatrix of a past period as the estimated correlation matrix of a future investment period.\nThe results also show that the utility of the RMT improves reliability in portfolio selection,\nas well as accuracy and stability in portfolio risk evaluation.\nThe primary objective of our study is to examine empirically whether the level of dis-\ntribution of investment weights for stocks in a portfolio can be improved by controlling the\ndivere properties included in the correlation matrix. Speci\ufb01cally, we believe that among the\nvarious properties in the correlation matrix, the market factor properties exert critical e\ufb00ects\non the degree of diversi\ufb01cation of investment weights in Markowitz\u2019s optimum portfolio. Ac-\ncordingly, we assessed the e\ufb00ects of changes in the in\ufb02uence of market factor properties on\nthe degree of diversi\ufb01cation of investment weights among stocks in a portfolio.\nFirst of all, we selected market factor as the properties that principally in\ufb02uence the\ncorrelation among stocks that constitute a portfolio. Many models developed in the \ufb01eld\nof \ufb01nance to elucidate the pricing mechanism of the stock market commonly utilize market\nfactor as an explanation variable. King previously provided empirical evidence suggesting\nthat stock price changes can be explained using market, industry, and company factors and\nmarket factor, in particular, account for a signi\ufb01cant portion of stock price changes in the\n3\n\nmarket [11]. These observed results a\ufb00ected the CAPM (capital asset pricing model) [12]\nand APM (arbitrage pricing model) [13]. In addition, studies that have applied the RMT\nmethod to the \ufb01eld of \ufb01nance determined that the largest eigenvalue has the properties of\nmarket factor [8, 9, 10], and similar results have been noted in other \ufb01nancial research [11,\n14, 15]. The eigenvector is utilized as the weight for creating time series data (corresponds\nto the factor score in the multivariate statistics of principal component analysis [17]) that\nre\ufb02ects the eigenvalue properties. The eigenvector distribution of the largest eigenvalue has\na higher average than the eigenvector distributions of other eigenvalues [10]. Moreover, it\nhas been veri\ufb01ed that the properties of the largest eigenvalue do not change, regardless of\nthe number and type of stocks in a correlation matrix [16]. Among the properties included\nin the correlation matrix, we conducted an empirical investigation of the e\ufb00ects of market\nfactor properties, in which de\ufb01ned by the largest eigenvalue via the RMT, on portfolio\ndiversi\ufb01cation, in which we comparatively examined the degree of diversi\ufb01cation of the\ninvestment weights among stocks in a portfolio by categorizing them into cases with and\nwithout market factor properties in the correlation matrix.\nNext, previous studies have reported that market factor exerts signi\ufb01cant e\ufb00ects on the\nstock market during a market crisis. King provided empirical evidence suggesting that the\ndegree of price \ufb02uctuation of stocks that could be explained by market factor (using the\nR2 determinant coe\ufb03cient) was relatively high during the Great Depression of the 1920s\n[11]. According to a study conducted by Onnela et al., the magnitude of the correlation\namong stocks has increased in times of market crisis, such as \u201dBlack Monday\u201d in the U.S.",
    "chunk_index": 2,
    "start_char": 5509,
    "end_char": 8838,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "without market factor properties in the correlation matrix.\nNext, previous studies have reported that market factor exerts signi\ufb01cant e\ufb00ects on the\nstock market during a market crisis. King provided empirical evidence suggesting that the\ndegree of price \ufb02uctuation of stocks that could be explained by market factor (using the\nR2 determinant coe\ufb03cient) was relatively high during the Great Depression of the 1920s\n[11]. According to a study conducted by Onnela et al., the magnitude of the correlation\namong stocks has increased in times of market crisis, such as \u201dBlack Monday\u201d in the U.S. in\nOctober 1987 [18, 19]. Eom et al. noted that the frequency of signi\ufb01cant information \ufb02ow\namong stocks increased rapidly in Korea during the Asian foreign exchange crisis of 1997\n[20]. Therefore, we have empirically assessed changes occurring in the in\ufb02uence of market\nfactor in Korean and Japanese stock markets in and around the Asian foreign exchange\ncrisis in December 1997, and the large Japanese recession in January 1990, respectively. We\nfurther attempted to determine the manner in which investment weight distributions among\nstocks in a portfolio were a\ufb00ected by market factor during market crisis situations.\nIn summary, our primary objective was to investigate empirically the usefulness of the\nRMT as a potential method for improving the degree of diversi\ufb01cation of the stocks in a\nportfolio constructed in accordance with conventional portfolio theory, by controlling market\nfactor among the properties included in the original correlation matrix. On the basis of the\n4\n\nobserved results, we were able to con\ufb01rm the empirical evidence suggesting that the in\ufb02u-\nence of market factor increases during a market crisis. We also noted that the distribution\nlevel of the investment weights among stocks in a portfolio constructed from the Markowitz\u2019s\noptimum portfolio is reduced when market factor exerts a signi\ufb01cant in\ufb02uence. However,\nthe distribution level of investment weights for stocks in a portfolio elicited from a corre-\nlation matrix without market factor properties by using the RMT method was insensitive\nto changes in the e\ufb00ects of market factor. Moreover, the risk of a portfolio elicited from\na correlation matrix without market factor properties was smaller than that of a portfolio\ndrawn from the conventional Markowitz\u2019s optimum portfolio. These results indicate that\nin a stock market, the market factor properties evidence a negative relationship with the\ndistribution level of investment weights of stocks in a portfolio. We also learned that the\ncombination of a correlation matrix controlled by the RMT method and the portfolio theory\nis useful for improving portfolio management in practical terms.\nThis paper is constructed as follows. The \ufb01rst chapter is the introduction and Chapter\n2 describes the data and methods employed in this study. Chapter 3 presents the results\nobserved according to the study objectives established previously. The \ufb01nal chapter o\ufb00ers\na summary of observed results and their implications.\nII.\nDATA AND METHODS\nA.\nData\nWe utilized daily price data of KOSPI 200 stocks in the Korean stock market and Nikkei\n225 stocks in the Japanese stock market.",
    "chunk_index": 3,
    "start_char": 8248,
    "end_char": 11450,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "terms.\nThis paper is constructed as follows. The \ufb01rst chapter is the introduction and Chapter\n2 describes the data and methods employed in this study. Chapter 3 presents the results\nobserved according to the study objectives established previously. The \ufb01nal chapter o\ufb00ers\na summary of observed results and their implications.\nII.\nDATA AND METHODS\nA.\nData\nWe utilized daily price data of KOSPI 200 stocks in the Korean stock market and Nikkei\n225 stocks in the Japanese stock market. Among the stocks included in the market indices,\nwe determined the ones to be utilized for the process on the basis of the following three\ncriteria. First, stocks with consecutive daily stock prices for the period were selected. Sec-\nond, stocks with statistical extremes in terms of skewness (> |2|) and kurtosis (> 30) were\nexcluded. Third, stocks in sectors with four or less companies were excluded. In turn, we\nselected 104 stocks from Korea\u2019s KOSPI200 and 183 from Japan\u2019s Nikkei225 that ful\ufb01lled\nthe three criteria.\nAccording to the objective of our study, we took the price data during the periods that in-\ncluded market crises when the in\ufb02uence of market factor in the stock markets increased. For\n5\n\nthe Korean stock market, this was between January 1990 to December 2007 (216 months),\na period that includes the foreign exchange crisis of December 1997. For the Japanese stock\nmarket, we compiled data from January 1983 to December 2000 (216 months), a duration\nwhich includes the beginning of the large recession in January 1990. We utilized the rolling\nsample method to assess changes in the e\ufb00ects of market factor, and to ensure variation\nin the degree of diversi\ufb01cation of investment weights of stocks within a portfolio from the\nperspective of the time series. For the entire period (216 months), we established a testing\nperiod duration of 5 years (60 months) and 1 month of duration shift. Ultimately, we noted\nthe time series variation in the results obtained from 157(=216-60+1) repetitions for the\nKorean and Japanese stock markets, respectively.\nB.\nMethods\nThe objective of this study was to investigate empirically the e\ufb00ects of changes in the\nin\ufb02uence of market factor properties in the stock market on the distribution level of in-\nvestment weights among stocks in a portfolio. Therefore, we required methods to measure\nquantitatively the degree of in\ufb02uence of market factor and the degree of diversi\ufb01cation of\ninvestment weights for stocks in a portfolio.\nInitially, we utilized two parameters for the quantitative measurement of the degree of\nin\ufb02uence of market factor. First, the largest eigenvalue, \u03bb1 was elicited via the RMT method,\nand second, the mean square error (MSE) of the di\ufb00erence between the original correlation\nmatrix, CO with the properties of market factor and the controlled correlation matrix, CM\nwithout the market factor properties. We employed the RMT method to control the market\nfactor included in the correlation matrix among stocks.\nThe \ufb01rst measurement used to quantify the degree of in\ufb02uence of market factor is the\nlargest eigenvalue.",
    "chunk_index": 4,
    "start_char": 10968,
    "end_char": 14043,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "the largest eigenvalue, \u03bb1 was elicited via the RMT method,\nand second, the mean square error (MSE) of the di\ufb00erence between the original correlation\nmatrix, CO with the properties of market factor and the controlled correlation matrix, CM\nwithout the market factor properties. We employed the RMT method to control the market\nfactor included in the correlation matrix among stocks.\nThe \ufb01rst measurement used to quantify the degree of in\ufb02uence of market factor is the\nlargest eigenvalue. By the statistical properties of the correlation matrix created by the\nrandom matrix, if the length of time series, L, and the number of data, N, is in\ufb01nite, the\nprobability density function PRM(\u03bb) of the random correlation matrix eigenvalue, \u03bb can be\nde\ufb01ned analytically as follows[21].\n6\n\nPRM(\u03bb) = Q\n2\u03c0\np\n(\u03bbRM\n+\n\u2212\u03bb)(\u03bb \u2212\u03bbRM\n\u2212)\n\u03bb\n(1)\n(\u03bbRM\n\u00b1\n= 1 + 1\nQ \u00b1 2\nr 1\nQ, Q \u2261L\nN > 1)\nand the range of the eigenvalue included in a random matrix is \u03bbRM\n\u2212\n\u2264\u03bbi \u2264\u03bbRM\n+ , where\n\u03bbRM\n+\nand \u03bbRM\n\u2212\ndenote the maximum and minimum eigenvalues, respectively.\nFrom previous studies, it is widely recognized that the largest eigenvalue among eigenval-\nues that exceed the range of a random matrix, \u03bbi > \u03bbRM\n+\nevidences market factor properties\n[6, 7, 8, 9, 10, 11, 14, 15, 16]. We utilized the largest eigenvalue as the proxy for market\nfactor, and noted the time series variation of its magnitude. In other words, an increasing\nmagnitude of the largest eigenvalue is indicative of a greater market factor in\ufb02uence in the\nstock market, and vice versa.\nThe second measurement is the MSE. Based on previous studies, we generated a correla-\ntion matrix, CM, from which the properties of the largest eigenvalue were removed via the\nRMT method [22]. This is the correlation matrix without market factor properties. The\nMSE is assessed using the o\ufb00-diagonal elements N(N \u22121)/2 of the controlled correlation\nmatrix, CM, and the original correlation matrix, CO, as follows.\nMSE =\nv\nu\nu\nt\n1\nN(N \u22121)/2\nN(N\u22121)/2\nX\nk=1\n(CO\nk \u2212CM\nk )2\n(2)\nin which a high MSE re\ufb02ects a high in\ufb02uence from market factor because the MSE is the\ndi\ufb00erence between CO with market factor and CM without market factor. If the in\ufb02uence\nof market factor increases, CM will decrease, causing the MSE to assume a large value. On\nthe other hand, a small MSE is re\ufb02ective of an insigni\ufb01cant market factor in\ufb02uence.\nNext, with regard to the quantitative measurement of the degree of diversi\ufb01cation of\ninvestment weights among stocks in a portfolio, we employed two parameters: the intra-\nportfolio correlation (IPC) and the concentration coe\ufb03cient (CC). These parameters were\ncalculated using the investment weights of stocks constituting a portfolio. Accordingly, we\nutilized a method predicated on Markowitz\u2019s portfolio theory to generate investment weights\nfor stocks in a portfolio.\n7",
    "chunk_index": 5,
    "start_char": 13556,
    "end_char": 16363,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "the degree of diversi\ufb01cation of\ninvestment weights among stocks in a portfolio, we employed two parameters: the intra-\nportfolio correlation (IPC) and the concentration coe\ufb03cient (CC). These parameters were\ncalculated using the investment weights of stocks constituting a portfolio. Accordingly, we\nutilized a method predicated on Markowitz\u2019s portfolio theory to generate investment weights\nfor stocks in a portfolio.\n7\n\n\u03c3p =\nsX\ni\nX\nj\nwp\ni wp\nj\u03c3i,j\n(\u03c3i,j = \u03c3i \u00b7 \u03c3j \u00b7 \u03c1i,j)\n(3)\nCONDITION 1 : E(Rp) =\nX\nj\nwp\njE(Rj) = RT\np\n(p = 1, 2, . . . , 10)\nCONDITION 2 :\nX\nj\nwp\nj \u22611.0\nCONDITION 3 : wp\nj \u22650.0 (j = 1, 2, . . . , N)\nwhere \u03c3i and \u03c3j are stock risks (standard deviation), and \u03c1i,j (= CO) denotes the correlation\namong stocks. Condition 1, which is concerned with the expected return of portfolio, E(Rp),\nspeci\ufb01es that expected returns, E(Rj) of stocks in a portfolio have a certain target return,\nRT\np . Then, a portfolio is generated with a minimum risk, \u03c3p according to the minimization\nobjective function of Eq. 3 for the target return; the investment weight, wp\nj for stocks in a\nportfolio are created in the process. Finally, connecting the combination points [\u03c3p, E(Rp)]\nof portfolio risks and returns created by varying the target return within a range, p =\n1, 2, . . . , 10 provides an e\ufb03cient investment curve, or an e\ufb03cient portfolio. Conditions 2 and\n3 indicate that short-selling is not permitted.\nIn order to obtain robust results, we conducted an identical testing process using the\ncontrolled correlation matrix from which the market factor, \u03bb1 were removed via the RMT\nmethod, as the correlation matrix input data \u03c1i,j \u2192CM of Eq. 3. For an objective compar-\nison between portfolios elicited from correlation matrices with the market factor properties\nand without the market factor properties, we set an identical target return for both cases.\nUsing investment weights for stocks in a portfolio calculated from the optimization func-\ntion of Eq. 3, we attempted to determine whether investment weights were well distributed\namong stocks with the IPC and the CC. The \ufb01rst measurement IPCp quanti\ufb01es the dis-\ntribution level of the investment weights among stocks in a portfolio.\nIPCp =\nPN\ni\nPN\nj wp\ni wp\nj\u03c1i,j\nPN\ni\nPN\nj wp\ni wp\nj\n(p = 1, 2, . . . , 10)\n(4)\nThe range of the IPC is \u22121 \u2264IPC \u22641. IPC = \u22121 indicates that the investment weights\nare well distributed among all of stocks comprising the portfolio, and IPC = +1 shows that\nthe investment weights are not distributed at all. Namely, the lower the IPC, the higher the\nlevel of distribution for the investment weights among stocks in a portfolio.\n8\n\nThe second measurement, CCp, as a complementary parameter to IPCp, measures the\ndegree of concentration of investment weights among stocks in a portfolio.",
    "chunk_index": 6,
    "start_char": 15944,
    "end_char": 18716,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "indicates that the investment weights\nare well distributed among all of stocks comprising the portfolio, and IPC = +1 shows that\nthe investment weights are not distributed at all. Namely, the lower the IPC, the higher the\nlevel of distribution for the investment weights among stocks in a portfolio.\n8\n\nThe second measurement, CCp, as a complementary parameter to IPCp, measures the\ndegree of concentration of investment weights among stocks in a portfolio.\nCCp = (\nN\nX\nj=1\n(wp\nj)2)\u22121 (p = 1, 2, . . . , 10)\n(5)\nThe range of the CC is 1 \u2264CC \u2264N. CC = 1 denotes that 100% of the investment is\nmade on one stock in a portfolio, and CC = N means that an equal investment weight\nwj =\n1\nN = w is applied to every stock in the portfolio. In other words, the higher the CC,\nthe lower the degree of concentration of investment weights among stocks in a portfolio.\nIII.\nRESULTS\nThis section presents the results of our examination of the e\ufb00ects of market factor prop-\nerties on the degree of diversi\ufb01cation of investment weights among stocks in a portfolio. The\nresults are divided into those obtained from the Korean stock markets and those obtained\nfrom Japanese stock markets, as is shown in Fig. 1 and Fig. 2, respectively. The results\nwere generated through 157(=216-60+1) repetitions (based on the rolling sample method)\nfor a testing period of 5 years and 1 month of duration shift, for a total of 18 years. Figs. 1\n& 2 (a) depict the trends in the market indices over the total period. We included Korea\u2019s\nforeign exchange crisis in December 1997 and the beginning of Japan\u2019s big recession in Jan-\nuary 1990 during this period. Figs. 1 & 2 (b) show the results of quantitative measurements\nof the in\ufb02uence of market factor in the stock market. Via the RMT method, variation in\nthe largest eigenvalue (\u03bb1, blue squares) and the MSE (red circles) were measured as the\ndi\ufb00erence between the controlled correlation matrix without market factor properties and\nthe original correlation matrix with market properties are indicated. Figs. 1 & 2 (c) & (e)\nshow the IPC and CC, which measure the degree of diversi\ufb01cation of investment weights\namong stocks in portfolios. According to the varying target returns, RT\np , we applied the\ncalculated investment weight wp\nj to Eqs. 4 and 5 to measure IPCp and CCp, and their\naverages IPC =\n1\n10\nP10\np=1 IPCp and CC =\n1\n10\nP10\np=1 CCp are shown in the graphs. These\nresults are observed for the original correlation matrix with the market factor properties\n(red circles) and the controlled correlation matrix without market factor properties (blue\nsquares). In Figs. 1 & 2 (d) & (f), the \ufb02uctuation ranges of IPC and CC during the total\n9\n\nperiod are displayed in box-plots.\nAccording to the observed results, we determined that the distribution level of investment\nweights for stocks in a portfolio",
    "chunk_index": 7,
    "start_char": 18259,
    "end_char": 21089,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "1\n10\nP10\np=1 CCp are shown in the graphs. These\nresults are observed for the original correlation matrix with the market factor properties\n(red circles) and the controlled correlation matrix without market factor properties (blue\nsquares). In Figs. 1 & 2 (d) & (f), the \ufb02uctuation ranges of IPC and CC during the total\n9\n\nperiod are displayed in box-plots.\nAccording to the observed results, we determined that the distribution level of investment\nweights for stocks in a portfolio according to the conventional portfolio theory is sensitive to\nchanges in the in\ufb02uence of market factor, and evidences a negative relationship. Figs. 1 &\n2 (a) & (b) demonstrate that market is volatile and time-varying, and the two parameters\n(\u03bb1, MSE) indicative of the in\ufb02uence of market factor increase rapidly during market crises.\nThese results constitute empirical evidence that the e\ufb00ects of market factor are increased\nduring times of market crisis. In Figs. 1 & 2 (c) & (e), during market crises when the\nin\ufb02uence of market factor increased, IPC (red circles in Figs (c)) evidenced a clear upward\ntrend, whereas CC (red circles in Figs (d)) evidenced a decreasing trend.\nNamely, the\ndegree of diversi\ufb01cation of investment weights for the stocks using the original correlation\nmatrix with the properties of market factor was quite low. On the other hand, when the\ndistribution level of investment weights for the stocks in a portfolio was elicited using the\ncorrelation matrix without market factor properties via the RMT method (IPC and CC,\nblue squares), no notable trend was observed. Moreover, in Figs. 1 & 2 (d) & (f), the\nmagnitude of IPC (CC) calculated with RMT for the entire period was signi\ufb01cantly less\n(greater) than that of IPC (CC) calculated in accordance with the conventional portfolio\ntheory. Also, we represent average value of IPC and CC in Table 1, in order to summary\nobserved results in Figs. 1 & 2. From the observed results, we found that the degree of\ndiversi\ufb01cation of investment weights for stocks from using the controlled correlation matrix\nwithout the market factor properties was relatively high. Furthermore, we were able to\ncon\ufb01rm empirically that the RMT method, which can control various properties included\nin a correlation matrix, is an e\ufb00ective means of improving the degree of diversi\ufb01cation for\ninvestment weights among the stocks in a portfolio constructed via the conventional portfolio\ntheory.\nIn order to provide robust empirical evidence for the observed results, we presented the\nrelationships among the largest eigenvalue \u03bb1 [Fig. 2(b)], IPC [Fig. 2(c)], and CC [Fig.\n2(e)] from the results of the Japanese stock market data in Fig. 3. In Fig. 3 (a) & (b),\nthe largest eigenvalue displays correlations of signi\ufb01cant 96.58% and signi\ufb01cant -73.84%\nwith IPC and CC at the 1 % level, respectively. It can be noted that as the magnitude\nof the largest eigenvalue that indicates the in\ufb02uence of market factor increases, the level\nof distribution of the investment weights for stocks in a portfolio clearly decreases (IPC\n10",
    "chunk_index": 8,
    "start_char": 20608,
    "end_char": 23663,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "CC [Fig.\n2(e)] from the results of the Japanese stock market data in Fig. 3. In Fig. 3 (a) & (b),\nthe largest eigenvalue displays correlations of signi\ufb01cant 96.58% and signi\ufb01cant -73.84%\nwith IPC and CC at the 1 % level, respectively. It can be noted that as the magnitude\nof the largest eigenvalue that indicates the in\ufb02uence of market factor increases, the level\nof distribution of the investment weights for stocks in a portfolio clearly decreases (IPC\n10\n\nincreases) and the concentration level of investment weights increases (CC decreases). In\naddition to these results, we observed \ufb02uctuation of portfolio risks in Fig. 4. Fig. 4 (a)\nand (b) are depicted using data from the Korean and Japanese stock markets, respectively.\nIn \ufb01gure, we also determined that the portfolio risk elicited from the controlled correlation\nmatrix without market factor (\u03c3M\np\n=\n1\n10\nP10\np=1 \u03c3p, blue squares) was decidedly lower than\nthe portfolio risk drawn from the conventional portfolio theory (\u03c3O\np , red circles) for a given\nreturn over the total period. That is to say, average value of \u03c3M\np\nis 0.0033 (t:43.08) within\nrange 0.0081 \u2264\u03c3M\np \u22640.0055 for data of Korean stock market, and 0.0018 (t:37.67) within\nrange 0.0010 \u2264\u03c3M\np \u22640.0032 for Japanese stock market. Otherwise, average value of \u03c3O\np is\n0.0131 (t:50.27)) within range 0.0073 \u2264\u03c3O\np \u22640.0180 for data of Korean stock market, and\n0.0089 (t:66.23) within range 0.0067 \u2264\u03c3O\np \u22640.0123 for Japanese stock market. Therefore\nwe robustly discovered that the properties of market factor causes to be lower degree of\ndiversi\ufb01cation, and then to increase the degree of portfolio risk. Also, these results show that\nthe RMT method, which can control market factor from a correlation matrix, is an e\ufb00ective\nmeans not only for improving the degree of diversi\ufb01cation for investment weights among\nstocks in a portfolio, which has been a practical limitation to the application of conventional\nportfolio theory, but also may prove useful in reducing the risk level of a portfolio.\nIV.\nCONCLUSIONS\nThis study comprised our empirical investigation of a method to improve the degree of\ndiversi\ufb01cation of investment weights for stocks that constitute a portfolio in the Korean and\nJapanese stock markets. An empirical test was conducted in order to determine whether the\ndistribution level of investment weights for stocks in a portfolio elicited in accordance with\nthe conventional Markowitz\u2019s portfolio theory could be improved by removing the market\nfactor properties included in the correlation matrix via the RMT method. The observed\nresults can be summarized as follows. We were able to con\ufb01rm that the in\ufb02uence of market\nfactor increased during market crises. We also determined that when the in\ufb02uence of market\nfactor was high, the distribution level of investment weights for stocks in a portfolio drawn\nfrom the Markowitz\u2019s optimum portfolio decreased. In other words, we noted a negative\nrelationship between the in\ufb02uence of market factor and the distribution level of investment\nweights for stocks in a portfolio. In order to acquire more robust results, we conducted an\n11",
    "chunk_index": 9,
    "start_char": 23205,
    "end_char": 26316,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "be summarized as follows. We were able to con\ufb01rm that the in\ufb02uence of market\nfactor increased during market crises. We also determined that when the in\ufb02uence of market\nfactor was high, the distribution level of investment weights for stocks in a portfolio drawn\nfrom the Markowitz\u2019s optimum portfolio decreased. In other words, we noted a negative\nrelationship between the in\ufb02uence of market factor and the distribution level of investment\nweights for stocks in a portfolio. In order to acquire more robust results, we conducted an\n11\n\nidentical testing process using the controlled correlation matrix from which market factor\nwere eliminated using the RMT method. The results revealed no signi\ufb01cant variations in\nterms of the distribution level of investment weights for the stocks in a portfolio during times\nof market crisis. Moreover, compared to the results observed from the conventional portfolio\ntheory, the results acquired via the RMT method evidenced a signi\ufb01cantly higher degree of\ndiversi\ufb01cation of investment weights for stocks in a portfolio, and also evidenced a lower\nportfolio risk. From these results, we evidence that the properties of market factor cause\nto decrease the degree of diversi\ufb01cation of investment weights among stocks in a portfolio\nand to increase portfolio risks. Also, we were able to verify that the RMT method that can\ncontrol various properties from the correlation matrix, is an e\ufb00ective means of improving the\npractical limitations to the application of the conventional portfolio theory. We also learned\nthat there is a clear necessity for more profound research e\ufb00orts in the future regarding the\nroles of market factor in the portfolio theory.\nAcknowledgments\nThis work was supported by the Korea Science and Engineering Foundation (KOSEF)\ngrant funded by the Korea government (MEST) (No. R01-2008-000-21065-0)\n[1] H. Markowitz, \u201cPortfolio Selection,\u201d the Journal of Finance 7(1), 1952, 77-91.\n[2] J. L. Evans and S. H. Archer, \u201cDiversi\ufb01cation and the Reduction of Dispersion: An Empirical\nAnalysis,\u201d the Journal of Finance 23(5), 1968, 761-767.\n[3] W. F. Sharpe, \u201cA Simplied Model for Portfolio Analysis,\u201d Management Science 9(2), 1963,\n277-293.\n[4] E. J. Elton and M. J. Gruber, \u201cRisk Reduction and Portfolio Size: An Analytical Solution,\u201d\nthe Journal of Business 50(4), 1977, 415-437.\n[5] M. Statman, \u201cHow Many Stocks Make a Diversi\ufb01ed Portfolio,\u201d the Journal of Financial and\nQuantitative Analysis 22(3), 1987, 353-363.\n[6] L. Laloux, P. Cizeau, J. Bouchaud, and M. Potters, \u201cNoise Dressing of Financial Correlation\nMatrices,\u201d Physical Review Letters 83(7), 1999, 1467-1470.\n12\n\n[7] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. N. Amaral, and H. E. Stanley, \u201cUniversal\nand Nonuniversal Properties of Cross Correlations in Financial Time Series,\u201d Physical Review\nLetters 83(7), 1999, 1471-1474.\n[8] L. Laloux, P. Cizeau, M. Potters, and J.-P. Bouchaud, \u201cRandom Matrix Theory and Financial\nCorrelations,\u201d International Journal of Theoretical and Applied Finance 3(3), 2000, 391-397.\n[9] B. Rosenow, V. Plerou, P. Gopikrishnan, and H. E. Stanley, \u201cPortfolio optimization and the\nrandom magnet problem,\u201d Europhysics Letters 59(4), 2002, 500-506.\n[10] S. Shari\ufb01, M. Crane, A. Shamaie, and H.",
    "chunk_index": 10,
    "start_char": 25782,
    "end_char": 29018,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "Nonuniversal Properties of Cross Correlations in Financial Time Series,\u201d Physical Review\nLetters 83(7), 1999, 1471-1474.\n[8] L. Laloux, P. Cizeau, M. Potters, and J.-P. Bouchaud, \u201cRandom Matrix Theory and Financial\nCorrelations,\u201d International Journal of Theoretical and Applied Finance 3(3), 2000, 391-397.\n[9] B. Rosenow, V. Plerou, P. Gopikrishnan, and H. E. Stanley, \u201cPortfolio optimization and the\nrandom magnet problem,\u201d Europhysics Letters 59(4), 2002, 500-506.\n[10] S. Shari\ufb01, M. Crane, A. Shamaie, and H. Rukin, \u201cRandom Matrix Theory for Portfolio Op-\ntimization: a stability approach,\u201d Physica A 335, 2004, 629-643.\n[11] B. F. King, \u201cMarket and Industry Factors in Stock Price Behavior,\u201d the Journal of Business\n39(1), 1966, 139-190.\n[12] F. Black, M. Jensen, and M. Scholes, \u201cThe Capitial Asset Pricing Model: Some Empirical\nTests,\u201d Working paper from SSR, 1972.\n[13] S. A. Ross, \u201cThe Arbitrage Theory of Capital Asset Pricing,\u201d Journal of Economic Theory\n13, 1976, 343-362.\n[14] C. Trzcinka, \u201cOn the Number of Factors in the Arbitrage Pricing Model,\u201d the Journal of\nFinance 41(2), 1986, 347-368.\n[15] S. T. Brown, \u201cThe Number of Factors in Security Returns,\u201d the Journal of Finance 44(5),\n1989, 1247-1262.\n[16] C. Eom, W.-S. Jung, T. Kaizoji, and S. Kim, \u201cE\ufb00ect on Eigenvalue by Changing Sample Size\nin the Korean and Japanese Stock Markets,\u201d preprint, available on web, arXiv.org:0811.4021,\n2008.\n[17] H. H. Harman (1976), Modern Factor Analysis, The University of Chicago Press.\n[18] J.-P. Onnela, A. Chakraborti, K. Kaski, and J. Kertesz, \u201cDynamic Asset Trees and Black\nMonday,\u201d Physica A 324, 2003, 247-252.\n[19] J.-P. Onnela, A. Chakraborti, K. Kaski, J. Kertesz, and A. Kanto, \u201cDynamics of Market\nCorrelations : Taxonomy and Portfolio Analysis,\u201d Physical Review E 68, 2003, 056110.\n[20] C. Eom, O. Kwon, and W.-S. Jung, \u201cStatistical Properties of Information Flow in Financial\nTime Series,\u201d preprint, available on web, arXiv.org:0811.0448, 2008.\n[21] A. M. Sengupta, and P. P. Mitra, \u201cDistributions of Singular Values for some Random Matri-\nces,\u201d Physical Review E 60, 1999, 389-392.\n13\n\n[22] C. Eom, G. Oh, W.-S. Jung, H. Jeong, and S. Kim, \u201cTopological Properties of Stock Networks\nbased on Minimal Spanning Tree and Random Matrix Theory in Financial Time Series,\u201d\nPhysica A 388, 2009, 900-926.\n14\n\n0\n50\n100\n150\n0\n50\n100\n150\n200\n250\n300\nRolling Sample\nMarket Index\nAsia FX Crisis in 1997\n0\n50\n100\n150\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\nRolling Sample\nMean Square Error (red circles)\nthe Largest Eigenvalue (blue squares)\n22\n24\n26\n28\n30\n32\n34\n36\n0\n50\n100\n150\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\nRolling Sample\nthe Original IPC (red circles)\nthe Controlled IPC (blue squares)\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\nx 10\n\u22123\nOriginal IPC \nControlled IPC\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\nthe IPCs\n0\n50\n100\n150\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\nRolling Sample\nthe Original CC (red circles)\nthe Controlled CC (blue squares)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nOriginal CC \nControlled CC\n10\n20\n30\n40\n50\n60\n70\n80\n90\nthe CCs\nO\u0088P\nO\u0089P\nO\u008aP\nO\u008bP\nO\u008cP\nO\u008dP\nFIG.",
    "chunk_index": 11,
    "start_char": 28505,
    "end_char": 31559,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "0.13\n0.14\n0.15\n0.16\nRolling Sample\nthe Original IPC (red circles)\nthe Controlled IPC (blue squares)\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\nx 10\n\u22123\nOriginal IPC \nControlled IPC\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\nthe IPCs\n0\n50\n100\n150\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\nRolling Sample\nthe Original CC (red circles)\nthe Controlled CC (blue squares)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nOriginal CC \nControlled CC\n10\n20\n30\n40\n50\n60\n70\n80\n90\nthe CCs\nO\u0088P\nO\u0089P\nO\u008aP\nO\u008bP\nO\u008cP\nO\u008dP\nFIG. 1: This shows the results from Korean stock markets. Fig. 1(a) depicts the market index\ntrends, during the period including Korea\u2019s foreign exchange crisis of December 1997. Fig. 1(b)\ndisplays the results of quantitative measurements of the in\ufb02uence of market factor in the stock\nmarket, using the largest eigenvalue(blue squares) and the MSE (red circles). Fig. 1 (c) & (e)\ndisplays the degree of diversi\ufb01cation of investment weights among stocks in portfolios using IPC\nand CC.\nThese results are provided for the original correlation matrix (red circles) and the\ncontrolled correlation matrix (blue squares). In Fig. 1 (d) & (f), the \ufb02uctuation ranges of IPC\nand CC during the total period are displayed in box-plots.\n15\n\n0\n50\n100\n150\n1\n1.5\n2\n2.5\n3\n3.5\n4\nx 10\n4\nRolling Sample\nMarket Index\nBig Recession in 1990\n0\n50\n100\n150\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\nRolling Sample\nMean Square Error (red circles)\nthe Largest Eigenvalue (blue squares)\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n0\n50\n100\n150\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nRolling Sample\nthe Original IPC (red circles)\nthe Controlled IPC (blue squares)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\nx 10\n\u22123\nOriginal IPC \nControlled IPC\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nthe IPCs\n0\n50\n100\n150\n5\n10\n15\n20\n25\n30\nRolling Sample\nthe Original CC (red circles)\nthe Controlled CC (blue squares)\n20\n40\n60\n80\n100\n120\n140\n160\n180\nOriginal CC \nControlled CC\n0\n20\n40\n60\n80\n100\n120\n140\n160\nthe CCs\nO\u0088P\nO\u0089P\nO\u008aP\nO\u008bP\nO\u008cP\nO\u008dP\nFIG. 2: This presents the results obtained from Japanese stock markets. Fig. 2(a) depicts the\nmarket index trends during the period including Japan\u2019s big recession of January 1990. Fig. 2(b)\ndisplays the results of quantitative measurements of the in\ufb02uence of market factor in the stock\nmarket, using the largest eigenvalue (blue squares) and the MSE (red circles). Fig. 2 (c) & (e)\ndisplays the degree of diversi\ufb01cation of investment weights among stocks in portfolios. These results\nare provided for the original correlation matrix (red circles) and the controlled correlation matrix\n(blue squares). In Fig. 2 (d) & (f), the \ufb02uctuation ranges of IPC and CC during the total period\nare displayed in box-plots.\n16",
    "chunk_index": 12,
    "start_char": 31111,
    "end_char": 33702,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "eigenvalue (blue squares) and the MSE (red circles). Fig. 2 (c) & (e)\ndisplays the degree of diversi\ufb01cation of investment weights among stocks in portfolios. These results\nare provided for the original correlation matrix (red circles) and the controlled correlation matrix\n(blue squares). In Fig. 2 (d) & (f), the \ufb02uctuation ranges of IPC and CC during the total period\nare displayed in box-plots.\n16\n\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nValues of the Largest Eigenvalue\nthe Classic IPCs\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n5\n10\n15\n20\n25\n30\nValues of the Largest Eigenvalue\nthe Classic CCs\nO\u0088P\nO\u0089P\nFIG. 3: This shows the relationships among the largest eigenvalue from the results of the Japanese\nstock market data in the largest eigenvalue, IPC, and CC. In Fig. 3, the X-axis denotes the\nlargest eigenvalue, and the Y-axis represents IPC (Fig. 3(a)) and CC (Fig. 3(b)).\n17\n\n0\n50\n100\n150\n0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\n0.016\n0.018\n0.02\nRolling Sample\nStandard Deviation\n \n \nRisk with Market\nRisk without Market\n0\n50\n100\n150\n0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\nRolling Sample\nStandard Deviation\n \n \nRisk with Market\nRisk without Market\nO\u0088P\nO\u0089P\nFIG. 4: This shows the results of comparison between portfolio risks from correlation matrices\nwith (blue squares) and without (red circles) the market factor properties. Fig. 4 (a) and (b) are\ndepicted using the data from the Korean and Japanese stock markets, respectively.\n18\n\nCorrelation Matrices\nOriginal\nControlled\nMeasurements\nCorrelation Matrix Correlation Matrix\nKorean Average of IPC\n0.1322\u2217\n-0.0047\u2217\nMarket\nAverage of CC\n15.24\u2217\n63.68\u2217\nJapanese Average of IPC\n0.2120\u2217\n-0.0015\u2217\nMarket\nAverage of CC\n15.30\u2217\n107.03\u2217\nTABLE I: This table represents the results of the average value of IPC and CC during rolling\nsample period. (\u2217: signi\ufb01cant at the 1% level)\n19",
    "chunk_index": 13,
    "start_char": 33302,
    "end_char": 35144,
    "paper_title": "The Effects of Market Properties on Portfolio Dive",
    "paper_category": "q-fin.PM",
    "paper_filename": "The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/The_Effects_of_Market_Properties_on_Portfolio_Dive.pdf"
  },
  {
    "text": "TRADINGGPT: MULTI-AGENT SYSTEM WITH LAYERED\nMEMORY AND DISTINCT CHARACTERS FOR ENHANCED\nFINANCIAL TRADING PERFORMANCE\nYang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah\nSchool of Business, Stevens Institute of Technology\nHoboken, NJ, United States\n{yli269, yyu44, hli113, zchen100, kkhashan}@stevens.edu\nABSTRACT\nLarge Language Models (LLMs), prominently highlighted by the recent evolution in the Generative\nPre-trained Transformers (GPT) series, have displayed significant prowess across various domains,\nsuch as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of\nGPTs lies in their ability to decode human instructions, achieved through comprehensively processing\nhistorical inputs as an entirety within their memory system. Yet, the memory processing of GPTs\ndoes not precisely emulate the hierarchical nature of human memory, which is categorized into long,\nmedium, and short-term layers. This can result in LLMs struggling to prioritize immediate and\ncritical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework\nendowed with layered memories. We assert that this framework is well-suited for stock and fund\ntrading, where the extraction of highly relevant insights from hierarchical financial data is imperative\nto inform trading decisions. Within this framework, one agent organizes memory into three distinct\nlayers, each governed by a custom decay mechanism, aligning more closely with human cognitive\nprocesses. Agents can also engage in inter-agent communication and debate. In financial trading\ncontexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system\nto integrate multi-source historical actions and market insights. This equips them to navigate financial\nchanges, formulate strategies, and debate with peer agents about investment decisions. Another\nstandout feature of our approach is to enable agents with individualized trading characters, which\nenrich the diversity of their highlighted essential memories and improve decision-making robustness.\nBy leveraging agents\u2019 layered memory processing and consistent information interchange, the entire\ntrading system demonstrates augmented adaptability to historical trades and real-time market cues.\nThis synergistic approach guarantees premier automated trading with heightened execution accuracy.\nKeywords Financial AI, Multi-Modal Learning, Trading Algorithms, Deep Learning, Financial Technology\n1\nIntroduction\nAs the influx of diverse data streams continues to rise, there is a growing need for individuals to effectively harness\ninformation. This trend is particularly pronounced in the realm of finance, where traders must consider multiple sources\nto inform their investment decisions. In light of this demand, researchers design intelligent trading robot-agents that can\nsynthesize and interpret data objectively[14, 5]. These robot-agents harness diverse machine algorithms, assimilate\na broader spectrum of data, autonomously refine trading strategies via methodical planning, and even potentially\ncollaborate [7]. Here, we introduce an advanced LLM-powered multi-agent trading agent framework, supported by\nlayered memories and customized characters. By employing a collaborative multi-agent system and capturing the\nintricate market dynamics from varied perspectives, this approach significantly enhances automated trading outcomes.\nThis approach substantially elevates the performance of automated trading by fostering collaborative interactions among\nagents and capturing the intricate dynamics of the market from diverse perspectives.\narXiv:2309.03736v1 [q-fin.PM] 7 Sep 2023",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3689,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "we introduce an advanced LLM-powered multi-agent trading agent framework, supported by\nlayered memories and customized characters. By employing a collaborative multi-agent system and capturing the\nintricate market dynamics from varied perspectives, this approach significantly enhances automated trading outcomes.\nThis approach substantially elevates the performance of automated trading by fostering collaborative interactions among\nagents and capturing the intricate dynamics of the market from diverse perspectives.\narXiv:2309.03736v1 [q-fin.PM] 7 Sep 2023\n\nPrevious studies have introduced multi-agent trading algorithms that employ machine learning techniques, such as\nreinforcement learning and have reported significant performance outcomes [5]. Yet, these methods exhibit limitations\nin precisely identifying, representing, and emulating crucial components of trading systems. This includes aspects like\nagents\u2019 memory archives and the evolving social interplay among agents.\nLLMs, with a particular focus on their recent advancements, such as the Generative Pre-trained Transformer (GPT),\nhave demonstrated remarkable effectiveness in enhancing human decision-making across various domains [9]. Notably,\na growing body of research has focused on harnessing this technology to make informed trading decisions for stocks\nand funds by continuously interacting with financial environment information [17, 16]. While current financial LLM\napplications predominantly operate within single-agent systems based on textual uni-modality, their immense potential\nto elevate trading performance is becoming increasingly evident. Moreover, these financial agent systems make trading\ndecisions relying solely on pre-trained LLMs or a memory system processing received information streams as an\nentirety. This can lead to a challenge for LLMs in efficiently prioritizing immediate and critical memory events for\noptimized trading.\nPark et al. [10] recently introduced a generative agent framework aiming to enhance the efficient retrieval of critical\nevents from agents empowered by LLMs. This structure comprises several agents, each distinguished by separate\nmemory streams and unique character profiles configured by LLMs. Each agent, owning its seed memories, not only\ntracks its actions but also monitors other agents and environmental behaviors. Faced with a task, agents sift through\nmemory segments to input into the language model, ranking them by recency, significance, and relevance. By archiving\nan agent\u2019s experiences, the system integrates individual weighted memories and the nuances of group dynamics. As a\nresult, agents can collaboratively strategize, leveraging their collective knowledge. Moreover, Du et al. [3] presented\na debate mechanism for LLM agents, emphasizing enhanced cooperative decision-making through debate phases in\ninter-agent memory interactions. These advancements align the LLM-driven multi-agent system more with human\nmemory structures, paving the way for a more adept financial automated trading system.\nLeveraging the capabilities of LLMs, we propose a novel trading agent framework, \"TradingGPT\". It offers a realistic\nscenario simulation through the integration of the trader\u2019s layered memory streams and character analysis. This\nframework is characterized by remarkable self-enhancement ability and performance to conduct automated trading and\noptimal execution. The primary contributions of our work include:\nThis represents a pioneering multi-agent trading system that integrates memory streams and debate mechanisms,\nanchored on LLMs. Building on Park et al.\u2019s weighted memory mechanisms, our system innovatively categorizes the\nagent\u2019s memories into short-term, middle-term, and long-term layers, which are closely aligned with the structure of the\nhuman cognitive system.",
    "chunk_index": 1,
    "start_char": 3130,
    "end_char": 6947,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "a realistic\nscenario simulation through the integration of the trader\u2019s layered memory streams and character analysis. This\nframework is characterized by remarkable self-enhancement ability and performance to conduct automated trading and\noptimal execution. The primary contributions of our work include:\nThis represents a pioneering multi-agent trading system that integrates memory streams and debate mechanisms,\nanchored on LLMs. Building on Park et al.\u2019s weighted memory mechanisms, our system innovatively categorizes the\nagent\u2019s memories into short-term, middle-term, and long-term layers, which are closely aligned with the structure of the\nhuman cognitive system. We adapt this layered memory framework to the financial trading system, equipping agents to\nreflect on past and present events, derive insights from trading performance, and leverage collective wisdom for future\ndecisions. This approach improves the system\u2019s robustness.\nThis marks the debut of the LLM agent trading system that incorporates the character design. The design assigns\nagents with different varying risk preferences, such as risk-seeking, risk-neutral, and risk-averse, and various investment\nsubscopes across industries. This design enables these collaborative agents to resonate more with human intuition and\npossess the potential to uncover latent market opportunities.\nOur trading system also integrates real-time multi-modal data from diverse information sources, offering a\ncomprehensive view of the financial landscape by encompassing both macro and micro perspectives, as well as historical\ntrading records. With updates available on both daily and minute-by-minute frequencies, our system ensures prompt\nreactions to daily trades and offers the capability for high-frequency trading.\nIn this paper, we commence with an in-depth exposition of TradingGPT. We then present multi-modal datasets for the\neffective training of TradingGPT. We methodically evaluate the pivotal components of the system, illustrating their\nability to yield notable results. We prospect that, when deployed on representative fund firms like ARK, TradingGPT\nwill markedly outperform other automated trading strategies.\n2\nRelated Work\n2.1\nLarge language models (LLMs)\nThe evolution of LLMs has reshaped artificial intelligence and natural language processing. From foundational\nembeddings like Word2Vec [4] and GloVe [11], the field advanced with the introduction of BERT [2]. Today, the\nnew-generation LLMs, like Generative Pre-trained Transformer series (GPTs) [12, 9] and Large Language Model Meta\nAI (Llamas) [15], demonstrate expressive proficiency across diverse applications.\n2\n\n2.2\nGenerative agent system with memory streams and customized character design\nPark et al. [10] introduced generative agents\u2019 memory streams and innovatively employed character design concepts\nfrom gaming, expanding LLM capabilities for the multi-agent system [13]. In their design, agents display human-like\nbehaviors while retaining individual characters. They dynamically interact with peers and their environment, forging\nmemories and relationships. Moreover, these agents coordinate collaborative tasks through natural language, creating a\ncaptivating fusion of artificial intelligence and interactive design.\n2.3\nMulti-agent debate mechanism\nDu et al. [3] introduced a debate mechanism leveraging multiple language models in a multi-agent system. Within this\nframework, various model instances propose debate and collaboratively converge to a unified answer. This approach\nbolsters mathematical and strategic reasoning while enhancing the factual accuracy of the generated content.\nFigure 1: TradingGPT Data Warehouse.",
    "chunk_index": 2,
    "start_char": 6276,
    "end_char": 9954,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "memories and relationships. Moreover, these agents coordinate collaborative tasks through natural language, creating a\ncaptivating fusion of artificial intelligence and interactive design.\n2.3\nMulti-agent debate mechanism\nDu et al. [3] introduced a debate mechanism leveraging multiple language models in a multi-agent system. Within this\nframework, various model instances propose debate and collaboratively converge to a unified answer. This approach\nbolsters mathematical and strategic reasoning while enhancing the factual accuracy of the generated content.\nFigure 1: TradingGPT Data Warehouse.\n3\nDataset and Database Structure\nFor TradingGPT\u2019s development, we systematically integrated an extensive array of multi-modal financial data from\nAugust 15, 2020, to August 15, 2023. These datasets were sourced from financial databases and APIs, exemplified by\nthe Databento Stock Price Database, Alpaca News API, publicly available daily holdings history records from ARK, etc.\nThis data serves two purposes: (a) to formulate multi-layer memories for agents, and (b) to train, guide, and back-test\nthe agents using ARK funds\u2019 historical trading records, refining their trading decisions and actions. In our study, we\nemployed FAISS[6], an open-source vector database, due to its capacity to store data as high-dimensional vectors,\nenabling semantic searches based on exact matches. Two primary reasons informed our decision: (a) The majority of\nour data, including audio transcriptions from ARK Invest videos (translated to texts via the Whisper API), benefits\nfrom FAISS\u2019s unique underlying structure to fast query data. (b) FAISS\u2019s compatibility incorporating OpenAI and\nefficient computation of cosine similarities for specific tickers. the Raw Input schema. This data is then channeled\ninto the Agents\u2019 Cognition Schema, guided by both the system\u2019s foundational logic and LLM-agent processing. A\ncomprehensive schema structure is in Figure. 1.\n4\nProposed Method\nOur methodology integrates LLM across multiple facets of the trading agent workflow. Details and associated notation\nare provided in the subsequent sections.\n3\n\n4.1\nTrading Agents Layered Generative Memory Formulation\nIn our LLM-based trading system, agents autonomously manage their actions and memory trajectories, engaging in\ncommunication and deliberation as needed.\n4.1.1\nLayered-memory structure\nEach agent within TradingGPT discerns and categorizes perceived information into three distinct memory layers:\nlong-term, middle-term, and short-term. Compared to the approach of extracting key insights through the computation\nof ranked retrieval scores from all memories in the generative agent system [10], this layered memory approach\nintroduces a more nuanced ranking mechanism for retrieving crucial events from individual layers. This closely aligns\nwith the human cognition proposed by Atkinson et al.[1]. Our framework initially categorizes memories into separate\nlists for each layer, guided by predefined rules tailored to specific situations and the nature of events. Subsequently,\nwithin each memory layer, we leverage three crucial metrics, inspired by the work of Park et al. - recency, relevancy,\nand importance - to establish the hierarchical arrangement of events within an agent\u2019s memory. However, we have\nreconstructed their mathematical representations to attain a more logical and advanced formulation.\nFor a memory event E within the memory layer i \u2208{short, middle, long}, upon the arrival of a prompt P from the\nLLM, the agent computes the recency score SE\nRecency as per Equation.1.",
    "chunk_index": 3,
    "start_char": 9356,
    "end_char": 12930,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "and the nature of events. Subsequently,\nwithin each memory layer, we leverage three crucial metrics, inspired by the work of Park et al. - recency, relevancy,\nand importance - to establish the hierarchical arrangement of events within an agent\u2019s memory. However, we have\nreconstructed their mathematical representations to attain a more logical and advanced formulation.\nFor a memory event E within the memory layer i \u2208{short, middle, long}, upon the arrival of a prompt P from the\nLLM, the agent computes the recency score SE\nRecency as per Equation.1. This score inversely correlates with the time\ndifference between the prompt\u2019s arrival and the event\u2019s memory timestamp, aligning with Ebbinghaus\u2019s forgetting\ncurve on memory decay [8]. Qi Equation.1 represents the stability term, employed to control the memory decay\nrates across layers. A higher stability value in the long-term memory layer compared to the short-term layer suggests\nthat memories persist longer in the former. The relevancy score SE\nrelevancy represents the cosine similarity between the\nembedding vectors for the textual content of the memory event mE and the prompt query mP. The importance score\nSE\nImportance is determined using a uniform piecewise function as described in Equation.3, adhering to the relationship\ncshort < cmiddle < clong. After normalizing their values to the [0,1] range using min-max scaling, these scores, SE\nRecency,\nSE\nRelevancy and SE\nImportance are linearly combined to produce the final ranking score \u03b3E\ni for each memory layer in the\nEquation. 4 (equivalent to retrieval score in the study of Park et al.). In our setup, the ranking score thresholds, \u03b3E\ni , are\n80 for long-term, 60 for middle-term, and 40 for short-term memory. Events scoring below 20 are removed.\nSE\nRecency = e\u2212\u03b4E\nQi\n\u03b4E = tP \u2212tE\n(1)\n, where Qlong = 365 for long-term, Qmiddle = 90 for middle-term, and Qshort = 3 for short-term events.\nSE\nRelevancy =\nmE \u00b7 mP\n\u2225mE\u22252 \u00d7 \u2225mP\u22252\n(2)\nSE\nImportance =\n\uf8f1\n\uf8f2\n\uf8f3\ncshort\nif short-term memory\ncmiddle\nif middle-term memory\nclong\nif long-term memory\n(3)\n, where cshort, cmiddle and clong are all constants.\n\u03b3E\ni = \u03b1E\ni \u00d7 SE\nRecencyi + \u03b2E\ni \u00d7 SE\nRelevancyi + \u03bbE\ni \u00d7 SE\nImportancei\n(4)\nwhere each memory event is only associated with one score, as it can only belong to one of the memory layers.\nTo ensure dynamic interactions across memory layers, we define upper and lower thresholds for memory event ranking\nscores in each layer. We also utilize an add-counter function to boost the scores of events that are triggered by trading\nexecutions resulting from significant trading profits and losses. This promotes frequent events to transition from\nshort-term to potentially longer-term memory, enhancing their retention and recall by agents. The hyperparameters \u03b1E\ni ,\n\u03b2E\ni , and \u03bbE\ni exhibit variations across different layers. The transferable layered memory system allows the agents to\ncapture and prioritize crucial memory events by considering both their types and frequencies when conducting queries.",
    "chunk_index": 4,
    "start_char": 12377,
    "end_char": 15390,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "the scores of events that are triggered by trading\nexecutions resulting from significant trading profits and losses. This promotes frequent events to transition from\nshort-term to potentially longer-term memory, enhancing their retention and recall by agents. The hyperparameters \u03b1E\ni ,\n\u03b2E\ni , and \u03bbE\ni exhibit variations across different layers. The transferable layered memory system allows the agents to\ncapture and prioritize crucial memory events by considering both their types and frequencies when conducting queries.\n4.1.2\nMemory formulated by individual experience\nIn the trading paradigm, macro-level market indicators are stored in the long-term memory, quarterly investment\nstrategies are allocated to the mid-term memory, and daily investment messages are channeled into the short-term\n4\n\nFigure 2: TradingGPT training and test workflow.\n1\nmemory. These three memory classes constitute the initial structure within the Agents\u2019 Cognition Schema of our data\nwarehouse in Figure. 1. In our trading system, agents make informed trading decisions relying on the outcomes of two\ndistinct workflows: the single-agent workflow and the multi-agent workflow, as depicted on the left side of Figure 2.\nIn the single-agent workflow, when presented with a specific stock ticker, agents\u2019 LLM core generates evaluations and\nreflections, which encompass trading recommendations and the reasons behind them, based on the essential events\nretrieved from their layered memory. Subsequently, the agent can proceed to execute trading actions in accordance with\nthese generated insights. The key features that empower our system are (a) Immediate reflection: Conducted daily, this\nmechanism allows agents to consolidate top-ranked events of each memory layer and market facts, such as daily stock\nprices and ARK fund trading records. Using LLM and specific prompts, agents generate five trading recommendations:\n\u201csignificantly increase position\u201d, \u201cslightly increase position\", \u201chold\u201d, \u201cslightly decrease position\u201d, and \u201csignificantly\ndecrease position\u201d, with its justification. Each option is associated with a predetermined trade value. which can be\nadjusted to suit the business scale represented by the agents. Additionally, this reflection captures the agent\u2019s trade\nvolumes and returns. (b)Extended reflection: This provides a broader performance overview over a designated period,\nlike a week. It includes stock prices, the agent\u2019s trading trends, and self-evaluation. The immediate reflection guides\ntrade execution directly, while the extended reflection acts as a supplementary reference for recalling recent investment\ntransactions. Both types of reflections are stored in the Agents\u2019 Cognition Schema\u2019s reflection index, as shown in\nFigure 1, distinguished by a specific flag.\n4.1.3\nMemory gained by interacting with other agents\nFor stocks that appear in multiple agents\u2019 trading portfolios, TradingGPT enables inter-agent dialogue via a debate\nmechanism. This mechanism encourages collaboration between agents typically specializing in distinct sectors, with\nthe goal of optimizing trading outcomes. Within these debates, agents present their top-K layered memories as well as\nimmediate reflections, encompassing recommendations, trade values, volumes, and returns, inviting feedback from\ntheir peers. All feedback is subsequently stored in the debate class of the Agents\u2019 Cognition Schema, tagged with the\nreceiver\u2019s index, as shown in Figure. 1.",
    "chunk_index": 5,
    "start_char": 14866,
    "end_char": 18317,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "This mechanism encourages collaboration between agents typically specializing in distinct sectors, with\nthe goal of optimizing trading outcomes. Within these debates, agents present their top-K layered memories as well as\nimmediate reflections, encompassing recommendations, trade values, volumes, and returns, inviting feedback from\ntheir peers. All feedback is subsequently stored in the debate class of the Agents\u2019 Cognition Schema, tagged with the\nreceiver\u2019s index, as shown in Figure. 1.\n4.2\nDesign of Training and Testing Workflows\nThe distinct design of our training and testing workflows is crucial for curating valuable past memory events and\nstrategizing optimal future trading actions.\n4.2.1\nTraining\nThe training process is twofold: a single-agent workflow followed by a multi-agent phase, as detailed in the left section\nof Fig. 2. In the single-agent phase, the LLM-driven agent is prompted with key data like stock ticker, date, and trader\ncharacters. Using this context, it evaluates top-K-ranked memories across each layer to derive preliminary investment\nsignals, where K is a predefined hyperparameter. The LLM then synchronizes and analyzes these signals with market\n1Data entities without specific timestamps are extracted as per the date displayed at the top of the plots.\n5\n\ndata, such as daily records from fund firms like ARK and stock closing prices, leading the agent to formulate an\nimmediate reflection and trade accordingly. Subsequently, the agent collaborates in the multi-agent phase, joining\ndebates with agents trading the same stock from varied sectors on that day (refer to 4.1.3).\n4.2.2\nTest\nThe testing process, illustrated in the right section of Figure. 2, blends single-agent and multi-agent operations. Both\nindividually processed memories and insights from inter-agent exchanges are concurrently inputted into the LLM to\ninform trading decisions. Key differences from the training phase include: (a) During testing, agents operate without the\nguidance of trading records from the representative fund firm, relying solely on daily stock prices as market facts. (b)\nTime series patterns of prior training reflections and debates, covering a week in our setup, act as auxiliary references\nin the absence of substantial market ground truths, as noted in (a). Other aspects of the test workflow align with the\ntraining phase.\nFigure 3: Prompt template for key steps of TradingGPT workflow.\n5\nCurrent Stage And Future work\nOur research consists of two phases: prompt design and ablation studies. We\u2019ve crafted efficient LLM prompts using\nGPT3.5 turbo as the backbone. Examples of prompts that encapsulate the necessary insights for each phase of the\nTradingGPT training and testing workflow. The specific design of these prompts is illustrated by examples in Figure. 3.\nWith our established prompt template, we\u2019re poised to undertake ablation studies to assess the trading efficacy of agent\nsystems based on various backbone models. This will involve comparisons within LLMs, such as GPT3.5 turbo versus\nCodeLlama 34B, and against models like multi-agent reinforcement learning. The training phase will utilize data\nspanning from August 15, 2020, to February 15, 2023, while the testing phase will extend until August 15, 2023. We\u2019ll\nassess performance using financial metrics like cumulative trade returns, volatility, and the Sharpe Ratio (see 4.1.2).",
    "chunk_index": 6,
    "start_char": 17825,
    "end_char": 21216,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "Figure. 3.\nWith our established prompt template, we\u2019re poised to undertake ablation studies to assess the trading efficacy of agent\nsystems based on various backbone models. This will involve comparisons within LLMs, such as GPT3.5 turbo versus\nCodeLlama 34B, and against models like multi-agent reinforcement learning. The training phase will utilize data\nspanning from August 15, 2020, to February 15, 2023, while the testing phase will extend until August 15, 2023. We\u2019ll\nassess performance using financial metrics like cumulative trade returns, volatility, and the Sharpe Ratio (see 4.1.2).\nHarnessing an innovative multi-layer memory system and character design, our main goal is to establish a state-of-the-art\nLLM-based multi-agent automated trading system adaptable to various LLMs as its core. This system aspires to achieve\nsuperior trading performance over other leading trading agent systems by emulating human traders\u2019 cognitive behaviors\nand ensuring responsiveness in the constantly changing market scenario. We also posit that this LLM-based multi-agent\ndesign can improve working efficiency and collaborative performance in artificial systems across diverse sectors.\nPotential applications range from character development in video games to the creation of robo-consultants in business,\nhealthcare, and technology domains.\n6\n\nReferences\n[1]\nRichard C Atkinson and Richard M Shiffrin. \u201cHuman memory: A proposed system and its control processes\u201d. In:\nPsychology of learning and motivation. Vol. 2. Elsevier, 1968, pp. 89\u2013195.\n[2]\nJacob Devlin et al. \u201cBert: Pre-training of deep bidirectional transformers for language understanding\u201d. In: arXiv\npreprint arXiv:1810.04805 (2018).\n[3]\nYilun Du et al. \u201cImproving Factuality and Reasoning in Language Models through Multiagent Debate\u201d. In: arXiv\npreprint arXiv:2305.14325 (2023).\n[4]\nYoav Goldberg and Omer Levy. \u201cword2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-\nembedding method\u201d. In: arXiv preprint arXiv:1402.3722 (2014).\n[5]\nZhenhan Huang and Fumihide Tanaka. \u201cMSPM: A modularized and scalable multi-agent reinforcement learning-\nbased system for financial portfolio management\u201d. In: Plos one 17.2 (2022), e0263689.\n[6]\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. \u201cBillion-scale similarity search with GPUs\u201d. In: IEEE Transac-\ntions on Big Data 7.3 (2019), pp. 535\u2013547.\n[7]\nYang Liu et al. \u201cAdaptive quantitative trading: An imitative deep reinforcement learning approach\u201d. In: Proceed-\nings of the AAAI conference on artificial intelligence. Vol. 34. 02. 2020, pp. 2128\u20132135.\n[8]\nJaap MJ Murre and Joeri Dros. \u201cReplication and analysis of Ebbinghaus\u2019 forgetting curve\u201d. In: PloS one 10.7\n(2015), e0120644.\n[9]\nOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].\n[10]\nJoon Sung Park et al. \u201cGenerative agents: Interactive simulacra of human behavior\u201d. In: arXiv preprint\narXiv:2304.03442 (2023).\n[11]\nJeffrey Pennington, Richard Socher, and Christopher D Manning. \u201cGlove: Global vectors for word representation\u201d.\nIn: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014,\npp. 1532\u20131543.\n[12]\nAlec Radford et al. \u201cImproving language understanding by generative pre-training\u201d. In: (2018).\n[13]\nMark Riedl and Vadim Bulitko. \u201cInteractive narrative: A novel application of artificial intelligence for computer\ngames\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 26. 1. 2012, pp. 2160\u20132165.\n[14]\nWonsup Shin, Seok-Jun Bu, and Sung-Bae Cho. \u201cAutomatic financial trading agent for low-risk portfolio\nmanagement using deep reinforcement learning\u201d.",
    "chunk_index": 7,
    "start_char": 20622,
    "end_char": 24237,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014,\npp. 1532\u20131543.\n[12]\nAlec Radford et al. \u201cImproving language understanding by generative pre-training\u201d. In: (2018).\n[13]\nMark Riedl and Vadim Bulitko. \u201cInteractive narrative: A novel application of artificial intelligence for computer\ngames\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 26. 1. 2012, pp. 2160\u20132165.\n[14]\nWonsup Shin, Seok-Jun Bu, and Sung-Bae Cho. \u201cAutomatic financial trading agent for low-risk portfolio\nmanagement using deep reinforcement learning\u201d. In: arXiv preprint arXiv:1909.03278 (2019).\n[15]\nHugo Touvron et al. \u201cLlama: Open and efficient foundation language models\u201d. In: arXiv preprint\narXiv:2302.13971 (2023).\n[16]\nShijie Wu et al. \u201cBloomberggpt: A large language model for finance\u201d. In: arXiv preprint arXiv:2303.17564\n(2023).\n[17]\nHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. \u201cFinGPT: Open-Source Financial Large Language\nModels\u201d. In: arXiv preprint arXiv:2306.06031 (2023).\n7",
    "chunk_index": 8,
    "start_char": 23636,
    "end_char": 24684,
    "paper_title": "TradingGPT Multi-Agent System with Layered Memory ",
    "paper_category": "q-fin.PM",
    "paper_filename": "TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.PM/TradingGPT_Multi-Agent_System_with_Layered_Memory_.pdf"
  },
  {
    "text": "Fair Estimation of Capital Risk Allocation\nTomasz R. Bielecki a,\nIgor Cialenco, a Marcin Pitera b,\nThorsten Schmidt c\nFirst Circulated: February 26, 2019\nThis Version: November 20, 2019\nAbstract:\nIn this paper we develop a novel methodology for estimation of risk capital allocation.\nThe methodology is rooted in the theory of risk measures. We work within a general,\nbut tractable class of law-invariant coherent risk measures, with a particular focus on\nexpected shortfall. We introduce the concept of fair capital allocations and provide\nexplicit formulae for fair capital allocations in case when the constituents of the risky\nportfolio are jointly normally distributed. The main focus of the paper is on the problem\nof approximating fair portfolio allocations in the case of not fully known law of the\nportfolio constituents. We de\ufb01ne and study the concepts of fair allocation estimators\nand asymptotically fair allocation estimators. A substantial part of our study is devoted\nto the problem of estimating fair risk allocations for expected shortfall.\nWe study\nthis problem under normality as well as in a nonparametric setup. We derive several\nestimators, and prove their fairness and/or asymptotic fairness. Last, but not least, we\npropose two backtesting methodologies that are oriented at assessing the performance\nof the allocation estimation procedure. The paper closes with a substantial numerical\nstudy of the subject.\nKeywords: capital allocation, fair capital allocation, asymptotic fairness, expected shortfall,\nrisk measures, Euler principle, value-at-risk, tail-value-at-risk, backtesting capital\nallocation.\n1\nIntroduction\nThe measurement and the management of risk is without doubt of highest importance in the\n\ufb01nancial and the insurance industries. Arguably, the theory and applications of risk measures are\nmost useful for this purpose. For early applications in the insurance context see [B\u00a8uh70, Ger74],\nand for a historical perspective in the \ufb01nancial context see [Gui16]. The seminal article [ADEH99]\nplaced risk measurements on an axiomatic foundation paving the way to coherent risk measures\nwhich have been treated in numerous works since then. We refer to [Del00, FS11, MFE15] for an\nin-depth treatment of the topic.\nThe application of risk measures to portfolio management naturally leads to the problem of\nallocating portions of the risk capital to the constituents of the portfolio, i.e. to the risk allocation\naDepartment of Applied Mathematics, Illinois Institute of Technology\n10 W 32nd Str, Building REC, Room 208, Chicago, IL 60616, USA\nEmails: tbielecki@iit.edu (T.R. Bielecki), and cialenco@iit.edu (I. Cialenco)\nURLs: http://math.iit.edu/~bielecki and http://math.iit.edu/~igor\nbInstitute of Mathematics, Jagiellonian University, Lojasiewicza 6, 30-348 Cracow, Poland\nEmail: marcin.pitera@im.uj.edu.pl, URL: http://www2.im.uj.edu.pl/MarcinPitera/\ncDepartment of Mathematical Stochastics, University of Freiburg, Eckerstr.1, 79104 Freiburg, Germany\nEmail: thorsten.schmidt@stochastik.uni-freiburg.de,\nURL: http://www.archiv.stochastik.uni-freiburg.de/homepages/schmidt/\n1\narXiv:1902.10044v2 [q-fin.RM] 21 Nov 2019\n\n2\nBielecki, Cialenco, Pitera, and Schmidt\nproblem. There are a number of di\ufb00erent approaches to risk capital allocation, depending on the one\nhand on the class of the used risk measures, and on the other hand on the used allocation principles.\nThe Euler principle, often used in risk management practice, is one example, see e.g. [Tas04, Tas07].\nFor coherent risk measures, the Euler principle coincides with the axiomatic approach proposed\nin [Kal05].",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3608,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "Eckerstr.1, 79104 Freiburg, Germany\nEmail: thorsten.schmidt@stochastik.uni-freiburg.de,\nURL: http://www.archiv.stochastik.uni-freiburg.de/homepages/schmidt/\n1\narXiv:1902.10044v2 [q-fin.RM] 21 Nov 2019\n\n2\nBielecki, Cialenco, Pitera, and Schmidt\nproblem. There are a number of di\ufb00erent approaches to risk capital allocation, depending on the one\nhand on the class of the used risk measures, and on the other hand on the used allocation principles.\nThe Euler principle, often used in risk management practice, is one example, see e.g. [Tas04, Tas07].\nFor coherent risk measures, the Euler principle coincides with the axiomatic approach proposed\nin [Kal05]. For the more general case of convex risk measures we refer to [Tsa09, MFE15] and\nreferences therein.\nRisk measures as we consider them here are mathematical tools which require as inputs probabil-\nity distributions of the underlying risk factors. In practical applications one is typically confronted\nwith the fact that these probability distributions are not fully speci\ufb01ed. For example, let X repre-\nsent a P&L, which is a function of some underlying risk factors, and let \u03c1 be the risk measure used\nto measure the riskiness of X, so that the desired quantity to compute is the risk \u03c1(X). Since the\nprobability laws of the risk factors are not fully speci\ufb01ed, then one needs to approximate \u03c1(X), per-\nhaps by estimating this quantity exploiting historical data. As a consequence, the risk allocations,\nwhich are usually computed in terms of risk measures, need to be approximated, in particular by\nestimation.\nThe problem of estimation of risk has, to a great extent, been neglected in the literature. In\nthe recent paper [PS18] a new statistical methodology for e\ufb03cient estimation of risk capital \u03c1(X)\nwas proposed.\nThe methodology introduced in that paper is based on the key concept, which\nthe authors call unbiased estimation of risk also introduced in [PS18], and is based on economic\nprinciple.1 Inspired by the ideas from [PS18], in this paper we develop a novel methodology for\nestimation of capital risk allocation.2 We work within a general, but tractable class of coherent risk\nmeasures, the so-called weighted value-at-risk measures introduced in [Che06], with focus on the\nexpected shortfall risk measure, which is broadly accepted in the risk management practice.\nThe underlying key concept introduced in this paper is the fair capital risk allocation, which\nbuilds upon the robust representation of coherent risk measures. Our concept of fairness aligns\nwell with what has been done in some of the existing literature. In particular, it implies fairness\nin the sense of fuzzy games introduced in [Del00].\nThe fair capital risk allocation can be also\nviewed as version of the Euler principle of risk allocation. The fair allocation principle used here\nhas been also applied in [BCF18] in the context of allocation of the total default fund among the\nclearing members of a CCP. For additional insight about fair risk allocation we refer to the recent\nwork [CD19]. We provide explicit formulae for fair capital allocations in case when the constituents\nof the portfolio are jointly normally distributed.",
    "chunk_index": 1,
    "start_char": 2954,
    "end_char": 6126,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "as version of the Euler principle of risk allocation. The fair allocation principle used here\nhas been also applied in [BCF18] in the context of allocation of the total default fund among the\nclearing members of a CCP. For additional insight about fair risk allocation we refer to the recent\nwork [CD19]. We provide explicit formulae for fair capital allocations in case when the constituents\nof the portfolio are jointly normally distributed.\nThe major focus of the paper is on the problem of approximating fair portfolio allocations when\nthe law of the portfolio constituents is not fully known. Motivated by the concept of the fair capital\nallocation, we de\ufb01ne and study the concepts of fair allocation estimators and asymptotically fair\nallocation estimators. A substantial portion of our study is devoted to the problem of estimating\nthe risk allocation under expected shortfall and normality. In addition we consider a nonparametric\napproach to this problem. We derive several estimators, and prove their fairness and/or asymptotic\nfairness. Last, but not least, we propose two backtesting methodologies that are oriented at assess-\ning the performance of the allocation estimation procedure. Finally, we perform relevant numerical\nstudies. The results of the numerical studies that we have conducted so far are encouraging for\npractical use of the estimation and backtesting of the capital allocation.\nThis work is a \ufb01rst step towards developing formal methodologies for estimating and backtesting\nof fair capital allocation. As such, it has potential to open new theoretical and practical research\navenues.\n1The concept of unbiased estimation of risk must not be confused with the classical concept of unbiased estimator.\n2In this paper we will occasionally write capital allocation or risk allocation in place of capital risk allocation.\n\nFair capital risk allocation\n3\n2\nThe fair allocation principle\nLet (\u2126, F, P) be an atomless probability space, and let E be the expectation under P. In what\nfollows, all needed integrability and regularity assumptions are taken for granted.\nWe consider a random vector X = (X1, . . . , Xd) whose components are interpreted as discounted\nfuture pro\ufb01ts and losses (P&Ls). The marginal random variable Xi (margin \u2013 for short) might\ncorrespond to the ith clearing member of a central clearing counterparty (CCP), to the ith position\nin the portfolio, to the ith trader portfolio in a trading desk, or to the ith desk in the \ufb01nancial\ninstitution portfolio. In the following, we will refer to X as portfolio and to Xi as the ith portfolio\nmargin or the ith portfolio constituent.\nLet L1 := L1(\u2126, F, P) and let \u03c1 : L1 \u2192R \u222a{+\u221e} be a normalized monetary risk measure.\nThat is: \u03c1 is monotone, i.e. \u03c1(U) \u2264\u03c1(V ) for all U, V \u2208L1 such that U \u2265V ; \u03c1 is cash-additive,\ni.e. \u03c1(U + c) = \u03c1(U) \u2212c for all c \u2208R and all U \u2208L1; \u03c1 is normalized, i.e. \u03c1(0) = 0.\nThe riskiness of the portfolio X is measured by applying the risk measure \u03c1 to the aggregated\nportfolio P&L denoted by\nS :=\nd\nX\ni=1\nXi.",
    "chunk_index": 2,
    "start_char": 5683,
    "end_char": 8704,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "Let L1 := L1(\u2126, F, P) and let \u03c1 : L1 \u2192R \u222a{+\u221e} be a normalized monetary risk measure.\nThat is: \u03c1 is monotone, i.e. \u03c1(U) \u2264\u03c1(V ) for all U, V \u2208L1 such that U \u2265V ; \u03c1 is cash-additive,\ni.e. \u03c1(U + c) = \u03c1(U) \u2212c for all c \u2208R and all U \u2208L1; \u03c1 is normalized, i.e. \u03c1(0) = 0.\nThe riskiness of the portfolio X is measured by applying the risk measure \u03c1 to the aggregated\nportfolio P&L denoted by\nS :=\nd\nX\ni=1\nXi.\nWe call the quantity \u03c1(S) the aggregated risk, or total risk, of the portfolio X.\nOur objective is to study the issue of allocating the aggregated risk of the portfolio to the\nindividual constituents of the portfolio. Speci\ufb01cally, we intend to \ufb01nd a vector a = (a1, . . . , ad) \u2208Rd,\ncalled a risk allocation, such that the following balance condition holds\n\u03c1(S) =\nd\nX\ni=1\nai.\n(2.1)\nThe component ai is interpreted as the risk contribution of Xi to the aggregated risk, and therefore\nXi + ai is interpreted as the ith secured margin of portfolio X. Correspondingly, we call X + a the\nsecured portfolio, and S + Pd\ni=1 ai the secured aggregated position.\nStated as such, the risk allocation problem is ill\u2013posed.\nIndeed, any collection of numbers\na1, . . . , ad satisfying the balance condition (2.1) constitutes a risk allocation.\nIn order to deal\nwith a meaningful risk allocation problem we need to impose additional conditions, that re\ufb02ect\nsome additional and desired features of the portfolio allocation. With this in mind, we impose an\nadditional condition on a, which we will call the fairness condition.\nTowards this end, we require more structure on the risk measure \u03c1. We additionally assume\nthat the monetary risk measure \u03c1 is \ufb01nite, law-invariant, comonotonic and coherent; see [Kus01] for\ndetails. In view of [Sha13, Theorem 2(iii)] we conclude that \u03c1 is a weighted value-at-risk measure,3 so\nthat it admits representation (1.1) in [Che06] for a \ufb01xed probability measure \u03bd on [0, 1]. Speci\ufb01cally,\nfor a continuously distributed random variable Y ,\n\u03c1(Y ) = \u03c1\u03bd(Y ) :=\nZ\n[0,1]\nES\u03b1(Y )\u03bd(d\u03b1),\nY \u2208L1,\n(2.2)\nwhere ES\u03b1 is the Expected Shortfall4 (ES) risk measure (sometimes also called tail value-at-risk or\nconditional value-at-risk) for reference level \u03b1 \u2208[0, 1]. Moreover, \u03c1 admits a robust-type represen-\ntation of the form\n\u03c1(Y ) = sup\nQ\u2208D\nEQ[\u2212Y ],\n(2.3)\n3Following the traditional nomenclature, we use the name \u2018weighted value-at-risk measure\u2019, although a more\nappropriate name would be \u2018weighted expected shortfall\u2019.\n4For a formal de\ufb01nition of expected shortfall in the context of this paper see (2.12).\n\n4\nBielecki, Cialenco, Pitera, and Schmidt\nwhere D is a determining family of probability measures absolutely continuous with respect to P.\nAs shown in [Che06, Theorem 6.3], for any Y \u2208L1 there exists a unique minimal extreme measure\nQY \u2208D such that5\n\u03c1(Y ) = EQY [\u2212Y ].",
    "chunk_index": 3,
    "start_char": 8305,
    "end_char": 11088,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "measure\u2019, although a more\nappropriate name would be \u2018weighted expected shortfall\u2019.\n4For a formal de\ufb01nition of expected shortfall in the context of this paper see (2.12).\n\n4\nBielecki, Cialenco, Pitera, and Schmidt\nwhere D is a determining family of probability measures absolutely continuous with respect to P.\nAs shown in [Che06, Theorem 6.3], for any Y \u2208L1 there exists a unique minimal extreme measure\nQY \u2208D such that5\n\u03c1(Y ) = EQY [\u2212Y ].\n(2.4)\nSometimes, we refer to QY as the worst-case scenario measure (for position Y ). We denote by ZY\nthe associated Radon-Nikodym derivative dQY /dP. In particular, as shown in [Che06] (cf. formula\n(6.2) there), if Y has a continuous distribution then we have\nZY = g(Y ),\nand\n\u03c1(Y ) = E[\u2212g(Y )Y ],\n(2.5)\nfor some Borel function g. For example if \u03c1 = ES\u03b1 is the expected shortfall at level \u03b1, then we have\nZY = 1\n\u03b11{Y <qY (\u03b1)},\n(2.6)\nwhere qY (\u03b1) is the \u03b1\u2013quantile of Y .\nIn what follows, for simplicity, we write ES instead of EQS. The value ES [Xi + ai] represents the\naverage performance of the secured margin Xi + ai under the extremal measure QS. The following\nfairness condition selects risk allocations which are comparable under the extremal measure of the\naggregated portfolio P&L.\nDe\ufb01nition 2.1. The capital allocation a = (a1, . . . , ad) is called fair, if\nES [Xi + ai] = ES [Xj + aj] ,\ni, j = 1, . . . , d.\n(2.7)\nThe economic intuition behind this de\ufb01nition is as follows: the worst-case-scenario QS is, in our\nsetting, the determining scenario of the capital allocation for the portfolio through \u03c1(S) = ES[\u2212S]\nresulting from Equation (2.4). A fair capital allocation is meant to create secured positions Xi+ai,\n1 \u2264i \u2264d, so that the averages of all secured positions with respect to the worst-case-scenario QS\nare all equal.\nSince \u03c1 is a monetary risk measure, the extremal measures for S and S + c, c \u2208R, coincide.\nThus, for any fair capital allocation a satisfying the balance condition in (2.1) we have\n0 = \u03c1\n\u0012\nd\nX\ni=1\n(Xi + ai)\n\u0013\n= \u2212ES\n\u0014\nd\nX\ni=1\n(Xi + ai)\n\u0015\n= \u2212\nd\nX\ni=1\nES [Xi + ai] ,\n(2.8)\nand consequently the risk allocations are given by\nai = \u2212ES[Xi] = \u2212E[ZSXi],\ni = 1, . . . , d.\n(2.9)\nIn view of (2.5), we also have that\nai = \u2212E\n\u0002\ng\n\u0000d\nX\nk=1\nXk\n\u0001\nXi\n\u0003\n,\ni = 1, . . . , d.\n(2.10)\nFirst, we note that the fair risk allocation is unique, which is due to the existence and uniqueness\nof the extreme measure QS.\nSecondly, we also note that the concept of fairness introduced in\n5Note that the set of extreme measures, i.e.\nthe set of measures that satisfy (2.4), might contain more than\none element. The term minimal corresponds to the minimal element with respect to the convex stochastic order;\nsee [Che06] for details.",
    "chunk_index": 4,
    "start_char": 10649,
    "end_char": 13327,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "\u0002\ng\n\u0000d\nX\nk=1\nXk\n\u0001\nXi\n\u0003\n,\ni = 1, . . . , d.\n(2.10)\nFirst, we note that the fair risk allocation is unique, which is due to the existence and uniqueness\nof the extreme measure QS.\nSecondly, we also note that the concept of fairness introduced in\n5Note that the set of extreme measures, i.e.\nthe set of measures that satisfy (2.4), might contain more than\none element. The term minimal corresponds to the minimal element with respect to the convex stochastic order;\nsee [Che06] for details.\n\nFair capital risk allocation\n5\nDe\ufb01nition 2.1 is actually equivalent to the concept of Euler risk allocation. This observation is\nreadily demonstrated by (2.9).\nHowever, it is the characterization of the fairness property of\nrisk allocation as presented in (2.7) that underlies the notion of fair allocation estimator given in\nDe\ufb01nition 3.1, which is the key de\ufb01nition in this paper. That is why we de\ufb01ned fairness of risk\nallocation via (2.7) rather than via (2.9).\nWe also note that the above notion of fairness implies fairness in the sense of fuzzy games\nintroduced in [Del00]. Indeed, this follows from Theorems 17 and 18 therein taking representation\n(2.3) into account. The fair allocation principle of De\ufb01nition 2.1 has been applied in [BCF18] in\nthe context of allocation of the total default fund among the clearing members of a CCP.\nThe following example illustrates the concept of fair allocation.\nExample 2.2 (Mean risk allocation). Consider expectation for measuring risk, i.e. \u03c1(Y ) = E[\u2212Y ],\nin which case D = {P}.\nThen, clearly, for any X = (X1, . . . , Xd), the capital allocation a =\n(a1, . . . , ad) given as\nai = \u2212E[Xi],\ni = 1, . . . , d,\nis fair.\n2.1\nRisk allocation under normality\nAs an example where explicit formulae can be obtained, we study the case of normally distributed\npro\ufb01ts and losses. In this regard, let us assume that the vector X is normally distributed under\nP with mean \u00b5 and covariance matrix \u03a3 and \ufb01x i \u2208{1, . . . , d}. Then, (Xi, S) is bivariate normal,\nand the conditional expectation E[Xi|S] takes the form\nE[Xi|S] = \u03b2iS + \u03b1i,\nwith \u03b2i = Cov(Xi,S)\nVar(S) , and \u03b1i = \u00b5i \u2212\u03b2i\nPd\nj=1 \u00b5j. Since this conditional expectation is the L2 :=\nL2(\u2126, F, P) orthogonal projection of Xi on the linear space spanned by S we obtain\nXi = \u03b2iS + \u03b1i + \u03f5i,\nwhere S and \u03f5i are independent under P, and E[\u03f5i] = 0. For any weighted value-at-risk measure \u03c1,\nEquation (2.9) implies that a fair capital allocation is given by\nai = \u2212ES[Xi] = \u2212\u03b1i \u2212\u03b2iES[S] \u2212ES[\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[ZS\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[g(S)\u03b5i] = \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[g(S)]E[\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S),\n(2.11)\nwhere we have used (2.5) in the fourth equality, independence of S and \u03f5i under P in the \ufb01fth\nequality, and the fact that \u03f5i has zero mean under P, in the last equality.",
    "chunk_index": 5,
    "start_char": 12840,
    "end_char": 15578,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "S and \u03f5i are independent under P, and E[\u03f5i] = 0. For any weighted value-at-risk measure \u03c1,\nEquation (2.9) implies that a fair capital allocation is given by\nai = \u2212ES[Xi] = \u2212\u03b1i \u2212\u03b2iES[S] \u2212ES[\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[ZS\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[g(S)\u03b5i] = \u2212\u03b1i + \u03b2i\u03c1(S) \u2212E[g(S)]E[\u03b5i]\n= \u2212\u03b1i + \u03b2i\u03c1(S),\n(2.11)\nwhere we have used (2.5) in the fourth equality, independence of S and \u03f5i under P in the \ufb01fth\nequality, and the fact that \u03f5i has zero mean under P, in the last equality. As expected, the total\nallocated risk is divided among constituents using the regression slope allocations which is typically\nreferred to as the covariance principle, see [MFE15, Section 8.5].\nExpected shortfall. To be more speci\ufb01c, we consider as an important example the expected\nshortfall (ES). In this regard, let \u03c1 = ES\u03b1 denote ES under P for the level \u03b1 \u2208(0, 1). Then, for a\ncontinuously distributed real valued random variable Y we have\nES\u03b1(Y ) = E[\u2212Y | Y \u2264qY (\u03b1)],\n(2.12)\n\n6\nBielecki, Cialenco, Pitera, and Schmidt\nwhere qY (\u03b1) is an \u03b1-quantile of Y . Thus, since S is normally distributed, (2.12) yields\nES\u03b1 (S) = \u2212\nd\nX\ni=1\n\u00b5i + 1\n\u03b1\np\nVar(S) \u03c6\n\u0000\u03a6\u22121(\u03b1)\n\u0001\n,\n(2.13)\nwhere \u03c6 and \u03a6 are the density and the cumulative distribution function of the standard normal\ndistribution; see [MFE15, Example 2.14]. Putting together (2.11) and (2.13) we see that the capital\nallocation for ES is given as\nai = \u2212\u00b5i + Cov(Xi, S)\n\u03b1\np\nVar(S)\n\u03c6(\u03a6\u22121(\u03b1)),\ni = 1, 2, . . . , d .\n(2.14)\n3\nFair allocation estimators\nIn practice, the probability distribution under P of X, the portfolio\u2019s P&L, is not fully speci\ufb01ed.\nSince, in view of (2.5) and (2.10), we have\n\u03c1(S) = \u2212E\n\u0014\ng\n\u0010\nd\nX\nk=1\nXk\n\u0011\nd\nX\nk=1\nXk\n\u0015\n,\nand ai = \u2212E\n\u0014\ng\n\u0010\nd\nX\nk=1\nXk\n\u0011\nXi\n\u0015\n,\ni = 1, . . . , d,\n(3.1)\nthen, in almost all practically relevant applications, neither the aggregated risk \u03c1(S) nor the fair\nrisk allocation a are known, and thus need to be estimated. Hence, appropriate estimation proce-\ndures have to be developed, in particular estimation procedures based on the historical data about\nrealizations of the portfolio. This will involve estimating, in some way, the probability distribution\nof X under P.\nIn the following, we set the relevant statistical framework and propose e\ufb03cient procedures to\ndeal with this estimation issue. We refer to X as to the population. Historical information about X\nis given in terms of a random sample of size n drawn from X, which we denote by X1, . . . , Xn, so\nthat X1, . . . , Xn are independently drawn copies of the random variable X. Our aim is to estimate\nthe aggregated risk \u03c1(S) using the information contained in the sample. Towards this end we let\nXn := {Xj = (Xj\n1, .",
    "chunk_index": 6,
    "start_char": 15114,
    "end_char": 17750,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "estimation issue. We refer to X as to the population. Historical information about X\nis given in terms of a random sample of size n drawn from X, which we denote by X1, . . . , Xn, so\nthat X1, . . . , Xn are independently drawn copies of the random variable X. Our aim is to estimate\nthe aggregated risk \u03c1(S) using the information contained in the sample. Towards this end we let\nXn := {Xj = (Xj\n1, . . . , Xj\nd), j = 1, . . . , n},\nrepresent the random sample, and let us denote its realization by\nxn := {xj = (xj\n1, . . . , xj\nd), j = 1, . . . , n},\n(3.2)\nwhere xj\nk corresponds to the j-th observed (realized) value of the portfolio\u2019s kth margin.\nThe formal statistical setup for this situation is as follows: consider a family of probability\nmeasures P := (P\u03b8)\u03b8\u2208\u0398 on (\u2126, F), where \u0398 denotes the parameter space. To avoid unnecessary\ntechnical di\ufb03culties, we assume that all measures in P are equivalent. Furthermore, we assume\nthat for any \u03b8 \u2208\u0398 the random sample X1, . . . , Xn is i.i.d. under P\u03b8. Moreover, we assume that\nP = P\u03b80 for some (unknown) parameter \u03b80 \u2208\u0398. We will denote by \u03c1\u03b8 and, respectively E\u03b8, the risk\nmeasure \u03c1, and respectively the expectation, under the probability measure P\u03b8. Similarly to the\nnotation QY and ZY , corresponding to the reference measure P, we will use notation Q\u03b8\nY and Z\u03b8\nY\nwith regard to the reference measure P\u03b8.\nGiven the random sample Xn, the allocation a is estimated using an allocation estimator \u02c6An =\n( \u02c6An\n1, . . . , \u02c6An\nd) de\ufb01ned as\n\u02c6An = \u03b7n(Xn),\n(3.3)\nfor some measurable function \u03b7n : Rd\u00d7n \u2192Rd.\nNext, we de\ufb01ne a property that should be satis\ufb01ed by any reasonable allocation estimator.\n\nFair capital risk allocation\n7\nDe\ufb01nition 3.1. An allocation estimator \u02c6An is called fair if, for all \u03b8 \u2208\u0398,\nE\u03b8h\nZ\u03b8\nS, \u02c6\nAn(Xi + \u02c6An\ni )\ni\n= 0,\ni = 1, . . . , d,\n(3.4)\nwhere Z\u03b8\nS, \u02c6\nAn := Z\u03b8\nS+Pd\ni=1 \u02c6\nAn\ni .\nWe emphasize that \u02c6An is a random variable, and Z\u03b8\nS, \u02c6\nAn is the Radon-Nikodym derivative cor-\nresponding to S + Pd\ni=1 \u02c6An\ni .\nWe stress that the de\ufb01nition of the fair allocation estimator requires that property (3.4) is\nsatis\ufb01ed for all populations from the population space \u0398, that is for all \u03b8 \u2208\u0398.\nIntuitively, the above de\ufb01nition means that an allocation estimator is fair if it mimics the\nbalanced fairness condition (2.9) for all relevant scenarios (given by probability distributions P\u03b8, \u03b8 \u2208\n\u0398). In particular, the aggregated risk estimator obtained from a fair allocation estimator \u02c6A by\nsummation turns out to be unbiased in the sense of [PS18, De\ufb01nition 4.1], namely, for any \u03b8 \u2208\u0398\nwe get\n\u03c1\u03b8\u0010\nS +\nd\nX\ni=1\n\u02c6An\ni\n\u0011\n= \u2212\nd\nX\ni=1\nE\u03b8h\nZ\u03b8\nS, \u02c6\nAn(Xi + \u02c6An\ni )\ni\n= 0.",
    "chunk_index": 7,
    "start_char": 17350,
    "end_char": 19968,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "all populations from the population space \u0398, that is for all \u03b8 \u2208\u0398.\nIntuitively, the above de\ufb01nition means that an allocation estimator is fair if it mimics the\nbalanced fairness condition (2.9) for all relevant scenarios (given by probability distributions P\u03b8, \u03b8 \u2208\n\u0398). In particular, the aggregated risk estimator obtained from a fair allocation estimator \u02c6A by\nsummation turns out to be unbiased in the sense of [PS18, De\ufb01nition 4.1], namely, for any \u03b8 \u2208\u0398\nwe get\n\u03c1\u03b8\u0010\nS +\nd\nX\ni=1\n\u02c6An\ni\n\u0011\n= \u2212\nd\nX\ni=1\nE\u03b8h\nZ\u03b8\nS, \u02c6\nAn(Xi + \u02c6An\ni )\ni\n= 0.\n(3.5)\nEquality (3.5) guarantees that the secured aggregated portfolio position S + Pd\ni=1 \u02c6Ai is accept-\nable in the sense that it bears no risk, while Equality (3.4) ensures that the average performance\nof the secured marginal positions under the worst-case scenario measure for the secured portfolio\nS are the same and that the joint position is secured. In particular, for d = 1, the de\ufb01nitions of\nfairness and unbiasedness coincide.\nIt should be noted that (3.5) means that a fair allocation estimator charges an adequate amount\nof capital to secure the portfolio. This is a consequence of (3.4), which means that a fair allocation\nestimator applies an adequate amount of capital charge to each position constituent.\nWe end this section with a simple example to illustrate the concept of fairness.\nExample 3.2. Consider the mean risk allocation given in Example 2.2. This leads to the family\nof risk measures \u03c1\u03b8(\u00b7) = \u2212E\u03b8[ \u00b7 ], \u03b8 \u2208\u0398. Then, the risk allocation estimator\n\u02c6\nMn\ni = \u22121\nn\nn\nX\nj=1\nXj\ni ,\nfor i = 1, 2, . . . , d ,\nis a fair allocation estimator. Indeed, note that here, for each \u03b8 \u2208\u0398, the extremal measure coincides\nwith the original probability measure P\u03b8, i.e. Z\u03b8\nS, \u02c6\nMn \u22611. Thus, for i \u2208{1, 2, . . . d} we obtain\nE\u03b8h\nZ\u03b8\nS, \u02c6\nMn(Xi + \u02c6\nMn\ni )\ni\n= E\u03b8h\nXi \u22121\nn\nn\nX\nj=1\nXj\ni\ni\n= 0.\n3.1\nEstimating capital allocation under expected shortfall and normality\nFollowing Section 2.1, we study the case where the d-dimensional random vector X is normally\ndistributed under every P\u03b8, and we assume that the risk is measured by the expected shortfall ES\u03b8\n\u03b1,\nat a \ufb01xed level \u03b1 \u2208(0, 1). In what follows, for the random sample Xn, we will use the notation\n\n8\nBielecki, Cialenco, Pitera, and Schmidt\nSj := Pd\ni=1 Xj\ni , j = 1, . . . , n, and we set6\n\u02c6\u00b5i := 1\nn\nPn\nj=1 Xj\ni ,\n\u02c6\u00b5S := 1\nn\nPn\nj=1 Sj = Pd\ni=1 \u02c6\u00b5i\n\u02c6\u03c32\nS := 1\nn\nPn\nj=1(Sj \u2212\u02c6\u00b5S)2,\nd\nCovXi,S := 1\nn\nPn\nj=1(Xj\ni \u2212\u02c6\u00b5i)(Sj \u2212\u02c6\u00b5S),\nto denote the sample mean of the ith constituent, the sample mean of the portfolio, the sam-\nple variance of the portfolio, and the sample covariance of the ith constituent and the portfolio,\nrespectively.",
    "chunk_index": 8,
    "start_char": 19434,
    "end_char": 22060,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "and Schmidt\nSj := Pd\ni=1 Xj\ni , j = 1, . . . , n, and we set6\n\u02c6\u00b5i := 1\nn\nPn\nj=1 Xj\ni ,\n\u02c6\u00b5S := 1\nn\nPn\nj=1 Sj = Pd\ni=1 \u02c6\u00b5i\n\u02c6\u03c32\nS := 1\nn\nPn\nj=1(Sj \u2212\u02c6\u00b5S)2,\nd\nCovXi,S := 1\nn\nPn\nj=1(Xj\ni \u2212\u02c6\u00b5i)(Sj \u2212\u02c6\u00b5S),\nto denote the sample mean of the ith constituent, the sample mean of the portfolio, the sam-\nple variance of the portfolio, and the sample covariance of the ith constituent and the portfolio,\nrespectively.\nMotivated by the Representation (2.11) we de\ufb01ne the allocation estimator \u02c6B = ( \u02c6B1, . . . , \u02c6Bd) as\n\u02c6Bi := \u2212\u02c6\u03b1i + \u02c6\u03b2i \u02c6R(S),\ni = 1, . . . , d ,\n(3.6)\nwhere \u02c6\u03b2i =\n1\n\u02c6\u03c32\nS\nd\nCovXi,S and \u02c6\u03b1i = \u02c6\u00b5i \u2212\u02c6\u03b2i\u02c6\u00b5S are the estimators of the slope and intercept regression\ncoe\ufb03cient from the L2 orthogonal projection of the ith margin of X onto S, and where \u02c6R(S) is an\nunbiased risk estimator (in the sense of [PS18]) for the Expected Shortfall of the secured position\nS. It has been shown in [PS18, Example 5.4] that \u02c6R(S) under normality can be represented as\n\u02c6R(S) = \u2212\u02c6\u00b5S + \u02c6\u03c3Sbn,\n(3.7)\nwhere bn \u2208R is deterministic, and depends only on the sample size n, and risk level \u03b1 \u2208(0, 1).\nConsequently, the estimator becomes\n\u02c6Bi = \u2212\u02c6\u00b5i +\nd\nCovXi,S\n\u02c6\u03c3S\nbn,\ni = 1, . . . , d.\nBefore we show that \u02c6B satis\ufb01es the fairness property, we show an important conditional un-\nbiasedness property of the estimators \u02c6\u03b2i and \u02c6\u03b1i, in the usual statistical sense. Towards this end,\nfor i = 1, 2, . . . , d, we use\n\u03b2\u03b8\ni := Cov\u03b8(Xi, S) \u00b7 (Var\u03b8(S))\u22121,\n\u03b1\u03b8\ni := E\u03b8(Xi) \u2212\u03b2\u03b8\ni\nd\nX\nk=1\nE\u03b8(Xk),\nto denote the true regression coe\ufb03cients of the L2\u2013orthogonal projection of ith margin of X onto\nS under P\u03b8, for \u03b8 \u2208\u0398; see Section 2.1. Note that, in view of our assumption that for any \u03b8 \u2208\u0398\nthe random sample X1, . . . , Xn is i.i.d. under P\u03b8, we get \u03b2\u03b8\ni = Cov\u03b8(Xj\ni , Sj) \u00b7 (Var\u03b8(Sj))\u22121 and\n\u03b1\u03b8\ni = E\u03b8(Xj\ni ) \u2212\u03b2\u03b8\ni\nPd\nk=1 E\u03b8(Xj\nk), for j = 1, . . . , n.\nProposition 3.3. For any \u03b8 \u2208\u0398 it holds that\nE\u03b8h\n\u02c6\u03b2i | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= \u03b2\u03b8\ni\nand\nE\u03b8h\n\u02c6\u03b1i | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= \u03b1\u03b8\ni ,\ni = 1, . . . , d.\n(3.8)\nProof. Recall from Section 2.1 that under normality, for j \u2208{1, . . . , n}, i \u2208{1, . . . , d}, and \u03b8 \u2208\u0398,\nwe have\nXj\ni = \u03b1\u03b8\ni + \u03b2\u03b8\ni Sj + \u03f5j,\u03b8\ni ,\n(3.9)\n6To ease the notation, we will drop the superscript n in the following. So, we will write \u02c6\u00b5i rather than \u02c6\u00b5n\ni , etc.",
    "chunk_index": 9,
    "start_char": 21658,
    "end_char": 23871,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "= 1, . . . , d.\n(3.8)\nProof. Recall from Section 2.1 that under normality, for j \u2208{1, . . . , n}, i \u2208{1, . . . , d}, and \u03b8 \u2208\u0398,\nwe have\nXj\ni = \u03b1\u03b8\ni + \u03b2\u03b8\ni Sj + \u03f5j,\u03b8\ni ,\n(3.9)\n6To ease the notation, we will drop the superscript n in the following. So, we will write \u02c6\u00b5i rather than \u02c6\u00b5n\ni , etc.\n\nFair capital risk allocation\n9\nwhere \u03f5j,\u03b8\ni\nis a zero mean Gaussian random variable independent of Sj. As a simple consequence of\n(3.9) we obtain that \u03f5j,\u03b8\ni\nis independent of \u00b5S and \u03c3S under P\u03b8 for all \u03b8 \u2208\u0398. Then, by de\ufb01nition,\nI1 := E\u03b8\n\u0014\n\u02c6\u03b2i | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014 1\nn\nn\nX\nj=1\n(Xj\ni \u2212\u02c6\u00b5i)(Sj \u2212\u02c6\u00b5S) | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n(3.10)\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014 1\nn\nn\nX\nj=1\nXj\ni Sj \u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n.\nInserting (3.9), and using that n\u22121 Pn\nj=1(Sj)2 = \u02c6\u03c32\nS + \u02c6\u00b52\nS, we obtain\nI1 =\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014 1\nn\nn\nX\nj=1\n(\u03b1\u03b8\ni + \u03b2\u03b8\ni Sj + \u03f5j,\u03b8\ni )Sj \u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni \u02c6\u00b5S + \u03b2\u03b8\ni (\u02c6\u03c32\nS + \u02c6\u00b52\nS) + 1\nn\nn\nX\nj=1\n\u03f5j,\u03b8\ni Sj \u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni \u02c6\u00b5S + \u03b2\u03b8\ni (\u02c6\u03c32\nS + \u02c6\u00b52\nS) + 1\nn\nn\nX\nj=1\nE\u03b8\u0002\n\u03f5j,\u03b8\ni Sj | Sj, \u02c6\u00b5S, \u02c6\u03c3S\n\u0003\n\u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni \u02c6\u00b5S + \u03b2\u03b8\ni (\u02c6\u03c32\nS + \u02c6\u00b52\nS) + 1\nn\nn\nX\nj=1\nSjE\u03b8\u0002\n\u03f5j,\u03b8\ni\n| Sj, \u02c6\u00b5S, \u02c6\u03c3S\n\u0003\n\u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni \u02c6\u00b5S + \u03b2\u03b8\ni (\u02c6\u03c32\nS + \u02c6\u00b52\nS) + 1\nn\nn\nX\nj=1\nSjE\u03b8\u0002\n\u03f5j,\u03b8\ni\n\u0003\n\u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n=\n1\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni \u02c6\u00b5S + \u03b2\u03b8\ni (\u02c6\u03c32\nS + \u02c6\u00b52\nS) \u2212\u02c6\u00b5i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n= \u03b2\u03b8\ni + \u02c6\u00b5S\n\u02c6\u03c32\nS E\u03b8\n\u0014\n\u03b1\u03b8\ni + \u03b2\u03b8\ni \u02c6\u00b5S \u2212\u02c6\u00b5i | \u02c6\u00b5S, \u02c6\u03c3S\n\u0015\n.\nWe use again (3.9) and obtain\n\u02c6\u00b5i = 1\nn\nn\nX\nj=1\nXj\ni = \u03b1\u03b8\ni + \u03b2\u03b8\ni\n1\nn\nn\nX\nj=1\nSj + \u03b7\u03b8,\n(3.11)\nwith \u03b7\u03b8 = 1\nn\nPn\nj=1 \u03f5j,\u03b8\ni\nsatisfying E\u03b8[\u03b7\u03b8 | \u02c6\u00b5S, \u02c6\u03c3S] = 0, so that\nE\u03b8\u0002\n\u02c6\u00b5i | \u02c6\u00b5S, \u02c6\u03c3S\n\u0003\n= \u03b1\u03b8\ni + \u03b2\u03b8\ni \u02c6\u00b5S,\n(3.12)\nand hence I1 = \u03b2\u03b8\ni yielding our \ufb01rst claim. With this result and using (3.11), we obtain\nE\u03b8 [\u02c6\u03b1i | \u02c6\u00b5S, \u02c6\u03c3S] = E\u03b8 h\n\u02c6\u00b5i \u2212\u02c6\u03b2i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= E\u03b8 h\n\u02c6\u00b5i \u2212\u03b2\u03b8\ni \u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= \u03b1\u03b8\ni\nwhich concludes the proof of (3.8).",
    "chunk_index": 10,
    "start_char": 23579,
    "end_char": 25383,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "\u03b7\u03b8 = 1\nn\nPn\nj=1 \u03f5j,\u03b8\ni\nsatisfying E\u03b8[\u03b7\u03b8 | \u02c6\u00b5S, \u02c6\u03c3S] = 0, so that\nE\u03b8\u0002\n\u02c6\u00b5i | \u02c6\u00b5S, \u02c6\u03c3S\n\u0003\n= \u03b1\u03b8\ni + \u03b2\u03b8\ni \u02c6\u00b5S,\n(3.12)\nand hence I1 = \u03b2\u03b8\ni yielding our \ufb01rst claim. With this result and using (3.11), we obtain\nE\u03b8 [\u02c6\u03b1i | \u02c6\u00b5S, \u02c6\u03c3S] = E\u03b8 h\n\u02c6\u00b5i \u2212\u02c6\u03b2i\u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= E\u03b8 h\n\u02c6\u00b5i \u2212\u03b2\u03b8\ni \u02c6\u00b5S | \u02c6\u00b5S, \u02c6\u03c3S\ni\n= \u03b1\u03b8\ni\nwhich concludes the proof of (3.8).\nProposition 3.3 shows that we can estimate the portfolio risk expressed through \u02c6\u00b5S and \u02c6\u03c3S\nwithout impacting the statistical unbiasedness property of the regression coe\ufb03cients; cf. Equation\n(3.7). Consequently, the risk allocation estimation procedure could be split into two independent\nsteps. First, we estimate the aggregated portfolio risk, and then we estimate the proper allocation\nof the risk within portfolio constituents. Now, we use this property to show that the allocation\nestimator given in (3.6) satis\ufb01es the fairness property.\n\n10\nBielecki, Cialenco, Pitera, and Schmidt\nTheorem 3.4. Assume that the allocation estimator \u02c6B = ( \u02c6B1, . . . , \u02c6Bd) is given by (3.6) with \u02c6R(S)\nas in (3.7). Then, the capital allocation \u02c6B is fair.\nProof. In what follows we will simply write \u02c6R instead of \u02c6R(S). We note that for any \u03b8 \u2208\u0398 the\nRadon-Nikodym density Z\u03b8\nS, \u02c6B is \u03c3(S+Pd\ni=1 \u02c6Bi)-measurable; see [Che06, Proposition 6.2] and recall\nthat \u02c6Bi = \u2212\u02c6\u03b1i + \u02c6\u03b2i \u02c6R. Moreover, since\nd\nX\ni=1\n\u02c6\u03b2i = 1\n\u02c6\u03c32\nS\nd\nX\ni=1\nd\nCovXi,S = \u02c6\u03c32\nS\n\u02c6\u03c32\nS\n= 1\nwe obtain that\nd\nX\ni=1\n\u02c6\u03b1i =\nd\nX\ni=1\n\u02c6\u00b5i \u2212\u02c6\u00b5S \u00b7\nd\nX\ni=1\n\u02c6\u03b2i = \u02c6\u00b5S \u2212\u02c6\u00b5S = 0.\nConsequently, as expected,\nd\nX\ni=1\n\u02c6Bi = \u02c6R\n(3.13)\nand Equation (3.7) yields that Z\u03b8\nS, \u02c6B is \u03c3(\u02c6\u00b5S, \u02c6\u03c3S, S)-measurable. With a view towards (3.4), we\ncompute\nE\u03b8h\nZ\u03b8\nS, \u02c6B \u02c6\u03b1i\ni\n= E\u03b8h\nZ\u03b8\nS, \u02c6BE\u03b8[\u02c6\u03b1i | \u02c6\u00b5S, \u02c6\u03c3S, S]\ni\n= E\u03b8h\nZ\u03b8\nS, \u02c6BE\u03b8[\u02c6\u03b1i | \u02c6\u00b5S, \u02c6\u03c3S]\ni\n= E\u03b8h\nZ\u03b8\nS, \u02c6B\u03b1\u03b8\ni\ni\n,\nby Proposition 3.3. Analogously,\nE\u03b8h\nZ\u03b8\nS, \u02c6B \u02c6\u03b2i\ni\n= E\u03b8h\nZ\u03b8\nS, \u02c6B\u03b2\u03b8\ni\ni\nand we obtain\nE\u03b8 h\nZ\u03b8\nS, \u02c6B\n\u0010\nXi + \u02c6Bi\n\u0011i\n= E\u03b8 h\nZ\u03b8\nS, \u02c6B\n\u0010\nXi \u2212\u02c6\u03b1i + \u02c6\u03b2i \u02c6R\n\u0011i\n= E\u03b8 h\nZ\u03b8\nS, \u02c6B\n\u0010\nXi \u2212\u03b1\u03b8\ni + \u03b2\u03b8\ni \u02c6R\n\u0011i\n.\n(3.14)\nNext, using (3.5) and (3.13) yields that\n0 = E\u03b8\n\u0014\nZ\u03b8\nS, \u02c6B\n\u0010\nS +\nd\nX\ni=1\n\u02c6Bi\n\u0011\u0015\n= E\u03b8\n\u0014\nZ\u03b8\nS, \u02c6B\n\u0010\nS + \u02c6R\n\u0011\u0015\n.\n(3.15)\nThis result, together with representation (3.9) for j = n + 1, and letting Xn+1 = X, imply that\n(3.14) = E\u03b8 h\nZ\u03b8\nS, \u02c6B\n\u0010\nXi \u2212\u03b1\u03b8\ni \u2212\u03b2\u03b8\ni S\n\u0011i\n= E\u03b8 h\nZ\u03b8\nS, \u02c6B\u03f5\u03b8\ni\ni\n= E\u03b8\u0002\nZ\u03b8\nS, \u02c6B\n\u0003\nE\u03b8\u0002\n\u03f5\u03b8\ni\n\u0003\n= 0,\n(3.16)\nwhere",
    "chunk_index": 11,
    "start_char": 25053,
    "end_char": 27303,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "Z\u03b8\nS, \u02c6B\n\u0010\nS +\nd\nX\ni=1\n\u02c6Bi\n\u0011\u0015\n= E\u03b8\n\u0014\nZ\u03b8\nS, \u02c6B\n\u0010\nS + \u02c6R\n\u0011\u0015\n.\n(3.15)\nThis result, together with representation (3.9) for j = n + 1, and letting Xn+1 = X, imply that\n(3.14) = E\u03b8 h\nZ\u03b8\nS, \u02c6B\n\u0010\nXi \u2212\u03b1\u03b8\ni \u2212\u03b2\u03b8\ni S\n\u0011i\n= E\u03b8 h\nZ\u03b8\nS, \u02c6B\u03f5\u03b8\ni\ni\n= E\u03b8\u0002\nZ\u03b8\nS, \u02c6B\n\u0003\nE\u03b8\u0002\n\u03f5\u03b8\ni\n\u0003\n= 0,\n(3.16)\nwhere we used the fact that (\u03f5\u03b8\ni , S) is bivariate normal with uncorrelated margins, so that \u03f5\u03b8\ni is\nindependent of S, and consequently from Z\u03b8\nS, \u02c6B. This concludes the proof.\n\nFair capital risk allocation\n11\n4\nAsymptotic fairness\nWe now introduce the de\ufb01nition of fairness for a sequence of estimators, ( \u02c6An)n\u2208N, and we de\ufb01ne\nthe notion of asymptotic fairness.\nDe\ufb01nition 4.1. A sequence of allocation estimators ( \u02c6An)n\u2208N will be called fair at n \u2208N, if \u02c6An is\nfair. If fairness holds for all n \u2208N, we call the sequence ( \u02c6An)n\u2208N fair. The sequence ( \u02c6An)n\u2208N is\ncalled asymptotically fair if\nE\u03b8h\nZ\u03b8\nS, \u02c6\nAn(Xi + \u02c6An\ni )\ni\nn\u2192\u221e\n\u2212\u2212\u2212\u2212\u21920,\ni = 1, 2, . . . , d, and \u03b8 \u2208\u0398.\n(4.1)\nIn view of Theorem 3.4 it is clear that the sequence of capital allocation estimators ( \u02c6Bn)n\u2208N\nde\ufb01ned in (3.6), for varying n, is a fair sequence.7\nIn the rest of the section we assume that the risk allocation is done using ES with reference\nlevel \u03b1.\n4.1\nAsymptotic fairness of capital allocation estimators under normality\nUsing (2.14), we now de\ufb01ne a sequence \u02c6Cn = ( \u02c6Cn\n1 , . . . , \u02c6Cn\nd ), n \u2208N, of \u201cplug-in type\u201d capital\nallocation estimators as\n\u02c6Cn\ni := \u2212\u02c6\u00b5i +\nd\nCovXi,S\n\u03b1\u02c6\u03c3S\n\u03c6(\u03a6\u22121(\u03b1)).\n(4.2)\nThe sequence ( \u02c6Cn)n\u2208N is not fair, in general, but it is asymptotically fair, as proven below.\nProposition 4.2. The sequence ( \u02c6Cn)n\u2208N is asymptotically fair.\nProof. Set \u02c6F n := \u2212\u02c6\u00b5S + \u02c6\u03c3S\n\u03c6(\u03a6\u22121(\u03b1))\n\u03b1\nand note that \u02c6Cn\ni = \u2212\u02c6\u03b1n\ni + \u02c6\u03b2n\ni \u02c6F n, i = 1, 2, . . . , d.\nProceeding analogously to the proof of Theorem 3.4, with \u02c6B replaced by \u02c6Cn and with \u02c6R replaced\nby \u02c6F n, we see that in order to prove proposition it is enough to show that for any \u03b8 \u2208\u0398 we have\nE\u03b8\n\u0014\nZ\u03b8\nS+ \u02c6F n\n\u0010\nS + \u02c6F n\u0011\u0015\nn\u2192\u221e\n\u2212\u2212\u2212\u2212\u21920.\n(4.3)\nNow, note that\nE\u03b8\n\u0014\nZ\u03b8\nS+ \u02c6F n\n\u0010\nS + \u02c6F n\u0011\u0015\n= \u03c1\u03b8(S + \u02c6F n),\nand, in the terminology of [PS18], \u02c6F n is the standard Gaussian expected shortfall plug-in estimator\nfor S. Consequently, noting that for d = 1 the de\ufb01nition of asymptotic fairness coincides with the\nde\ufb01nition of asymptotic unbiasedness given in [PS18, De\ufb01nition 6.1], and using [PS18, Proposition\n6.4] we conclude the proof.",
    "chunk_index": 12,
    "start_char": 27028,
    "end_char": 29368,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "that\nE\u03b8\n\u0014\nZ\u03b8\nS+ \u02c6F n\n\u0010\nS + \u02c6F n\u0011\u0015\n= \u03c1\u03b8(S + \u02c6F n),\nand, in the terminology of [PS18], \u02c6F n is the standard Gaussian expected shortfall plug-in estimator\nfor S. Consequently, noting that for d = 1 the de\ufb01nition of asymptotic fairness coincides with the\nde\ufb01nition of asymptotic unbiasedness given in [PS18, De\ufb01nition 6.1], and using [PS18, Proposition\n6.4] we conclude the proof.\n4.2\nAsymptotic fairness of non-parametric capital allocation estimators\nWe assume throughout this section that the population X, and hence the aggregated portfolio S,\nare continuous random variables under any \u03b8 \u2208\u0398. Given that the ES is used to determine the\n7Recall that the superscript n is omitted in (3.6) for the ease of notation.\n\n12\nBielecki, Cialenco, Pitera, and Schmidt\nrisk allocation, and taking (2.6) and (2.9) into account, we consider two natural non-parametric\nexpected shortfall capital allocation estimators\n\u02c7Dn\ni := \u2212\nPn\nk=1 Xk\ni 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\nn\u03b1\n,\ni = 1, . . . , d,\n(4.4)\n\u02c6Dn\ni := \u2212\nPn\nk=1 Xk\ni 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\nPn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n,\ni = 1, . . . , d,\n(4.5)\nwhere\n\u02c6\nV@Rn\n\u03b1 := \u2212S(\u230an\u03b1\u230b+1), with S(j) denoting the jth order statistics, and \u230az\u230bdenoting the largest\ninteger less or equal than z.\nProposition 4.3. The sequences ( \u02c6Dn)n\u2208N and ( \u02c7Dn)n\u2208N are asymptotically fair.\nWe will show only that \u02c6Dn\ni is asymptotically fair. The proof for \u02c7Dn\ni follows by similar arguments.\nBefore we prove Proposition 4.3, let us introduce supplementary notation and a lemma that will be\nuseful for the proof. For any \u03b8 \u2208\u0398 we use a\u03b8 = (a\u03b8\n1, . . . , a\u03b8\nn) to denote the true expected shortfall\nallocation for X under \u03b8 and so we have (cf. (2.6))\nZ\u03b8\nS,a\u03b8 = 1\n\u03b11\u001a\nS+Pd\ni=1 a\u03b8\ni \u2264q\u03b8\nS+Pd\ni=1 a\u03b8\ni\n(\u03b1)\n\u001b,\nwhere q\u03b8\nS+Pd\ni=1 a\u03b8\ni (\u03b1) denotes the true \u03b1-quantile of S + Pd\ni=1 a\u03b8\ni under P\u03b8. Similarly, we have\nZ\u03b8\nS, \u02c6D = 1\n\u03b11\u001a\nS+Pd\ni=1 \u02c6Dn\ni \u2264q\u03b8\nS+Pd\ni=1 \u02c6\nDn\ni\n(\u03b1)\n\u001b.\nLemma 4.4. For any \u03b8 \u2208\u0398 we get Z\u03b8\nS, \u02c6Dn\nP\u03b8\n\u2212\u2212\u2192Z\u03b8\nS,a\u03b8, as n \u2192\u221e.\nProof. Let us \ufb01x \u03b8 \u2208\u0398. For brevity we will use the notation r := Pd\ni=1 a\u03b8 and Rn := Pd\ni=1 \u02c6Dn\ni .\nFirst, using classical trimmed-mean convergence arguments (see e.g. [Sti97]) we will show that\nRn\nP\u03b8\n\u2212\u2212\u2192r,\nn \u2192\u221e.\n(4.6)\nLet In := Pn\nk=1 1{Sk\u2212q\u03b8\nS(\u03b1)\u22640} and an := \u230an\u03b1\u230b+ 1, n \u2208N. Since an = Pn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}, we get\nRn = \u2212\nPn\nk=1 Sk1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\nPn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n= \u22121\nan\nan\nX\nk=1\nS(k) = \u22121\nan\nIn\nX\nk=1\nS(k) + \u03f5n,\nwhere \u03f5n := \u22121\nan\n\u0010\n1{an>In}\nPan\nk=In+1 S(k) \u22121{an<In}\nPIn\nk=an+1 S(k)\u0011\n.\nNext, we will show that\n\u03f5n\nP\u03b8\n\u2212\u2212\u21920.",
    "chunk_index": 13,
    "start_char": 28992,
    "end_char": 31449,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "\u2212\u2212\u2192r,\nn \u2192\u221e.\n(4.6)\nLet In := Pn\nk=1 1{Sk\u2212q\u03b8\nS(\u03b1)\u22640} and an := \u230an\u03b1\u230b+ 1, n \u2208N. Since an = Pn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}, we get\nRn = \u2212\nPn\nk=1 Sk1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\nPn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n= \u22121\nan\nan\nX\nk=1\nS(k) = \u22121\nan\nIn\nX\nk=1\nS(k) + \u03f5n,\nwhere \u03f5n := \u22121\nan\n\u0010\n1{an>In}\nPan\nk=In+1 S(k) \u22121{an<In}\nPIn\nk=an+1 S(k)\u0011\n.\nNext, we will show that\n\u03f5n\nP\u03b8\n\u2212\u2212\u21920. Due to the consistency of the empirical quantiles, we have that S(an)\nP\u03b8\n\u2212\u2192q\u03b8\nS(\u03b1) and\nS(In) P\u03b8\n\u2212\u2192q\u03b8\nS(\u03b1), as n \u2192\u221e. Hence, noting that\n0 \u2264|\u03f5n| \u2264\n\f\f\f\f\nIn \u2212an\nan\n\f\f\f\f max\nn\f\f\fS(an)\f\f\f ,\n\f\f\fS(In)\f\f\f\no\n,\nit is su\ufb03cient to prove that\n\f\f\f In\u2212an\nan\n\f\f\f\nP\u03b8\n\u2212\u2212\u21920. For this, we observe that\nIn \u2212an\nan\n= n\nan\n\u0012 1\nnIn \u2212\u03b1\n\u0013\n+ n\u03b1 \u2212an\nan\n.\n\nFair capital risk allocation\n13\nSince limn\u2192\u221en\u03b1\u2212an\nan\n= 0, limn\u2192\u221en\nan = 1\n\u03b1, and, by the Law of Large Numbers,\n\u0000 1\nnIn \u2212\u03b1\n\u0001\nP\u03b8\n\u2212\u2212\u21920,\nwe have that \u03f5n\nP\u03b8\n\u2212\u2212\u21920. Also, by the Law of Large Numbers we get at once that\n1\nan\nIn\nX\nk=1\nS(k) = n\nan\n \n1\nn\nn\nX\nk=1\nSk1{Sk\u2264q\u03b8\nS(\u03b1)}\n!\nP\u03b8\n\u2212\u2212\u2212\u21921\n\u03b1E\nh\nS1{S\u2264q\u03b8\nS(\u03b1)}\ni\n= \u2212r,\nwhich concludes the proof of (4.6).\nNext, for a \ufb01xed \u03f5 \u2208( 1\n\u03b1, 0), we get\nP\u03b8 h\n|ZS, \u02c6Dn \u2212Z\u03b8\nS,a\u03b8| > \u03f5\ni\n= P\u03b8 h\n|ZS, \u02c6Dn \u2212Z\u03b8\nS,a\u03b8| \u0338= 0\ni\n= P\u03b8 h\n{S + Rn \u2264q\u03b8\nS+Rn(\u03b1)} \u2229{S + r > q\u03b8\nS+r(\u03b1)}\ni\n(4.7)\n+ P\u03b8 h\n{S + Rn > q\u03b8\nS+Rn(\u03b1)} \u2229{S + r \u2264q\u03b8\nS+r(\u03b1)}\ni\n.\n(4.8)\nWe want to show that (4.7) and (4.8) go to zero as n \u2192\u221e. For brevity, we show the proof only\nfor (4.7); the proof for (4.8) is analogous. For any \u03f52 > 0 we get\n(4.7) = P\u03b8 h\n{q\u03b8\nS+r(\u03b1) < S + r \u2264q\u03b8\nS+r\u2212(r\u2212Rn)(\u03b1) + (r \u2212Rn)}\ni\n\u2264P\u03b8 h\n{q\u03b8\nS+r(\u03b1) < S + r \u2264q\u03b8\nS+r\u2212(r\u2212Rn)(\u03b1) + |r \u2212Rn|}\ni\n\u2264P\u03b8 [{|r \u2212Rn| \u2265\u03f52}] + P\u03b8 h\n{q\u03b8\nS+r(\u03b1) < S + r \u2264q\u03b8\nS+r\u2212(r\u2212Rn)(\u03b1) + \u03f52}\ni\n.\n(4.9)\nUsing (4.6), and recalling that convergence in probability implies convergence in distribution which\nin turn implies convergence of quantiles (at continuity points) for n \u2192\u221ewe get\nP\u03b8 [{|r \u2212Rn| \u2265\u03f52}] \u21920\nand\nq\u03b8\nS+r\u2212(r\u2212Rn)(\u03b1) \u2192q\u03b8\nS+r(\u03b1).\n(4.10)\nCombining (4.9) with (4.10), noting that the choice of \u03f52 was arbitrary, and that S is continuous,\nwe conclude the proof.\nNow, we are ready to prove Proposition 4.3.\nProof of Proposition 4.3. Let us \ufb01x \u03b8 \u2208\u0398 and i \u2208{1, . . . , d}. We want to show that\nE\u03b8h\nZ\u03b8\nS, \u02c6Dn(Xi + \u02c6Dn\ni )\ni\n\u21920,\nn \u2192\u221e.",
    "chunk_index": 14,
    "start_char": 31111,
    "end_char": 33224,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "with (4.10), noting that the choice of \u03f52 was arbitrary, and that S is continuous,\nwe conclude the proof.\nNow, we are ready to prove Proposition 4.3.\nProof of Proposition 4.3. Let us \ufb01x \u03b8 \u2208\u0398 and i \u2208{1, . . . , d}. We want to show that\nE\u03b8h\nZ\u03b8\nS, \u02c6Dn(Xi + \u02c6Dn\ni )\ni\n\u21920,\nn \u2192\u221e.\nNoting that\nE\u03b8h\nZ\u03b8\nS, \u02c6Dn(Xi + \u02c6Dn\ni )\ni\n= E\u03b8h\nZ\u03b8\nn(Xi + \u02c6Dn\ni )\ni\n+ E\u03b8h\nZ\u03b8\nS,a\u03b8(Xi + \u02c6Dn\ni )\ni\n,\nwhere Z\u03b8\nn := Z\u03b8\nS, \u02c6Dn \u2212Z\u03b8\nS,a\u03b8, we need to prove that\nE\u03b8h\nZ\u03b8\nn(Xi + \u02c6Dn\ni )\ni\n\u21920,\nn \u2192\u221e,\n(4.11)\nand\nE\u03b8h\nZ\u03b8\nS,a\u03b8(Xi + \u02c6Dn\ni )\ni\n\u21920,\nn \u2192\u221e.\n(4.12)\n\n14\nBielecki, Cialenco, Pitera, and Schmidt\nWe start with the proof of (4.11). Noting that for any n \u2208N we have |Z\u03b8\nn| \u22641\n\u03b1 and\nn\nX\nk=1\n1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640} = \u230an\u03b1\u230b+ 1,\nwe get\n\f\f\fE\u03b8h\nZ\u03b8\nn(Xi + \u02c6Dn\ni )\ni\f\f\f \u2264E\u03b8h\n|Z\u03b8\nn|(|Xi| + | \u02c6Dn\ni |)\ni\n\u22641\n\u03b1\n\u0010\nE\u03b8h\n1{|Z\u03b8n|\u0338=0}|Xi|\ni\n+ E\u03b8h\n1{|Z\u03b8n|\u0338=0}| \u02c6Dn\ni |\ni\u0011\n\u22641\n\u03b1\n \nE\u03b8h\n1{|Z\u03b8n|\u0338=0}|Xi|\ni\n+\n1\n\u230an\u03b1\u230b+ 1E\u03b8h\n1{|Z\u03b8n|\u0338=0}\nn\nX\nk=1\n|Xk\ni |1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\ni!\n\u22641\n\u03b1\n \nE\u03b8h\n1{|Z\u03b8n|\u0338=0}|Xi|\ni\n+\n1\n\u230an\u03b1\u230b+ 1E\u03b8h\n1{|Z\u03b8n|\u0338=0}\nn\nX\nk=1\n|Xk\ni |\ni!\n\u22641\n\u03b1\n\u0012\nE\u03b8h\n1{|Z\u03b8n|\u0338=0}|Xi|\ni\n+\nn\n\u230an\u03b1\u230b+ 1E\u03b8h\n1{|Z\u03b8n|\u0338=0}|X1\ni |\ni\u0013\n.\n(4.13)\nNow, noting that 1{|Z\u03b8n|\u0338=0} = 1{|Z\u03b8n|< 1\n2\u03b1} and using Lemma 4.4 we get\nP\u03b8 h\n|Z\u03b8\nn| \u0338= 0\ni\n\u21920,\nn \u2192\u221e.\nCombining this with (4.13), noting that |Xi| and |X1\ni | are integrable, and\nn\n\u230an\u03b1\u230b+1 \u21921\n\u03b1 as n \u2192\u221e,\nwe conclude the proof of (4.11).\nNext, we prove (4.12). Recalling that a\u03b8 is a true allocation for X under \u03b8 we get\nE\u03b8h\nZ\u03b8\nS,a\u03b8(Xi + \u02c6Dn\ni )\ni\n= E\u03b8h\nZ\u03b8\nS,a\u03b8(Xi + a\u03b8\ni )\ni\n+ E\u03b8h\nZ\u03b8\nS,a\u03b8( \u02c6Dn\ni \u2212a\u03b8\ni )\ni\n= E\u03b8h\nZ\u03b8\nS,a\u03b8( \u02c6Dn\ni \u2212a\u03b8\ni )\ni\n.\nConsequently, noting that Z\u03b8\nS,a\u03b8 and \u02c6Dn\ni are independent under P\u03b8 we get\nE\u03b8h\nZ\u03b8\nS,a\u03b8(Xi + \u02c6Dn\ni )\ni\n= E\u03b8h\nZ\u03b8\nS,a\u03b8\ni\nE\u03b8h\n\u02c6Dn\ni \u2212a\u03b8\ni\ni\n= \u2212E\u03b8\n\"Pn\nk=1 Xk\ni 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\nPn\nk=1 1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n+ a\u03b8\ni\n#\n= \u2212\n1\n\u230an\u03b1\u230b+ 1E\u03b8\n\" n\nX\nk=1\n(Xk\ni + a\u03b8\ni )1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n#\n= \u2212\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n(X1\ni + a\u03b8\ni )1{S1+\n\u02c6\nV@Rn\n\u03b1\u22640}\ni\n= \u2212\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n(X1\ni + a\u03b8\ni )\n\u0010\n1{S1+\n\u02c6\nV@Rn\n\u03b1\u22640} \u22121{S1\u2264q\u03b8\nS(\u03b1)}\n\u0011i\n.\n\u2264\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n|X1\ni + a\u03b8\ni |1An\ni\n,\n(4.14)\nwhere An := {q\u03b8\nS(\u03b1) < S1 \u2264\u2212\n\u02c6\nV@Rn\n\u03b1} \u222a{\u2212\n\u02c6\nV@Rn\n\u03b1 < S1 \u2264q\u03b8\nS(\u03b1)}, and where in the last equality\nwe used the property E\u03b8 h\n(X1\ni + a\u03b8\ni )1{S1\u2264q\u03b8\nS(\u03b1)}\ni\n= 0.",
    "chunk_index": 15,
    "start_char": 32951,
    "end_char": 35037,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "i + a\u03b8\ni )1{Sk+\n\u02c6\nV@Rn\n\u03b1\u22640}\n#\n= \u2212\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n(X1\ni + a\u03b8\ni )1{S1+\n\u02c6\nV@Rn\n\u03b1\u22640}\ni\n= \u2212\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n(X1\ni + a\u03b8\ni )\n\u0010\n1{S1+\n\u02c6\nV@Rn\n\u03b1\u22640} \u22121{S1\u2264q\u03b8\nS(\u03b1)}\n\u0011i\n.\n\u2264\nn\n\u230an\u03b1\u230b+ 1E\u03b8 h\n|X1\ni + a\u03b8\ni |1An\ni\n,\n(4.14)\nwhere An := {q\u03b8\nS(\u03b1) < S1 \u2264\u2212\n\u02c6\nV@Rn\n\u03b1} \u222a{\u2212\n\u02c6\nV@Rn\n\u03b1 < S1 \u2264q\u03b8\nS(\u03b1)}, and where in the last equality\nwe used the property E\u03b8 h\n(X1\ni + a\u03b8\ni )1{S1\u2264q\u03b8\nS(\u03b1)}\ni\n= 0. By similar reasoning as in (4.9), we get\nP\u03b8[An] \u2264P\u03b8[|q\u03b8\nS(\u03b1) +\n\u02c6\nV@Rn\n\u03b1| > \u03f5] + P\u03b8[|S1 +\n\u02c6\nV@Rn\n\u03b1| < \u03f5],\n\nFair capital risk allocation\n15\nfor any \u03f5 > 0. Since \u2212\n\u02c6\nV@Rn\n\u03b1 is a consistent estimator of q\u03b8\nS(\u03b1), we conclude that P\u03b8[|q\u03b8\nS(\u03b1) +\n\u02c6\nV@Rn\n\u03b1| > \u03f5] \u21920, as n \u2192\u221e. Consequently, as the choice of \u03f5 was arbitrary and S1 is continuous,\nwe obtain that P\u03b8[An] \u21920, as n \u2192\u221e. Combining this with (4.14), and since |X1\ni +a\u03b8\ni | is integrable,\nand\nn\n\u230an\u03b1\u230b+1 \u21921\n\u03b1 as n \u2192\u221e, the proof of (4.12) is complete.\n5\nBacktesting and numerical examples\nIn this section we analyze the proposed fair capital allocation methodology via examples using\nsimulated data and real market data. It goes without saying that any quantitative methodology\nused for measuring and allocating risk relies on an adopted formal model. It also goes without\nsaying that actual results of risk measurement and/or risk allocation need to be tested for their\nadequacy. Often, testing adequacy of the results of risk measurement is done in practice using\nbacktesting, and we will use this approach in testing the estimation procedures of fair capital\nallocation introduced in the previous sections.\nBacktesting, applied for risk measurement in the \ufb01nancial context, can be summarized as fol-\nlows: given a time series of capital forecasts, one compares these forecasts with the realized losses;\nthe accumulated performance is the key ingredient of the backtesting. Backtesting might be also\ntreated as a speci\ufb01c case of assessment of quality of a point forecast, which aims at assessing\nwhether the forecasted capital is su\ufb03cient; see [Zie16, SKG15, NZ17]. In particular, backtesting\nvalue-at-risk goes back to [Kup95] and recently has gained a lot of practical and theoretical interest;\nsee [AS14, PS18] for further details on this topic and the related literature. Undoubtedly, simi-\nlar backtesting procedure should be developed for testing the adequacy of risk capital allocation\nmethodologies.\nWe focus our attention on assessing the performance of a statistical capital allocation method-\nology when the underlying reference risk measure is expected shortfall at the \ufb01xed level \u03b1 \u2208(0, 1],\nused in computing of the values of our estimators. For this purpose we propose two backtesting\nframeworks:\n\u2022 absolute deviation from fairness backtesting;\n\u2022 risk level shifts adjustments backtesting.\nThe backtesting framework adopted for assessment of adequacy of estimators of capital allocations,\nsay eA = ( eA1, .",
    "chunk_index": 16,
    "start_char": 34678,
    "end_char": 37506,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "be developed for testing the adequacy of risk capital allocation\nmethodologies.\nWe focus our attention on assessing the performance of a statistical capital allocation method-\nology when the underlying reference risk measure is expected shortfall at the \ufb01xed level \u03b1 \u2208(0, 1],\nused in computing of the values of our estimators. For this purpose we propose two backtesting\nframeworks:\n\u2022 absolute deviation from fairness backtesting;\n\u2022 risk level shifts adjustments backtesting.\nThe backtesting framework adopted for assessment of adequacy of estimators of capital allocations,\nsay eA = ( eA1, . . . , eAd), that were created using some capital allocation methodology,8 uses as its\ninput the observations of past P&Ls.\nThe key ingredient to both backtesting methods is the\nestimation of\nd\nX\ni=1\nE\u03b80h\nZ\u03b80\nS, \u02c6\nA(Xi + eAi)\ni\n,\n(5.1)\nand the estimation of\nE\u03b80h\nZ\u03b80\nS, \u02c6\nA(Xi + eAi)\ni\n, i = 1, 2, . . . , d.\n(5.2)\nWe assume that the length of the backtesting window is m days. With each day k = 1, . . . , m,\nwe associate the P&Ls Xk\ni and allocation estimators eAk\ni , i = 1, . . . , d. The estimators eAk\ni can be\nobtained in various ways. One way is to proceed in accordance to what was proposed previously in\nthis paper. Speci\ufb01cally, to produce allocation estimators eAk\ni on day k one uses market observations\nfrom the previous n days. We denote these observations as Xk,n = (Xk\u2212n\ni\n, . . . , Xk\ni , i = 1, . . . , d).\n8We refer to such methodology as to an Internal Capital Allocation Model (ICAM).\n\n16\nBielecki, Cialenco, Pitera, and Schmidt\nBased on these observations, and following (3.3), we compute the estimators of the allocations as\neAk = \u03b7n(Xk,n).\nThe realizations of Xk\ni and eAk\ni are denoted as xk\ni and eak\ni , respectively. We set y := (y1, . . . , ym),\nwhere yk = (yk\n1, . . . , yk\nd), k = 1, . . . , m, and yk\ni := xk\ni + eak\ni ,\ni = 1, 2, . . . , d. We also let \u03bek :=\nPd\ni=1 yk\ni ,\nk = 1, 2, . . . , m, to denote the realized aggregated secured position on day k, and we set\n\u03be := (\u03be1, . . . , \u03bem).\nIn order to proceed we introduce the following functions of \u03b2 \u2208(0, 1],\nG\u03b2( \u02dcA) := \u2212\nPm\nk=1 \u03bek1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\nPm\nk=1 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\n,\n(5.3)\nand\nGi\n\u03b2( \u02dcA) := \u2212\nPm\nk=1 yk\ni 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\nPm\nk=1 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\n,\ni = 1 . . . , d,\n(5.4)\nwith\n\u02c6\nV@R\u03b2 being the empirical value-at-risk at level \u03b2 \u2208(0, 1]. Note that yi\nks are computed using\nas the reference risk measure ES at the \ufb01xed risk level \u03b1.",
    "chunk_index": 17,
    "start_char": 36914,
    "end_char": 39334,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "following functions of \u03b2 \u2208(0, 1],\nG\u03b2( \u02dcA) := \u2212\nPm\nk=1 \u03bek1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\nPm\nk=1 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\n,\n(5.3)\nand\nGi\n\u03b2( \u02dcA) := \u2212\nPm\nk=1 yk\ni 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\nPm\nk=1 1{\u03bek+\n\u02c6\nV@R\u03b2(\u03be)\u22640}\n,\ni = 1 . . . , d,\n(5.4)\nwith\n\u02c6\nV@R\u03b2 being the empirical value-at-risk at level \u03b2 \u2208(0, 1]. Note that yi\nks are computed using\nas the reference risk measure ES at the \ufb01xed risk level \u03b1. If no confusions arise, we will write G\u03b2,\nrespectively Gi\n\u03b2, instead of G\u03b2( \u02dcA), respectively Gi\n\u03b2( \u02dcA).\nNow, similarly to the derivation of \u02c6Dn\ni , we estimate the expectation in (5.1) as \u2212G\u03b1, and we\nestimate (5.2) as \u2212Gi\n\u03b1.\nDeviation from fairness backtesting. If the capital allocation methodology is fair, then the\nobtained empirical values Gi\n\u03b1, i = 1, . . . , d, should be close to zero, for the \ufb01xed reference level\n\u03b1; the bigger the obtained estimate, the bigger the potential (true) deviation from fairness for the\nith margin. The deviation from fairness backtest assesses proximity to zero of Gi\n\u03b1, i = 1, . . . , d.\nA comprehensive study of properties of Gi\n\u03b1s, such as \u2018how far from zero is an acceptable value\u2019 is\nbeyond the scope of this manuscript. Nevertheless, the following backtesting methodology is one\nway to address this question.\nRisk level shift backtesting. Instead of measuring the deviation from fairness directly, it is\nnatural to \ufb01nd the reference risk level \u03b2 \u2208(0, 1] that makes Gi\n\u03b2 closest to zero; equivalently, we\nwant to answer the question by how much one needs to shift the reference risk level \u03b1 to make the\nposition acceptable. This approach hinges on duality-based performance measurement introduced\nin [PM18]. It should be noted that this approach is di\ufb00erent from the elicitability-based backtests\nas it focuses on capital conservativeness assessment rather than the general forecast \ufb01t; cf. [NZ17].\nFormally, for the estimators of capital allocation \u02dcA, we de\ufb01ne\n\u03a5( \u02dcA) := inf{\u03b2 \u2208(0, 1] : G\u03b2( \u02dcA) \u22640},\n(5.5)\nW i\n\u2212( \u02dcA) := inf{\u03f5 \u2208[0, \u03b1] : Gi\n\u03b1( \u02dcA) \u00b7 Gi\n\u03b1\u2212\u03f5( \u02dcA) \u22640},\n(5.6)\nW i\n+( \u02dcA) := inf{\u03f5 \u2208[0, 1 \u2212\u03b1] : Gi\n\u03b1( \u02dcA) \u00b7 Gi\n\u03b1+\u03f5( \u02dcA) \u22640},\n(5.7)\nwhere in (5.6) we use the convention inf \u2205= \u03b1, and correspondingly, in (5.7) we put inf \u2205= 1 \u2212\u03b1.\nSimilar to Gi\n\u03b2, G\u03b2, we may simple write \u03a5, and W i\n\u00b1.\nNote that G\u03b2 is a monotone decreasing\nfunction in \u03b2, while Gi\n\u03b2 generally speaking is not monotone. Hence, the quantities W \u00b1 are de\ufb01ned\nas the smallest shift in the reference risk level from \u03b1, to the right or to the left, that makes the\nith secured position acceptable. Thus, the closer \u03a5 is to the initial reference risk level \u03b1 the better\nis the total risk estimation procedure. Similarly, the closer W \u00b1 are to zero, the better is the risk\nallocation procedure.",
    "chunk_index": 18,
    "start_char": 38965,
    "end_char": 41637,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "Note that G\u03b2 is a monotone decreasing\nfunction in \u03b2, while Gi\n\u03b2 generally speaking is not monotone. Hence, the quantities W \u00b1 are de\ufb01ned\nas the smallest shift in the reference risk level from \u03b1, to the right or to the left, that makes the\nith secured position acceptable. Thus, the closer \u03a5 is to the initial reference risk level \u03b1 the better\nis the total risk estimation procedure. Similarly, the closer W \u00b1 are to zero, the better is the risk\nallocation procedure. One can look at W as the performance index that is dual to the ES family;\nsee [PM18, Proposition 4.3] for more details.\n\nFair capital risk allocation\n17\nFinally, by combining the left and right minimal shifts, we de\ufb01ne the the minimal shift estimator\nas\nW i( \u02dcA) :=\n(\n\u2212W i\n\u2212,\nif W i\n\u2212< W i\n+\nW i\n+,\nif W i\n\u2212\u2265W i\n+\n,\ni = 1, 2, . . . , d.\n(5.8)\nBefore moving to numerical examples, several comments on backtesting procedure are in order.\n(a) It goes without saying that the results produced by the deviation from fairness and the risk\nlevel shift approaches should be compared with each other for consistency and reality check.\n(b) It is worth mentioning that the two proposed backtesting methodologies can be applied to any\nICAM, not necessarily those discussed in this paper.\n(c) Our study of the backtesting procedure of the estimation of the risk capital allocation is pre-\nliminary. A thorough investigation of the statistical properties of Gi\n\u03b1 and W i is deferred to\nfuture studies.\nNext we will illustrate the performance of the capital allocation estimators \u02c6Bn, \u02c6Cn, and \u02c6Dn on\nsimulated data by applying the two backtesting procedures described above. For brevity and to\nease the notation, we will write \u02c6Bn, \u02c6Cn, and \u02c6Dn as \u02c6B, \u02c6C, and \u02c6D, respectively.\nFor simulations, we consider two cases of probability distributions of the P&Ls vector X - the\nGaussian distribution and the Student\u2019s t-distribution. We also \ufb01x the reference level \u03b1 = 0.05.\nAll numerical evaluations are performed using R statistical software; the source codes are available\nfrom the authors upon request.\nExample 5.1 (Gaussian P&Ls). We assume that the portfolio X of eight (discounted) P&Ls\nfollows an eight dimensional Gaussian distribution N(\u00b5, \u03a3), with the (true) mean\n\u00b5 = (0.000786, 0.001549, 0.001660, 0.000195, 0.000650, 0.000413, \u22120.000401, \u22120.001146),\nand the (true) variance-covariance matrix\n\u03a3 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0.000226\n0.000174\n0.000104\n0.000066\n0.000069\n0.000019\n-0.000077\n-0.000135\n0.000174\n0.000346\n0.000135\n0.000068\n0.000091\n0.000022\n-0.000082\n-0.000195\n0.000104\n0.000135\n0.000257\n0.000065\n0.000084\n0.000034\n-0.000093\n-0.000111\n0.000066\n0.000068\n0.000065\n0.000133\n0.000048\n0.000025\n-0.000058\n-0.000064\n0.000069\n0.000091\n0.000084\n0.000048\n0.000137\n0.000034\n-0.000065\n-0.000081\n0.000019\n0.000022\n0.000034\n0.000025\n0.000034\n0.000061\n-0.000022\n-0.000031\n-0.000077\n-0.000082\n-0.000093\n-0.000058\n-0.000065\n-0.000022\n0.000149\n0.000085\n-0.000135\n-0.000195\n-0.000111\n-0.000064\n-0.000081\n-0.000031\n0.000085\n0.000202\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nFor the purpose of obtaining the above mean vector and the variance-covariance matrix we used\nvalues of daily returns of eight stocks from S&P 500 index, namely: AAPL, AMZN, BA, DIS, HD,\nKO, JPM, and MSFT;",
    "chunk_index": 19,
    "start_char": 41171,
    "end_char": 44363,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "0.000135\n0.000068\n0.000091\n0.000022\n-0.000082\n-0.000195\n0.000104\n0.000135\n0.000257\n0.000065\n0.000084\n0.000034\n-0.000093\n-0.000111\n0.000066\n0.000068\n0.000065\n0.000133\n0.000048\n0.000025\n-0.000058\n-0.000064\n0.000069\n0.000091\n0.000084\n0.000048\n0.000137\n0.000034\n-0.000065\n-0.000081\n0.000019\n0.000022\n0.000034\n0.000025\n0.000034\n0.000061\n-0.000022\n-0.000031\n-0.000077\n-0.000082\n-0.000093\n-0.000058\n-0.000065\n-0.000022\n0.000149\n0.000085\n-0.000135\n-0.000195\n-0.000111\n-0.000064\n-0.000081\n-0.000031\n0.000085\n0.000202\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nFor the purpose of obtaining the above mean vector and the variance-covariance matrix we used\nvalues of daily returns of eight stocks from S&P 500 index, namely: AAPL, AMZN, BA, DIS, HD,\nKO, JPM, and MSFT; these data were taken for the period from January 2015 till December 2018.\nWe will use this sample again in Example 5.4. The \ufb01rst six stocks represent long positions in our\nportfolio and the last two represent short positions; this gives the negative entries in \u00b5 and \u03a3. The\npositions in each stock are equally weighted with nominal (absolute) value $1.\nWe took the learning period of n = 500 days, and the backtesting period of m = 5,000 days.\nBelow, we present the results for the Gaussian plug-in estimator \u02c6C and the non-parametric estimator\n\u02c6D; we omit results for estimators \u02c6B and \u02c7D, since, due to large size of the learning period, the results\nare almost identical to \u02c6C and \u02c6D, respectively. Additionally, for comparison, we present results for\nthe true allocations a; these allocations were obtained by plugging-in true mean and covariance\nmatrix into (2.14).\n\n18\nBielecki, Cialenco, Pitera, and Schmidt\nRisk allocations using a\n-0.05\n0.05\n0.15\nRisk allocations using \u02c6C\n-0.05\n0.05\n0.15\nRisk allocations using \u02c6D\n-0.05\n0.05\n0.15\n0\n1000\n2000\n3000\n4000\n5000\n0.00 0.05 0.10 0.15\nAggregated risk using a\nk\n0\n1000\n2000\n3000\n4000\n5000\n0.00 0.05 0.10 0.15\nAggregated risk using \u02c6C\nk\n0\n1000\n2000\n3000\n4000\n5000\n0.00 0.05 0.10 0.15\nAggregated risk using \u02c6D\nk\nFigure 1: Example 5.1. Top row: estimated risk allocations for the eight portfolio constituents\n(indexed by color) at each backtesting day, k = 1, . . . , m for the true allocation a, and the estimated\nrisk allocations \u02c6C and \u02c6D; the height of each horizontal layer represents the risk allocated to one of\nthe constituents. Bottom row: estimated aggregated risk at each backtesting day. The estimated\nrisk allocations are close to the reference allocations. The Gaussian plug-in estimator \u02c6C slightly\noutperforms the non-parametric method \u02c6D.\nThe obtained results validate, as expected, the proposed methods. In Figure 1 we present the\nvalues of the risk allocation to each constituent (top row), and the aggregated risk (bottom row).\nIn this example, the fair risk allocation a computed with the true underlying distribution can be\nconsidered as reference for the backtesting results. The estimated risk allocations using \u02c6C and \u02c6D are\nclose to the reference allocations, and as expected, the results computed using the non-parametric\nmethod \u02c6D are not as close to the reference results as those obtained using \u02c6C that explicitly exploits\nthe Gaussian distribution structure of the data.",
    "chunk_index": 20,
    "start_char": 43634,
    "end_char": 46813,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "each constituent (top row), and the aggregated risk (bottom row).\nIn this example, the fair risk allocation a computed with the true underlying distribution can be\nconsidered as reference for the backtesting results. The estimated risk allocations using \u02c6C and \u02c6D are\nclose to the reference allocations, and as expected, the results computed using the non-parametric\nmethod \u02c6D are not as close to the reference results as those obtained using \u02c6C that explicitly exploits\nthe Gaussian distribution structure of the data.\nTable 1 contains the summary of the estimated backtesting measures G0.05, Gi\n0.05, W i and \u03a5.\nFirst, we note that the values of G0.05(a), Gi\n0.05(a) and W i(a) corresponding to backtesting the fair\nallocation are, as expected, close to zero. In addition, \u03a5(a) is close to \u03b1 = 0.05. This indicates that\nthe proposed backtesting methodologies are adequate. The obtained values give the benchmark for\nthe following results produced by using \u02c6C and \u02c6D. We note that indeed, the values of G0.05, Gi\n0.05, W i\nand \u03a5 corresponding to \u02c6C and \u02c6D are in the same ballpark as for a, indicating that \u02c6C and \u02c6D are\nsuitable risk allocation methodologies.\nWe also provide a graphical representation of Gi\n0.05 in\nFigure 2 (top row), and in Figure 2 (bottom row) we Gi\n\u03b2( \u02c6D), i = 1, . . . , 8 as function of \u03b2.\nFor convenience, we additionally present several graphical representations of the backtesting\nmetrics. In Figure 3 we plot G\u03b2 and Gi\n\u03b2 as functions of \u03b2, for the three risk allocation methods\na, \u02c6C, \u02c6D. All these functions should take zero value around \u03b2 = \u03b1 = 0.05, which is clearly the case.\nFinally, Figure 4 is dedicated to risk level shift backtesting. The top row shows the values of W i\nfor risk allocations estimated using a, \u02c6C, and \u02c6D. The blue dots in the bottom graphs in Figure 4\ndepict the values of \u03b1\u00b1W, all of them being close to the reference risk value \u03b1 = 0.05, which again\nindicates adequacy of risk allocation estimation procedure \u02c6D.\n\nFair capital risk allocation\n19\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nS\nGi\n0.05(a)\n-0.00038\n0.00017\n-0.00054\n-0.00039\n-0.00021\n-0.00048\n0.00076\n-0.00011\nG0.05(a)\n-0.00118\nW i(a)\n-0.013\n0.001\n-0.002\n-0.002\n-0.003\n-0.012\n-0.017\n0.001\n\u03a5(a)\n0.047\nGi\n0.05( \u02c6C)\n-0.00090\n-0.00037\n-0.00014\n-0.00022\n0.00043\n-0.00058\n0.00088\n0.00008\nG0.05( \u02c6C)\n-0.00081\nW i( \u02c6C)\n-0.009\n-0.004\n-0.002\n-0.002\n0.004\n-0.016\n-0.013\n-0.005\n\u03a5( \u02c6C)\n0.048\nGi\n0.05( \u02c6D)\n0.00032\n0.00110\n0.00031\n0.00014\n-0.00003\n0.00010\n-0.00011\n-0.00088\nG0.05( \u02c6D)\n0.00094\nW i( \u02c6D)\n0.003\n0.005\n0.002\n0.005\n-0.001\n-0.001\n0.002\n0.008\n\u03a5( \u02c6D)\n0.053\nTable 1: Summary of the estimated backtesting measures for Example 5.1: In the \ufb01rst columns\nwe show Gi\n0.05 and W i, i = 1, . . . , 8, for the true allocation a, and the estimated risk allocations\n\u02c6C and \u02c6D, corresponding to backtesting the fair allocation. The values are close to zero, indicating\nthat the proposed backtesting methodologies are adequate.",
    "chunk_index": 21,
    "start_char": 46294,
    "end_char": 49203,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "0.00014\n-0.00003\n0.00010\n-0.00011\n-0.00088\nG0.05( \u02c6D)\n0.00094\nW i( \u02c6D)\n0.003\n0.005\n0.002\n0.005\n-0.001\n-0.001\n0.002\n0.008\n\u03a5( \u02c6D)\n0.053\nTable 1: Summary of the estimated backtesting measures for Example 5.1: In the \ufb01rst columns\nwe show Gi\n0.05 and W i, i = 1, . . . , 8, for the true allocation a, and the estimated risk allocations\n\u02c6C and \u02c6D, corresponding to backtesting the fair allocation. The values are close to zero, indicating\nthat the proposed backtesting methodologies are adequate. The last column shows the aggregated\nquantities G0.05 and the risk level shift \u03a5. Here, \u03a5 is close to \u03b1 = 0.05, as expected.\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05(a)\ni\n-0.004\n0.000\n0.004\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6C)\ni\n-0.004\n0.000\n0.004\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6D)\ni\n-0.004\n0.000\n0.004\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 1\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 2\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 3\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 4\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 5\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 6\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 7\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 8\nGi\n\u03b2( \u02c6D)\nFigure 2:\nExample 5.1.\nGraphical representation of the deviation from fairness backtesting\nmethod, compare Table 1: the \ufb01rst row shows Gi\n0.05, i = 1, . . . , 8, for the true allocation a, and\nthe estimated risk allocations \u02c6C and \u02c6D. The values are close to zero, indicating that the proposed\nbacktesting methodologies are adequate. The second and third row shows Gi\n\u03b2 as function of \u03b2\nfor each constituent. The red dots in the bottom rows represent the values of Gi\n0.05 using \u02c6D. An\naggregated plot together with G\u03b2 is given in Figure 3.\n\n20\nBielecki, Cialenco, Pitera, and Schmidt\n-0.004\n0.000\n0.004\nGi\n\u03b2(a) and G\u03b2(a)\n\u03b2\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n-0.004\n0.000\n0.004\nGi\n\u03b2( \u02c6C) and G\u03b2( \u02c6C)\n\u03b2\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n-0.004\n0.000\n0.004\nGi\n\u03b2( \u02c6D) and G\u03b2( \u02c6D)\n\u03b2\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nFigure 3: Example 5.1: the estimated backtesting measures as function of the risk level \u03b2 for\nthe true allocation a, and the estimated risk allocations \u02c6C and \u02c6D (compare Table 1 for values\ncorresponding to \u03b2 = 0.05). The measures Gi\n\u03b2, i = 1, 2, . . . , 8 for the di\ufb00erent constituents are\nindicated by color while the bold red line represents the backtesting measure G\u03b2 at portfolio level.\nAll these functions should be zero around \u03b2 = \u03b1 = 0.05, which is clearly the case.",
    "chunk_index": 22,
    "start_char": 48713,
    "end_char": 51115,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "to \u03b2 = 0.05). The measures Gi\n\u03b2, i = 1, 2, . . . , 8 for the di\ufb00erent constituents are\nindicated by color while the bold red line represents the backtesting measure G\u03b2 at portfolio level.\nAll these functions should be zero around \u03b2 = \u03b1 = 0.05, which is clearly the case.\n8\n7\n6\n5\n4\n3\n2\n1\nW i(a)\ni\n-0.04\n-0.02\n0.00\n0.02\n0.04\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6C)\ni\n-0.04\n-0.02\n0.00\n0.02\n0.04\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6D)\ni\n-0.04\n-0.02\n0.00\n0.02\n0.04\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 1\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 2\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 3\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 4\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 5\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 6\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 7\n0.03\n0.05\n0.07\n-0.004\n0.000\n0.004\n\u03b2\ni = 8\nGi\n\u03b2( \u02c6D)\nFigure 4:\nExample 5.1.\nGraphical representation of the risk level shift backtesting method,\ncompare Table 1: the \ufb01rst row shows W i, i = 1, . . . , 8, values being close to zero. The second\nand third row shows Gi\n\u03b2( \u02c6D) as function of \u03b2 for each constituent, blue dots represent the values of\nW i( \u02c6D).\n\nFair capital risk allocation\n21\nRisk allocations using \u02c6C\n-0.05\n0.05\n0.15\nRisk allocations using \u02c6D\n-0.05\n0.05\n0.15\nFigure 5: Example 5.2. Estimated risk allocations for the eight portfolio constituents (indexed\nby colour) at each backtesting day, k = 1, . . . , mfor the estimated risk allocations \u02c6C and \u02c6D; the\nheight of each coloured horizontal layer represents the risk allocated to one of the constituents. It\nis apparent that the estimated risk allocation by these two methods are quite di\ufb00erent.\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nS\nGi\n0.05( \u02c6C)\n0.00209\n0.00363\n0.00152\n0.00127\n0.00093\n0.00088\n-0.00084\n-0.00076\nG0.05( \u02c6C)\n0.00872\nW i( \u02c6C)\n0.016\n0.025\n0.013\n0.012\n0.012\n0.021\n0.014\n0.012\n\u03a5( \u02c6C)\n0.069\nGi\n0.05( \u02c6D)\n0.00074\n0.00062\n-0.00030\n0.00028\n0.00049\n0.00014\n0.00001\n0.00013\nG0.05( \u02c6D)\n0.00212\nW i( \u02c6D)\n0.004\n0.003\n-0.001\n0.008\n0.007\n0.001\n0.000\n-0.001\n\u03a5( \u02c6D)\n0.054\nTable 2: Summary of the estimated backtesting measures for Example 5.2: In the \ufb01rst columns\nwe show Gi\n0.05 and W i, i = 1, . . . , 8, for the estimated risk allocations \u02c6C and \u02c6D, corresponding to\nbacktesting the fair allocation. The values of Gi\n0.05( \u02c6C) are of one order of magnitude further away\nfrom zero than Gi\n0.05( \u02c6D), indicating that indeed risk allocation methodology \u02c6D is more adequate\nfor this experiment. The last column shows the aggregated quantities G0.05 and the risk level shift\n\u03a5. Here, \u03a5( \u02c6D) is close to \u03b1 = 0.05, as expected.",
    "chunk_index": 23,
    "start_char": 50845,
    "end_char": 53344,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "= 1, . . . , 8, for the estimated risk allocations \u02c6C and \u02c6D, corresponding to\nbacktesting the fair allocation. The values of Gi\n0.05( \u02c6C) are of one order of magnitude further away\nfrom zero than Gi\n0.05( \u02c6D), indicating that indeed risk allocation methodology \u02c6D is more adequate\nfor this experiment. The last column shows the aggregated quantities G0.05 and the risk level shift\n\u03a5. Here, \u03a5( \u02c6D) is close to \u03b1 = 0.05, as expected.\nExample 5.2 (Student t-distributed P&Ls). Similar to the previous example we consider a port-\nfolio of eight constituents and with discounted P&L following a t-distribution with \ufb01ve degrees of\nfreedom. For comparison reasons, the distribution of (X1, . . . , X8) is modi\ufb01ed so that it has the\nsame mean and variance covariance structure as in Example 5.1.\nFirst, note that there is no available counterpart of a for this setup. Second, as we will show\nbelow, since X does not follow a Gaussian distribution, one should not use \u02c6C to estimate the risk\nallocation, and only \u02c6D is an appropriate methodology in estimating risk allocation. In Figure 5, we\npresent the estimated risk allocations computed using \u02c6C and \u02c6D, over the entire backtesting period\nk = 1, . . . , m. It is apparent that the estimated risk allocation by these two methods are quite\ndi\ufb00erent. Table 2 contains the values of the estimated backtesting metrics, and for the reader\u2019s\nconvenience Gi\n0.05 and W i are represented graphically in Figure 6. The values of Gi\n0.05( \u02c6C) are of\none order of magnitude further away from zero than Gi\n0.05( \u02c6D), indicating that indeed risk allocation\nmethodology \u02c6D is more adequate for this experiment. We also note that magnitude of Gi\n0.05( \u02c6D) in\nthis example aligns with the benchmark values from Example 5.1. Similar arguments hold true for\nW i and \u03a5.\n\n22\nBielecki, Cialenco, Pitera, and Schmidt\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6C)\ni\n-0.004\n0.000\n0.004\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6D)\ni\n-0.004\n0.000\n0.004\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6C)\ni\n-0.04\n-0.02\n0.00\n0.02\n0.04\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6D)\ni\n-0.04\n-0.02\n0.00\n0.02\n0.04\nFigure 6: Estimated backtesting measures for Example 5.2, compare Figures 1 and 3. The \ufb01rst\nrow represents the deviation from fairness backtesting method and shows Gi\n0.05 for each constituent\ni = 1, . . . , 8 for the risk allocations \u02c6C and \u02c6D. The second row represents the risk level shift back-\ntesting method and shows W i, respectively. The values of Gi\n0.05( \u02c6C) are of one order of magnitude\nfurther away from zero than Gi\n0.05( \u02c6D), indicating that indeed risk allocation methodology \u02c6D is more\nadequate for this experiment. We also note that magnitude of Gi\n0.05( \u02c6D) in this example aligns with\nthe benchmark values from Example 5.1.",
    "chunk_index": 24,
    "start_char": 52912,
    "end_char": 55603,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": ". . , 8 for the risk allocations \u02c6C and \u02c6D. The second row represents the risk level shift back-\ntesting method and shows W i, respectively. The values of Gi\n0.05( \u02c6C) are of one order of magnitude\nfurther away from zero than Gi\n0.05( \u02c6D), indicating that indeed risk allocation methodology \u02c6D is more\nadequate for this experiment. We also note that magnitude of Gi\n0.05( \u02c6D) in this example aligns with\nthe benchmark values from Example 5.1. Similar arguments hold true for W i.\nExample 5.3 (Fairness and asymptotic fairness). In this example we illustrate the fairness and\nthe asymptotic fairness properties.\nAgain, for the sake of a reference statistic which eases the\npresentation, we work under the normality assumption. Moreover, we consider only the \ufb01rst three\nconstituents from Example 5.1, that is (X1, X2, X3), because the other constituents show similar\nbehavior. The numerical results presented below con\ufb01rm that allocations a and \u02c6B are fair. In\naddition, these results con\ufb01rm that the allocations \u02c6C and \u02c6D are asymptotically fair even though\nthey are not fair in this example.\nFigure 7 deals with the issue of a short learning period, that is a small sample size, of n = 250.\nWe see that for allocations a and \u02c6B the Gi\n0.05\u2019s and W i\u2019s are getting close to zero with increasing\nm, and that \u03a5 gets close to 0.05 with increasing m, con\ufb01rming that these are fair allocations. We\nalso see that Gi\n0.05\u2019s and W i\u2019s stay away from zero, and \u03a5 stays away from 0.05 with increasing m\nfor allocations \u02c6C and \u02c6D, indicating that these are not fair allocations.\nFigure 8 illustrates the asymptotic fairness of \u02c6Dn with n \u2192\u221e. The left panel shows that\nGi\n0.05( \u02c6D) get closer to zero for large m with increasing n. Similarly for the right panel, with regard\nto W i and \u03a5.\n\nFair capital risk allocation\n23\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05(a)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6B)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6C)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6D)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i(a) and \u03a5(a)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6B) and \u03a5( \u02c6B)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6C) and \u03a5( \u02c6C)\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6D) and \u03a5( \u02c6D)\nm\nFigure 7: Example 5.3 (small sample size): We \ufb01x the learning period of size n = 250 and consider\nincreasing lengths of backtesting intervals, i.e. we let m run. On the \ufb01rst two rows we plot Gi\n0.05,\ni = 1, 2, 3, for the true allocation a, and the estimated risk allocations \u02c6B, \u02c6C and \u02c6D. On the last two\nrows we plot W i, i = 1, 2, 3,.",
    "chunk_index": 25,
    "start_char": 55161,
    "end_char": 57889,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6D) and \u03a5( \u02c6D)\nm\nFigure 7: Example 5.3 (small sample size): We \ufb01x the learning period of size n = 250 and consider\nincreasing lengths of backtesting intervals, i.e. we let m run. On the \ufb01rst two rows we plot Gi\n0.05,\ni = 1, 2, 3, for the true allocation a, and the estimated risk allocations \u02c6B, \u02c6C and \u02c6D. On the last two\nrows we plot W i, i = 1, 2, 3,. For allocations a and \u02c6B the measures are getting close to zero with\nincreasing m, and \u03a5 gets close to 0.05, con\ufb01rming that these are fair allocations. For allocations\n\u02c6C and \u02c6D the opposite is true, indicating that these are not fair allocations.\n\n24\nBielecki, Cialenco, Pitera, and Schmidt\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6D); n = 250\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6D) and \u03a5( \u02c6D); n = 250\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6D); n = 1000\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6D) and \u03a5( \u02c6D); n = 1000\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-4e-04\n0e+00\n4e-04\nGi\n0.05( \u02c6D); n = 4000\nm\n0e+00\n2e+05\n4e+05\n6e+05\n8e+05\n1e+06\n-0.004\n0.000\n0.004\nW i( \u02c6D) and \u03a5( \u02c6D); n = 4000\nm\nFigure 8: Example 5.3 (asymptotic fairness): in the left panel we plot Gi\n0.05( \u02c6D) as function of m,\nfor di\ufb00erent values of learning period, n = 250 (top) to n = 4000 (bottom). These plots con\ufb01rm\nthat Gi\n0.05( \u02c6D) get closer to zero with increasing n, which yields asymptotic fairness. The pictures\nin the right panel contain values of W i( \u02c6D) and \u03a5( \u02c6D) as functions of m, and for n = 250, 1000 and\n4000 (from top to bottom). We obtain similar results here, showing that W i and \u03a5 get closer to\nzero with increasing n.\n\nFair capital risk allocation\n25\n2015\n2016\n2017\n-0.2\n-0.1\n0.0\n0.1\n0.2\ndate\nbacktesting period\n2017\n2018\n2019\n-0.2\n-0.1\n0.0\n0.1\n0.2\ndate\nbacktesting period\nFigure 9:\nAggregated portfolio P&Ls split into Dataset 1 (January 2015 - December 2016,\nleft panel) and Dataset 2(January 2017 - December 2018, right panel).\nThe estimated volatil-\nities are 0.0425 (0.0456/0.0392)(firsthalf/secondhalf) in Dataset 1 and in the right panel\n0.0415 (0.0267/0.0522).\nExample 5.4 (Market data example). In this example we analyze the performance of the backtest-\ning methodologies on market data. We consider the same portfolio formation as in Example 5.1,\nby taking eight stocks (AAPL, AMZN, BA, DIS, HD, KO, JPM, and MSFT) from the S&P 500\nindex, and form an equally weighted long-short portfolio. Namely, we hold a long position in the\n\ufb01rst six stocks, and a short position in the last two stocks, with nominal (absolute) value $1 in\neach stock.",
    "chunk_index": 26,
    "start_char": 57487,
    "end_char": 60134,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "performance of the backtest-\ning methodologies on market data. We consider the same portfolio formation as in Example 5.1,\nby taking eight stocks (AAPL, AMZN, BA, DIS, HD, KO, JPM, and MSFT) from the S&P 500\nindex, and form an equally weighted long-short portfolio. Namely, we hold a long position in the\n\ufb01rst six stocks, and a short position in the last two stocks, with nominal (absolute) value $1 in\neach stock. For this study, we use the daily stock returns, for the period January 2015 - December\n2018. Throughout we set both, learning period (n) and backtesting period (m), equal to 250 days.\nWe split the dataset into two subsets: January 2015 - December 2016 (Dataset 1), and January\n2017 - December 2018 (Dataset 2). As before, for each dataset, we use the standard 1-day rolling\nwindow and compare forecasted capital allocations with realized portfolio values.\nOne reason to split the data into these two time frames stems from the distinctively di\ufb00erent\npatterns of the the aggregated P&L of the portfolio; see Figure 9. Dataset 1 is more homogeneous,\nwith slightly larger volatility in the \ufb01rst half. Speci\ufb01cally, the sample standard deviation of the\naggregated P&L portfolio for Dataset 1 is equal to 0.0425; the sample standard deviation for the\n\ufb01rst half is 0.0456, and for the second half is 0.0392. Dataset 2 exhibits a higher volatility in the\nsecond half compared to its \ufb01rst half and compared to Dataset 1; the standard deviation for the\n\ufb01rst half is 0.0267, and for the second half is 0.0522. As we will show later, these di\ufb00erences will\npropagate into the capital risk allocation and they will be picked up by the backtesting procedure.\nSimilar to the previous examples, for both datasets we will use the risk allocation estimators bC\nand bD, and we will use both backtesting procedures proposed in Section 5. We also performed the\nJarque-Bera normality test for the aggregated portfolio P&Ls for both datasets, which was rejected\nat signi\ufb01cance level 0.01.\nIn the following, we will analyze each dataset separately.\nA \ufb01rst overview is presented in\nFigure 10, where the \ufb01rst two columns (left panel) correspond to Dataset 1, and the rightmost two\ncolumns (right panel) to Dataset 2.\nDataset 1, January 2015 - December 2016, Figure 10, left panel, and Table 3. The aggregated\n(total) risk of the portfolio is displayed in the \ufb01rst row of Figure 10, which was computed by using\nestimators bC and bD. The aggregated portfolio risk seems to be well estimated by both bC and bD.\nThe noticeable slight decrease in time of the aggregated risk is partially due to the lower volatility of\nthe returns in the second part of the Dataset 1. The estimated risk capital allocations are presented\n\n26\nBielecki, Cialenco, Pitera, and Schmidt\nDataset 1\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nS\nGi\n0.05( \u02c6C)\n-0.00107\n0.00221\n0.00110\n-0.00036\n-0.00052\n0.00255\n0.00653\n-0.00797\nG0.05\n0.00247\nW i( \u02c6C)\n-0.002\n0.010\n0.010\n-0.002\n-0.026\n0.014\n-0.050\n0.018",
    "chunk_index": 27,
    "start_char": 59720,
    "end_char": 62659,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "both bC and bD.\nThe noticeable slight decrease in time of the aggregated risk is partially due to the lower volatility of\nthe returns in the second part of the Dataset 1. The estimated risk capital allocations are presented\n\n26\nBielecki, Cialenco, Pitera, and Schmidt\nDataset 1\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nS\nGi\n0.05( \u02c6C)\n-0.00107\n0.00221\n0.00110\n-0.00036\n-0.00052\n0.00255\n0.00653\n-0.00797\nG0.05\n0.00247\nW i( \u02c6C)\n-0.002\n0.010\n0.010\n-0.002\n-0.026\n0.014\n-0.050\n0.018\n\u03a5\n0.056\nGi\n0.05( \u02c6D)\n-0.00653\n-0.00855\n-0.00692\n-0.00525\n-0.00419\n0.00102\n0.01707\n0.00339\nG0.05\n-0.00996\nW i( \u02c6D)\n-0.022\n-0.022\n-0.026\n-0.030\n-0.026\n0.006\n-0.050\n-0.010\n\u03a5\n0.044\nTable 3: Summary of backtesting statistics for Example 5.4, Dataset 1, split into the \ufb01rst period\n(\ufb01rst two rows) and the second period (last two rows): in the \ufb01rst columns we show Gi\n0.05 and\nW i, i = 1, . . . , 8, for the estimated risk allocations \u02c6C and \u02c6D, corresponding to backtesting the fair\nallocation. Overall, the capital allocations are well estimated by both bC and bD, with exception\nof the seventh constituent, for which W 7\n0.05( \u02c6C) = \u22120.05 and W 7\n0.05( \u02c6D) = \u22120.05. For i = 7 the\ncorrelation di\ufb00erence is noticeably higher than for the rest of the sample which might be a result\nof a structural change. The last column shows the aggregated quantities G0.05 and the risk level\nshift \u03a5. Here, \u03a5 is close to \u03b1 = 0.05, as expected.\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\n01/2015 \u2013 12/2015\n0.70\n0.66\n0.75\n0.66\n0.68\n0.58\n-0.54\n-0.38\n01/2016 \u2013 12/2016\n0.59\n0.65\n0.56\n0.6\n0.59\n0.52\n-0.18\n-0.33\nDi\ufb00erence\n0.11\n0.01\n0.18\n0.06\n0.09\n0.06\n-0.36\n-0.04\nTable 4: Estimated correlations between each portfolio constituent (Xi) and aggregated portfolio\n(S), for two separate time periods for Dataset 1 in Example 5.4. One can that the biggest di\ufb00erence\nis observed for i = 7; this might indicate a structural change, a possible explanation of the results\nin Table 3.\nin the second row, and the backtesting statistics G, Gi, and W, W i are graphically displayed in rows\n3-5 of Figure 10 and the numerical values are presented in Table 3.\nOverall, the capital allocations are well estimated by both9 bC and bD, with exception of the\nseventh constituent, for which W 7\n0.05( \u02c6C) = \u22120.05 and W 7\n0.05( \u02c6D) = \u22120.05. To see whether this is a\nproblem with the estimator or a result of time-correlation structure change we checked the sample\ncorrelations between each constituent and the portfolio for two disjoint subsets. The results are\npresented in Table 4. One could see that for i = 7 the correlation di\ufb00erence is noticeably higher\nthan for the rest which might be a result of a structural change. Consequently, we believe that the\nproposed backtesting procedures correctly identi\ufb01ed a wrong allocation in this particular case.\nDataset 2, January 2017 - December 2018, Figure 10, right panel, and Table 5.",
    "chunk_index": 28,
    "start_char": 62200,
    "end_char": 65026,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "structure change we checked the sample\ncorrelations between each constituent and the portfolio for two disjoint subsets. The results are\npresented in Table 4. One could see that for i = 7 the correlation di\ufb00erence is noticeably higher\nthan for the rest which might be a result of a structural change. Consequently, we believe that the\nproposed backtesting procedures correctly identi\ufb01ed a wrong allocation in this particular case.\nDataset 2, January 2017 - December 2018, Figure 10, right panel, and Table 5.\nDue to the\nincrease of the volatility in the second half of the Dataset 2, the aggregated portfolio risk increases\nthroughout the backtesting period; see Figure 10, \ufb01rst row. In the second row of the same \ufb01gure\nwe present the nominal value of the allocated risk among constituents computed by using risk\nallocation estimators bC and bD. In contrast to Dataset 1, the backtesting results for Dataset 2\nreveal a signi\ufb01cant underestimation of the aggregated risk. This can be seen by noticing that the\nvalues of G0.05 and \u03a5, for both bC and bD, are far from zero; see last column in Table 5, or the\nthird and fourth rows of Figure 10, right panel. The graph of function \u03b2 \u2192G\u03b2 is plotted in the\nthird row of Figure 10, solid red line, and the value of \u03a5 corresponds to the red vertical line in\nthe last row. Comparing these plots with the corresponding plots from previous examples and\ndatasets, we also conclude that the aggregated risk is signi\ufb01cantly underestimated. Inevitably, this\n9We note that while data is not normally distributed, the estimator bC performed similarly well as the nonpara-\nmetric estimator bD.\n\nFair capital risk allocation\n27\nDataset 2\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nGi\n0.05( \u02c6C)\n0.01187\n0.02618\n0.01512\n0.00562\n0.01229\n0.00404\n-0.01356\n-0.01785\nG0.05\n0.0437\nW i( \u02c6C)\n0.234\n0.210\n0.146\n0.166\n0.222\n0.110\n0.278\n0.274\n\u03a5\n0.204\nGi\n0.05( \u02c6C)\n0.00680\n0.01669\n0.01633\n0.00301\n0.00876\n0.00181\n-0.00809\n-0.01106\nG0.05\n0.03425\nW i( \u02c6D)\n0.158\n0.130\n0.102\n0.078\n0.074\n0.042\n0.118\n0.086\n\u03a5\n0.156\nTable 5: Summary of backtesting statistics for Example 5.4, Dataset 2, split into the \ufb01rst period\n(\ufb01rst two rows) and the second period (last two rows): in the \ufb01rst columns we show Gi\n0.05 and\nW i, i = 1, . . . , 8, for the estimated risk allocations \u02c6C and \u02c6D, corresponding to backtesting the\nfair allocation. In contrast to Dataset 1, the backtesting results for Dataset 2 reveal a signi\ufb01cant\nunderestimation of the aggregated risk.\nInevitably, this error propagates to the risk allocation\nestimation.\nClearly, the values of Gi\n0.05 and \u03a5 are signi\ufb01cantly di\ufb00erent from zero.\nThe risk\nallocation using the nonparametric estimators bD performs better than that one using bC.\nerror propagates to the risk allocation estimation, as shown in the plots from rows 3-5. Clearly,\nthe values of Gi\n0.05 and \u03a5 are signi\ufb01cantly di\ufb00erent from zero (see also Table 5), in comparison to\nthose from Dataset 1 and the previous examples.",
    "chunk_index": 29,
    "start_char": 64518,
    "end_char": 67448,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "Inevitably, this error propagates to the risk allocation\nestimation.\nClearly, the values of Gi\n0.05 and \u03a5 are signi\ufb01cantly di\ufb00erent from zero.\nThe risk\nallocation using the nonparametric estimators bD performs better than that one using bC.\nerror propagates to the risk allocation estimation, as shown in the plots from rows 3-5. Clearly,\nthe values of Gi\n0.05 and \u03a5 are signi\ufb01cantly di\ufb00erent from zero (see also Table 5), in comparison to\nthose from Dataset 1 and the previous examples. On the other hand, arguably, the risk allocation\nusing the nonparametric estimators bD performs better than that one using bC; see for instance the\nvalues of W i( bD) and Gi\n0.05( bD) versus W i( bC) and Gi\n0.05( bC). Finally, we note that, for the estimator\nbD, the zeros of functions \u03b2 \u2192Gi\n\u03b2( bD), i = 1, . . . , 8, are essentially the same as the zero of the\nfunction \u03b2 \u2192G\u03b2( bD), indicating that the risk allocation itself (as proportion of the total risk) is\ndone properly, and failure of the backtesting procedure is due to underestimation of the total risk.\nAcknowledgments\nTomasz R. Bielecki and Igor Cialenco acknowledge support from the National Science Founda-\ntion grant DMS-1907568. Marcin Pitera acknowledges support from the National Science Centre,\nPoland, via project 2016/23/B/ST1/00479. The authors would also like to thank the anonymous\nreferees, the associate editor and the editor for their helpful comments and suggestions which\nimproved greatly the \ufb01nal manuscript.\nReferences\n[ADEH99] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath. Coherent measures of risk. Math.\nFinance, 9(3):203\u2013228, 1999.\n[AS14]\nC. Acerbi and B. Sz\u00b4ekely. Back-testing expected shortfall. Risk magazine, (November),\n2014.\n[BCF18]\nT. R Bielecki, I. Cialenco, and S. Feng. A dynamic model of Central Counterparty Risk.\nInternational Journal of Theoretical and Applied Finance, 21(8):1850050, 2018.\n[B\u00a8uh70]\nH. B\u00a8uhlmann. Mathematical methods in risk theory. Springer, Berlin, 1970.\n[Che06]\nA. Cherny. Weighted VaR and its properties. Finance Stoch., 10(3):367\u2013393, 2006.\n[CD19]\nD. Coculescu and F. Delbaen, Surplus Sharing with Coherent Utility Functions. Risks,\n7, 7, 2019.\n\n28\nBielecki, Cialenco, Pitera, and Schmidt\nDataset 1\n0\n50\n100\n150\n200\n250\n-0.05\n0.05\n0.15\nAggregated risk using \u02c6C\nk\n0\n50\n100\n150\n200\n250\n-0.05\n0.05\n0.15\nAggregated risk using \u02c6D\nk\nRisk allocations using \u02c6C\n-0.05\n0.05\n0.15\nRisk allocations using \u02c6D\n-0.05\n0.05\n0.15\n-0.04\n0.00\n0.04\nGi\n\u03b2( \u02c6C) and G\u03b2( \u02c6C)\n\u03b2\n0.1\n0.2\n0.3\n0.4\n0.5\n-0.04\n0.00\n0.04\nGi\n\u03b2( \u02c6D) and G\u03b2( \u02c6D)\n\u03b2\n0.1\n0.2\n0.3\n0.4\n0.5\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6C)\ni\n-0.02 0.00\n0.02\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6D)\ni\n-0.02 0.00\n0.02\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6C)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6D)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\nDataset 2\n0\n50\n100\n150\n200\n250\n-0.05\n0.05\n0.15\nAggregated risk using \u02c6C",
    "chunk_index": 30,
    "start_char": 66961,
    "end_char": 69768,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "0.5\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6C)\ni\n-0.02 0.00\n0.02\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6D)\ni\n-0.02 0.00\n0.02\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6C)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6D)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\nDataset 2\n0\n50\n100\n150\n200\n250\n-0.05\n0.05\n0.15\nAggregated risk using \u02c6C\nk\n0\n50\n100\n150\n200\n250\n-0.05\n0.05\n0.15\nAggregated risk using \u02c6D\nk\nRisk allocations using \u02c6C\n-0.05\n0.05\n0.15\nRisk allocations using \u02c6D\n-0.05\n0.05\n0.15\n-0.04\n0.00\n0.04\nGi\n\u03b2( \u02c6C) and G\u03b2( \u02c6C)\n\u03b2\n0.1\n0.2\n0.3\n0.4\n0.5\n-0.04\n0.00\n0.04\nGi\n\u03b2( \u02c6D) and G\u03b2( \u02c6D)\n\u03b2\n0.1\n0.2\n0.3\n0.4\n0.5\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6C)\ni\n-0.02 0.00\n0.02\n1\n2\n3\n4\n5\n6\n7\n8\nGi\n0.05( \u02c6D)\ni\n-0.02 0.00\n0.02\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6C)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\n8\n7\n6\n5\n4\n3\n2\n1\nW i( \u02c6D)\ni\n-0.1\n0.0\n0.1\n0.2\n0.3\nFigure 10: Aggregated risk, risk allocation and the backtesting metrics for portfolios in Exam-\nple 5.4. Two columns on the left (left panel) correspond to Dataset 1, while rightmost two columns\n(right panel) correspond to Dataset 2. The results are obtained by using the risk allocation esti-\nmator bC and the nonparametric risk allocation estimators bD.\n\nFair capital risk allocation\n29\n[Del00]\nF. Delbaen. Coherent risk measures. Scuola Normale Superiore, 2000.\n[FS11]\nH. F\u00a8ollmer and A. Schied. Stochastic \ufb01nance: an introduction in discrete time. Walter\nde Gruyter, 3rd edition, 2011.\n[Ger74]\nH. U. Gerber.\nOn additive premium calculation principles.\nASTIN Bulletin: The\nJournal of the IAA, 7(3):215\u2013222, 1974.\n[Gui16]\nGene D Guill. Bankers trust and the birth of modern risk management. Journal of\napplied corporate \ufb01nance, 28(1):19\u201329, 2016.\n[Kal05]\nM. Kalkbrener. An axiomatic approach to capital allocation. Mathematical Finance,\n15(3):425\u2013437, 2005.\n[Kup95]\nP. H. Kupiec. Techniques for verifying the accuracy of risk measurement models. The\nJournal of Derivatives, 3(2):73\u201384, 1995.\n[Kus01]\nS. Kusuoka. On law invariant coherent risk measures. In Advances in mathematical\neconomics, Vol. 3, volume 3 of Adv. Math. Econ., pages 83\u201395. Springer, 2001.\n[MFE15]\nA.J. McNeil, R. Frey, and P. Embrechts.\nQuantitative risk management: concepts,\ntechniques, and tools. Princeton university press, \ufb01rst revised edition, 2015.\n[NZ17]\nN. Nolde and J. F. Ziegel.\nElicitability and backtesting: Perspectives for banking\nregulation. The Annals of Applied Statistics, 11(4):1833\u20131874, 2017.\n[PM18]\nM. Pitera and F. Moldenhauer. Backtesting Expected Shortfall: a simple recipe? Jour-\nnal of Risk, 22(1):17\u201342, 2019.\n[PS18]\nM. Pitera and T. Schmidt. Unbiased estimation of risk. Journal of Banking & Finance,\n91:133\u2013145, 2018.\n[Sha13]\nA. Shapiro. On Kusuoka representation of law invariant risk measures. Mathematics of\nOperations Research, 38(1):142\u2013152, 2013.\n[SKG15]\nP. Schmidt, M. Katzfuss, and T. Gneiting. Interpretation of point forecasts with unkown\ndirective Preprint, 2015.",
    "chunk_index": 31,
    "start_char": 69503,
    "end_char": 72306,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "backtesting: Perspectives for banking\nregulation. The Annals of Applied Statistics, 11(4):1833\u20131874, 2017.\n[PM18]\nM. Pitera and F. Moldenhauer. Backtesting Expected Shortfall: a simple recipe? Jour-\nnal of Risk, 22(1):17\u201342, 2019.\n[PS18]\nM. Pitera and T. Schmidt. Unbiased estimation of risk. Journal of Banking & Finance,\n91:133\u2013145, 2018.\n[Sha13]\nA. Shapiro. On Kusuoka representation of law invariant risk measures. Mathematics of\nOperations Research, 38(1):142\u2013152, 2013.\n[SKG15]\nP. Schmidt, M. Katzfuss, and T. Gneiting. Interpretation of point forecasts with unkown\ndirective Preprint, 2015.\n[Sti97]\nS. M. Stigler.\nThe Asymptotic Distribution of the Trimmed Mean\nThe Annals of\nStatistics, 1(3):472\u2013477, 1973.\n[Tas04]\nD. Tasche. Allocating portfolio economic capital to sub-portfolios. Economic capital: a\npractitioner guide, pages 275\u2013302, 2004.\n[Tas07]\nD. Tasche. Euler allocation: Theory and practice. Preprint, 2007.\n[Tsa09]\nA. Tsanakas. To split or not to split: Capital allocation with convex risk measures.\nInsurance: Mathematics and Economics, 44(2):268\u2013277, 2009.\n[Zie16]\nJ. F. Ziegel. Coherence and elicitability. Mathematical Finance, 26:901 \u2013 918, 2016.",
    "chunk_index": 32,
    "start_char": 71709,
    "end_char": 72879,
    "paper_title": "Fair Estimation of Capital Risk Allocation",
    "paper_category": "q-fin.RM",
    "paper_filename": "Fair_Estimation_of_Capital_Risk_Allocation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.RM/Fair_Estimation_of_Capital_Risk_Allocation.pdf"
  },
  {
    "text": "A Clustering Algorithm for Correlation Quickest\nHub Discovery Mixing Time Evolution and\nRandom Matrix Theory\n1st Alejandro Rodr\u00b4\u0131guez Dom\u00b4\u0131nguez\nMiralta Finance Bank S.A., Spain\narodriguez@miraltabank.com\n2nd David Stynes\nMunster Technological University, Ireland\ndavid.stynes@mtu.ie\nAbstract\u2014We present a geometric version of Quickest Change\nDetection (QCD) and Quickest Hub Discovery (QHD) tests in\ncorrelation structures that allows us to include and combine\nnew information with distance metrics. The topic falls within\nthe scope of sequential, nonparametric, high-dimensional QCD\nand QHD, from which state-of-the-art settings developed global\nand local summary statistics from asymptotic Random Matrix\nTheory (RMT) to detect changes in random matrix law. These\nsettings work only for uncorrelated pre-change variables. With\nour geometric version of the tests via clustering, we can test\nthe hypothesis that we can improve state-of-the-art settings for\nQHD, by combining QCD and QHD simultaneously, as well as\nincluding information about pre-change time-evolution in corre-\nlations. We can work with correlated pre-change variables and\ntest if the time-evolution of correlation improves performance.\nWe prove test consistency and design test hypothesis based on\nclustering performance. We apply this solution to \ufb01nancial time\nseries correlations. Future developments on this topic are highly\nrelevant in \ufb01nance for Risk Management, Portfolio Management,\nand Market Shocks Forecasting which can save billions of\ndollars for the global economy. We introduce the Diversi\ufb01cation\nMeasure Distribution (DMD) for modeling the time-evolution of\ncorrelations as a function of individual variables which consists\nof a Dirichlet-Multinomial distribution from a distance matrix\nof rolling correlations with a threshold. Finally, we are able to\nverify all these hypotheses.\nIndex Terms\u2014Clustering, Correlation, Distribution functions,\nFinancial, Graphs and networks, Quickest change detection,\nQuickest hub discovery, Risk management, Sequential analysis\nI. INTRODUCTION\nFinancial assets portfolios are combinations of individual\nassets (Stocks, Bonds, Commodities, etc) that, when combined\ncan bene\ufb01t from risk diversi\ufb01cation, and correlation struc-\ntures play a crucial role in this, as seen in H. Markowitz\n[1]. Therefore, knowing in advance the future behavior of\ncorrelation matrices is key for the risk management of port-\nfolios. We focus on Quickest Change Detection (QCD) on\ncorrelation structures and Quickest Hub Discovery (QHD),\nvariables changing their correlation. CPD and QHD allow\nmeasuring changes in the behavior of correlations to antic-\nipate their future behavior. They also are risk management\ncontingency measures to reallocate portfolios for better diver-\nsi\ufb01cation and expected risk-adjusted returns when correlations\nchange thereby improving the portfolio allocation process.\nAlso, changes in many elements of the correlation matrix all\nat once are related to \ufb01nancial market shocks as can be seen in\nL. S. Junior and I. D. P. Franca [2], therefore, CPD and QHD\nin correlations are important too for economic and \ufb01nancial\nshock detection, which helps to preserve wealth in \ufb01nancial\nmarkets and the economy. As can be seen in A.G Tartakovsky\n[3], the existing literature in CPD/QCD focuses on a setup\nwhere:\n\u2022 The pre- and post- means and covariance matrices are\nunknown.\n\u2022 It does not allow for high-dimensional settings where\np >> n, p variables, and n timestamps.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3482,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "S. Junior and I. D. P. Franca [2], therefore, CPD and QHD\nin correlations are important too for economic and \ufb01nancial\nshock detection, which helps to preserve wealth in \ufb01nancial\nmarkets and the economy. As can be seen in A.G Tartakovsky\n[3], the existing literature in CPD/QCD focuses on a setup\nwhere:\n\u2022 The pre- and post- means and covariance matrices are\nunknown.\n\u2022 It does not allow for high-dimensional settings where\np >> n, p variables, and n timestamps.\n\u2022 It assumes variables are i.i.d, a critical problem for de-\ntecting changes in correlations that assumes dependence.\nAs mentioned in T. Banerjee and A. Hero [4], to tackle\nthis intractability, the area called sequential analysis in the\nliterature focus on sub-optimal statistical tests with thresholds\nthat improve performance and help with the test design. From\nthe existing literature, the only optimal test designed for corre-\nlations and that can tackle most of the issues described, comes\nfrom work developed over the last decade by T. Banerjee et\nal. [5], T. Banerjee and A. Hero [4], and A. Hero and B.\nRajaratnam [6], [7]. The objective is to detect a change in the\ndistribution of the data matrices as quickly as possible subject\nto a constraint in the false alarm rate. In this line, the authors\nobtain an asymptotic distribution for a global and a family of\nlocal summary statistics that measures a change in the law of\nthe Random Matrix as a way to detect the change points and\nisolated hubs [4].\nThe key limitation of [4], [5] is that they assume variables\nare uncorrelated before the change and correlated after. We\napproach the QCD/QHD test from a geometric point of view\nvia clustering techniques. We design the geometric version\nof the test and prove consistency. This allows us to combine\nand incorporate new information into the test by de\ufb01ning\ndifferent distance metrics. We design the test hypothesis for\nour geometric version via clustering performance. This allows\nus to compare state-of-the-art QHD test performance with\narXiv:2210.03988v1 [q-fin.ST] 8 Oct 2022\n\na version including information about QCD to see if we\ncan improve QHD performance. We also test the hypothesis\nthat we can improve QHD test performance by including\nthe pre-change time-evolution in correlation with two goals:\nto tackle the state-of-the-art limitation of uncorrelated pre-\nchange variables and to see if time-evolution in correlation\ncan improve QHD and QHD+QCD performances.\nII. RELATED LITERATURE\nIn S. Aminikhanghahi and D. Cook [8], we can see there\nare multiple methods and approaches for Time Series CPD.\nAuthors mention supervised methods, such as Decision Trees,\nNaive Bayes, Bayesian Net, SVM, Nearest Neighbor, Hidden\nMarkov Models, and Gaussian Mixture Models. Another group\nincludes unsupervised methods, like Likelihood Ratio Meth-\nods(LR), Subspace Models Methods, Probabilistic Methods,\nKernel-Based Methods, Graph-Based Methods, and Clustering\nMethods. Supervised, Probabilistic, and Clustering methods do\nnot require extra data apart from the \ufb01tting window. Whereas\nLR, Subspace model, Kernel-based methods require to include\npost-change data [8].",
    "chunk_index": 1,
    "start_char": 3021,
    "end_char": 6152,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "Time Series CPD.\nAuthors mention supervised methods, such as Decision Trees,\nNaive Bayes, Bayesian Net, SVM, Nearest Neighbor, Hidden\nMarkov Models, and Gaussian Mixture Models. Another group\nincludes unsupervised methods, like Likelihood Ratio Meth-\nods(LR), Subspace Models Methods, Probabilistic Methods,\nKernel-Based Methods, Graph-Based Methods, and Clustering\nMethods. Supervised, Probabilistic, and Clustering methods do\nnot require extra data apart from the \ufb01tting window. Whereas\nLR, Subspace model, Kernel-based methods require to include\npost-change data [8]. All these methods are focused on uni-\nvariate time series, but our focus is on CPD in correlation\nstructures.\nFrom CPD literature, sequential analysis solutions are the\nmost suited for our problem. QCD tries to identify the closest\nchange point in a sequential setting. For detection to be quick,\nwe need high-dimensional settings with p >> n, with n\ntimestamps and p variables. In A. G. Tartakovsky [9] we \ufb01nd a\ndetailed description of sequential multi-decision for CPD and\nQCD. Changes in statistical properties of distributions from\npreviously identical populations distributions are monitored\nto detect CPD, subject to lower levels of false alarms and\ndelays. Methodologies rely on two standardized methods and\ntheir variants, the Page Cumulative SUM (CUSUM), and\nthe Shiryaev-Roberts. Both had been introduced by the same\nauthor in A. G. Tartakovsky [9]. The problem with this setup is\nthat [4], stream independence is assumed but we want to detect\nchanges in the level of streams\u2019 dependence, and the setting is\nnot high-dimensional. Alternatives that can tackle dependence\nin the sequential analysis literature are based on a sub-optimal\ntest, which provides a performance analysis of the test which\nis then used to design the test by choosing thresholds. The\nef\ufb01ciency of these tests is veri\ufb01ed by simulations [4]. However,\nthese solutions are nonoptimal, not high-dimensional, and not\nfocused on correlation structures. To cope with these three\naspects, we need to focus on work developed by T. Banerjee\nand A. Hero [5], T. Banerjee et al. [4], and A. Hero and B.\nRajaratnam [6], [7].\nAs a preamble, V. Veeravalli and T.Banerjee [10] focused\n\ufb01rst on CPD in a parametric setting, on Bayesian CPD trying\nto minimize Average Detection Delay (ADD) subject to a\nconstraint in the Probability of False Alarm (PFA). They\nalso focused on a second solution, Minimax CPD, in which\nLorden\u00b4s test from G. Lorden et al. [11] is applied. Lorden\ndeveloped the \ufb01rst minimax theory for delays in CPD, \u201din\nwhich he proposed a measure of detection delay obtained\nby taking the supremum (over all possible change points)\nof a worst-case delay over all possible realizations of the\nobservations, conditioned on the change point\u201d [10]. Lorden\u2019s\ntest is important to understand their posterior work.\nIn [6], a discovery is a correlation above a threshold, they\nderive an asymptotic expression for the mean number of\ndiscoveries that is a function of the number of samples. It is\nshown that the mean number of discoveries is in\ufb02uenced by the\npopulation covariance matrix by the Bhattacharyya measure\nof the average pairwise dependency of the p-multivariate U-\nscores de\ufb01ned on the (n-2)-dimensional hypersphere [6].",
    "chunk_index": 2,
    "start_char": 5582,
    "end_char": 8853,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "delay over all possible realizations of the\nobservations, conditioned on the change point\u201d [10]. Lorden\u2019s\ntest is important to understand their posterior work.\nIn [6], a discovery is a correlation above a threshold, they\nderive an asymptotic expression for the mean number of\ndiscoveries that is a function of the number of samples. It is\nshown that the mean number of discoveries is in\ufb02uenced by the\npopulation covariance matrix by the Bhattacharyya measure\nof the average pairwise dependency of the p-multivariate U-\nscores de\ufb01ned on the (n-2)-dimensional hypersphere [6]. Un-\nder weak dependency assumptions, the number of discoveries\nis asymptotically represented by a Poisson distribution. For\nauto-correlation and cross-correlation discoveries this Poisson\ndistribution is measured by the number of positive vertex\ndegrees in the associated sample correlation graph [6]. In [7],\nthey focus on hub discovery in partial correlation graphs, and\nan extension for variables with a speci\ufb01c degree of connectiv-\nity. A hub is de\ufb01ned broadly as any variable that is correlated\nwith at least \u03b4 other variables having a magnitude correlation\nexceeding \u03c1. Their setup is the \ufb01rst high-dimensional of its\nkind. They show that the count N\u03b4,\u03c1p of the number of groups\nof \u03b4 mutually coincident edges in the correlation graph (and\npartial) with correlation threshold \u03c1 converges to a Poisson\nvariable:\nP(N\u03b4,\u03c1p > 0) \u2192exp(\u2212\u039b/\u03d5(\u03b4))\n(1)\nThis had implications for future work developed in [4], [5].\nIn [5], authors introduce a nonparametric QCD test for large-\nscale random matrices based on a global summary statistic\nfrom asymptotic properties of RMT. It is assumed pre- and\npost- change distributions of the i.i.d random matrices rows\nare unknown or belong to an elliptically contoured family [5].\nIf pre- and post- change densities f 0\nX and f 1\nX are known,\nand the mean \u00b5m is constant before and after the change,\nalgorithms such as Cumulative Sum (CumSum) or Shiryaev-\nRoberts (SR) can be ef\ufb01ciently used as both have optimal\nproperties respect to Lorden formulations. In this case, it\nis a parametric CPD problem, and asymptotically optimally\nsolved by Generalized Likelihood Ratio (GLR) tests. For [5],\npre- and post- change densities are unknown and present an\noptimal nonparametric solution, as an asymptotically optimal\nsolution to the minimax CPD in the random matrix setup using\nlarge-scale RMT. The framework in [5] is suited for high-\ndimensional settings. Therefore, a summary to justify why we\nfocus on nonparametric methods with RMT like [4], [5]:\n\u2022 Pre- and post- change densities f 0\nX and f 1\nX are NOT\nknown, and \u00b5m is NOT constant before and after the\nchange.\n\u2022 Need to focus on high-dimensional settings p >> n for\nQCD and QHD in correlation structures.\n\u2022 Other CPD/QHD methods are rejected too because cannot\ntackle dependence.\n\nIII. FRAMEWORK DESCRIPTION\nWe now explain the approach in [4], [5], how the densities\nof the summary statistics are obtained and used to test for\nQCD and QHD.\nA. QCD\nGiven an elliptically distributed random data matrix X, such\nthat, X = [X1, .",
    "chunk_index": 3,
    "start_char": 8279,
    "end_char": 11362,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "and \u00b5m is NOT constant before and after the\nchange.\n\u2022 Need to focus on high-dimensional settings p >> n for\nQCD and QHD in correlation structures.\n\u2022 Other CPD/QHD methods are rejected too because cannot\ntackle dependence.\n\nIII. FRAMEWORK DESCRIPTION\nWe now explain the approach in [4], [5], how the densities\nof the summary statistics are obtained and used to test for\nQCD and QHD.\nA. QCD\nGiven an elliptically distributed random data matrix X, such\nthat, X = [X1, . . . , Xp] =\nh\nXT\n(1), . . . , XT\n(n)\niT\n.\nXi = [X1i, . . . , Xni]T , is the ith column and, X(i) =\n[Xi1, . . . , Xip], is the ith row. The sample covariance matrix\nis:\nS =\n1\nn \u22121\nn\nX\ni=1\n(X(i) \u2212\n\u00afX)\nT (X(i) \u2212\n\u00afX)\n(2)\n\u00afX is the sample mean of the n rows of X. The sample\ncorrelation matrix is:\nR = D\u22121/2\nS\nS D\u22121/2\nS\n(3)\nwith DS the diagonal matrix of S. Rij, the element in the ith\nrow and jth column of the matrix R, is the sample correlation\ncoef\ufb01cient between the ith and jth columns of X.\nThen, d(k)\nNN(i) is the k-nearest neighbor of the ith column of\nX in correlation distance: d(k)\nNN(i) = kth largest order statistic\nof {|Rij| ; j \u0338= i}. In other words, is the k-nearest neighbor in\ncorrelation distance of the variable ith from the data matrix\nX, with Xi = [X1i, ..., Xni]T , and n samples.\nThen the summary statistic is, for a \ufb01xed k [5]:\nVk(X) = max\ni\nd(k)\nNN(i)\n(4)\nThe distribution of the statistic Vk can be related to the\ndistribution of an integer value random variable N\u03b4,\u03c1. This\nnumber is the total number of hubs in the correlation graph\nGp(R), a hub being a vertex with degree \u03b4, if \u03b4i \u2265\u03b4 for\nvertex i with degree \u03b4i in the correlation graph Gp(R). For a\nthreshold correlation \u03c1 \u2208[0, 1], Gp(R) is the correlation graph\nassociated with correlation matrix R, as an undirected graph\nwith p vertices, each being the columns of data matrix X.\nAn edge is present between vertices i and j if the magnitude\nof the sample correlation coef\ufb01cient between the ith and jth\ncomponents of the random vector X is greater than \u03c1, |Rij| \u2265\n\u03c1, i \u0338= j [5]. N\u03b4,\u03c1 is the total number of hubs in the correlation\ngraph, N\u03b4,\u03c1 = card {i : \u03b4i \u2265\u03b4}. The events, V\u03c1(X) \u2265\u03c1, and,\nN\u03b4,\u03c1 > 0, are equivalent. And so we have [5]:\nP(V\u03c1(X) \u2265\u03c1) = P(N\u03b4,\u03c1 > 0)\n(5)\nIn a high-dimensional setting p \u2192\u221efor n \ufb01xed, P(N\u03b4,\u03c1 >\n0) is asymptotically approximated by relating N\u03b4,\u03c1 to a Pois-\nson distribution. The proof is in [7]. Using this theorem, the\nlarge p distribution of Vk de\ufb01ned in (5) can be approximated,\nfor k = \u03b4, by:\nP(V\u03b4(X) \u2264\u03c1) = exp(\u2212\u039b(\u03c1) JX/\u03c6(\u03b4)), \u03c1 \u2208[0, 1]\n(6)\nFor",
    "chunk_index": 4,
    "start_char": 10896,
    "end_char": 13413,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "\u2265\u03b4}. The events, V\u03c1(X) \u2265\u03c1, and,\nN\u03b4,\u03c1 > 0, are equivalent. And so we have [5]:\nP(V\u03c1(X) \u2265\u03c1) = P(N\u03b4,\u03c1 > 0)\n(5)\nIn a high-dimensional setting p \u2192\u221efor n \ufb01xed, P(N\u03b4,\u03c1 >\n0) is asymptotically approximated by relating N\u03b4,\u03c1 to a Pois-\nson distribution. The proof is in [7]. Using this theorem, the\nlarge p distribution of Vk de\ufb01ned in (5) can be approximated,\nfor k = \u03b4, by:\nP(V\u03b4(X) \u2264\u03c1) = exp(\u2212\u039b(\u03c1) JX/\u03c6(\u03b4)), \u03c1 \u2208[0, 1]\n(6)\nFor \u03c1 > 0 and large p, V\u03b4 has density:\nfV (\u03c1) = \u2212\u039b\n\u2032(\u03c1)\n\u03c6(\u03b4) JX exp(\u2212\u039b(\u03c1)\n\u03c6(\u03b4) JX), \u03c1 \u2208(0, 1]\n(7)\nIf \u03b4 = 1 \ufb01xed, V\u03b4 reduces to the nearest neighbor (correla-\ntion) distance:\nV (X) = max\ni\u0338=j |Rij|\n(8)\nand the density reduces to:\nfV (\u03c1; J) = C\n2 (1 \u2212\u03c12)\nn\u22124\n2\nJ exp(\u2212C\n2 JT(\u03c1)), \u03c1 \u2208(0, 1]\n(9)\nThe QCD test is performed by computing the sequence\nof global summary statistics {V\u03b4(X(m))}m\u22651 from the data\nmatrix sequence. J0 and J1 are the values of J before and\nafter the change. If the pre-change dispersion matrix \u03a30 is\ndiagonal, J0 = 1. The QCD test takes the form, de\ufb01ned by\nstopping time \u03c4G:\n\u03c4G = inf\nm\u22651\n(\nmax\n1\u2264l\u2264m\nsup\nJ:|J\u22121|\u2265\u03f5\nm\nX\ni=l\nlog fV (V (i); J)\nfV (V (i); 1) > A\n)\n(10)\nThe parameter A is a threshold to control for the false alarm\nrate, \u03f5 is the minimum magnitude of change in correlation.\nB. QHD\nIn [4], for a nondiagonal covariance matrix \u03a3 with correla-\ntion coef\ufb01cients \u03c1ki:\nVk(\u03a3) = max\ni\u0338=k |\u03c1ki|,\nk \u2208{1, . . . , p}\n(11)\nis the maximum magnitude correlation coef\ufb01cient for the kth\nvariable. Hubs are de\ufb01ned as:\nH =\n\u001a\nk: Vk =\nmax\n1\u2264j\u2264p Vj\n\u001b\n(12)\nThe local summary statistics are de\ufb01ned:\nVk(X) = Vk(R) = max\ni\u0338=k |Rki| ,\nk \u2208{1, . . . , p}\n(13)\nThe large p distribution of Vk can be approximated:\nP(Vk(X) \u2264\u03c1) \u223cexp(\u2212\u039bk,\u03c1) = exp(\u2212(p \u22121)P0(\u03c1)Jk),\n\u03c1 \u2208[0, 1]\n(14)\nFor each k, the density fVk is a member of a one-parameter\nexponential family with Jk as the unknown parameter. We\nhave for \u03c1 \u2208[0, 1], the exponential family form of the density\nfV with parameter Jk:\nfV (\u03c1; Jk) = 2(p \u22121)Jk(1 \u2212\u03c12)\nn\u22124\n2\nB((n \u22122)/2, 1/2)\nexp(\u2212(p \u22121)JkP0(\u03c1))\n(15)\n\nFig. 1. Jk values for 100 variables. Adapted from [4]\nthe variable V tends to take on higher values as the param-\neter Jk increases [4]. The QHD is reduced to the following\nGLR-based family of QCD tests:\n\u03c4V = min\nk \u03c4 (k)\nV\n= min\n\u001a\nm :\nmax\n1<k<p Gk(m) > Av\n\u001b\n(16)\nwhere:\nGk(m) = max\n1\u2264l\u2264m\nsup\nJk:|Jk\u22121|\u2265\u03f5v\nm\nX\ni=l\nlog\n\u0012fV (Vk (i) ; Jk)\nfV (Vk (i) ; 1)\n\u0013\n(17)\nthe test \u03c4V can be used to detect if the parameter of any\nof the p variables is affected by the change [4].",
    "chunk_index": 5,
    "start_char": 12997,
    "end_char": 15430,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "The QHD is reduced to the following\nGLR-based family of QCD tests:\n\u03c4V = min\nk \u03c4 (k)\nV\n= min\n\u001a\nm :\nmax\n1<k<p Gk(m) > Av\n\u001b\n(16)\nwhere:\nGk(m) = max\n1\u2264l\u2264m\nsup\nJk:|Jk\u22121|\u2265\u03f5v\nm\nX\ni=l\nlog\n\u0012fV (Vk (i) ; Jk)\nfV (Vk (i) ; 1)\n\u0013\n(17)\nthe test \u03c4V can be used to detect if the parameter of any\nof the p variables is affected by the change [4]. \u03f5V represents\nthe minimum magnitude of change, away from Jk = 1, that\nthe user wishes to detect.Av is the threshold to control for\nfalse alarms and delays. Finally, for the joint detection and\nhub discovery, the authors combine local (QHD) and global\n(QCD) tests solutions with the maximum stopping times:\n\u03c4HB = max {\u03c4V , \u03c4U}\n(18)\nIf, Dk(X(1), . . . , X(\u03c4HB)), is the binary decision variable\nwith 1 if k is a hub. The rule for hub discovery is [4], for a\n\ufb01x positive integer q:\nDk(X(1), . . . , X(\u03c4HB)) = I{Gk(\u03c4HB) is top q statistic} (19)\nC. Geometric QCD and QHD via clustering\nIn this section, we focus on our contribution, which consists\nof a geometric version of the tests via clustering. For that, we\nprove consistency and design test hypotheses.\nLemma III.1. The times with maximum parameter J and\nmaximum density of the global summary statistic converges in\nprobability, and more if J is closer to its maximum. These times\ncoincide in probability with the optimal solution for QCD in\ncorrelation structures:\nFig. 2. Gk values at the time of stopping \u03c4HB for 100 variables. Adapted\nfrom [4]\n\u0010\nmax\ni\n(J(i))\np\u2212\u2192max\ni\nfV (V (i); J)\n\u0011\n| J \u2192max\nJ\n(J(i)),\n\u2200i = 1, . . . , m\n(20)\nProof: The proof can be found in Appendix A\nThe same applies for QHD test for both, Jk and Gk. Cluster-\ning variables based on the value of Jk and Gk, or equivalently\nmaximum density values (fV (\u03c1; Jk), is statistically consistent\nwith tests in [4], [5]. From Figures 1 and 2, we can see that\n\u201dwhat is clear is that high Gk values at the time of stopping\ncorrespond to high Jk values of the summary statistics, and\nthese values together have signi\ufb01cantly higher magnitude than\nthe values for the rest of the variables. This motivates the\npick-the-top approach. In fact, in practice, one can plot the\nGk values this way and identify all the variables that have\nexperienced a change in correlation\u201d as stated in [4].\nLemma III.2. The rule for joint detection and hub discovery\nin the geometric test is given by the following decision\nvariable:\nDk(X(1), . . . , X(\u03c4HB)) = I{k\u2208Sj(\u03c4HB)},\n{k : {Gk(\u03c4HB) is top q statistic}; j = 1, . . . , w} \u2282\n\u2282Sj(\u03c4HB)\n(21)\nwith change point given in this case by:\n\u03c4HB = {\u03c4U \u2228\u03c4V }\n(22)",
    "chunk_index": 6,
    "start_char": 15102,
    "end_char": 17625,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "in correlation\u201d as stated in [4].\nLemma III.2. The rule for joint detection and hub discovery\nin the geometric test is given by the following decision\nvariable:\nDk(X(1), . . . , X(\u03c4HB)) = I{k\u2208Sj(\u03c4HB)},\n{k : {Gk(\u03c4HB) is top q statistic}; j = 1, . . . , w} \u2282\n\u2282Sj(\u03c4HB)\n(21)\nwith change point given in this case by:\n\u03c4HB = {\u03c4U \u2228\u03c4V }\n(22)\n\nProof: We apply lemma III.1 to Gk:\n\u2200i1 \u0338= i2 \u2203k :\n\u0014\u0012\n\u03c4v\np\u2212\u2192\n\u0014\nmax\nk\n(Gk(i2))\n\u0015\u0013\n\u21ce\n\u0012\n\u03c4v\np\u2212\u2192\n\u0014\nmax\nk\n(Gk(i1))\n\u0015\u0013\u0015\n\u21d2[i1 = i2 = i, i \u2208{\u03c4v, \u03c4U, t}] ,\n[{t \u0338= \u03c4v} \u2227{t \u0338= \u03c4U} : i = {\u03c4v \u2228\u03c4U \u2228t}]\n\u21d2[\u03c4HB = i : i = {\u03c4v \u2228\u03c4U}]\n(23)\nLemma III.3. Geometric test consistency via clustering:\nA\n=\n1\nnT\nPn\nk=1\nPT\nm=1 Dk(X(1), . . . , X(\u03c4HB = m))\n1\nnT\nPn\nk=1\nPT\nm=1 I {k\u2208Sj(HB); \u03c4HB=m}\n,\nA\np\u2212\u21921 as T \u2192\u221e; A \u21921 as T, Gk, Jk \u2192\u221e,\n\u2200k = 1, . . . , n, \u2200Sj , j = 1, . . . , w,\n\u03c4HB = {\u03c4U(m) \u2228\u03c4V (m)} , m = 1, . . . , T\n(24)\nProof: We apply lemmas III.1 and III.2 and the Law of\nLarge Numbers\nLemma III.4. Test hypothesis for geometric QHD comparison\nis based on clustering performance.\nProof:\n\u2200{a1, . . . , ap} \u2208Hm,\nCPL \u2261{j : {as : Gs(\u03c4HB) is top 1 statistic } \u2282Sj} ,\n\u2200m = 1, . . . , T, {\u2200\u03c4HB| {\u03c4HB = {\u03c4U \u2228\u03c4V } , \u03c4HB \u2208m}},\n\u2200Sj , j = 1, . . . , w, \u2200p \u2264n :\nLPM(\u03c4HB) =\nPp\ni=1 I{ ai\u2282Sj | j=CP L}\np\nC(\u03c4HB) =\nm\nX\ni=1\nI{\u03c4HB\u2261i}\nMCP({1, . . . , m}) =\n1\nC(\u03c4HB)\nC(\u03c4HB)\nX\ni=1\nLPM(\u03c4HB(i))\n(25)\nLocal Performance Metric (LPM) is the clustering perfor-\nmance at \u03c4HB. Mean Clustering Performance (MCP) is the\naverage performance over all \u03c4HB in a sample with n variables,\nm timestamps, and hubs Hm of p variables.\nDe\ufb01nition III.1. The QCD + QHD distance metric between\ntwo variables X and Y at a time m is given by:\n(QCD + QHD)ij(m) = F\n\u0000QHDij(m), gU(U(m); J), \u03b8\n\u0001\n=\n= \u03b81QHDij(m)+\n+\u03b82\n|maxk fV (Vi(k); Ji) \u2212maxk fV (Vj(k); Jj)|\nmaxk gU(U(k); Jk)\n,\n\u2200i, j = 1, . . . , n, i \u0338= j, k = 1, . . . , m; m = 1, . . . , T,\n\u03b81 + \u03b82 = 1\n(26)\nThe Euclidean Distance between two rolling correlations is\nde\ufb01ned as:\nTEij(k) =\nv\nu\nu\nt\nk\nX\nk=t\u2212w\n(rcik \u2212rcjk)2\n\u2200i, j = (p, m) |p \u0338= m,\ni \u0338= j\n(27)\nwith k being the timestamp when the distance is computed,\nbased on previous rolling correlation time series. i and j the\ntuples of variables p and m.\nDe\ufb01nition III.2. The Diversi\ufb01cation Measure Distribution\n(DMD) is a node degree distribution of a composite network.",
    "chunk_index": 7,
    "start_char": 17293,
    "end_char": 19525,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": ". , T,\n\u03b81 + \u03b82 = 1\n(26)\nThe Euclidean Distance between two rolling correlations is\nde\ufb01ned as:\nTEij(k) =\nv\nu\nu\nt\nk\nX\nk=t\u2212w\n(rcik \u2212rcjk)2\n\u2200i, j = (p, m) |p \u0338= m,\ni \u0338= j\n(27)\nwith k being the timestamp when the distance is computed,\nbased on previous rolling correlation time series. i and j the\ntuples of variables p and m.\nDe\ufb01nition III.2. The Diversi\ufb01cation Measure Distribution\n(DMD) is a node degree distribution of a composite network.\nThe \ufb01rst subnetwork is an unweighted network of distance\nlabels from a \ufb01ltered distance matrix of rolling correlations\nwith a closeness threshold. Nodes are the rolling correlations\npair variables and edge the distances labels. The second\nsubnetwork (the asset variables network) is an unweighted\nnetwork of rolling correlations labels that are the result of the\n\ufb01rst subnetwork, with nodes the individual assets and edges\nthe labels of the rolling correlations.\nLemma III.5. The link between both subnetworks is given\nby the Bayes Theorem. The conjugacy by Bayes Theorem\nshows, as a result, the Dirichlet-Multinomial distribution, a\ncompound distribution which is the degree distribution of\nthe compounded network, from two distributions which are\nthe degree distributions of each subnetwork (a subnetwork\nof rolling correlations through time with the Categorical\ndistribution and the subnetwork of asset variables with the\nDirichlet distribution).\nProof: The proof can be found in Appendix B.\nP (\u03b8 |D) \u223cP (D|\u03b8) P (\u03b8) =\nn\nY\ni=1\nm\nY\nk=1\n\u03b8rck+ ak\u22121\nk\n\u223c\n\u223cDir (\u03b8 |rc + a) = Dir (d \u2264\u03a6 |rc + a) ,\nrck = I [rci = k1] + I [rci = k2] ,\nak = I [ai = k11] + I [ai = k12] + I [ai = k21] + I [ai = k22]\n(28)\nwhere ai, is the asset variable i, d is the distance TE from\n(27) on each iteration, n is the number of asset variables in the\nsample, m is the number of correlations associated with the\nvariables dataset. k11, . . . , k22 are the different names of the\nvariables that are part of the two pairs of rolling correlations\nfor distances d = TEk1k2(k), with k1 and k2 correlation labels\n(pair of variables). Equation (28) is the distribution that counts\nthe number of times ai appears in any of k11, . . . , k22, for\ndistances d = TEk1k2(k) below a closeness threshold \u03a6, or\nrolling correlations with similar time-evolution.\nDe\ufb01nition III.3. The similarity metric in terms of time-\nevolution of correlation is given by the difference in DMD:\n\nFig. 3. DMD for a particular timestamp. DMD in vertical axis and \ufb01nancial\nassets in horizontal axis\nDDij(m) = (P(d \u2264\u03a6|ai, rck) \u2212P(d \u2264\u03a6|aj, rcp)) =\n\uf8ee\n\uf8f0\n n\nY\ni=1\nm\nY\nk=1\n\u03b8rck+ ak\u22121\nk\n!\n\u2212\n\uf8eb\n\uf8ed\nn\nY\nj=1\nm\nY\np=1\n\u03b8rcp+ ap\u22121\np\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb,\n\u2200i, j = 1, . . . , N, i \u0338= j, k = 1, .",
    "chunk_index": 8,
    "start_char": 19087,
    "end_char": 21725,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "evolution of correlation is given by the difference in DMD:\n\nFig. 3. DMD for a particular timestamp. DMD in vertical axis and \ufb01nancial\nassets in horizontal axis\nDDij(m) = (P(d \u2264\u03a6|ai, rck) \u2212P(d \u2264\u03a6|aj, rcp)) =\n\uf8ee\n\uf8f0\n n\nY\ni=1\nm\nY\nk=1\n\u03b8rck+ ak\u22121\nk\n!\n\u2212\n\uf8eb\n\uf8ed\nn\nY\nj=1\nm\nY\np=1\n\u03b8rcp+ ap\u22121\np\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb,\n\u2200i, j = 1, . . . , N, i \u0338= j, k = 1, . . . , M, p = 1, . . . , M,\nk \u0338= p\n(29)\nWe standardize the distribution with respect to the sample\nof 1500 timestamps and show the distribution for a particular\ntimestamp in Figure 3, with assets in the horizontal axis. The\nhigher the bar for an asset, the less diversi\ufb01ed is in terms\nof time-evolution of correlation. We show the distribution in\ntime for the 1500 timestamps in Figure 4. The Test Hypothesis\ndistance metric is given by:\nTHij(k) = G\n\u0010\n(QCD + QHD)ij(k), DDij(k), \u03b3\n\u0011\n=\n\u03b31(QCD + QHD)ij(k) + \u03b32DDij(k),\n\u2200i, j = 1, . . . , N, i \u0338= j, k = 1, . . . , M,\n\u03b31 + \u03b32 = 1, \u03b81 + \u03b82 = \u03b31\n(30)\nGiven a set of variables k, k = {1, ..., N}, candidates\nsolutions for QHD in correlations, we aim to partition the\nk into w(\u0338= k) sets S = {S1, .., Sw } so as to minimize the\nwithin-cluster sum of squares. The objective is, for a sample\nof time stamps m, m = {1, ..., T}, to run sequentially the\nfollowing algorithm:\nmin\nS(m)\nw\nX\nj=1\n1\n|Sj(m)|\nT\nX\np=1\nX\nk1,k2 \u2208Sj(m)\n(TH)k1,k2(m),\n\u2200k1, k2 \u2208k; k1 \u0338= k2| [( k1 \u2208x1) \u2228( k2 \u2208x1)] \u2227\n\u2227[( k1 \u2208x2) \u2228( k2 \u2208x2)]\n(31)\nFig. 4.\nDMD for a sample of 1500 timestamps. The two horizontal axis\nare the categories (\ufb01nancial assets;right axis) and the timestamps with dates\n(hourly time series;left axis)\nIV. NUMERICAL RESULTS\nTest hypotheses are carried on \ufb01nancial asset correlations\nhourly time series from a dataset of 38 assets of different\nclasses and 3500 hourly prices. Tables I and II show uni-\nsample test hypotheses for different model con\ufb01gurations. We\ncompare QHD, QHD+QCD, and QHD+QCD+DD, with labels\n1, 2, and 3 respectively for the Tables. Each row represents\na different model con\ufb01guration, with an array of parameters\n(second column). We compute the mean (MCP), minimum,\nand standard deviation of the LPM for a 100-hours sample. In\nTable III, we show test hypotheses from multiple 100-hours\nsamples. Model con\ufb01guration is given as: Jk, threshold Ak,\n\u03b8 and \u03b3 the distance metric parameters in (30), K number of\nclusters, \u03a6 the time-evolution threshold, in this order.\nTABLE I\nUNI-SAMPLE TEST HYPOTHESES.",
    "chunk_index": 9,
    "start_char": 21399,
    "end_char": 23771,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "sample. In\nTable III, we show test hypotheses from multiple 100-hours\nsamples. Model con\ufb01guration is given as: Jk, threshold Ak,\n\u03b8 and \u03b3 the distance metric parameters in (30), K number of\nclusters, \u03a6 the time-evolution threshold, in this order.\nTABLE I\nUNI-SAMPLE TEST HYPOTHESES. UNCUT DATASET\nLabel\nJk Ak \u03b8 \u03b3 K \u03a6\nMean\nMin\nStd\n1\n2 0\n1\n0\n5 2\n75,00%\n44%\n21%\n3\n2 0 0.95 0.10 5 2\n81,94%\n44%\n21%\n3\n2 0 0.98 0.05 5 2\n83,80%\n39%\n22%\n3\n2 0 0.98 0.05 5 4\n92,67%\n63%\n15%\n1\n2 3\n1\n0\n5 2\n89,23%\n46%\n22%\n3\n2 3 0.98 0.05 5 4\n96,59%\n50%\n11%\n1\n5 0\n1\n0\n5 2\n74,39%\n43%\n24%\n2\n5 0 0.02\n0\n5 2\n87,83%\n33%\n23%\n1\n5 3\n1\n0\n5 2\n92,22%\n30%\n22%\n2\n5 3 0.10\n0\n5 2\n92,59%\n33%\n21%\n3\n5 3 0.02 0.05 5 2\n96,22%\n36%\n15%\n3\n5 3 0.95 0.05 5 4\n97,58%\n78%\n7%\n1\n10 3\n1\n0\n5 2\n92,31%\n62%\n15%\n2\n10 3\n0.50 0\n5 4\n93,55%\n68%\n13%\n3\n10 3 0.95 0.05 5 4\n95,38%\n31%\n17%\n1\n100 0\n1\n0\n5 2\n85,38%\n67%\n13%\n3\n100 0 0.98 0.10 5 2\n85,94%\n43%\n22%\n1\n100 3\n1\n0\n5 2\n98,08%\n92%\n3%\n1\n1000 0\n1\n0\n5 2\n97,44%\n92%\n4%\n1\n1000 3\n1\n0\n5 2\n100,00%\n100%\n0%\n\nTABLE II\nUNI-SAMPLE TEST HYPOTHESES. CUT DATASET\nLabel\nJk Ak \u03b8 \u03b3 K \u03a6\nMean\nMin\nStd\n1\n2 0\n1\n0\n5 2\n85,71%\n50%\n23%\n2\n2 0\n0.98 0\n5 2\n91,87%\n59%\n16%\n3\n2 0 0.50 0.15 5 4\n92,53%\n55%\n17%\n1\n2 3\n1\n0\n5 2\n91,95%\n52%\n18%\n3\n2 3 0.98 0.05 5 2\n92,26%\n61%\n15%\n3\n2 3 0.50 0.15 5 4\n94,25%\n66%\n13%\n1\n5 0\n1\n0\n5 2\n88,51%\n31%\n26%\n2\n5 0\n0.95 0\n5 2\n89,10%\n46%\n20%\n3\n5 0 0.98 0.05 5 2\n90,21%\n50%\n19%\n3\n5 0 0.95 0.05 5 2\n91,67%\n33%\n22%\n1\n5 3\n1\n0\n5 2\n95,40%\n72%\n10%\n2\n5 3 0.95 0\n5 2\n96,25%\n50%\n11%\n3\n5 3 0.98 0.05 5 2\n96,30%\n70%\n10%\n1\n10 0\n1\n0\n5 2\n90,80%\n45%\n21%\n1\n10 3\n1\n0\n5 2\n94,83%\n69%\n12%\n3\n10 3 0.95 0.02 5 4\n95,00%\n55%\n14%\n3\n10 3 0.98 0.05 5 4\n95,37%\n44%\n15%\n1\n1000 0\n1\n0\n5 2\n100,00%\n100%\n0%\n1\n1000 3\n1\n0\n5 2\n100,00%\n100%\n0%\nTABLE III\nMULTIPLE-SAMPLE TEST HYPOTHESES (CUT)\nLabel\nJk Ak \u03b8 \u03b3 K \u03a6\nMean\nMin\nStd\n1\n2 0\n1\n0\n5 2\n88,81%\n47%\n19%\n2\n2 0 0.98 0\n5 2\n89,27%\n45%\n21%",
    "chunk_index": 10,
    "start_char": 23490,
    "end_char": 25313,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "2\n94,83%\n69%\n12%\n3\n10 3 0.95 0.02 5 4\n95,00%\n55%\n14%\n3\n10 3 0.98 0.05 5 4\n95,37%\n44%\n15%\n1\n1000 0\n1\n0\n5 2\n100,00%\n100%\n0%\n1\n1000 3\n1\n0\n5 2\n100,00%\n100%\n0%\nTABLE III\nMULTIPLE-SAMPLE TEST HYPOTHESES (CUT)\nLabel\nJk Ak \u03b8 \u03b3 K \u03a6\nMean\nMin\nStd\n1\n2 0\n1\n0\n5 2\n88,81%\n47%\n19%\n2\n2 0 0.98 0\n5 2\n89,27%\n45%\n21%\n3\n2 0 0.95 0.01 5 2\n89,50%\n47%\n20%\n3\n2 0 0.98 0.05 5 2\n90,27%\n46%\n20%\n1\n2 3\n1\n0\n5 2\n97,53%\n63%\n9%\n3\n2 3 0.98 0.05 5 2\n96,97%\n63%\n10%\n3\n2 3 0.02 0.05 5 2\n97,92%\n63%\n8%\n1\n5 0\n1\n0\n5 2\n86,02%\n43%\n21%\n2\n5 0 0.02 0\n5 2\n87,16%\n37%\n22%\n1\n5 3\n1\n0\n5 2\n93,73%\n44%\n16%\n2\n5 3 0.02 0\n5 2\n94,59%\n47%\n15%\n3\n5 3 0.02 0.05 5 2\n96,27%\n50%\n13%\n1\n10 0\n1\n0\n5 2\n82,85%\n42%\n23%\n1\n10 3\n1\n0\n5 2\n92,36%\n40%\n17%\n2\n10 3\n0.02\n0\n5 2\n96,24%\n50%\n13%\n3\n10 3 0.02 0.05 5 2\n96,24%\n50%\n13%\nDue to limited space, we have included enough representa-\ntive results to verify all the hypotheses and empirically show\ngeometric test consistency. In Tables I, II and III, we see test\nconsistency with performance increasing with Jk and Ak, and\nreaching 100% for Jk between 100 and 1000. The hypothesis\nthat QHD+QCD improves QHD is veri\ufb01ed (label 2 > 1) for\na range of parameters. The same applies for the hypothesis\nthat the previous time-evolution of correlation improves QHD\nand QHD+QCD performances (label 3 > 2 > 1). In Table III,\nwe see that all hypotheses are veri\ufb01ed for multiple sampling\nwith a \ufb01xed model con\ufb01guration. Distance metric parameters\nwere not scaled but could be which explains values close to 0\nand 1 (> 100 orders of magnitude). The best performance\nis for \u03a6\n=\n4, with a less restrictive threshold for the\nrolling correlation network closeness in DMD. Results are not\nsensitive to the number of clusters, and we show them for\nK=5. Different asset classes have different market opening and\nclosing hours that could have an impact on our analysis. For\nthat, we use a cut version in Table II and III, with no missing\nvalues, and the uncut version with the full dataset in Table I.\nPerformance patterns are the same, but calibration parameters\nchange. Finally, the clustering algorithm used is Kmedoids.\nV. CONCLUSION\nWe can conclude that it is possible to improve the QHD\nin correlation structures for non-parametric, high-dimensional\nsettings by combining it with QCD in a geometric version\nof the test via clustering. Also, we can improve both, QHD\nand QHD+QCD, by adding the pre-change time-evolution of\ncorrelation to the test with the geometric solution and solving\nthe state-of-the-art issue of uncorrelated pre-change variables.",
    "chunk_index": 11,
    "start_char": 25017,
    "end_char": 27524,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "uncut version with the full dataset in Table I.\nPerformance patterns are the same, but calibration parameters\nchange. Finally, the clustering algorithm used is Kmedoids.\nV. CONCLUSION\nWe can conclude that it is possible to improve the QHD\nin correlation structures for non-parametric, high-dimensional\nsettings by combining it with QCD in a geometric version\nof the test via clustering. Also, we can improve both, QHD\nand QHD+QCD, by adding the pre-change time-evolution of\ncorrelation to the test with the geometric solution and solving\nthe state-of-the-art issue of uncorrelated pre-change variables.\nOur geometric version of the test opens new doors by allowing\nus to include new sources of information. We have proved and\nempirically veri\ufb01ed consistency and test hypothesis designs\nfor the geometric test via clustering. This geometric solution\ncan be applied to other statistical tests with similar properties\nsuch that consistency based on lemmas III.1, III.2 and III.3\ncan be proved. DMD is a new way to represent time-evolution\nof correlation respect individual asset variables. It is a useful\nmeasure of risk diversi\ufb01cation for portfolios to avoid crowded\nrisk positions. We veri\ufb01ed that DMD improves QHD and\nQHD+QCD performances.\nFor future work it would be interesting to analyze the\nasymptotic high-dimensional (p >> n) properties of the DMD\nto come up with local summary statistics like in [4] that\ncould allow for a geometric solution based entirely on high-\ndimensional properties.\nACKNOWLEDGMENT\nThe authors would like to thank Miralta Bank for the\n\ufb01nancial data, cloud services and discussions.\nREFERENCES\n[1] H. Markowitz, \u201cPORTFOLIO SELECTION,\u201d The Journal of Finance,\nvol. 7, no. 1, pp. 77\u201391, mar 1952.\n[2] L. S. Junior and I. D. P. Franca, \u201cCorrelation of \ufb01nancial markets in\ntimes of crisis,\u201d Physica A 391 (2012) 187\u2013208, 2011.\n[3] A. G. Tartakovsky, \u201cMultidecision quickest change-point detection:\nPrevious achievements and open problems,\u201d Sequential Analysis, vol. 27,\nno. 2, pp. 201\u2013231, 2008.\n[4] T. Banerjee and A. O. Hero, \u201cQuickest hub discovery in correlation\ngraphs,\u201d in 2016 50th Asilomar Conference on Signals, Systems and\nComputers.\nIEEE, 2016, pp. 1248\u20131255.\n[5] T. Banerjee, H. Firouzi, and A. O. Hero, \u201cNon-parametric quickest\nchange detection for large scale random matrices,\u201d in 2015 IEEE\nInternational Symposium on Information Theory (ISIT).\nIEEE, 2015.\n[6] A. Hero and B. Rajaratnam, \u201cLarge-scale correlation screening,\u201d Journal\nof the American Statistical Association, vol. 106, no. 496, pp. 1540\u2013\n1552, 2011.\n[7] A. O. Hero and B. Rajaratnam, \u201cHub discovery in partial correlation\ngraphs,\u201d IEEE Transactions on Information Theory, vol. 58, pp. 6064\u2013\n6078, 2012.\n[8] S. Aminikhanghahi and D. J. Cook, \u201cA survey of methods for time series\nchange point detection,\u201d Knowledge and information systems, vol. 51,\nno. 2, pp. 339\u2013367, 2017.\n[9] A. Tartakovsky, \u201cAsymptotic properties of cusum and shiryaev\u2019s proce-\ndures for detecting a change in a nonhomogeneous gaussian process,\u201d\nMathematical Methods of Statistics, vol. 4, 04 1999.\n[10] V. V. Veeravalli and T. Banerjee, \u201cQuickest change detection,\u201d arXiv,\n2012.\n[11] G. Lorden et al., \u201cProcedures for reacting to a change in distribution,\u201d\nThe Annals of Mathematical Statistics, vol. 42, no. 6, pp. 1897\u20131908,\n1971.",
    "chunk_index": 12,
    "start_char": 26922,
    "end_char": 30228,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "Cook, \u201cA survey of methods for time series\nchange point detection,\u201d Knowledge and information systems, vol. 51,\nno. 2, pp. 339\u2013367, 2017.\n[9] A. Tartakovsky, \u201cAsymptotic properties of cusum and shiryaev\u2019s proce-\ndures for detecting a change in a nonhomogeneous gaussian process,\u201d\nMathematical Methods of Statistics, vol. 4, 04 1999.\n[10] V. V. Veeravalli and T. Banerjee, \u201cQuickest change detection,\u201d arXiv,\n2012.\n[11] G. Lorden et al., \u201cProcedures for reacting to a change in distribution,\u201d\nThe Annals of Mathematical Statistics, vol. 42, no. 6, pp. 1897\u20131908,\n1971.\n\nAPPENDIX A\nPROOF OF LEMMA III.1\nProof:\n\u0010\n\u03c4G\np\u2212\u2192\nh\nmax\nJ\n(J(i))\ni\u0011\n| \u2200i = 1, . . . , m\n(32)\n\u0010\n\u03c4G\np\u2212\u2192\nh\nmax\ni\nfV (V (i); J)\ni\u0011\n| J \u2192max\nJ\n(J(i))\n(33)\n\u0010\nJ\np\u2212\u2192\u03b8| \u03b8 = max\ni\n(J(i))\n\u0011\n\u21d0\u21d2\n\u0010\n\u03c4G\np\u2212\u2192\nh\nmax\ni\nfV (V (i); J)\ni\u0011\n(34)\n\u0010\nJ\np\u2212\u2192\u03b8 | \u03b8 = max\ni\n(J(i))\n\u0011\n\u21d0\u21d2\n\u0010\n\u03c4G\np\u2212\u2192\nh\nmax\ni\n(J(i))\ni\u0011\n(35)\n\u0010\nmax\ni\n(J(i))\np\u2212\u2192max\ni\nfV (V (i); J)\n\u0011\n| J \u2192max\nJ\n(J(i)),\n\u2200i = 1, . . . , m\n(36)\nAPPENDIX B\nPROOF OF LEMMA III.5\nProof: Categorical Distribution. RC subnetwork:\nX1, . . . . . . , Xn \u223cCat (\u03b8),\nP (Xi = j | \u03b8j)\n(37)\nD = (rc1, . . . , rcn),\nrci = {1, . . . , n}\n(38)\nWith rc for rolling correlations. The probability of encoun-\ntering a rc label (pair of variables) in a distance of rc through\ntime from a distance matrix with threshold T, can be modeled\nby a categorical distribution:\nP(D |\u03b8) =\nn\nY\ni=1\nP (RCi = rci | \u03b8) =\nn\nY\ni=1\n\u03b8rci =\nn\nY\ni=1\nn\nY\nj=1\nn\nY\np=1j\u0338=p\n(djp \u2264T)I[rci=j]+I[rci=p] =\nn\nY\ni=1\nn\nY\nk1=1\nn\nY\nk2=1k1\u0338=k2\n(dk1k2 \u2264T)I[rci=k1]+I[rci=k2] =\nn\nY\ni=1\nm\nY\nk=k1,k2k1\u0338=k2\n(dk \u2264T)I[rci=k1]+I[rci=k2] =\nn\nY\ni=1\nn\nY\nk=1\n\u03b8k\nrck\n(39)\nWith d, the distance between rolling correlations (27), rc\nthe pair of variable names in each correlation, k the pair of\nrolling correlations names (4 variable names), and I indicator\nfunction.\nDirichlet Distribution. Assets subnetwork:\n\u03b8 \u223cDir (\u03b1),\nP (\u03b8 |\u03b1) \u223c\nn\nY\nj=1\n\u03b8\u03b1j\u22121\nj\nI\n\uf8eb\n\uf8ed\nn\nX\nj=1\n\u03b8j = 1, \u2200\u03b8i\n\uf8f6\n\uf8f8\n(40)\nThe probability of an individual asset being part of the pair\nof variables in a rolling correlation, rc, from a pair of rc in the\ndistance d (27), part of the distance matrix with a threshold\nT, can be modeled by a Dirichlet distribution:\nP (\u03b8 |a) = P (d \u2264T |a) \u223c\nn\nY\ni=1\nn\nY\nk=1\n\u03b8ak\u22121\nk\n=\n=\nn\nY\ni=1\nm\nY\nk=k1,k2;k11\u0338=k12;k21\u0338=k22\n(dk \u2264T)A,\nA = I [ai = k11] + I [ai = k12] + I [ai = k21] +\n+I [ai = k22] \u22121\n(41)\nLetter a, is the individual asset variable name.",
    "chunk_index": 13,
    "start_char": 29661,
    "end_char": 32023,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "of rc in the\ndistance d (27), part of the distance matrix with a threshold\nT, can be modeled by a Dirichlet distribution:\nP (\u03b8 |a) = P (d \u2264T |a) \u223c\nn\nY\ni=1\nn\nY\nk=1\n\u03b8ak\u22121\nk\n=\n=\nn\nY\ni=1\nm\nY\nk=k1,k2;k11\u0338=k12;k21\u0338=k22\n(dk \u2264T)A,\nA = I [ai = k11] + I [ai = k12] + I [ai = k21] +\n+I [ai = k22] \u22121\n(41)\nLetter a, is the individual asset variable name. With d and k\nas before, k1 being the asset names of one rolling correlation,\nk2 the other rc pair of name variables, k11 the name of an\nasset variable that is part of the rolling correlation k1, same\nlogic for other variables and indexes. By Bayes Theorem:\nn\nY\ni=1\nCat (rc | d \u2264T) Dir(d \u2264T |a) = Dir(d \u2264T|rc + a)\n(42)\nwe want to prove the following link, for distributions N1 (for\nsubnetwork 1 of rolling correlations), N2 (subnetwork 2 of\nasset variables), and the compound network, as N1+N2:\nN1 \u223cCat (rc | d \u2264T),\nN2 \u223cDir(d \u2264T |a) ,\nN1 + N2 \u223cDir(d \u2264T|rc + a)\n(43)\nP (a |rc) =\nZ\nP (a |d \u2264T, rc) P (d \u2264T|rc) d (d \u2264T) =\nZ\nP (a |d \u2264T) P (d \u2264T|rc) d (d \u2264T) =\nZ\nP (d \u2264T|a)\nP (a)\nP (d \u2264T)P (rc|d \u2264T) P (d \u2264T)\nP (rc)\nd (d \u2264T)\n=\nZ\nP(d \u2264T| a) P(rc |d \u2264T) P (a)\nP (rc) d (d \u2264T) =\n=\nn\nY\ni=1\nm\nY\nk=k1,k2;k11\u0338=k12;k21\u0338=k22\n(dk \u2264T)A\u00d7\n\u00d7\nn\nY\nk1=1\nn\nY\nk2=1,k1\u0338=k2\n(dk1k2 \u2264T)B =\n= Dir(d \u2264T|rc + a) P (a)\nP (rc),\nA = I [ai = k11] + I [ai = k12] + I [ai = k21] +\n+I [ai = k22] \u22121,\nB = I [rci = k1] + I [rci = k2]\n(44)",
    "chunk_index": 14,
    "start_char": 31681,
    "end_char": 33021,
    "paper_title": "A Clustering Algorithm for Correlation Quickest Hu",
    "paper_category": "q-fin.ST",
    "paper_filename": "A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/A_Clustering_Algorithm_for_Correlation_Quickest_Hu.pdf"
  },
  {
    "text": "Analyzing order \ufb02ows in limit order books\nwith ratios of Cox-type intensities\nIoane Muni Toke\u22171 and Nakahiro Yoshida\u20202\n1Math\u00b4ematiques et Informatique pour la Complexit\u00b4e et les Syst`emes, CentraleSup\u00b4elec,\nUniversit\u00b4e Paris-Saclay, France\n2Graduate School of Mathematical Sciences, University of Tokyo, Japan.\n1,2CREST, Japan Science and Technology Agency, Japan.\nAugust 23, 2019\nAbstract\nWe introduce a Cox-type model for relative intensities of orders \ufb02ows in a limit order book.\nThe model assumes that all intensities share a common baseline intensity, which may for ex-\nample represent the global market activity. Parameters can be estimated by quasi-likelihood\nmaximization, without any interference from the baseline intensity. Consistency and asymptotic\nbehavior of the estimators are given in several frameworks, and model selection is discussed with\ninformation criteria and penalization. The model is well-suited for high-frequency \ufb01nancial data:\n\ufb01tted models using easily interpretable covariates show an excellent agreement with empirical\ndata. Extensive investigation on tick data consequently helps identifying trading signals and\nimportant factors determining the limit order book dynamics. We also illustrate the potential\nuse of the framework for out-of-sample predictions.\nKeywords : order book models; point processes; Cox processes; Hawkes processes; ratio models;\ntrading signals; imbalance; spread.\n1\nIntroduction\nThe limit order book is the central structure that aggregates all orders submitted by market par-\nticipants to buy or sell a given asset on a \ufb01nancial market. Buy o\ufb00ers form the bid side, sell o\ufb00ers\n\u2217Laboratoire MICS et Chaire de Finance Quantitative, CentraleSup\u00b4elec, B\u02c6atiment Bouygues, 3 rue Joliot Curie,\n91190 Gif-sur-Yvette, France. ioane.muni-toke@centralesupelec.fr\n\u2020Graduate School of Mathematical Sciences, University of Tokyo: 3-8-1 Komaba, Meguro-ku, Tokyo 153- 8914,\nJapan. nakahiro@ms.u-tokyo.ac.jp\n1\narXiv:1805.06682v3 [q-fin.ST] 22 Aug 2019\n\nform the ask side. Simpli\ufb01ed representations of possible interactions with a limit order book usually\nconsider three archetypal sorts of orders : limit orders, market orders, and cancellations. Buy (resp.\nsell) limit orders are orders to buy (resp. sell) a given quantity of the asset with a given limit price\nstrictly lower (resp. greater) than the current best ask (resp. bid) quote. Limit orders are stored\nin the book until matched or canceled. Buy (resp. sell) market orders are submitted without any\nlimit price, and are thus matched against the current best ask (resp. bid) orders. Cancellations\nremove a non-executed limit order from the book.\nThe rise of electronic markets, the accelerating rate of trading on \ufb01nancial markets, the de-\nvelopment of optimal trading strategies by brokers, etc., have pushed for better investigation and\nmodeling of limit order books. Chakraborti et al. (2011), Gould et al. (2013) and Abergel et al.\n(2016) provide some overview on recent modeling e\ufb00orts. Cont et al. (2010) has been a seminal\nmodel using Poisson processes. Huang et al.\n(2015) investigate Markovian dynamics dependent\non the size of the queues of the limit order book. Muni Toke & Yoshida (2017) propose a state-\ndependent model that makes orders intensities depend on \ufb01nancial signals such as the bid-ask\nspread.\nA common di\ufb03culty in estimating these models is the necessity to cope with the fact\nthat market activity is highly \ufb02uctuating during the day, at all timescales.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3480,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "et al.\n(2016) provide some overview on recent modeling e\ufb00orts. Cont et al. (2010) has been a seminal\nmodel using Poisson processes. Huang et al.\n(2015) investigate Markovian dynamics dependent\non the size of the queues of the limit order book. Muni Toke & Yoshida (2017) propose a state-\ndependent model that makes orders intensities depend on \ufb01nancial signals such as the bid-ask\nspread.\nA common di\ufb03culty in estimating these models is the necessity to cope with the fact\nthat market activity is highly \ufb02uctuating during the day, at all timescales. Daily market activity\n(for example measured in number or volume of trades, or in number or volume of all orders) is\nknown to globally exhibit a U-shaped pattern, with a lower activity in the middle of the day, and a\nmuch higher activity in the morning after the market opening, and in the afternoon before market\nclose. This seasonality e\ufb00ect is non-smooth: exogenous news (announcements of company results,\nof acquisitions, of macroeconomic indicators, etc.) occur all day long, at random or predetermined\ntimes, and may incur activity bursts. In Europe, openings of American markets are usually fol-\nlowed by some activity increase. It may be quite di\ufb03cult to incorporate such variations in a model.\nIn order to avoid such problems, one may try to remove the most hectic parts of the samples,\nand/or focus on a limited time interval, and/or split trading days in several parts. Examples can\nbe found in, e.g., the growing literature on Hawkes processes in \ufb01nance: Bacry et al. (2012) limits\ntests on their estimation procedure to two-hour periods to avoid seasonality e\ufb00ects; Lallouache &\nChallet (2016) uses time-dependent piecewise-linear baseline intensity (with nodes spaced every few\nhours). However, even at these scales, global market activity varies and may limit the reliability of\nestimation procedures.\nIn this work we continue previous investigations on the in\ufb02uences of \ufb01nancial variables (state\nof the order book or other trading signals) on the processes of order submissions.\nWe assume\nthat orders submissions are modeled by point processes with Cox-like intensities depending on\ngiven covariates.\nHowever, we do not directly try to model and fully estimate each and every\nintensities of the model, as in e.g. Muni Toke & Yoshida (2017), but we rather try to estimate\nthe relative in\ufb02uences of given covariates on the intensities.\nTo this end we assume that their\nexists a possibly random baseline intensity that is common to all processes under investigation,\n2\n\nand that could for example incorporate the seasonality e\ufb00ects and changing activities that we have\ndescribed. By dealing with ratios of intensities, we can then remove this baseline intensity from\nour estimation procedure, and thereby obtain a model that focuses only on the \ufb01nancial covariates\nunder investigation.\nIn Section 2 we describe the general model of ratios of intensities. In Section 3 we show that\nthe quasi-likelihood estimators of the model are consistent and asymptotically normal. Section\n4 discusses the method with respect to information criteria and penalization.",
    "chunk_index": 1,
    "start_char": 2931,
    "end_char": 6046,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "that we have\ndescribed. By dealing with ratios of intensities, we can then remove this baseline intensity from\nour estimation procedure, and thereby obtain a model that focuses only on the \ufb01nancial covariates\nunder investigation.\nIn Section 2 we describe the general model of ratios of intensities. In Section 3 we show that\nthe quasi-likelihood estimators of the model are consistent and asymptotically normal. Section\n4 discusses the method with respect to information criteria and penalization.\nFinally, Section\n5 illustrates the bene\ufb01ts of the model with several examples of limit order book analysis. The\nratio model is able to reproduce empirical observations on trading signals in orders \ufb02ows such\nas imbalance, spread, quantities available, etc. Estimation results on more than 30 stocks in the\nParis Stock Exchange for most of the year 2015 show a very good agreement between empirical\nobservations and the proposed ratio model.\n2\nModel description\nLet I = {0, . . . ,\u00afi} and J = {1, . . . ,\u00afj} for some strictly positive integers \u00afi and \u00afj. Let (Ni\nt)t\u22650, i \u2208I,\nbe some counting processes. We assume that the intensities \u03bbi(t), i \u2208I, of these counting processes\nshare a common baseline intensity \u03bb0(t). \u03bb0(t) is neither observable nor speci\ufb01ed as a function of\nobservables and parameters. For any i \u2208I, the intensity \u03bbi(t) is written :\n\u03bbi(t, \u03d1) = \u03bb0(t) exp\n\uf8eb\n\uf8edX\nj\u2208J\n\u03d1i\njXj(t)\n\uf8f6\n\uf8f8,\n(1)\nwhere (Xj(t))t\u22650 is the j-th observable covariate process and \u03d1 = (\u03d1i\nj)i\u2208I,j\u2208J a parameter vector.\nWe are not interested in the value of the coe\ufb03cient \u03d1i\nj, modeling the speci\ufb01c response of the\ncounting process Ni to the covariate Xj, but rather in the relative responses of the intensities\ncompared to each other. Let\n\u03b8i\nj = \u03d1i\nj \u2212\u03d10\nj,\n(i \u2208I, j \u2208J)\n(2)\nbe the relative response of process i compared to process 0 with respect to covariate j, j \u2208J.\nObviously, \u2200j \u2208J, \u03b80\nj = 0 since the process 0 is taken as an arbitrary reference, and \u2200j \u2208J, \u2200(i, i\u2032) \u2208\nI2, \u03b8i\u2032\nj \u2212\u03b8i\nj = \u03d1i\u2032\nj \u2212\u03d1i\nj, i.e. the di\ufb00erences between absolute and relative responses are equal. Instead\nof the standard intensities de\ufb01ned at Equation (1), and since we are only interested in the relative\nresponses, we will consider the intensities ratios\nri(t, \u03b8) =\n\u03bbi(t, \u03d1)\nP\ni\u2032\u2208I \u03bbi\u2032(t, \u03d1)\n(3)\nwhere \u03b8 = (\u03b81\n1, . . . , \u03b81\u00afj, . . . , \u03b8\u00afi\n1, . . . , \u03b8\u00afi\u00afj) denotes the new parameter vector. Notation is justi\ufb01ed by\n3\n\nthe following computation:\nri(t, \u03b8) =\nexp\n\u0010P\nj\u2208J \u03d1i\njXj(t)\n\u0011\nP\ni\u2032\u2208I exp\n\u0010P\nj\u2208J \u03d1i\u2032\nj Xj(t)\n\u0011\n=\n\uf8ee\n\uf8f0exp\n\uf8eb\n\uf8ed\u2212\nX\nj\u2208J\n\u03d1i\njXj(t)\n\uf8f6\n\uf8f8X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n\u03d1i\u2032\nj Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f0X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03d1i\u2032\nj \u2212\u03d1i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f0X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f01 +\nX\ni\u2032\u2208I\\{i}\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n.",
    "chunk_index": 2,
    "start_char": 5549,
    "end_char": 8268,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "denotes the new parameter vector. Notation is justi\ufb01ed by\n3\n\nthe following computation:\nri(t, \u03b8) =\nexp\n\u0010P\nj\u2208J \u03d1i\njXj(t)\n\u0011\nP\ni\u2032\u2208I exp\n\u0010P\nj\u2208J \u03d1i\u2032\nj Xj(t)\n\u0011\n=\n\uf8ee\n\uf8f0exp\n\uf8eb\n\uf8ed\u2212\nX\nj\u2208J\n\u03d1i\njXj(t)\n\uf8f6\n\uf8f8X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n\u03d1i\u2032\nj Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f0X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03d1i\u2032\nj \u2212\u03d1i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f0X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n=\n\uf8ee\n\uf8f01 +\nX\ni\u2032\u2208I\\{i}\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\u22121\n.\n(4)\nAs hinted in the introduction, this framework is convenient to model a limit order book since daily\nmovements are known to exhibit both intraday seasonality and sudden bursts of activities, some-\ntimes in response to exogenous news, sometimes occurring for reasons less obvious to the outside\nobserver. If we assume that such global market activity equally a\ufb00ects all types of orders, then such\nvariations of market activity are taken into account by the non speci\ufb01ed, possibly stochastic, base-\nline intensity \u03bb0. Therefore, we can estimate relative responses \u03b8 of order \ufb02ows to speci\ufb01c covariates\nwithout having to account for the common intensity \u03bb0 due to global market activity. If di\ufb00erent\npatterns of background intensities are observed in the data, then it is possible to incorporate these\ndi\ufb00erences into covariates.\nOne should also note that by construction P\ni\u2208I ri(t, \u03b8) = 1.\nThis gives another important\nfeature of the model, which is that the intensities ratios ri are directly interpretable in terms of\nprobability. Let us assume that the counting process Ni, i \u2208I, are counting the number of events\nof (mutually exclusive) type i occurring in the limit order book. Then the intensities ratio ri is\nthe instantaneous probability that the next occurring event will be of type i. Our ratio model can\nthus estimate relative event probabilities independently of the variations of market activity that are\nassumed to be shared by the intensities of all processes. Several examples are provided in Section\n5.\n3\nLikelihood analysis\nIn this section, we show that the quasi-maximum likelihood estimator and the quasi-Bayesian\nestimator of the ratio model of Equation (3) are consistent and asymptotically normal.\nTwo\n4\n\nclose formulations are provided. In the \ufb01rst one, stationarity of covariates is assumed to obtain\nconsistency and asymptotic normality.\nIn the second one, this assumption is discarded, but it\nis assumed that the sample consists of repeated i.i.d. measurements to again obtain consistency\nand asymptotic normality. This second formulation is in agreement with the common practice in\nempirical \ufb01nance to glue several trading days, or parts of trading days, into one single sample.\nLet us introduce some notations used in the following analysis. For a tensor T = (Ti1,...,ik)i1,...,ik,\nwe write\nT[u1, ..., uk] = T[u1 \u2297\u00b7 \u00b7 \u00b7 \u2297uk] =\nX\ni1,...,ik\nTi1,...,ikui1\n1 \u00b7 \u00b7 \u00b7 uik\nk\n(5)\nfor u1 = (ui1\n1 )i1,..., uk = (uik\nk )ik.",
    "chunk_index": 3,
    "start_char": 7864,
    "end_char": 10712,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "consists of repeated i.i.d. measurements to again obtain consistency\nand asymptotic normality. This second formulation is in agreement with the common practice in\nempirical \ufb01nance to glue several trading days, or parts of trading days, into one single sample.\nLet us introduce some notations used in the following analysis. For a tensor T = (Ti1,...,ik)i1,...,ik,\nwe write\nT[u1, ..., uk] = T[u1 \u2297\u00b7 \u00b7 \u00b7 \u2297uk] =\nX\ni1,...,ik\nTi1,...,ikui1\n1 \u00b7 \u00b7 \u00b7 uik\nk\n(5)\nfor u1 = (ui1\n1 )i1,..., uk = (uik\nk )ik. Brackets [ , ..., ] stand for a multilinear mapping. We denote by\nu\u2297r = u \u2297\u00b7 \u00b7 \u00b7 \u2297u the r times tensor product of u. Let \u03b9(\u03b8) = (0j, \u03b8), where 0k = 0 \u2208Rk. We will\nwrite \u03bbi(t, \u03b8) for \u03bbi(t, \u03b9(\u03b8)) for notational simplicity. Furthermore we write \u03b8i = (\u03b8i\nj)j\u2208J for i \u2208I.\nSince \u03b80 = 0, we will use the notation I0 = I \\ {0} = {1, . . . ,\u00afi} and consider a bounded domain\n\u0398 \u2282Rp, with p = \u00afi \u00d7 \u00afj as the parameter space of \u03b8 = (\u03b8i\nj)i\u2208I0,j\u2208J.\n3.1\nCase of stationary covariates\nLet T \u2208R\u2217\n+. To estimate \u03b8 \u2208\u0398 based on the observations on [0, T], we consider the quasi-log\nlikelihood (log partial likelihood)\nHT (\u03b8) =\nX\ni\u2208I\nZ T\n0\nlog ri(t, \u03b8) dNi\nt.\n(6)\nObviously, the model is continuously extended to the closure \u0398, and HT is extended to there as\na continuous function. A quasi-maximum likelihood estimator (QMLE) is a measurable mapping\n\u02c6\u03b8M\nT : \u2126\u2192\u0398 satisfying\nHT (\u02c6\u03b8M\nT ) = max\n\u03b8\u2208\u0398\nHT (\u03b8)\n(7)\nfor all \u03c9 \u2208\u2126. The quasi-Bayesian estimator (QBE) with respect to a prior density \u03d6 is de\ufb01ned by\n\u02c6\u03b8B\nT =\n\u0014 Z\n\u0398\nexp\n\u0000HT (\u03b8)\n\u0001\n\u03d6(\u03b8)d\u03b8\n\u0015\u22121 Z\n\u0398\n\u03b8 exp\n\u0000HT (\u03b8)\n\u0001\n\u03d6(\u03b8)d\u03b8.\n(8)\nThe QBE takes values in C[\u0398], the convex hull of \u0398. We assume \u03d6 is continuous and satis\ufb01es 0 <\ninf\u03b8\u2208\u0398 \u03d6(\u03b8) \u2264sup\u03b8\u2208\u0398 \u03d6(\u03b8) < \u221e. These estimators are called together quasi-likelihood estimators.\nWe now investigate asymptotic properties of the estimators when T \u2192\u221ein two cases.\nLet X = (Xj)j\u2208J. In this \ufb01rst step, in order to simplify the statements, we assume that the\nprocess (\u03bb0, X) is stationary. Denote by \u03b8\u2217= ((\u03d1\u2217)i\nj \u2212(\u03d1\u2217)0\nj)i\u2208I0,j\u2208J the true value of \u03b8, and assume\n5\n\nthat \u03b8\u2217\u2208\u0398. Denote by BI the \u03c3-\ufb01eld generated by {\u03bb0(t), X(t); t \u2208I} for I \u2282R+. Let\n\u03b1(h) = sup\nt\u2208R+\nsup\nA\u2208B[0,t],B\u2208B[t+h,\u221e)\n\f\fP[A \u2229B] \u2212P[A]P[B]\n\f\f\n(9)\nfor h > 0. We consider the following conditions.\n[A1] The process (\u03bb0, X) is stationary, \u03bb0(0) \u2208L\u221e\u2212= \u2229p>1Lp and exp(|Xj(0)|) \u2208L\u221e\u2212for all\nj \u2208J.\n[A2] The function \u03b1 is rapidly decreasing, that is, lim suph\u2192\u221ehL\u03b1(h) < \u221efor every L > 0.\nLet\n\u03c1i(x, \u03b8) =\n\"X\ni\u2032\u2208I\nexp\n\u0012\nx\n\u0002\n\u03b8i\u2032 \u2212\u03b8i\u0003\u0013#\u22121\n(10)\nfor x \u2208Rj and \u03b8 = (\u03b8i)i\u2208I0 \u2208Rp. Let\n\u039b(w, x) = w\nX\ni\u2208I\nexp\n\u0000x\n\u0002\n\u03d1\u2217i\u0003\u0001\n(11)\nfor w \u2208R+ and x \u2208Rj.",
    "chunk_index": 4,
    "start_char": 10218,
    "end_char": 12765,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "P[A \u2229B] \u2212P[A]P[B]\n\f\f\n(9)\nfor h > 0. We consider the following conditions.\n[A1] The process (\u03bb0, X) is stationary, \u03bb0(0) \u2208L\u221e\u2212= \u2229p>1Lp and exp(|Xj(0)|) \u2208L\u221e\u2212for all\nj \u2208J.\n[A2] The function \u03b1 is rapidly decreasing, that is, lim suph\u2192\u221ehL\u03b1(h) < \u221efor every L > 0.\nLet\n\u03c1i(x, \u03b8) =\n\"X\ni\u2032\u2208I\nexp\n\u0012\nx\n\u0002\n\u03b8i\u2032 \u2212\u03b8i\u0003\u0013#\u22121\n(10)\nfor x \u2208Rj and \u03b8 = (\u03b8i)i\u2208I0 \u2208Rp. Let\n\u039b(w, x) = w\nX\ni\u2208I\nexp\n\u0000x\n\u0002\n\u03d1\u2217i\u0003\u0001\n(11)\nfor w \u2208R+ and x \u2208Rj. Denote by V(x, \u03b8) the variance matrix of the (1+i)-dimensional multinomial\ndistribution M(1; \u03c00, \u03c01, ..., \u03c0i) with \u03c0i = \u03c1i(x, \u03b8), i \u2208I, and V(x, \u03b8)\u03b1,\u03b1\u2032 is V(x, \u03b8)\u2019s (\u03b1 + 1, \u03b1\u2032 + 1)-\nelement. Let V0(x, \u03b8) = (V(x, \u03b8)\u03b1,\u03b1\u2032)\u03b1,\u03b1\u2032\u2208I0. De\ufb01ne a symmetric tensor \u0393 by\n\u0393[u\u22972] = E\n\u0014\u0012\nV0(X(0)) \u2297X(0)\u22972\n\u0013\n[u\u22972]\u039b(\u03bb0(0), X(0))\n\u0015\n(12)\nfor u \u2208Rp, where V0(x) = V0(x, \u03b8\u2217). The matrix \u0393 is nonnegative de\ufb01nite. We assume\n[A3] det \u0393 > 0.\nThe non-degeneracy of \u0393 is a local condition. However, the global identi\ufb01ability condition follows\nfrom this condition in the present model, as seen later.\nLet \u03b8\u2217\u2208\u0398 denote the true value of \u03b8. Denote by Cp(Rp) the space of continuous functions on\nRp of at most polynomial growth. We write \u02c6uM\nT =\n\u221a\nT\n\u0000\u02c6\u03b8M\nT \u2212\u03b8\u2217\u0001\nand \u02c6uB\nT =\n\u221a\nT\n\u0000\u02c6\u03b8B\nT \u2212\u03b8\u2217\u0001\n. Denote\nby \u03b6 a p-dimensional standard Gaussian vector. The quasi-likelihood analysis ensures convergence\nof moments as well as asymptotic normality of the quasi-likelihood estimators.\nTheorem 3.1. Suppose that [A1], [A2] and [A3] are satis\ufb01ed. Then\nE\n\u0002\nf(\u02c6uA\nT )\n\u0003\n\u2192E\n\u0002\nf(\u0393\u22121/2\u03b6)\n\u0003\n(13)\nfor any f \u2208Cp(Rp) and A \u2208{M, B}.\n6\n\nProof is given in the appendix.\nExample 1. Let H be a Hawkes process with exponential kernel with parameters (\u00b5, \u03b1, \u03b2) \u2208(R\u2217\n+)3.\nFor the sake of illustrating Theorem 3.1, we consider a version of model (1) with \u00afi = 1, \u00afj = 1, and\nin which the common baseline intensity \u03bb0 is the intensity of the process H. We thus have:\n\u03bbi(t, \u03d1) =\n\u0014\n\u00b5 +\nZ t\n0\n\u03b1e\u2212\u03b2(t\u2212s) dH(s)\n\u0015\nexp\n\u0000\u03d1i\n1X1(t)\n\u0001\n,\ni = 0, 1,\n(14)\nwhere X1 is a Markov chain with states {\u22121, 1} and in\ufb01nitesimal generator\n \n\u03bbX\n\u2212\u03bbX\n\u2212\u03bbX\n\u03bbX\n!\n. Then\nwe obviously have p = 1, i.e. \u03b8 = \u03d11\n1 \u2212\u03d10\n1 is the single parameter to be \ufb01t. In this speci\ufb01c case, one\nmay explicitly compute the asymptotic variance of Equation (12):\n\u0393 =\n\u00b5\n1 \u2212\u03b1\n\u03b2\ne\u03b8\u2217\n(1 + e\u03b8\u2217)2\n\u0002\ncosh((\u03d1\u2217)1\n0) + cosh((\u03d1\u2217)1\n1)\n\u0003\n\u2208R\u2217\n+.\n(15)\nWe run 1000 simulations of the model with Hawkes parameters \u00b5 = 0.5, \u03b1 = 1, \u03b2 = 2, covariate\nparameter \u03bbX = 0.5, and horizon T = 1000. True values of the parameters are (\u03d1\u2217)1\n1 = 0.75,\n(\u03d1\u2217)0\n1 = \u22120.75 so that \u03b8\u2217= 1.5. For each simulation we estimate \u03b8.",
    "chunk_index": 5,
    "start_char": 12363,
    "end_char": 14840,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "compute the asymptotic variance of Equation (12):\n\u0393 =\n\u00b5\n1 \u2212\u03b1\n\u03b2\ne\u03b8\u2217\n(1 + e\u03b8\u2217)2\n\u0002\ncosh((\u03d1\u2217)1\n0) + cosh((\u03d1\u2217)1\n1)\n\u0003\n\u2208R\u2217\n+.\n(15)\nWe run 1000 simulations of the model with Hawkes parameters \u00b5 = 0.5, \u03b1 = 1, \u03b2 = 2, covariate\nparameter \u03bbX = 0.5, and horizon T = 1000. True values of the parameters are (\u03d1\u2217)1\n1 = 0.75,\n(\u03d1\u2217)0\n1 = \u22120.75 so that \u03b8\u2217= 1.5. For each simulation we estimate \u03b8. The empirical density of the\nestimated values of \u03b8 \u2212\u03b8\u2217is plotted on Figure 1 in dots and full line. On the same plot is provided\nin dashed lines a Gaussian distribution \ufb01tted on these estimated values, as well as the theoretical\nGaussian distribution given by Theorem 3.1, i.e. with the asymptotic variance given at Equation\n(12). Experimental values agree with the results of Theorem 3.1.\nExample 2. This paper does not focus on the baseline intensity. However, in the case of a para-\nmetric model where the baseline intensity is fully speci\ufb01ed, then the ratio approach is particularly\nhelpful as it reduces the dimension of the space of the parameters, and can thus help improving\nnumerical estimations by choosing appropriate starting points for the optimization routines. As an\nillustration, let us consider a model similar to the previous example, with a Hawkes process H with\n\u00afk = 2 exponential kernels as baseline intensity, \u00afi = 2, i.e. 3 processes, and \u00afj = 2 covariates (same\nMarkov chains as in the previous example):\n\u03bbi(t, \u03d1) =\n\uf8ee\n\uf8f01 +\n\u00afk\nX\nk=1\nZ t\n0\n\u03b1ke\u2212\u03b2k(t\u2212s) dH(s)\n\uf8f9\n\uf8fbexp\n\uf8eb\n\uf8ed\n\u00afj\nX\nj=1\n\u03d1i\njXj\n\uf8f6\n\uf8f8.\n(16)\nMaximum likelihood estimators of this model are directly computable when the process H is ob-\nservable. This requires an optimization on 10 parameters. As an alternative estimation procedure,\nwe can estimate the ratio model (4 parameters \u03b8i\nj, i = 1, 2, j = 1, 2) and then estimate the full\nmodel likelihood with the constraints that the di\ufb00erences \u03d1i\nj \u2212\u03d10\nj are kept equal to the estimated\nvalues \u03b8i\nj (dimension 6). All 10 parameters are thus estimated. The estimated values can \ufb01nally be\n7\n\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n\u22120.15\n\u22120.10\n\u22120.05\n0.00\n0.05\n0.10\n0.15\n0\n2\n4\n6\n8\nEstimators distribution T=1000\nError\nDensity\nG\nEmpirical density\nGaussian fit\nTheoretical density\nFigure 1: Distribution of the estimator of the estimation error \u03b8 \u2212\u03b8\u2217for the model de\ufb01ned in\nExample 1. Asymptotic variance de\ufb01ned at Equation (12) is retrieved experimentally.\nused as a starting point for a \ufb01nal maximization of the likelihood of the full model, in order to en-\nsure that we keep the desired properties of the maximum likelihood estimators. Estimation results\non simulations of the model (16) are presented in Table 1.",
    "chunk_index": 6,
    "start_char": 14464,
    "end_char": 17068,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "de\ufb01ned in\nExample 1. Asymptotic variance de\ufb01ned at Equation (12) is retrieved experimentally.\nused as a starting point for a \ufb01nal maximization of the likelihood of the full model, in order to en-\nsure that we keep the desired properties of the maximum likelihood estimators. Estimation results\non simulations of the model (16) are presented in Table 1. \u201cFull\u201d denotes the standard maximum\n\u03b10\n\u03b11\n\u03b20\n\u03b21\n\u03d10\n0\n\u03d10\n1\n\u03d11\n0\n\u03d11\n1\n\u03d12\n0\n\u03d12\n1\nTrue\n1.000\n2.000\n2.000\n10.000\n0.500\n1.000\n0.500\n-1.000\n-0.500\n1.000\nFull\n0.942\n1.507\n2.131\n5.647\n0.499\n1.011\n0.517\n-1.003\n-0.504\n1.015\n(0.819)\n(1.150)\n(0.878)\n(2.492)\n(0.013)\n(0.013)\n(0.049)\n(0.020)\n(0.004)\n(0.019)\nCombined\n0.994\n1.993\n1.981\n9.923\n0.500\n1.000\n0.500\n-1.000\n-0.500\n1.000\n(0.155)\n(0.154)\n(0.233)\n(1.034)\n(0.005)\n(0.005)\n(0.004)\n(0.005)\n(0.004)\n(0.005)\nTable 1: Numerical results for the estimation of the 10 parameters of the model de\ufb01ned at Equation\n(16). Standard deviations a given in parenthesis. See text for details.\nlikelihood estimation. \u201cCombined\u201d denotes the mixed method involving a ratio estimation as a \ufb01rst\nstep. Optimizations are carried out with a Nelder-Mead algorithm (Python scipy implementation)\nwith random starting points (using a standard Gaussian distribution). With these parameters and\nan horizon T = 10 000, samples have roughly 33 000 data points in average. Out of 197 tests, the\n\u201cFull\u2019 estimation converged only 10 times, while the \u201cCombined\u201d method converged 192 times out\nof 202. Estimates produced by the \u201cCombined\u201d method are closer to the true values and have a\nsmaller empirical standard deviations by a factor 2 to 10 (except for \u03d12\n0, already well-estimated in\nthe \u201cFull\u201d case). Strong improvements are observed in the Hawkes parameters, where the standard\n\u201cFull\u201d method struggles to produce close estimates, while the \u201cCombined\u201d method starting with a\n8\n\nratio estimation yields much more accurate results.\n3.2\nRepeated measurements\nWe complete the previous analysis with a formulation of our estimation result that may be con-\nvenient in \ufb01nance and in other \ufb01elds of applications.\nWe consider a sequence of observations\nof intraday data.\nThe observations may be non-ergodic each day.\nWe shall consider intervals\nI(k) = [Ok, Ck] (k \u2208N) of the same length such that 0 \u2264O1 < C1 \u2264O2 < C2 \u2264\u00b7 \u00b7 \u00b7 . We consider\nthe counting processes Ni = (Ni\nt)t\u2208R+ of the previous section. However, in this section, the obser-\nvations are ((Ni\nt)i\u2208I, X(t))t\u2208I(k), k = 1, ..., T. As before, it is assumed that the point processes Ni\n(i \u2208I) have no common jumps. The stationarity of each process\n\u0000\u03bbi(t, \u03d1\u2217)\n\u0001\nt\u2208I(k) is not assumed\nhere.\nWe are interested in estimation of the parameter \u03b8 = (\u03b8i\nj)i\u2208I0, j\u2208J de\ufb01ned earlier by Equation\n(2). The estimation will be based on the random \ufb01eld HT re-de\ufb01ned by\nHT (\u03b8) =\nT\nX\nk=1\nX\ni\u2208I\nZ\nI(k) log ri(t, \u03b8)dNi\nt.\n(17)\nThen the QMLE and the QBE are de\ufb01ned by Equations (7) and (8), respectively, but for HT given\nby Equation (17).",
    "chunk_index": 7,
    "start_char": 16716,
    "end_char": 19645,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "stationarity of each process\n\u0000\u03bbi(t, \u03d1\u2217)\n\u0001\nt\u2208I(k) is not assumed\nhere.\nWe are interested in estimation of the parameter \u03b8 = (\u03b8i\nj)i\u2208I0, j\u2208J de\ufb01ned earlier by Equation\n(2). The estimation will be based on the random \ufb01eld HT re-de\ufb01ned by\nHT (\u03b8) =\nT\nX\nk=1\nX\ni\u2208I\nZ\nI(k) log ri(t, \u03b8)dNi\nt.\n(17)\nThen the QMLE and the QBE are de\ufb01ned by Equations (7) and (8), respectively, but for HT given\nby Equation (17).\nLet Xk =\n\u0000\u03bb0(t), X(t)\n\u0001\nt\u2208I(k). Let Gk = \u03c3[X1, ..., Xk] and let Hk = \u03c3[Xk, Xk+1, ...]. The \u03b1-mixing\ncoe\ufb03cient for X = (Xk)k\u2208N is de\ufb01ned by\n\u03b1X (h) = sup\nk\u2208N\nsup\nA\u2208Gk, B\u2208Hk+h\n\f\fP[A \u2229B] \u2212P[A]P[B]\n\f\f.\n(18)\nLet us then de\ufb01ne the new conditions.\n[C1] The sequence (Xk)k\u2208N is identically distributed.\nMoreover, supt\u2208I(1) \u2225\u03bb0(t)\u2225p < \u221eand\nmaxj\u2208J supt\u2208I(1) \u2225exp(p|Xj(t)|)\u22251 < \u221efor all p > 1.\n[C2] For every L > 0, lim suph\u2192\u221ehL\u03b1X (h) < \u221e.\nDe\ufb01ne the symmetric tensor \u0393 by\n\u0393[u\u22972] = E\n\u0014 Z\nI(1)\n\u0012\nV0(X(t)) \u2297X(t)\u22972\n\u0013\n[u\u22972]\u039b(\u03bb0(t), X(t))dt\n\u0015\n(19)\nfor u \u2208Rp. The matrix \u0393 is nonnegative de\ufb01nite. More strongly we assume\n[C3] det \u0393 > 0.\nIn a way similar to Theorem 3.1, it is possible to prove the following theorem.\n9\n\nTheorem 3.2. Suppose that [C1], [C2] and [C3] are satis\ufb01ed. Then\nE\n\u0002\nf(\u02c6uA\nT )\n\u0003\n\u2192E\n\u0002\nf(\u0393\u22121/2\u03b6)\n\u0003\n(20)\nas T \u2192\u221efor any f \u2208Cp(Rp) and A \u2208{M, B}.\nThe proof is omitted.\n4\nInformation criteria and penalization\nSince the ratio model \ufb02exibly incorporates various covariates processes and since it may have a large\nnumber of parameters, we need information criteria for model selection and other regularization\nmethods for sparse estimation. Though the inference of the ratio model is based on the quasi-\nlikelihood analysis, we can still apply information criterion like CAIC (consistent AIC) and BIC by\nusing HT . See Bozdogan (1987) for exposition of information criteria for model selection.\nLet aT be a sequence of positive numbers such that aT \u2192\u221eand aT /T \u21920 as T \u2192\u221e. Let\nK \u2282I0 \u00d7 J. We consider a sub-model SK of \u0398 such that\nSK = {\u03b8 \u2208\u0398; \u03b8i\nj = 0 ((i, j) \u2208Kc)}.\n(21)\nLet\nCT (SK) = \u22122HT (\u02c6\u03b8K) + d(SK)aT\n(22)\nwhere \u02c6\u03b8K (depending on T) is denoting the QMLE or QBE in the sub-model SK and d(SK) is the\ndimension of SK. More precisely, the QMLE \u02c6\u03b8M\nK is de\ufb01ned as an estimator that satis\ufb01es\nHT (\u02c6\u03b8M\nK ) = max\n\u03b8\u2208SK\nHT (\u03b8),\n(23)\nand the QBE \u02c6\u03b8B\nK is de\ufb01ned by\n\u02c6\u03b8B\nK =\n\u0014 Z\nSK\nexp\n\u0000HT (\u03b8)\n\u0001\n\u03d6SK(\u03b8)\n\u0015\u22121 Z\nSK\n\u03b8 exp\n\u0000HT (\u03b8)\n\u0001\n\u03d6SK(\u03b8)d\u03b8\n(24)\nfor a continuous prior density \u03d6SK on SK satisfying 0 < inf\u03b8\u2208SK \u03d6SK(\u03b8) \u2264sup\u03b8\u2208SK \u03d6SK(\u03b8) < \u221e.\nWe denote by SK\u2217the minimum model that includes \u03b8\u2217, in other words, (\u03b8\u2217)i\nj \u0338= 0 if and only if\n(i, j) \u2208K\u2217.",
    "chunk_index": 8,
    "start_char": 19245,
    "end_char": 21772,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": ") = max\n\u03b8\u2208SK\nHT (\u03b8),\n(23)\nand the QBE \u02c6\u03b8B\nK is de\ufb01ned by\n\u02c6\u03b8B\nK =\n\u0014 Z\nSK\nexp\n\u0000HT (\u03b8)\n\u0001\n\u03d6SK(\u03b8)\n\u0015\u22121 Z\nSK\n\u03b8 exp\n\u0000HT (\u03b8)\n\u0001\n\u03d6SK(\u03b8)d\u03b8\n(24)\nfor a continuous prior density \u03d6SK on SK satisfying 0 < inf\u03b8\u2208SK \u03d6SK(\u03b8) \u2264sup\u03b8\u2208SK \u03d6SK(\u03b8) < \u221e.\nWe denote by SK\u2217the minimum model that includes \u03b8\u2217, in other words, (\u03b8\u2217)i\nj \u0338= 0 if and only if\n(i, j) \u2208K\u2217. The QMLE and QBE for \u03b8 restricted to the sub-model SK\u2217are generically denoted by\n\u02c6\u03b8K\u2217. In particular,\nCT (SK\u2217) = \u22122HT (\u02c6\u03b8K\u2217) + d(SK\u2217)aT .\n(25)\nProposition 4.1. Suppose that [A1], [A2] and [A3] are satis\ufb01ed. Then\n10\n\n(i) If \u03b8\u2217\u0338\u2208SK, then\nCT (SK) \u2212CT (SK\u2217) \u2192p \u221e\n(T \u2192\u221e)\n(26)\n(ii) If \u03b8\u2217\u2208SK and SK \u0338= SK\u2217, then\nCT (SK) \u2212CT (SK\u2217) \u2192p \u221e\n(T \u2192\u221e)\n(27)\nA proof of Proposition 4.1 is given in Appendix for selfcontainedness. According to Proposition\n4.1, we should select a model SbK that attains min{CT (SK); K \u2282I0 \u00d7 J}.\nThen the selection\nconsistency holds as\nP\n\u0002bK = K\u2217\u0003\n\u21921\n(T \u2192\u221e).\n(28)\nBased on the quasi-likelihood function HT , the criterion CT (SK) gives the quasi-consistent AIC\n(QCAIC) when aT = log T + 1, and the quasi-BIC (QBIC) when aT = log T among many other\npossible choices of aT . A Hannan and Quinn type of quasi-information criterion (QHQ) is the case\nwhere aT = c log log T for c > 2. The quasi-AIC (QAIC) is the case where aT = 2 but it cannot\nexclude over\ufb01tting though it excludes misspeci\ufb01ed models, as usual and as suggested in the proof\nof Proposition 4.1.\nRecently penalized quasi-likelihood analysis for sparse estimation has been developed.\nWe\nconsider a penalty function p\u03bb such that p\u03bb(x) = p\u03bb(\u2212x), p\u03bb(0) = 0, p\u03bb is non-decreasing on x \u22650,\np\u03bb is di\ufb00erentiable except for x = 0, and limx\u21930 x\u2212qp\u03bb(x) = \u03bb > 0 for some q \u2208(0, 1]. This class\nof penalty functions includes the ones of LASSO (Tibshirani (1996)), Bridge (Frank & Friedman\n(1993)) and SCAD (Fan & Li (2001)). Suppose that we have a QLA based on the random \ufb01eld\nHT . Then we have the penalized contrast function\n\u2212H\u2020\nT (\u03b8) = \u2212HT (\u03b8) +\n\u221a\nT\nX\ni\u2208I0,j\u2208J\np\u03bb(\u03b8i\nj)\n(29)\nand the penalized QMLE \u02c6\u03b8\u03bb (depending on T) is associated by\n\u02c6\u03b8\u03bb \u2208argmin\u03b8\u2208\u0398\n\b\n\u2212H\u2020\nT (\u03b8)\n \n.\n(30)\nIn the case q = 1, the polynomial type large deviation inequality is inherited from HT to H\u2020\nT , as a\nresult, we obtain asymptotic properties of \u02c6\u03b8\u03bb as well as Lp-boundedness of the error. Asymptotic\ndistribution becomes slightly involved. In a similar way, in the case q < 1, it is possible to derive\nasymptotic properties of \u02c6\u03b8\u03bb. A prominent property is selection consistency. Thanks to the QLA\ntheory having a polynomial type large deviation inequality, we can prove that the probability of\ncorrect model selection is 1 \u2212O(T \u2212L) as T \u2192\u221efor every L > 0.",
    "chunk_index": 9,
    "start_char": 21441,
    "end_char": 24062,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "inherited from HT to H\u2020\nT , as a\nresult, we obtain asymptotic properties of \u02c6\u03b8\u03bb as well as Lp-boundedness of the error. Asymptotic\ndistribution becomes slightly involved. In a similar way, in the case q < 1, it is possible to derive\nasymptotic properties of \u02c6\u03b8\u03bb. A prominent property is selection consistency. Thanks to the QLA\ntheory having a polynomial type large deviation inequality, we can prove that the probability of\ncorrect model selection is 1 \u2212O(T \u2212L) as T \u2192\u221efor every L > 0.\nSee Kinoshita & Yoshida\n(2018) for details. An advantage of working with the QLA theory is that asymptotic properties of\nregularization methods are obtained in a uni\ufb01ed manner without using any speci\ufb01c nature of the\n11\n\nstochastic process. The QLA was used in Umezu et al. (2015) for a generalized linear model.\nAnother approach is toward the adaptive LASSO (Zou (2006)) and the least square approxi-\nmation method (Wang & Leng (2007)). De Gregorio & Iacus (2012) took this approach to sparse\nestimation of ergodic di\ufb00usions. Since we can assume strong mode of convergence for the initial\nestimator constructed within the QLA framework, we can obtain selection consistency with er-\nror probability that is of O(T \u2212L) for any L > 0 as well as the oracle properties of the penalized\nestimator. For details, see Suzuki & Yoshida (2018).\nRelated to regularization methods for point processes, Fan & Li (2002) extended the nonconcave\npenalized likelihood approach to the Cox proportional hazards model. Yue & Loh (2015) treated\nvariable selection in spatial point processes. Hansen et al. (2015) derived probabilistic inequalities\nfor multivariate point processes in the context of LASSO.\n5\nEmpirical results - Dependencies analysis\nThis section describes the high-frequency \ufb01nancial data (Section 5.1) and several empirical studies\nconducted with it. A detailed example of \ufb01nancial analysis allowed by the ratio model is provided\nin Section 5.2, where the determination of the sign of the next trade in a limit order book is carried\nout with several covariates. Other examples are provided in subsequent sections, illustrating the\nin\ufb02uence of the spread and queue sizes in the order book.\n5.1\nData\nWe use Reuters TRTH tick by tick data for 36 stocks traded on the Paris stock Exchange on most\nof the year 2015. The list of the stocks is given in Appendix B. For each stock and each trading\nday, the orders \ufb02ow is reconstructed using the method described in Muni Toke (2016), and we keep\nall trades and quotes recorded between 9:30 am and 5:00 pm1. When adding all stocks and trading\ndays, the sample represents more than one billion events (more than 50 millions market orders,\nmore than 500 millions limit orders, and more than 450 millions cancellations).\nAll models are numerically estimated using R by quasi-likelihood maximization. For practical\npurposes, we provide practical explicit expressions for the log partial likelihood. Using Equation\n1Removing the beginning and the trading day has been done as a usual precautionary step when dealing with\nhigh-frequency trades and quotes databases, as one may sometimes be concerned with data quality in very busy\nperiods.",
    "chunk_index": 10,
    "start_char": 23576,
    "end_char": 26732,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "and 5:00 pm1. When adding all stocks and trading\ndays, the sample represents more than one billion events (more than 50 millions market orders,\nmore than 500 millions limit orders, and more than 450 millions cancellations).\nAll models are numerically estimated using R by quasi-likelihood maximization. For practical\npurposes, we provide practical explicit expressions for the log partial likelihood. Using Equation\n1Removing the beginning and the trading day has been done as a usual precautionary step when dealing with\nhigh-frequency trades and quotes databases, as one may sometimes be concerned with data quality in very busy\nperiods. However, this precaution may actually not be necessary. For example, when recomputing Figure 4 below\nusing the whole trading day, no we observe no signi\ufb01cant visual di\ufb00erence with the version presented in this paper.\n12\n\n(4), one may write Equation (6) with respect to \u03b8 as:\nHT (\u03b8) = \u2212\nX\ni\u2208I\nZ T\n0\nlog\n\uf8ee\n\uf8f0X\ni\u2032\u2208I\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbdNi(t)\n= \u2212\nX\ni\u2208I\nZ T\n0\nlog\n\uf8ee\n\uf8f01 +\nX\ni\u2032\u2208I\\{i}\nexp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b8i\u2032\nj \u2212\u03b8i\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbdNi(t)\n(31)\nEquation (17) is similarly written. In the example case I = {0, 1, 2} with three counting processes,\nthe quasi-log likelihood given at Equation (31) is written:\nHT (\u03b8) = \u2212\nZ T\n0\nlog\n\uf8ee\n\uf8f01 + exp\n\uf8eb\n\uf8edX\nj\u2208J\n\u03b81\njXj(t)\n\uf8f6\n\uf8f8+ exp\n\uf8eb\n\uf8edX\nj\u2208J\n\u03b82\njXj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbdN0(t)\n\u2212\nZ T\n0\nlog\n\uf8ee\n\uf8f01 + exp\n\uf8eb\n\uf8ed\u2212\nX\nj\u2208J\n\u03b81\njXj(t)\n\uf8f6\n\uf8f8+ exp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b82\nj \u2212\u03b81\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbdN1(t)\n\u2212\nZ T\n0\nlog\n\uf8ee\n\uf8f01 + exp\n\uf8eb\n\uf8ed\u2212\nX\nj\u2208J\n\u03b82\njXj(t)\n\uf8f6\n\uf8f8+ exp\n\uf8eb\n\uf8edX\nj\u2208J\n(\u03b81\nj \u2212\u03b82\nj)Xj(t)\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbdN2(t)\n(32)\nAn important feature of the model may be underlined here. Many mathematical models of\nlimit order books, and models in high-frequency \ufb01nance in general, rely on simple point processes,\ni.e. processes for which one may not observe two simultaneous events. Confronting such modeling\nto empirical data thus often requires that the timestamps of all recorded events are di\ufb00erent. This\ncondition, even with resolutions down to milliseconds or even microseconds, is not veri\ufb01ed on\n\ufb01nancial markets, where burst of activities are translated into data \ufb01les by multiple events at the\nsame timestamp. Randomization, i.e. adding a small random quantity to the observed timestamps,\nis often proposed as a patch to force all timestamps to be di\ufb00erent. A nice feature of the setting\nproposed here is that such patches are not necessary : likelihood (31) does not depend on interval\ntimes between consecutive events, but only on the number of events. Therefore, the model can be\napplied even on data with low resolution timestamps.\n5.2\nTrades signs in a limit order book\nIt is well known that imbalance is good proxy to the sign of the next trade. Empirical observations\ncan be found in e.g.",
    "chunk_index": 11,
    "start_char": 26093,
    "end_char": 28803,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "timestamps to be di\ufb00erent. A nice feature of the setting\nproposed here is that such patches are not necessary : likelihood (31) does not depend on interval\ntimes between consecutive events, but only on the number of events. Therefore, the model can be\napplied even on data with low resolution timestamps.\n5.2\nTrades signs in a limit order book\nIt is well known that imbalance is good proxy to the sign of the next trade. Empirical observations\ncan be found in e.g. Lipton et al. (2013). Lehalle & Mounjid (2017), among others, is an attempt\nto incorporate this signal in trading strategies.\nLet us de\ufb01ne the imbalance in the order book\nobserved at time t by:\ni(t) = qB\n1 (t) \u2212qA\n1 (t)\nqB\n1 (t) + qA\n1 (t),\n(33)\n13\n\nwhere qA\n1 (t) and qB\n1 (t) are the number of shares available at time t at the best quote on the ask side\nand bid side respectively. An imbalance close to +1 indicates a very small available volume on the\nask side and consequently a probable upward price move, while an imbalance close to \u22121 increases\nthe probability of a downward price move. Using our model, we can infer how the imbalance a\ufb00ects\nthe relative intensities of bid and ask market orders. Let us assume that counting processes of ask\nmarket orders NMA and bid market orders NMB are point processes with intensities :\n\u03bbMA(t, \u03d1MA) = \u03bb0(t) exp\n\u0002\n\u03d1MA\n0\n+ \u03d1MA\n1\ni(t)\n\u0003\n,\n\u03bbMB(t, \u03d1MB) = \u03bb0(t) exp\n\u0002\n\u03d1MB\n0\n+ \u03d1MB\n1\ni(t)\n\u0003\n,\n(34)\nwhere \u03bb0(t) is a potentially random baseline intensity. Parameters \u03b8i = \u03b8MA\ni\n\u2212\u03b8MB\ni\n, i = 0, 1 can\nbe straightforwardly estimated by maximization of the quasi-likelihood. As mentioned above, a\nvery interesting feature of our intensity model is that the intensities ratios rMA and rMB are then\ndirectly interpretable as the instantaneous probability that the next market order will occur on the\nask side and on the bid side respectively.\nThe model is \ufb01tted monthly for all stocks, from January 2015 to November 2015, which, ex-\ncluding gaps in the database, gives 390 \ufb01ts of the model. On Figure 2, we plot for two samples\nthe empirical probability that for a given imbalance, the next trade occurs on the ask side, and\nthe numerical estimation of the theoretical probability rMA(i) = [1 + exp(\u2212\u03b80 \u2212\u03b81i)]\u22121 for a level\ni of imbalance given by the \ufb01t of our ratio model. For representativity we rank the 390 \ufb01ts by\nincreasing mean L2 distance between empirical and \ufb01tted probabilities, and we select two samples\nrepresenting the 20% and 80% quantiles of this error distribution. All 390 \ufb01ts are available upon\nrequest.\nThe simple ratio model with one covariate provides a very good \ufb01t of the probability of the next\ntrade. But we can obviously investigate further possible factors in\ufb02uencing the sign of the next\ntrade.",
    "chunk_index": 12,
    "start_char": 28339,
    "end_char": 31058,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "For representativity we rank the 390 \ufb01ts by\nincreasing mean L2 distance between empirical and \ufb01tted probabilities, and we select two samples\nrepresenting the 20% and 80% quantiles of this error distribution. All 390 \ufb01ts are available upon\nrequest.\nThe simple ratio model with one covariate provides a very good \ufb01t of the probability of the next\ntrade. But we can obviously investigate further possible factors in\ufb02uencing the sign of the next\ntrade. It has been shown that times series of signs of trades (\u03f5(t) = +1 for an ask trade at time\nt, \u22121 for a buy trade) exhibit slowly decreasing autocorrelation functions (see e.g., Lillo & Farmer\n(2004); Bouchaud et al. (2004)). We can include the sign of the last trade in the ratio model, and\nassume that counting processes of ask market orders NMA and bid market orders NMB are point\nprocesses with intensities :\n\u03bbMA(t, \u03d1MA) = \u03bb0(t) exp\n\u0002\n\u03d1MA\n0\n+ \u03d1MA\n1\ni(t) + \u03d1MA\n2\n\u03f5(t)\n\u0003\n,\n\u03bbMB(t, \u03d1MB) = \u03bb0(t) exp\n\u0002\n\u03d1MB\n0\n+ \u03d1MB\n1\ni(t) + \u03d1MB\n2\n\u03f5(t)\n\u0003\n,\n(35)\nFigure 3 plots the updated results for two samples. For representativity, we again select the two\nsamples representing the 20% and 80% quantile measured in mean L2 error.\nAll 390 \ufb01ts are\navailable upon request. It is interesting to observe the strong e\ufb00ect of the sign of the last trade on\nthe imbalance predicting power. While a close to zero imbalance unconditionally indicate a close\n14\n\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nCAPP.PA 201511\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nDANO.PA 201503\nFigure 2: Empirical probability (in red) and \ufb01tted probability (in blue) that the next market order\nis an ask market order, given the observed imbalance. Selected stocks show the 20% (left) and\n80%(right) quantiles measured in mean L2 error.\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nAXAF.PA 201502\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG",
    "chunk_index": 13,
    "start_char": 30610,
    "end_char": 32865,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nAXAF.PA 201502\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nSCHN.PA 201502\nFigure 3: Empirical probability (in red) and \ufb01tted probability (in blue) that the next market order\nis an ask market order, given the observed imbalance. Upward (resp. downward) triangles indicate\nthat the last trade was an ask (resp. bid) market order. Selected stocks show the 20% (left) and\n80%(right) quantiles measured in mean L2 error.\n15\n\nto 50% probability of an ask trade (see Figure 2), this dramatically changes when the sign of the\nlast trade comes into play. We now observe on Figure 3 two probability curves, one for each sign\nof the preceding trade, and these curves highlight a kind of hysteresis e\ufb00ect in sign determination\nwith respect to the imbalance. For example, during a series of bid (resp. ask) market orders, the\nimbalance level at which an ask (resp. bid) market order becomes more probable is not around 0,\nbut higher (resp.lower). On the selected graphs the 50% crossing point for the imbalance level is\naround \u00b10.5 (this value may be stock and date dependent).\nIn a further step, we can investigate the role of the spread on these dynamics. In the case of a\nlarge spread observation, then priority can be achieved with limit orders, it is thus expected that\nthe imbalance e\ufb00ect will be less pronounced (as found for example in Stoikov (2017)) but that it\nwill interact with \u03f5. We can thus use the following ratio model :\n\u03bbMA(t, \u03d1MA) = \u03bb0(t) exp\n\u0002\n\u03d1MA\n0\n+ \u03d1MA\n1\ni(t) + \u03d1MA\n2\n\u03f5(t) + \u03d1MA\n3\n\u03f5(t)s(t)\n\u0003\n,\n\u03bbMB(t, \u03d1MB) = \u03bb0(t) exp\n\u0002\n\u03d1MB\n0\n+ \u03d1MB\n1\ni(t) + \u03d1MB\n2\n\u03f5(t) + \u03d1MA\n3\n\u03f5(t)s(t)\n\u0003\n,\n(36)\nwhere s(t) = +1 if the observed spread is larger than its mean, and \u22121 if it is smaller. Obviously,\none could use the spread value in ticks for \ufb01ner models, but we choose categorical variable for\ngraphic illustration purposes. Note that the spread distribution is very stable, so that the mean\ncan for example be computed on the previous sample in the case of repeated \ufb01ts. Note also that\nthe spread distribution is positively skewed and bounded below by zero, so that s(t) = \u22121 can be\ninterpreted as the usual spread case, and s(t) = +1 as the large spread case. Figure 4 plots the\nupdated results for two samples.",
    "chunk_index": 14,
    "start_char": 32635,
    "end_char": 35110,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "purposes. Note that the spread distribution is very stable, so that the mean\ncan for example be computed on the previous sample in the case of repeated \ufb01ts. Note also that\nthe spread distribution is positively skewed and bounded below by zero, so that s(t) = \u22121 can be\ninterpreted as the usual spread case, and s(t) = +1 as the large spread case. Figure 4 plots the\nupdated results for two samples. For representativity, we again select the two samples representing\nthe 20% and 80% quantiles measured in mean L2 error. All 390 \ufb01ts are available upon request. We\nnow have four curves representing the e\ufb00ect of the imbalance on the next trade sign, depending\non the last trade sign and the current spread. Empirical curves are noisier as the samples are\nsplit into subsamples to compute conditional probabilities. However, we observe as expected that a\nlarge spread \ufb02attens the imbalance e\ufb00ect and reinforces the last sign importance. On the provided\nexample \ufb01ts, if the spread is larger than usual and the last trade was a bid, an ask market order\nbecomes more probable than a bid market order when the imbalance nearly reaches 1, compared\nto roughly 0.5 when the spread is at its usual values, and to 0 when neither the spread nor the last\nsign are taken into account.\nSince we are exploring the role of several covariates, it is natural to apply the information\ncriteria mentioned in Proposition 4.1. Among other possible sequences aT , we use the QAIC for\naT = 2, the QCAIC for aT = log T + 1, and the QBIC for aT = log T, all based on the QMLE\n\u02c6\u03b8K. As an illustration, Table 2 (left panel) shows the number of times various models are selected\namong the 390 samples, according to these information criteria. Several comments can be made\nabout this illustration. First of all, it turns out that the simplest model depending only on the\nimbalance is never selected. Recall for example that the weighted mid-price, commonly used in\n16\n\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nSCHN.PA 201510\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG",
    "chunk_index": 15,
    "start_char": 34712,
    "end_char": 37032,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "G\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nSCHN.PA 201510\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\nObserved imbalance\nProbability of ask market\nVLOF.PA 201501\nFigure 4: Empirical probability (in red) and \ufb01tted probability (in blue) that the next market order\nis an ask market order, given the observed imbalance. Upward (resp. downward) triangles indicate\nthat the last trade was an ask (resp. bid) market order. Dashed (resp. full) lines represent large\n(resp. usual) spread values. Selected stocks show the 20% (left) and 80%(right) quantiles measured\nin mean L2 error.\nd\nModel\nQAIC\nQCAIC\nQBIC\nQAIC\nQCAIC\nQBIC\n2\ni\n0\n0\n0\n0\n0\n0\n3\ni.\u03f5\n0\n2\n2\n0\n2\n2\n4\ni.\u03f5.s\n1\n1\n1\n1\n1\n1\n4\ni.\u03f5.\u03f5s\n79\n162\n158\n48\n118\n114\n7\ni.\u03f5.s + int\n310\n225\n229\n218\n175\n178\n11\ni.\u03f5.s.\u03b4 + int\n\u2014\n\u2014\n\u2014\n123\n94\n95\nTable 2: Number of times a model for the intensity of bid and ask market orders is selected among\nthe 390 samples, according to several information criteria. Models are named by the names of the\ncovariates separated with a dot, with the notations de\ufb01ned in the text. \u201c+int\u201d means that all\ninteraction terms are included in the model. E.g., \u201ci.\u03f5.\u03f5s\u201d is the model de\ufb01ned at Equations (36).\nLeft panel investigates covariates i, \u03f5 and s only, while right panel adds the last price \u03b4.\n17\n\nmicrostructure as a proxy for a \u201cfuture\u201d price, depends only on the imbalance. Our observation\npleads for the incorporation of more covariates in de\ufb01ning such proxies. It also highlights that,\nas explained above, the spread acts primarily on the intensity of submission in interaction with\nthe last sign: model with covariates i, \u03f5 and s is (nearly) never selected, in contrast to the model\nwith covariates i, \u03f5 and \u03f5s. Finally, we observe as expected that QAIC has a tendency to select the\nmodel with the greatest number of parameters ; however, using QCAIC or QBIC, the simple model\nof Equation (36) is still selected on more than 40% of the samples over the full model with all\nterms. These results show that in future works further investigations could be made in determining\nrelevant covariates and possibly analyze a stock- or time- dependency. Among several possibilities,\nTable 2 (right panel) gives selection results when we add the last price movement (denoted \u03b4) to\nratio model.",
    "chunk_index": 16,
    "start_char": 36802,
    "end_char": 39265,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "however, using QCAIC or QBIC, the simple model\nof Equation (36) is still selected on more than 40% of the samples over the full model with all\nterms. These results show that in future works further investigations could be made in determining\nrelevant covariates and possibly analyze a stock- or time- dependency. Among several possibilities,\nTable 2 (right panel) gives selection results when we add the last price movement (denoted \u03b4) to\nratio model.\nOur investigation on the in\ufb02uence of the imbalance signal on the intensities of bid and ask\nmarket orders can also illustrate the use of the penalized QMLE described in Section 4. In this\nexample, we use the ratio model to try to decide whether traders use the imbalance as a trading\nsignal on a given stock looking at the \ufb01rst level only, or at the \ufb01rst two levels, etc. Let us denote ik(t)\nthe imbalance observed a time t computed using the cumulative quantities at the best quotes up to\nthe k-th level, k = 1, . . . , 10. i1 is thus the imbalance previously investigated. For k = 2, . . . , 10, let\n\u2206ik = ik \u2212ik\u22121 the corrective term between the imbalances computed with k and k \u22121 limits. We\nextend the basic model (34) to include the standard imbalance as well as all the corrective terms\nas potential trading signals:\n\u03bbT (t, \u03b8T ) = \u03bb0(t) exp\n\"\n\u03b8T\n0 + \u03b8T\n1 i1(t) +\n10\nX\nk=2\n\u03b8T\nk \u2206ik(t)\n#\n,\n(37)\nwhere T \u2208{MB, MA}. Let \u03b8k = \u03b8MA\nk\n\u2212\u03b8MB\nk\n, k = 1, 10 be our ratio parameters, to be estimated\nby likelihood maximization. We estimate the model using both standard quasi-likelihood maxi-\nmization, as well as a penalized estimation using the penarized QMLE \u02c6\u03b8\u03bb of Equations (29) and\n(30). We compute daily \ufb01ts of the model and plot the mean values of the estimated parameters\nfor each stock. Figure 5 shows the \ufb01tted parameters \u03b8k as a function of the level k, as well as the\n95% Gaussian empirical con\ufb01dence interval computed with the empirical standard deviation. For\nbrevity only two stocks are shown, and for simplicty we choose the same ones as in Figure 4 but\nresults for all 36 stocks are similar. It turns out that the parameters associated to the imbalance at\nlevel 1 is always very signi\ufb01cant, but that the additional information given by the corrective terms\n\u2206ik, k = 2, . . . , 10 is not translated into signi\ufb01cant \u03b8k, k = 2, . . . , 10. Penalization greatly helps\nreducing the con\ufb01dence intervals of estimates. This results thus seem to be in favor of arguing that\ntraders use imbalance at the \ufb01rst level as a signi\ufb01cant trading signal, but do not signi\ufb01cantly use\ninformation of higher levels.\n18",
    "chunk_index": 17,
    "start_char": 38814,
    "end_char": 41375,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "translated into signi\ufb01cant \u03b8k, k = 2, . . . , 10. Penalization greatly helps\nreducing the con\ufb01dence intervals of estimates. This results thus seem to be in favor of arguing that\ntraders use imbalance at the \ufb01rst level as a signi\ufb01cant trading signal, but do not signi\ufb01cantly use\ninformation of higher levels.\n18\n\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\n\u22122\n0\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nLOB level (1=best quote)\nYearly mean theta\nEstimation\nG\nG Penalized\nStandard\nSCHN.PA \u2212 Relative theta w.r.t. LOB level\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nGG\n\u22124\n\u22122\n0\n2\n4\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nLOB level (1=best quote)\nYearly mean theta\nEstimation\nG\nG Penalized\nStandard\nVLOF.PA \u2212 Relative theta w.r.t. LOB level\nFigure 5: Mean daily estimate (curves) and 95% con\ufb01dence interval (shaded areas) of \u03b8k for the\nimbalance model with corrective terms of Equation (37). Same stocks as in Figure 4. In these\nnumerical examples, we use q = 1 and \u03bb = 50T \u22121\n2 .\n5.3\nSpread and order \ufb02ows at the best quotes\nSpread as a potential trading signal seems less investigated in the microstructure literature than\nother factors. One potential reason is that the spread of many highly traded stocks is almost always\nequal to one tick (so-called large tick stocks). This property has given birth to speci\ufb01c models of\nsuch limit order books assuming a constant spread. However, in the studied sample of 36 stocks\ntraded on the Paris stock exchange in 2015, spread cannot be assume equal to one. As such, it must\nbe considered a possible trading signal, in\ufb02uencing the order \ufb02ows. If the observed spread is large,\nthen a trader wanting to buy a share would rather submit a bid limit order inside the spread than\nan ask market order. That way the trader will secure priority for the next sell market order, while\nobtaining a lower price. A market order should only be used by an impatient trader, or a trader\nbelieving in an incoming and lasting upward market movement. On the contrary, a spread equal to\none tick prevents the use of limit orders to secure priority in future executions. Such mechanisms\nhave been observed in Muni Toke & Yoshida (2017) where data analysis shows that the empirical\nintensity of market orders increases when the spread decreases to one tick.\nWe can therefore propose a version of our ratio model to investigate the in\ufb02uence of the spread\non orders \ufb02ows occurring at the best quotes. Let us consider a model of best quotes of the limit\norder book where market orders, limit orders and cancellations are submitted with intensities \u03bbM,\n\u03bbL and \u03bbC respectively, all sharing an unobserved baseline intensity \u03bb0(t) and depending on the\n19",
    "chunk_index": 18,
    "start_char": 41065,
    "end_char": 43680,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "spread decreases to one tick.\nWe can therefore propose a version of our ratio model to investigate the in\ufb02uence of the spread\non orders \ufb02ows occurring at the best quotes. Let us consider a model of best quotes of the limit\norder book where market orders, limit orders and cancellations are submitted with intensities \u03bbM,\n\u03bbL and \u03bbC respectively, all sharing an unobserved baseline intensity \u03bb0(t) and depending on the\n19\n\nobserved spread s(t) \u2208N\u2217(i.e expressed in number of ticks) :\n\u03bbM(t, \u03d1M) = \u03bb0(t) exp\n\u0002\n\u03d1M\n0 + \u03d1M\n1 log s(t) + \u03d1M\n2 (log s(t))2\u0003\n,\n(38)\n\u03bbL(t, \u03d1L) = \u03bb0(t) exp\n\u0002\n\u03d1L\n0 + \u03d1L\n1 log s(t) + \u03d1L\n2 (log s(t))2\u0003\n,\n(39)\n\u03bbC(t, \u03d1C) = \u03bb0(t) exp\n\u0002\n\u03d1C\n0 + \u03d1C\n1 log s(t) + \u03d1C\n2 (log s(t))2\u0003\n.\n(40)\nLetting \u03b8L\nj = \u03d1L\nj \u2212\u03d1M\nj\nand \u03b8C\nj = \u03d1C\nj \u2212\u03d1M\nj , j = 0, 1, 2, our intensities ratio model is easily estimated\nby maximizing the quasi-log likelihood.\nWe estimate the model for each stock and each trading days, which gives after data cleaning 8052\ndi\ufb00erent samples and associated model \ufb01ts. As in the previous cases, we compute the intensities\nratios to estimate the probabilities of each event given the observed spread. Figure 6 plots the\nprobabilities of market orders, limit orders and cancellations occurring at the best quotes given the\nobserved spread, and compare them to the empirical probabilities computed on the sample. Again\nfor the sake of brevity and representativity we rank the \ufb01ts by increasing mean L2 error between\nmodel and data, and select four stocks representing the 20%, 40%, 60% and 80% quantiles of the\nerror distribution. It is satisfying to observe that the model provide good \ufb01ts in a wide range\nof spread distribution, from a large tick stock (Figure 6, bottom left) where the spread is almost\nalways equal to one tick, to the more interesting case of small tick stocks (e.g., Figure 6, top right),\nwhere the probability is correctly estimated up to 9 ticks and more.\n5.4\nEquilibrium behaviour with respect to the queues sizes\nWe propose a \ufb01nal illustration of the ratio model to investigate the role of the observed queue\nsize in determining the \ufb02ows of limit orders and cancellations.\nSome empirical observations of\nthe role of the queue size can be observed in Huang et al.\n(2015) as well as in Muni Toke &\nYoshida (2017). A basic equilibrium argument suggests that when a queue size at a given price is\nsmall, then providing liquidity with limit orders is interesting as it secures a relative priority for the\ntrader. On the contrary, when a queue size is large, then cancellations should be more frequent than\nlimit orders. Furthermore, such an argument should be \u201cmore\u201d valid inside the book, where only\nlimit and market orders are allowed, than closer to the best quotes, where market orders remove\nimportant portions of liquidity and other trading signals, such as the spread and the imbalance\npreviously investigated, play a signi\ufb01cant role.",
    "chunk_index": 19,
    "start_char": 43261,
    "end_char": 46140,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "when a queue size at a given price is\nsmall, then providing liquidity with limit orders is interesting as it secures a relative priority for the\ntrader. On the contrary, when a queue size is large, then cancellations should be more frequent than\nlimit orders. Furthermore, such an argument should be \u201cmore\u201d valid inside the book, where only\nlimit and market orders are allowed, than closer to the best quotes, where market orders remove\nimportant portions of liquidity and other trading signals, such as the spread and the imbalance\npreviously investigated, play a signi\ufb01cant role.\nWe investigate these insights by building one simple ratio model for limit orders and cancella-\ntions occurring inside the book, from level 2 to 10. Level 1 (best quote) is left aside as its dynamics\nalso involves market orders. For each level \u03b1 \u2208{2, . . . , 10}, let NL\u03b1 and NC\u03b1 be the counting\n20\n\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n1\n2\n3\n4\n5\n6\n7\nObserved spread\nProbability\nG\nG\nG\nMarket orders\nLimit orders\nCancel orders\nTECF.PA 20150324\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n1\n2\n3\n4\n5\n6\n7\n8\n9\nObserved spread\nProbability\nG\nG\nG\nMarket orders\nLimit orders\nCancel orders\nSTM.PA 20150520\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n1\n2\n3\nObserved spread\nProbability\nG\nG\nG\nMarket orders\nLimit orders\nCancel orders\nLVMH.PA 20150908\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n1\n2\n3\n4\nObserved spread\nProbability\nG\nG\nG\nMarket orders\nLimit orders\nCancel orders\nSAF.PA 20150603\nFigure 6: Empirical probability (dashed lines) and \ufb01tted probability (full lines) that the next order\nis a market order (red), a limit order (blue) or an cancellation (green), given the observed spread.\nx-axis spans more than 99% of the empirical spread distribution, and the shaded area on the right\nsignal the 95% and 99% quantiles of this distribution. Selected samples represent the 20% (top\nleft), 40% (top right), 60% (bottom left) and 80% (bottom right) quantiles of the mean L2 error\ndistribution among the 8052 tested samples.\n21\n\nprocesses of limits orders and cancellations occurring at this level, with intensities:\n\u03bbL\u03b1(t, \u03b8L\u03b1) = \u03bb0(t) exp\nh\n\u03b8L\u03b1\n0\n+ \u03b8L\u03b1\n1\nlog q\u03b1(t) + \u03b8L\u03b1\n2 (log q\u03b1(t))2i\n,\n\u03bbC\u03b1(t, \u03b8C\u03b1) = \u03bb0(t) exp\nh\n\u03b8C\u03b1\n0\n+ \u03b8C\u03b1\n1\nlog q\u03b1(t) + \u03b8C\u03b1\n2 (log q\u03b1(t))2i\n.",
    "chunk_index": 20,
    "start_char": 45559,
    "end_char": 48004,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "spread.\nx-axis spans more than 99% of the empirical spread distribution, and the shaded area on the right\nsignal the 95% and 99% quantiles of this distribution. Selected samples represent the 20% (top\nleft), 40% (top right), 60% (bottom left) and 80% (bottom right) quantiles of the mean L2 error\ndistribution among the 8052 tested samples.\n21\n\nprocesses of limits orders and cancellations occurring at this level, with intensities:\n\u03bbL\u03b1(t, \u03b8L\u03b1) = \u03bb0(t) exp\nh\n\u03b8L\u03b1\n0\n+ \u03b8L\u03b1\n1\nlog q\u03b1(t) + \u03b8L\u03b1\n2 (log q\u03b1(t))2i\n,\n\u03bbC\u03b1(t, \u03b8C\u03b1) = \u03bb0(t) exp\nh\n\u03b8C\u03b1\n0\n+ \u03b8C\u03b1\n1\nlog q\u03b1(t) + \u03b8C\u03b1\n2 (log q\u03b1(t))2i\n.\n(41)\nwhere \u03bb0(t) is a potentially random baseline intensity and q\u03b1(t) is the volume standing in the book\nat the level of submission in group \u03b1 and time t. q\u03b1(t) is expressed in integer multiple of the median\ntrade size (see e.g., Huang et al. (2015); Muni Toke & Yoshida (2017) about this normalization).\nLet \u03b8\u03b1\nj = \u03b8L\u03b1\nj\n\u2212\u03b8C\u03b1\nj , j = 0, 2, \u03b1 \u2208{2, . . . , 10}, be the relative parameters of the ratio model, to be\nestimated by the maximization of quasi-log likelihood.\nAs in the previous examples, we estimate the model for each stock and each month, and then\ncompute the intensities ratios to estimate the probabilities of each event given the observed volume\nof the level of submission. Each monthly \ufb01t gives 9 \ufb01ts, one for each level, but for the sake of\nreadability, we plot only three levels: level 2 at the head of the book, level 5 at mid-depth and level\n8 at the back of the book. Figure 7 plots the probabilities of limit orders occurring inside the book\nat these three levels, given the observed volume of the level, and compare them to the empirical\nprobabilities computed on the sample. We show four examples of \ufb01ts (again selected by computing\nthe quantiles of the mean L2 error of the \ufb01ts), but all monthly \ufb01ts are quite similar and available.\nIt is interesting that the probabilities of occurrence of a limit order as a function of the observed\nvolume show similar patterns across stocks. The model obviously recovers the expected equilibrium\nproperty that decreases the interest (hence the probability) of a limit order when liquidity is already\nhere (i.e. q\u03b1 is large). Furthermore, the use of a common x-axis makes the the probability curves\nre\ufb02ect the general shape of a limit order book. Using a rule of thumb that the equilibrium size of\neach level should be roughly at the crossing of the .5 probability line, the respective position of the\nthree probability curves re\ufb02ects the well-known humped shaped of the limit order book (Bouchaud\net al. (2002)): the mid-book (level 5) is fatter than the head (level 2) and tail (level 8) of the book.\n6\nEmpirical results : Prediction\nEmpirical results from Section 5 are in-sample analysis aimed at providing new descriptions of some\ndependencies observed on \ufb01nancial markets.",
    "chunk_index": 21,
    "start_char": 47424,
    "end_char": 50244,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "equilibrium size of\neach level should be roughly at the crossing of the .5 probability line, the respective position of the\nthree probability curves re\ufb02ects the well-known humped shaped of the limit order book (Bouchaud\net al. (2002)): the mid-book (level 5) is fatter than the head (level 2) and tail (level 8) of the book.\n6\nEmpirical results : Prediction\nEmpirical results from Section 5 are in-sample analysis aimed at providing new descriptions of some\ndependencies observed on \ufb01nancial markets. We now turn to the possible use of the ratio model\nas a prediction tool. We consider the problem of the sign of the next trade, previously introduced\nin Section 5.2. The ratio model has helped us model how this sign depends on the state of the\norder book, through e.g., the imbalance, the spread or the last trade sign. Time series of trade\nsigns are known to be highly auto-correlated, and to be well described by Hawkes processes. In this\nsetting, market orders can be described by a two-dimensional Hawkes process Z = (ZB, ZA) with\n22\n\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nGGGGGGGGGGGGG\nGGGG\nGG\nGGGG\nGGGGGG\nGGGGGGG\nGGGGGGG\nGGGGG\nG\nGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGGGG\nGG\nGGGGGGGG\nGGGG\nG\nGGGGGGGGG\nGGG\nG\nG\nGGGG\nGG\nGG\nGG\nGGG\nGGG\nG\nG\nG\nGGG\nG\nGGGGGGG\nGG\nGG\nGGG\nG\nGG\nGGG\nG\nG\nG\nGG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGGG\nG\nGG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25\n50\n75\nObserved q\nProbability that next order is limit vs cancel\nLevels\nG\nG\nG\n2\n5\n8\nAIRP.PA 201511\nG\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG G G\nG G G G G G G G G G\nG G G G\nG\nG\nG\nG G\nG\nG\nG G\nG\nG G\nG G\nG\nG\nG G\nG G\nG G\nG G G\nG G\nG G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG G\nG G\nG G\nG\nG G\nG G\nG\nG\nG G G\nG\nG\nG\nG G G\nG\nG\nG G\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n10\n20\n30\n40\nObserved q\nProbability that next order is limit vs cancel\nLevels\nG\nG\nG\n2\n5\n8\nSGOB.PA 201504\nG\nG\nG\nG\nG\nG",
    "chunk_index": 22,
    "start_char": 49744,
    "end_char": 51693,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG G\nG G\nG G\nG\nG G\nG G\nG\nG\nG G G\nG\nG\nG\nG G G\nG\nG\nG G\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n10\n20\n30\n40\nObserved q\nProbability that next order is limit vs cancel\nLevels\nG\nG\nG\n2\n5\n8\nSGOB.PA 201504\nG\nG\nG\nG\nG\nG\nG G\nG G G G G G\nG G G G G\nG G G G G G\nG G\nG G\nG G G\nG G\nG G G G G\nG\nG\nG\nG\nG\nG G\nG\nG G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG G G G G G G G\nG G\nG\nG\nG G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G G\nG G G\nG G\nG G\nG\nG G G\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n10\n20\n30\n40\nObserved q\nProbability that next order is limit vs cancel\nLevels\nG\nG\nG\n2\n5\n8\nSAF.PA 201511\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n10\n20\nObserved q\nProbability that next order is limit vs cancel\nLevels\nG\nG\nG\n2\n5\n8\nBOUY.PA 201509\nFigure 7: Empirical probability (dots and dashed lines) and \ufb01tted probability (lines) that the next\norder is a limit order, given the observed level of the queue of submission. x-axis spans more than\n99% of the empirical volume distribution (at all levels), and the shaded area on the right signal the\n95% and 99% quantiles of this distribution.\n23\n\nexponential kernels, its conditional intensity \u03bbZ = (\u03bbZB, \u03bbZA) being written in vector notations as\n\u03bbZ(t) = \u00b5Z +\nZ t\n0\nK(t \u2212s) dZs,\n(42)\nwhere \u00b5Z = (\u00b5ZB, \u00b5ZA) is the baseline intensity and the kernel matrix\nK(t) =\n \n\u03b1BBe\u2212\u03b2BBt\n\u03b1BAe\u2212\u03b2BAt\n\u03b1ABe\u2212\u03b2ABt\n\u03b1AAe\u2212\u03b2AAt\n!\n(43)\ndescribes the self- and cross- excitation parts.\nA simple ratio model with only current observations of the state and not taking into account\nthe history of the order \ufb02ow (except for the last trade sign) can probably not compete with the\nHawkes description. However, we can incorporate some history into covariates of the ratio model.",
    "chunk_index": 23,
    "start_char": 51461,
    "end_char": 53286,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "\u00b5ZA) is the baseline intensity and the kernel matrix\nK(t) =\n \n\u03b1BBe\u2212\u03b2BBt\n\u03b1BAe\u2212\u03b2BAt\n\u03b1ABe\u2212\u03b2ABt\n\u03b1AAe\u2212\u03b2AAt\n!\n(43)\ndescribes the self- and cross- excitation parts.\nA simple ratio model with only current observations of the state and not taking into account\nthe history of the order \ufb02ow (except for the last trade sign) can probably not compete with the\nHawkes description. However, we can incorporate some history into covariates of the ratio model.\nOne may for example consider the following covariates:\nHA(t) = log\n\u0012\n\u00b5A +\nZ t\n0\n\u03b1Ae\u2212\u03b2A(t\u2212s) dNA\ns\n\u0013\n,\n(44)\nHB(t) = log\n\u0012\n\u00b5B +\nZ t\n0\n\u03b1Be\u2212\u03b2B(t\u2212s) dNB\ns\n\u0013\n,\n(45)\nwhere (NB, NA) counts the number of bid and ask market orders. These covariates add some\nself-exciting history into the ratio model.\nWe can thus examine di\ufb00erent methods to predict the trade sign, given that one trade is\nobserved a given time. This is of course a theoretical exercise, as we predict the trade sign with all\nthe information available just before its occurrence, not taking into account latency, information\ndelays, reaction times, etc., that would hinder the performances in a more realistic setting. We test\nseven methods for this exercise:\n\u2022 Last : the trade sign is set to be the same as the last observed trade sign ;\n\u2022 Imbalance : the trade sign is set to \u22121 if the imbalance observed before its submission is\nnegative, +1 otherwise;\n\u2022 Hawkes Full : at each time we compute the intensity \u03bbZ of the Hawkes process described at\nEquations (42)-(43), given the observed history, and set the sign to be \u22121 if \u03bbZB > \u03bbZA, +1\notherwise.\n\u2022 Hawkes NoCross : same as the previous method, except that we independently \ufb01t two Hawkes\nprocesses for the bid and ask market orders, i.e. we set \u03b1BA = \u03b1AB = 0;\n\u2022 Ratio (i, \u03f5, \u03f5s) : we use the probabilities given by the basic ratio model of Equation (36) and\nset the sign to be \u22121 if the probability of an ask market order given by the ratio model is\n24\n\nFigure 8: Fraction of correctly predicted trade sign per method and per stock, averaged on all\navailable trading days.\nlower than 0.5, and +1 otherwise ;\n\u2022 Ratio (HB, HA) : same as the previous method, but we use only the covariates (HB, HA) in\nthe ratio model, instead of the covariates describing the state of the book ;\n\u2022 Ratio (HB, HA, i, \u03f5, \u03f5s) : same as the previous method, but using both (HA, HB) and the\ncovariates (i, \u03f5, \u03f5s) in the ratio model.\nWe test these methods on the sample described at Section 5.1. \u201cLast\u201d and \u201cImbalance\u201d methods\ndo no require any calibration. For the other methods, calibration is carried out on the trading\nday preceding the day at which prediction performance is evaluated. Models \u201cHawkes Full\u201d and\n\u201cHawkes NoCross\u201d are \ufb01tted with a maximum-likelihood estimation (see e.g. Ogata (1978), Ozaki\n(1979) and later Bowsher (2007), Bacry et al.",
    "chunk_index": 24,
    "start_char": 52843,
    "end_char": 55629,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "the previous method, but using both (HA, HB) and the\ncovariates (i, \u03f5, \u03f5s) in the ratio model.\nWe test these methods on the sample described at Section 5.1. \u201cLast\u201d and \u201cImbalance\u201d methods\ndo no require any calibration. For the other methods, calibration is carried out on the trading\nday preceding the day at which prediction performance is evaluated. Models \u201cHawkes Full\u201d and\n\u201cHawkes NoCross\u201d are \ufb01tted with a maximum-likelihood estimation (see e.g. Ogata (1978), Ozaki\n(1979) and later Bowsher (2007), Bacry et al. (2013) or Muni Toke & Pomponio (2012) in a \ufb01nancial\nsetting). The parameters (\u00b5A, \u03b1A, \u03b2A) and (\u00b5B, \u03b1B, \u03b2B) used to compute the covariates HA and\nHB are obtained by the same method, i.e. the estimated values of the parameters (\u00b5A, \u03b1A, \u03b2A)\nused to compute HA correspond to the estimated values (\u00b5ZA, \u03b1AA, \u03b2AA) of the \u201cHawkes NoCross\u201d\nmodel. Note that the preceding day in the sample means Friday for Mondays, and possibly some\nother previous day in case of missing trading days in the data. Recall that this sample represents\nroughly 54 millions trades.\nResults are illustrated in Figure 8. For all stocks, the \u201cLast\u201d indicator correctly predicts more\nthan 80% of the trade signs, e.g. less than one trade out of 5 has a sign di\ufb00erent from the preceding\n25\n\nFigure 9: Fraction of correctly predicted trade sign per method and per stock, averaged on all\navailable trading days.\ntrade. The \u201cImbalance\u201d indicator is less performant, signing correctly about 70% \u221280% of the\ntrades. The model \u201cRatio (i, \u03f5, \u03f5s)\u201d of Section 5.2 builds on these two indicators to improve the\nsigning performance to about 85%. Hawkes models perform as the \u201cImbalance\u201d: the performance\nof the \u201cHawkes Full\u201d method lies in the range 73% \u221278%, less than the \u201cHawkes NoCross\u201d model\nthat actually does better in this prediction exercise, always around 75% \u221280%. This result is an\nillustration of the observation that parcimony is often crucial for prediction. Now, it is interesting to\nobserve that the ratio model is actually able to match and improve the Hawkes performances. Using\nonly the covariates (HB, HA), the \u201cRatio (HB, HA)\u201d method actually mimicks the Hawkes process,\nthe two performances curves being very close. The advantage of the ratio model is the possibility\nto consider both the history covariates (HB, HA) and the state covariates (i, \u03f5, \u03f5s). The model\n\u201cRatio (HB, HA, i, \u03f5, \u03f5s)\u201d turns out to deliver the best prediction of the trade sign, improving the\nperformance of the \u201cRatio (i, \u03f5, \u03f5s)\u201d model by a bit more than 2.5% in average on all the stock-days.\nWe can analyse a bit further these performances. We now focus on the prediction of a sign\nchange, i.e. we compute the performance using only the trades that have a sign di\ufb00erent from the\nprevious trade (roughly 20% of the sample, given the observation above). Results are averaged for\neach stock on Figure 9. The performance of the \u201cImbalance\u201d signal is roughly similar.",
    "chunk_index": 25,
    "start_char": 55113,
    "end_char": 58044,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "\u03f5s)\u201d model by a bit more than 2.5% in average on all the stock-days.\nWe can analyse a bit further these performances. We now focus on the prediction of a sign\nchange, i.e. we compute the performance using only the trades that have a sign di\ufb00erent from the\nprevious trade (roughly 20% of the sample, given the observation above). Results are averaged for\neach stock on Figure 9. The performance of the \u201cImbalance\u201d signal is roughly similar. The \u201cLast\u201d\nindicator has obviously a performance equal to 0, as it never predicts a sign change. The ratio\nmodel without any history also performs poorly (around 40%) in this speci\ufb01c subsample, since its\nonly history relies on the last trade sign. Both Hawkes model are also less performant, always even\n26\n\nworse than the basic \u201cImbalance\u201d signal. The interesting point is that in this speci\ufb01c subsample,\nthe combination of the Hawkes covariates and the state covariates (i, \u03f5, \u03f5s) described in Section 5.2\nsigni\ufb01cantly improves the Hawkes models and the \u201cRatio (i, \u03f5, \u03f5s)\u201d model. The average performance\nincrease on all the stock-days of the sub-sample for the \u201cRatio (HB, HA, i, \u03f5, \u03f5s)\u201d model with respect\nto the best Hawkes model is more than 13%. In brief, trade signs are quite well represented by\nHawkes processes, but the addition of a state-dependency by means of a ratio model allows a better\ntracking of the sign changes, explaining an overall better performance.\n7\nConclusion\nWe have presented a model based on ratios of Cox-type intensities sharing a common, possibly\nrandom, baseline intensity. Consistency and asymptotic normality of the estimators have been\nproved.\nSuch a model may be very useful in cases where one tries to investigate the role of\ngiven covariates on point processes in a \ufb02uctuating environment, but in which (some of) these\n\ufb02uctuations are assumed to equally in\ufb02uence all the processes under scrutiny. This is therefore\na plausible framework in \ufb01nance, where global market activity varies wildly during the trading\nday. The proposed setting removes this common baseline intensity from the estimation procedure,\nallowing to focus on the role of covariates. Using this method we have in particular been able to\nhighlight how signals such as market imbalance, bid-ask spread and sizes of limit order book queues\ndo in\ufb02uence trading activity. This framework may hopefully be helpful in many other studies in\nhigh-frequency \ufb01nance. Several directions of research and applications may indeed be followed,\namong which model selection and the identi\ufb01cation of signi\ufb01cant trading signals.\nAcknowledgements\nThis work was in part supported by Japan Science and Technology Agency CREST JPMJCR14D7;\nJapan Society for the Promotion of Science Grants-in-Aid for Scienti\ufb01c Research No. 17H01702\n(Scienti\ufb01c Research), No. 26540011 (Challenging Exploratory Research); and by a Cooperative\nResearch Program of the Institute of Statistical Mathematics.\nA\nMathematical proofs\nA.1\nProof of Theorem 3.1\nWe have\nri(t, \u03b8) = \u03c1i(X(t), \u03b8)\n(i \u2208I, \u03b8 \u2208\u0398)\n(46)\nand\n\u03bbi(t, \u03d1\u2217) = \u03c1i(X(t), \u03b8\u2217)\u039b(\u03bb0(t), X(t)).\n(i \u2208I)\n(47)\n27",
    "chunk_index": 26,
    "start_char": 57605,
    "end_char": 60668,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "part supported by Japan Science and Technology Agency CREST JPMJCR14D7;\nJapan Society for the Promotion of Science Grants-in-Aid for Scienti\ufb01c Research No. 17H01702\n(Scienti\ufb01c Research), No. 26540011 (Challenging Exploratory Research); and by a Cooperative\nResearch Program of the Institute of Statistical Mathematics.\nA\nMathematical proofs\nA.1\nProof of Theorem 3.1\nWe have\nri(t, \u03b8) = \u03c1i(X(t), \u03b8)\n(i \u2208I, \u03b8 \u2208\u0398)\n(46)\nand\n\u03bbi(t, \u03d1\u2217) = \u03c1i(X(t), \u03b8\u2217)\u039b(\u03bb0(t), X(t)).\n(i \u2208I)\n(47)\n27\n\nSimple calculus yields\n\u2202\u03b8\u03b1 log \u03c1i(x, \u03b8) =\n\b\n\u03b4\u03b1,i \u2212\u03c1\u03b1(x, \u03b8)\n \nx\n(i \u2208I, \u03b1 \u2208I0, \u03b8 \u2208Rp)\n(48)\nand\n\u2202\u03b8\u03b1\u2032\u2202\u03b8\u03b1 log \u03c1i(x, \u03b8) = \u2212\n\b\n\u03b4\u03b1\u2032,\u03b1\u03c1\u03b1(x, \u03b8) \u2212\u03c1\u03b1\u2032(x, \u03b8)\u03c1\u03b1(x, \u03b8)\n \nx\u22972\n(i \u2208I; \u03b1\u2032, \u03b1 \u2208I0, \u03b8 \u2208Rp)\n(49)\nwhere \u03b8\u03b1 = (\u03b8\u03b1\nj )j\u2208J. In other words,\n\u2202\u03b8\u03b1\u2032\u2202\u03b8\u03b1 log \u03c1i(x, \u03b8) = \u2212V(x, \u03b8)\u03b1\u2032,\u03b1x\u22972\n(i \u2208I; \u03b1\u2032, \u03b1 \u2208I0, \u03b8 \u2208Rp).\n(50)\nFor the closed convex hull C[\u0398] of \u0398, let\nY(\u03b8) = E\n\u0014 X\ni\u2208I\nlog \u03c1i(X(0), \u03b8)\n\u03c1i(X(0), \u03b8\u2217) \u03c1i(X(0), \u03b8\u2217)\u039b(\u03bb0(0), X(0))\n\u0015\n(\u03b8 \u2208C[\u0398])\n(51)\nThen\nY(\u03b8) \u22640\n(\u03b8 \u2208C[\u0398])\n(52)\nand the equality holds if and only if\n\u03c1i(X(0), \u03b8) = \u03c1i(X(0), \u03b8\u2217)\n\u03bb0(0)dP-a.e.\n(\u2200i \u2208I).\n(53)\nCondition (53) implies that\nexp\n\u0000X(0)[\u03b8i] \u2212X(0)[\u03b8\u2217i]\n\u0001\n= exp\n\u0000X(0)[\u03b80] \u2212X(0)[\u03b8\u22170]\n\u0001\n= 1\n\u03bb0(0)dP-a.e.\n(\u2200i \u2208I)\n(54)\ndue to \u03b80 = \u03b8\u22170 = 0. Therefore,\nE\n\u0014\nV0(X(0))i,i\n\u0000X(0)\n\u0002\n\u03b8i \u2212\u03b8\u2217i\u0003\u00012\u039b(\u03bb0(0), X(0))\n\u0015\n= 0\n(\u2200i \u2208I0)\n(55)\nand hence \u03b8i = \u03b8\u2217i for all i \u2208I0 due to Condition [A3] applied to u =\n\u0000\u03b4i,i\u2032(\u03b8i\nj \u2212\u03b8\u2217i\nj )\n\u0001\ni\u2032\u2208I0,j\u2208J.\nWe see \u0393 = \u2212\u22022\n\u03b8Y(\u03b8\u2217). Since \u0393 is positive de\ufb01nite and Y(\u03b8) \u0338= 0 for all \u03b8 \u2208\u0398\\{\u03b8\u2217}, there exists\na constant \u03c70 > 0 such that\nY(\u03b8) = Y(\u03b8) \u2212Y(\u03b8\u2217) \u2264\u2212\u03c70\n\f\f\u03b8 \u2212\u03b8\u2217\f\f2\n(56)\nfor all \u03b8 \u2208C[\u0398].\nLet UT = {u \u2208Rp; \u03b8\u2020\nT (u) \u2208\u0398}, where \u03b8\u2020\nT (u) = \u03b8\u2217+ T \u22121/2u. The quasi-likelihood ratio random\n28\n\n\ufb01eld is de\ufb01ne by\nZT (u) = exp\n\u0000HT (\u03b8\u2217+ T \u22121/2u) \u2212HT (\u03b8\u2217)\n\u0001\n(u \u2208UT ).\n(57)\nWe will use also\nZT (u) = exp\n\u0000HT (\u03b8\u2217+ T \u22121/2u) \u2212HT (\u03b8\u2217)\n\u0001\n(u \u2208C[UT ]).\n(58)\nFor this extension, we notice ri(t, \u03b8) and hence HT is naturally extended to C[UT ]. The random\n\ufb01eld ZT will be used to obtain the so-called polynomial type large deviation inequality for ZT .\nDe\ufb01ne YT by\nYT (\u03b8) = T \u22121\u0000HT (\u03b8) \u2212HT (\u03b8\u2217)\n\u0001\n(\u03b8 \u2208C[\u0398])\n(59)\nwith the naturally extended HT . De\ufb01ne a p-dimensional random variable \u2206T and a p \u00d7 p random\nmatrix \u0393T by\n\u2206T = T \u22121/2\u2202\u03b8HT (\u03b8\u2217)\n(60)\nand\n\u0393T = \u2212T \u22121\u22022\n\u03b8HT (\u03b8\u2217)\n(61)\nrespectively. Let \u0393T (\u03b8) = \u2212T \u22121\u22022\n\u03b8HT (\u03b8) for \u03b8 \u2208C[\u0398]. Then\n\u0393T (\u03b8)[u\u22972] = 1\nT\nZ T\n0\n\u0012\nV0(X(t), \u03b8) \u2297X(t)\u22972\n\u0013\n[u\u22972]\nX\ni\u2208I\ndNi\nt.\n(62)\nTake parameters \u03b1, \u03b21, \u03c11 and \u03c12 so that\n0 < \u03b21 < 1\n2,\n0 < \u03c11 < min\n\u001a\n1, \u03b2, 2\u03b21\n1 \u2212\u03b1\n\u001b\n,\n0 < 2\u03b1 < \u03c12,\n1 \u2212\u03c12 > 0,\n(63)\nwhere \u03b2 = \u03b1/(1 \u2212\u03b1).",
    "chunk_index": 27,
    "start_char": 60195,
    "end_char": 62656,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "\u00d7 p random\nmatrix \u0393T by\n\u2206T = T \u22121/2\u2202\u03b8HT (\u03b8\u2217)\n(60)\nand\n\u0393T = \u2212T \u22121\u22022\n\u03b8HT (\u03b8\u2217)\n(61)\nrespectively. Let \u0393T (\u03b8) = \u2212T \u22121\u22022\n\u03b8HT (\u03b8) for \u03b8 \u2208C[\u0398]. Then\n\u0393T (\u03b8)[u\u22972] = 1\nT\nZ T\n0\n\u0012\nV0(X(t), \u03b8) \u2297X(t)\u22972\n\u0013\n[u\u22972]\nX\ni\u2208I\ndNi\nt.\n(62)\nTake parameters \u03b1, \u03b21, \u03c11 and \u03c12 so that\n0 < \u03b21 < 1\n2,\n0 < \u03c11 < min\n\u001a\n1, \u03b2, 2\u03b21\n1 \u2212\u03b1\n\u001b\n,\n0 < 2\u03b1 < \u03c12,\n1 \u2212\u03c12 > 0,\n(63)\nwhere \u03b2 = \u03b1/(1 \u2212\u03b1). We have\nLemma A.1. Suppose that [A1] and [A2] are satis\ufb01ed. Let p be any positive number. Then\n(i) supT>1\n\r\r\u2206T\n\r\r\np < \u221e.\n(ii) supT>1\n\r\r\r\r sup\u03b8\u2208C[\u0398] T\n1\n2 \u0000YT (\u03b8) \u2212Y(\u03b8)\n\u0001\r\r\r\r\np\n< \u221e.\n(iii) supT>1\n\r\r\r\rT \u22121 sup\u03b8\u2208C[\u0398] |\u22023\n\u03b8HT (\u03b8)\n\f\f\n\r\r\r\r\np\n< \u221e.\n(iv) supT>1 E\n\r\rT \u03b21|\u0393T \u2212\u0393|\n\r\r\np < \u221e.\nProof. Let \u03c1(x, \u03b8) =\n\u0000\u03c1i(x, \u03b8)\n\u0001\ni\u2208I0. Let ei = (\u03b4\u03b1,i)\u03b1\u2208I0 for i \u2208I. De\ufb01ne gi \u2208Ri \u2297Rj (i \u2208I) by\ngi(x, \u03b8) =\n\u00001{i\u0338=0}ei \u2212\u03c1(x, \u03b8)\n\u0001\n\u2297x\n(i \u2208I)\n(64)\n29\n\nThen\n\u2206T = T \u22121/2 X\ni\u2208I\nZ T\n0\ngi(X(t), \u03b8\u2217)d \u02dcNi\nt,\n(65)\nwhere\n\u02dcNi\nt = Ni\nt \u2212\nZ t\n0\n\u03bbi(s, \u03d1\u2217)ds\n(i \u2208I)\n(66)\nWe have\n\u03bbi(t, \u03b8) \u2264\u03bb0(t)\nX\ni\u2208I\nexp\n\u0000X(t)[\u03d1\u2217i]\n\u0001\n, |gi(X(t), \u03b8)| \u22642|X(t)|\n(67)\nand\n| log ri(t, \u03b8)| = | log \u03c1i(X(t), \u03b8)| \u2264Ci (1 + |X(t)||\u03b8|)\n(68)\nfor some constant Ci depending on i.\nBy the Burkholder-Davis-Gundy inequality, we obtain\nE\n\u0014\f\f\f\fT \u22121/2\nZ T\n0\nh(X(t))d \u02dcNi\nt\n\f\f\f\f\n2k\u0015\n\u2264CkE\n\u0014\f\f\f\fT \u22121\nZ T\n0\nh(X(t))2dNi\nt\n\f\f\f\f\nk\u0015\n\u2264CkE\n\u0014\f\f\f\fT \u22121\nZ T\n0\nh(X(t))2d \u02dcNi\nt\n\f\f\f\f\nk\u0015\n+ CkE\n\u0014\nT \u22121\nZ T\n0\n\f\fh(X(t))\n\f\f2k\u03bbi(t, \u03d1\u2217)kdt\n\u0015\n\u2264CkE\n\u0014\f\f\f\fT \u22121\nZ T\n0\nh(X(t))2d \u02dcNi\nt\n\f\f\f\f\nk\u0015\n+ CkE\n\u0014\f\fh(X(0))\n\f\f2k\u03bbi(0, \u03d1\u2217)k\n\u0015\n(69)\nfor any k \u2208N and any measurable function h of at most polynomial growth. By Equation (65) and\ninduction, we obtain (i).\nSet\nMT (\u03b8) =\nX\ni\u2208I\nT \u22121\nZ T\n0\nlog \u03c1i(X(t), \u03b8)\n\u03c1i(X(t), \u03b8\u2217)d \u02dcNi\nt\n(70)\nand\nKT (\u03b8) = T \u22121\nZ T\n0\nf(\u03bb0(t), X(t), \u03b8)dt,\n(71)\nwhere\nf(w, x, \u03b8) =\nX\ni\u2208I\nlog \u03c1i(x, \u03b8)\n\u03c1i(x, \u03b8\u2217)\u03c1i(x, \u03b8\u2217)\u039b(w, x)\n\u2212E\n\u0014 X\ni\u2208I\nlog \u03c1i(X(0), \u03b8)\n\u03c1i(X(0), \u03b8\u2217)\u03c1i(X(0), \u03b8\u2217)\u039b(\u03bb0(0), X(0))\n\u0015\n.\n(72)\n30\n\nThen\nYT (\u03b8) \u2212Y(\u03b8) = MT (\u03b8) + KT (\u03b8).\n(73)\nSimilarly to the proof of (i), we obtain\nX\nk=0,1\nsup\n\u03b8\u2208C[\u0398]\nsup\nT>1\n\u2225T 1/2\u2202k\n\u03b8 MT (\u03b8)\u2225p < \u221e\n(74)\nfor every p \u22652. Moreover, applying Theorem 6.3 of Rio (2017) under [A1] and [A2], we obtain\nX\nk=0,1\nsup\n\u03b8\u2208C[\u0398]\nsup\nT>1\n\u2225T 1/2\u2202k\n\u03b8 KT (\u03b8)\u2225p < \u221e\n(75)\nfor every p \u22652. Now Sobolev\u2019s embedding inequality in W 1,p(C[\u0398]) ,\u2192C(C[\u0398]) for p > p \u22282 gives\n(ii). In a similar fashion, it is possible to prove (iii).",
    "chunk_index": 28,
    "start_char": 62305,
    "end_char": 64509,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "proof of (i), we obtain\nX\nk=0,1\nsup\n\u03b8\u2208C[\u0398]\nsup\nT>1\n\u2225T 1/2\u2202k\n\u03b8 MT (\u03b8)\u2225p < \u221e\n(74)\nfor every p \u22652. Moreover, applying Theorem 6.3 of Rio (2017) under [A1] and [A2], we obtain\nX\nk=0,1\nsup\n\u03b8\u2208C[\u0398]\nsup\nT>1\n\u2225T 1/2\u2202k\n\u03b8 KT (\u03b8)\u2225p < \u221e\n(75)\nfor every p \u22652. Now Sobolev\u2019s embedding inequality in W 1,p(C[\u0398]) ,\u2192C(C[\u0398]) for p > p \u22282 gives\n(ii). In a similar fashion, it is possible to prove (iii). The proof of (iv) is also similar to that of (ii),\nand rather simpler.\nLemma A.2. Suppose that [A1]-[A3] are satis\ufb01ed. Then\n(i) For any L > 0,\nsup\nT>1\nsup\nr>0\nrLP\n\u0014\nsup\nu\u2208VT (r)\nZT (u) \u2265exp\n\u0000\u22122\u22121r2\u2212(\u03c11\u2228\u03c12)\u0001\u0015\n< \u221e,\n(76)\nwhere VT (r) = {u \u2208UT ; |u| \u2265r}.\n(ii) ZT admits a locally asymptotically normal representation\nZT (u) = exp\n\u0012\n\u2206T [u] \u22121\n2\u0393[u\u22972] + rT (u)\n\u0013\n(77)\nwith rT (u) \u2192p 0 as T \u2192\u221efor every u \u2208Rp and \u2206T \u2192d Np(0, \u0393) as T \u2192\u221e.\nProof. We will verify the conditions of Theorem 3 (c) of Yoshida (2011) for the naturally extended\nrandom \ufb01eld HT over C[\u0398]. Condition [A1\u2032\u2032] therein holds according to Lemma A.1 (iii) and (iv).\nCondition [A4\u2032] therein is satis\ufb01ed with \u03b22 = 0 under Equation (63).\nLemma A.1 (i) and (ii)\nensures Condition [A6] therein. Condition [B1] therein is obvious and Condition [B2] therein is\n(56). Therefore, by Theorem 3 of Yoshida (2011), we obtain\nsup\nT>1\nsup\nr>0\nrLP\n\u0014\nsup\nu\u2208C[UT ]:|u|\u2265r\nZT (u) \u2265exp\n\u0000\u22122\u22121r2\u2212(\u03c11\u2227\u03c12)\u0001\u0015\n< \u221e,\n(78)\nwhich gives (i).\n31\n\nThe term rT (u) is de\ufb01ned by (77) for u \u2208UT . For each u \u2208Rp, for su\ufb03ciently large T, rT (u)\nadmits the representation\nrT (u) =\nZ 1\n0\n(1 \u2212s)\n\u001a\n\u0393[u\u22972] \u2212\u0393T (\u03b8\u2020\nT (su))[u\u22972]\n\u001b\nds.\n(79)\nThen Lemma A.1 (iii) and (iv) verify the convergence rT (u) \u2192p 0 as T \u2192\u221e.\nFor u = (ui\nj)i\u2208I0,j\u2208J and x = (xj)j\u2208J,\nX\ni\u2208I\n\u001a\nX\ni1\u2208I0,j1\u2208J\n\u0000\u03b4i,i1 \u2212\u03c1i1(x, \u03b8)\n\u0001\nxj1ui1\nj1\n\u001b2\n\u03c1i(x, \u03b8)\n=\nX\ni1,i2\u2208I0\nj1,j2\u2208J\n\u001a X\ni\u2208I\n\u0000\u03b4i,i1 \u2212\u03c1i1(x, \u03b8)\n\u0001\u0000\u03b4i,i2 \u2212\u03c1i2(x, \u03b8)\n\u0001\n\u03c1i(x, \u03b8)\n\u001b\nxj1xj2ui1\nj1ui2\nj2\n=\nX\ni1,i2\u2208I0\nj1,j2\u2208J\n\u001a\n\u03b4i1,i2\u03c1i1(x, \u03b8) \u2212\u03c1i1(x, \u03b8)\u03c1i2(x, \u03b8)\n\u001b\nxj1xj2ui1\nj1ui2\nj2\n=\n\u0000V0(x, \u03b8) \u2297x\u22972\u0001\n[u\u22972].\n(80)\nSince it is assumed that Ni (i \u2208I) do not have common jumps,\n\u001c\nT \u22121/2 X\ni\u2208I\nZ \u00b7\n0\ngi(X(t), \u03b8\u2217)[u]d \u02dcNi\nt\n\u001d\nT\n= T \u22121 X\ni\u2208I\nZ T\n0\n\u0000gi(X(t), \u03b8\u2217)[u]\n\u00012\u03c1i(X(t), \u03b8\u2217)\u039b(\u03bb0(t), X(t))dt\n= T \u22121\nZ T\n0\n\u0012\nV0(X(t), \u03b8) \u2297X(t)\u22972\n\u0013\n[u\u22972]\u039b(\u03bb0(t), X(t))dt.\n(81)\nUnder [A1] and [A2], the term on the right-hand side converges in probability to \u0393[u\u22972] as T \u2192\u221e.\nThe conditional type Lindeberg condition is easily veri\ufb01ed by dividing the range [0, T] of the integral\n(65) into \u2308T\u2309subintervals, and as a result, we see \u2206T \u2192d Np(0, \u0393), which concludes the proof of\n(ii).",
    "chunk_index": 29,
    "start_char": 64128,
    "end_char": 66594,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "0\n\u0000gi(X(t), \u03b8\u2217)[u]\n\u00012\u03c1i(X(t), \u03b8\u2217)\u039b(\u03bb0(t), X(t))dt\n= T \u22121\nZ T\n0\n\u0012\nV0(X(t), \u03b8) \u2297X(t)\u22972\n\u0013\n[u\u22972]\u039b(\u03bb0(t), X(t))dt.\n(81)\nUnder [A1] and [A2], the term on the right-hand side converges in probability to \u0393[u\u22972] as T \u2192\u221e.\nThe conditional type Lindeberg condition is easily veri\ufb01ed by dividing the range [0, T] of the integral\n(65) into \u2308T\u2309subintervals, and as a result, we see \u2206T \u2192d Np(0, \u0393), which concludes the proof of\n(ii).\nIt is possible to extend ZT to Rp so that the extension has a compact support and\nsup\nu\u2208Rp\\UT\nZT (u) \u2264max\nu\u2208\u2202UT ZT (u).\n(82)\nWe will denote this extended random \ufb01eld by the same ZT . Then ZT is a random variable taking\nvalues in the Banach space \u02c6C = {f \u2208C(Rp); lim|u|\u2192\u221ef(u) = 0} equipped with sup-norm. On\n32\n\nsome probability space, we prepare a random \ufb01eld\nZ(u) = exp\n\u0012\n\u2206[u] \u22121\n2\u0393[u\u22972]\n\u0013\n(u \u2208Rp)\n(83)\nwhere \u2206\u223cNp(0, \u0393). By Lemma A.2 (ii), we have a \ufb01nite-dimensional convergence\nZT \u2192df Z\n(84)\nas T \u2192\u221e.\nFor \u03b4 > 0 and c > 0, de\ufb01ne wT (\u03b4, c) by\nwT (\u03b4, c) = sup\n\u001a\f\f log ZT (u2) \u2212log ZT (u1)\n\f\f; u1, u2 \u2208Bc, |u2 \u2212u1| \u2264\u03b4\n\u001b\n(85)\nfor large T, where Bc = {u \u2208Rp; |u| \u2264c}. Then by Lemma A.1 (iii) and the de\ufb01nition of ZT , we\nhave\nlim\n\u03b4\u21930 lim sup\nT\u2192\u221e\nP\n\u0002\nwT (\u03b4, c) > \u03f5\n\u0003\n= 0\n(86)\nfor every \u03f5 > 0 and c > 0.\nAccording to e.g. Theorem 4 of Yoshida (2011), we obtain (13) for \u02c6uM\nT since Conditions [C1]\nand [C3] therein are ensured by (86) and by (84), respectively, and Condition [C2] is trivial now.\nThe properties (86) and (84) give functional convergence\nZT |Bc \u2192d ZT |Bc\nin\nC(Bc)\n(87)\nas T \u2192\u221efor each c > 0. Moreover, Lemma A.2 already provided the PLD inequality for ZT .\nThus e.g. Theorem 10 of Yoshida (2011) proves (13) for \u02c6uB\nT once the estimate\nsup\nT>1\nE\n\u0014\u0012 Z\nUT\nZT (u)du\n\u0013\u22121\u0015\n< \u221e\n(88)\nis established. However, (88) can be veri\ufb01ed e.g. with Lemma 2 of Yoshida (2011). This completes\nthe proof of Theorem 3.1.\n33\n\nA.2\nProof of Proposition 4.1\nSuppose that \u03b8\u2217\u0338\u2208SK. Then\n1\nT\n\u0000CT (SK) \u2212CT (SK\u2217)\n\u0001\n= \u22122\nT\n\u0000HT (\u02c6\u03b8K) \u2212HT (\u03b8\u2217)\n\u0001\n+ 2\nT\n\u0000HT (\u02c6\u03b8K\u2217) \u2212HT (\u03b8\u2217)\n\u0001\n+ aT\nT\n\u0000d(SK) \u2212d(S\u2217\nK)\n\u0001\n\u22652\u03c70 inf\n\u03b8\u2208SK\n\f\f\u03b8 \u2212\u03b8\u2217\f\f2 + op(1)\n(89)\nby Lemma A.1 (ii) and (56) applied to the sub-models SK and SK\u2217in place of \u0398. Then (i) holds\nsince inf\u03b8\u2208SK\n\f\f\u03b8 \u2212\u03b8\u2217\f\f > 0.\nNext, suppose that \u03b8\u2217\u2208SK and SK \u0338= SK\u2217. Obviously, d(SK) > d(SK\u2217). Denote by \u0393K the\nmatrix consisting of the elements of \u0393 with indices in K. Then [A3] implies det \u0393K > 0. Thus, we\ncan obtain the same results as in Theorem 3.1 for \u02c6\u03b8K.",
    "chunk_index": 30,
    "start_char": 66177,
    "end_char": 68566,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "(89)\nby Lemma A.1 (ii) and (56) applied to the sub-models SK and SK\u2217in place of \u0398. Then (i) holds\nsince inf\u03b8\u2208SK\n\f\f\u03b8 \u2212\u03b8\u2217\f\f > 0.\nNext, suppose that \u03b8\u2217\u2208SK and SK \u0338= SK\u2217. Obviously, d(SK) > d(SK\u2217). Denote by \u0393K the\nmatrix consisting of the elements of \u0393 with indices in K. Then [A3] implies det \u0393K > 0. Thus, we\ncan obtain the same results as in Theorem 3.1 for \u02c6\u03b8K. In particular,\n2\n\u0000HT (\u02c6\u03b8K) \u2212HT (\u03b8\u2217)\n\u0001\n= \u0393K\n\u0002\n(\u02c6uK)\u22972\u0003\n+ op(1) = Op(1)\n(90)\nfor both QMLE and QBE for the model SK since \u03b8\u2217\u2208SK, where \u02c6uK =\n\u221a\nT(\u02c6\u03b8K \u2212\u03b8). This is also\nvalid for the model SK\u2217with \u02c6\u03b8K\u2217. Therefore,\nCT (SK) \u2212CT (SK\u2217) = Op(1) +\n\u0000d(SK) \u2212d(SK\u2217)\n\u0001\naT \u2192p \u221e\n(91)\nas T \u2192\u221e, which completes the proof.\nB\nList of stocks\nTable 3 lists all the stocks investigated in the paper.\n34\n\nRIC\nCompany\nSector\nNumber of trading\ndays in sample\nAIRP.PA\nAir Liquide\nHealthcare / Energy\n239\nBNPP.PA\nBNP Paribas\nBanking\n224\nEDF.PA\nElectricite de France\nEnergy\n237\nLAGA.PA\nLagard`ere\nMedia\n143\nCARR.PA\nCarrefour\nRetail\n230\nBOUY.PA\nBouygues\nConstruction / Telecom\n229\nALSO.PA\nAlstom\nTransport\n230\nACCP.PA\nAccor\nHotels\n228\nALUA.PA\nAlcatel\nNetworks / Telecom\n235\nAXAF.PA\nAxa\nInsurance\n237\nCAGR.PA\nCr\u00b4edit Agricole\nBanking\n236\nCAPP.PA\nCap Gemini\nTechnology Consulting\n233\nDANO.PA\nDanone\nFood\n230\nESSI.PA\nEssilor\nOptics\n229\nLOIM.PA\nKlepierre\nFinance\n222\nLVMH.PA\nLouis Vuitton Mo\u00a8et Hennessy\nLuxury\n234\nMICP.PA\nMichelin\nTires\n230\nOREP.PA\nL\u2019Or\u00b4eal\nCosmetics\n234\nPERP.PA\nPernod Ricard\nSpirits\n225\nPEUP.PA\nPeugeot\nAutomotive\n152\nPRTP.PA\nKering\nLuxury\n228\nPUBP.PA\nPublicis\nCommunication\n224\nRENA.PA\nRenault\nAutomotive\n229\nSAF.PA\nSafran\nAerospace / Defense\n233\nTECF.PA\nTechnip\nEnergy\n226\nTOTF.PA\nTotal\nEnergy\n233\nVIE.PA\nVeolia\nEnergy / Environment\n235\nVIV.PA\nVivendi\nMedia\n235\nVLLP.PA\nVallourec\nMaterials\n229\nVLOF.PA\nValeo\nAutomotive\n222\nSASY.PA\nSano\ufb01\nHealthcare\n230\nSCHN.PA\nSchneider Electric\nEnergy\n225\nSGEF.PA\nVinci\nConstruction\n230\nSGOB.PA\nSaint Gobain\nMaterials\n235\nSOGN.PA\nSoci\u00b4et\u00b4e G\u00b4en\u00b4erale\nBanking\n230\nSTM.PA\nST Microelectronics\nSemiconductor\n228\nTable 3: List of stocks investigated in this paper. Sample consists of the whole year 2015, repre-\nsenting roughly 230 trading days for all stocks except LAGA.PA and PEUP.PA which are missing\nroughly 70 trading days.\n35\n\nReferences\nAbergel, Fr\u00b4ed\u00b4eric, Anane, Marouane, Chakraborti, Anirban, Jedidi, Aymen, & Muni Toke, Ioane.\n2016. Limit order books. Cambridge University Press, Dehli.\nBacry, Emmanuel, Dayri, Khalil, & Muzy, Jean-Fran\u00b8cois. 2012. Non-parametric kernel estimation\nfor symmetric Hawkes processes. Application to high frequency \ufb01nancial data. The European\nPhysical Journal B-Condensed Matter and Complex Systems, 85(5), 1\u201312.\nBacry, Emmanuel, Delattre, Sylvain, Ho\ufb00mann, Marc, & Muzy, Jean-Fran\u00b8cois. 2013. Modelling\nmicrostructure noise with mutually exciting point processes. Quantitative \ufb01nance, 13(1), 65\u201377.\nBouchaud, Jean-Philippe, M\u00b4ezard, Marc, Potters, Marc, et al. . 2002. Statistical properties of stock\norder books: empirical results and models. Quantitative \ufb01nance, 2(4), 251\u2013256.\nBouchaud, Jean-Philippe, Gefen, Yuval, Potters, Marc, & Wyart, Matthieu. 2004. Fluctuations and\nresponse in \ufb01nancial markets: the subtle nature of \u2018random\u2019price changes. Quantitative \ufb01nance,\n4(2), 176\u2013190.\nBowsher, C. G. 2007.",
    "chunk_index": 31,
    "start_char": 68204,
    "end_char": 71433,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "1\u201312.\nBacry, Emmanuel, Delattre, Sylvain, Ho\ufb00mann, Marc, & Muzy, Jean-Fran\u00b8cois. 2013. Modelling\nmicrostructure noise with mutually exciting point processes. Quantitative \ufb01nance, 13(1), 65\u201377.\nBouchaud, Jean-Philippe, M\u00b4ezard, Marc, Potters, Marc, et al. . 2002. Statistical properties of stock\norder books: empirical results and models. Quantitative \ufb01nance, 2(4), 251\u2013256.\nBouchaud, Jean-Philippe, Gefen, Yuval, Potters, Marc, & Wyart, Matthieu. 2004. Fluctuations and\nresponse in \ufb01nancial markets: the subtle nature of \u2018random\u2019price changes. Quantitative \ufb01nance,\n4(2), 176\u2013190.\nBowsher, C. G. 2007. Modelling security market events in continuous time: Intensity based, mul-\ntivariate point process models. Journal of Econometrics, 141, 876\u2013912.\nBozdogan, Hamparsum. 1987.\nModel selection and Akaike\u2019s information criterion (AIC): The\ngeneral theory and its analytical extensions. Psychometrika, 52(3), 345\u2013370.\nChakraborti, Anirban, Muni Toke, Ioane, Patriarca, Marco, & Abergel, Fr\u00b4ed\u00b4eric. 2011. Econo-\nphysics review: I. Empirical facts. Quantitative Finance, 11(7), 991\u20131012.\nCont, Rama, Stoikov, Sasha, & Talreja, Rishi. 2010. A stochastic model for order book dynamics.\nOperations research, 58(3), 549\u2013563.\nDe Gregorio, Alessandro, & Iacus, Stefano M. 2012. Adaptive LASSO-type estimation for multi-\nvariate di\ufb00usion processes. Econometric Theory, 28(4), 838\u2013860.\nFan, Jianqing, & Li, Runze. 2001. Variable selection via nonconcave penalized likelihood and its\noracle properties. Journal of the American statistical Association, 96(456), 1348\u20131360.\nFan, Jianqing, & Li, Runze. 2002. Variable selection for Cox\u2019s proportional hazards model and\nfrailty model. Annals of Statistics, 30(1), 74\u201399.\nFrank, Lldiko E., & Friedman, Jerome H. 1993. A statistical view of some chemometrics regression\ntools. Technometrics, 35(2), 109\u2013135.\nGould, Martin D, Porter, Mason A, Williams, Stacy, McDonald, Mark, Fenn, Daniel J, & Howison,\nSam D. 2013. Limit order books. Quantitative Finance, 13(11), 1709\u20131742.\n36\n\nHansen, Niels Richard, Reynaud-Bouret, Patricia, Rivoirard, Vincent, et al. . 2015. Lasso and\nprobabilistic inequalities for multivariate point processes. Bernoulli, 21(1), 83\u2013143.\nHuang, Weibing, Lehalle, Charles-Albert, & Rosenbaum, Mathieu. 2015. Simulating and analyzing\norder book data: The queue-reactive model. Journal of the American Statistical Association,\n110(509), 107\u2013122.\nKinoshita, Yoshiki, & Yoshida, Nakahiro. 2018. Penalized quasi-likelihood estimation for variable\nselection. preprint.\nLallouache, Mehdi, & Challet, Damien. 2016.\nThe limits of statistical signi\ufb01cance of Hawkes\nprocesses \ufb01tted to \ufb01nancial data. Quantitative Finance, 16(1), 1\u201311.\nLehalle, Charles-Albert, & Mounjid, Othmane. 2017. Limit order strategic placement with adverse\nselection risk and the role of latency. Market Microstructure and Liquidity, 3(01), 1750009.\nLillo, Fabrizio, & Farmer, J Doyne. 2004. The long memory of the e\ufb03cient market. Studies in\nnonlinear dynamics & econometrics, 8(3), 1\u201333.\nLipton, A., Pesavento, U., & Sotiropoulos, M. G. 2013. Trade arrival dynamics and quote imbalance\nin a limit order book. arxiv preprint arxiv:1312.0514.\nMuni Toke, Ioane. 2016.\nReconstruction of Order Flows using Aggregated Data.\nMarket Mi-\ncrostructure and Liquidity, 02(02), 1650007.\nMuni Toke, Ioane, & Pomponio, Fabrizio. 2012. Modelling trades-through in a limited order book\nusing Hawkes processes. Economics -journal, 6, 22.\nMuni Toke, Ioane, & Yoshida, Nakahiro. 2017. Modelling intensities of order \ufb02ows in a limit order\nbook. Quantitative Finance, 17(5), 683\u2013701.\nOgata, Yosihiko. 1978. The asymptotic behaviour of maximum likelihood estimators for stationary\npoint processes. Annals of the Institute of Statistical Mathematics, 30(1), 243\u2013261.",
    "chunk_index": 32,
    "start_char": 70833,
    "end_char": 74589,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "Ioane. 2016.\nReconstruction of Order Flows using Aggregated Data.\nMarket Mi-\ncrostructure and Liquidity, 02(02), 1650007.\nMuni Toke, Ioane, & Pomponio, Fabrizio. 2012. Modelling trades-through in a limited order book\nusing Hawkes processes. Economics -journal, 6, 22.\nMuni Toke, Ioane, & Yoshida, Nakahiro. 2017. Modelling intensities of order \ufb02ows in a limit order\nbook. Quantitative Finance, 17(5), 683\u2013701.\nOgata, Yosihiko. 1978. The asymptotic behaviour of maximum likelihood estimators for stationary\npoint processes. Annals of the Institute of Statistical Mathematics, 30(1), 243\u2013261.\nOzaki, T. 1979. Maximum likelihood estimation of Hawkes\u2019 self-exciting point processes. Annals\nof the Institute of Statistical Mathematics, 31(1), 145\u2013155.\nRio, Emmanuel. 2017. Asymptotic Theory of Weakly Dependent Random Processes. Probability\nTheory and Stochastic Modelling. Springer, Berlin.\nStoikov, Sasha. 2017. The Micro-Price: A High Frequency Estimator of Future Prices. SSRN\npreprint.\nSuzuki, Takumi, & Yoshida, Nakahiro. 2018. Penalized least squares approximation methods and\ntheir applications to stochastic processes. arXiv:1811.09016.\n37\n\nTibshirani, Robert. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), 58(1), 267\u2013288.\nUmezu, Yuta, Shimizu, Yusuke, Masuda, Hiroki, & Ninomiya, Yoshiyuki. 2015. AIC for non-concave\npenalized likelihood method. arxiv preprint arxiv:1509.01688.\nWang, Hansheng, & Leng, Chenlei. 2007. Uni\ufb01ed LASSO estimation by least squares approxima-\ntion. Journal of the American Statistical Association, 102(479), 1039\u20131048.\nYoshida, Nakahiro. 2011. Polynomial type large deviation inequalities and quasi-likelihood analysis\nfor stochastic di\ufb00erential equations. Annals of the Institute of Statistical Mathematics, 63(3),\n431\u2013479.\nYue, Yu Ryan, & Loh, Ji Meng. 2015. Variable selection for inhomogeneous spatial point process\nmodels. Canadian Journal of Statistics, 43(2), 288\u2013305.\nZou, Hui. 2006. The adaptive lasso and its oracle properties. Journal of the American statistical\nassociation, 101(476), 1418\u20131429.\n38",
    "chunk_index": 33,
    "start_char": 73999,
    "end_char": 76120,
    "paper_title": "Analyzing order flows in limit order books with ra",
    "paper_category": "q-fin.ST",
    "paper_filename": "Analyzing_order_flows_in_limit_order_books_with_ra.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Analyzing_order_flows_in_limit_order_books_with_ra.pdf"
  },
  {
    "text": "arXiv:0910.2909v3 [q-fin.ST] 6 Jul 2010\nCompensating asynchrony e\ufb00ects in the calculation of \ufb01nancial correlations\nMichael C. M\u00a8unnix\u2217, Rudi Sch\u00a8afer, Thomas Guhr\nFakult\u00a8at f\u00a8ur Physik, Universit\u00a8at Duisburg-Essen, Germany\nAbstract\nWe present a method to compensate statistical errors in the calculation of correlations on asynchronous time series.\nThe method is based on the assumption of an underlying time series. We set up a model and apply it to \ufb01nancial data\nto examine the decrease of calculated correlations towards smaller return intervals (Epps e\ufb00ect). We show that this\nstatistical e\ufb00ect is a major cause of the Epps e\ufb00ect. Hence, we are able to quantify and to compensate it using only\ntrading prices and trading times.\nKeywords: Financial correlations, Epps e\ufb00ect, Market emergence, Covariance estimation, Asynchronous time series\n1. Introduction\nThe decrease of calculated correlations in \ufb01nancial data towards smaller return (or \u201csampling\u201d-) intervals has been\nof interest since Epps discovered this phenomenon in 1979 [1]. Ever since, this behavior was found in data of di\ufb00erent\nstock exchanges [2, 3, 4, 5] and foreign exchange markets [6, 7].\nMany economists as well as physicists addressed this phenomenon, since a precise calculation of correlations\nis of major importance for the estimation of \ufb01nancial risk [8, 9, 10]. While the physicists\u2019 approach is often to\nconstruct a model which o\ufb00ers an explanation for this phenomenon, the standard economy approach is to work\non estimators with the aim to suppress the Epps e\ufb00ect. Recently, Hayashi and Yoshida introduced a cumulative\nestimator [11], only involving returns whose time intervals are overlapping. This estimator has been supplemented\nwith di\ufb00erent adjustments, such as bias compensation and lead-lag treatment [5, 12, 13]. A very similar approach\non a completly di\ufb00erent topic is the \u201cdiscrete correlation function\u201d in astrophysics which was introduced in 1988\nby Edelson and Krolik [14]. Other approaches to estimate correlation coe\ufb03cients involve Previous-Tick-Estimators\n[15, 16] or realized kernel functions [17].\nAn extensive study of microscopic causes leading to the Epps e\ufb00ect has been performed by Ren`o [18], while\nanother work by T\u00b4oth et. al. introduce a model for the Epps e\ufb00ect which is based on the phenomenon of lagged\ncorrelations [19].\nHowever, certainly miscellaneous mechanisms are contributing to the Epps e\ufb00ect. Thus our approach is di\ufb00erent.\nFirst, we will introduce a simple model which o\ufb00ers an explanation for the statistical part of the Epps e\ufb00ect, based\non a central assumption of an underlying time series. Secondly, based on that model, we will present an estimator,\nwith which these e\ufb00ects can be compensated. Finally, we will quantify the impact of this phenomenon on the Epps\ne\ufb00ect in recent empirical data and show that it can be a major cause for the Epps e\ufb00ect, especially when looking at\nless frequently traded securities.\nThis paper is organized as followed: In section 2, we develop the model for correlations in asynchronous time\nseries. Within the model, we observe a decay of correlations towards smaller return intervals, similar to the Epps\ne\ufb00ect. We then derive the method to compensate this phenomenon.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3228,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "of this phenomenon on the Epps\ne\ufb00ect in recent empirical data and show that it can be a major cause for the Epps e\ufb00ect, especially when looking at\nless frequently traded securities.\nThis paper is organized as followed: In section 2, we develop the model for correlations in asynchronous time\nseries. Within the model, we observe a decay of correlations towards smaller return intervals, similar to the Epps\ne\ufb00ect. We then derive the method to compensate this phenomenon. In section 3, this method is applied to recent\nempirical data to estimate the impact of the observed e\ufb00ect on the Epps e\ufb00ect. We discuss the results in section 4.\n\u2217Corresponding author. Tel.: +49 203 379 4727; Fax: +49 203 379 4732.\nEmail address: michael@muennix.com (Michael C. M\u00a8unnix)\nPreprint submitted to Physica A\nOctober 25, 2018\n\nFigure 1: Illustration of the model for asynchronous trading times of two stocks. Shown above are the prices \u02dcS on the underlying timescale. The\n\u201csampling\u201d of theses prices \u02dcS to prices S on simulated trading times are shown below.\n2. Statistical e\ufb00ects in asynchronous time series\nIn section 2.1, we set up our model and develop a compensation formalism for asynchrony e\ufb00ects in section 2.2.\n2.1. Model\nThe central assumption of our model is the existence of an underlying non-lagged time series of prices. The\nassumption of a \ufb01ner [19] or even continuous [11, 20, 18] underlying timescale is a common approach in the estimation\nof correlations. This approach is also intuitive, as most stocks are traded at several stock exchanges simultaneously.\nTo simulate asynchrony e\ufb00ects we generate an underlying correlated time series using the Capital Asset Pricing\nModel (CAPM) [21], which is also known as Noh\u2019s model [22] in physics,\n\u02dcr(i)(t) = \u221ac \u03b7(t) +\n\u221a\n1 \u2212c \u03b5(i)(t) ,\n(1)\nwhere \u02dcr(i) stands for the relative price change, the so-called return of the i-th stock and c is the correlation coe\ufb03cient.\nThe random variables \u03b7 and \u03b5(i) are taken from a compound distribution as observed on market data by Gopikrishnan\net. al. with power-law tails and a central Levy distribution (for details see Ref. [23]). We have chosen this approach\nto keep our model initially as simple as possible. We note, however, that return time series can also be autocorre-\nlated. While \ufb01rst order autocorrelations are in this context insigni\ufb01cantly small [24], second order autocorrelations or\n\u201cvolatility clustering\u201d represent a strong characteristic of return time series and led to the development of autoregres-\nsive models, such as GARCH [25, 26]. For this reason we also test our compensation in a more realistic setup against\na GARCH(1,1) generated time series of underlying returns, given by\n\u02dcr(i)(t) = \u03c3(i)(t)\n\u0010 \u221ac \u03b7(t) +\n\u221a\n1 \u2212c \u03b5(i)(t)\n\u0011\n(2)\nwith\n\u0010\n\u03c3(i)(t)\n\u00112 = \u03b10 + \u03b11\n\u0010\n\u02dcr(i)(t \u22121)\n\u00112 + \u03b21\n\u0010\n\u03c3(i)(t \u22121)\n\u00112 .\n(3)\nThe initial parameters of the GARCH process have been chosen as \u03b10 = 2.4 \u00b7 10\u22124, \u03b11 = 0.15 and \u03b21 = 0.84.\n2",
    "chunk_index": 1,
    "start_char": 2758,
    "end_char": 5673,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "also test our compensation in a more realistic setup against\na GARCH(1,1) generated time series of underlying returns, given by\n\u02dcr(i)(t) = \u03c3(i)(t)\n\u0010 \u221ac \u03b7(t) +\n\u221a\n1 \u2212c \u03b5(i)(t)\n\u0011\n(2)\nwith\n\u0010\n\u03c3(i)(t)\n\u00112 = \u03b10 + \u03b11\n\u0010\n\u02dcr(i)(t \u22121)\n\u00112 + \u03b21\n\u0010\n\u03c3(i)(t \u22121)\n\u00112 .\n(3)\nThe initial parameters of the GARCH process have been chosen as \u03b10 = 2.4 \u00b7 10\u22124, \u03b11 = 0.15 and \u03b21 = 0.84.\n2\n\nFigure 2: Scaling behavior of the correlation coe\ufb03cient on a simulated asynchronous time series. The length of the underlying time series was set\nto 7.2 106 (c = 0.4, \u00b5(1) = 15 and \u00b5(2) = 25). The correlation coe\ufb03cient was calculated on return-intervals from 60 data points (corresponding to 1\nminute) to 1800 data points (corresponding to 30 minutes).\nTwo return time series \u02dcr(1) and \u02dcr(2) are generated representing two correlated stocks. The lengths of these underly-\ning time series are chosen as 7.2 \u00b7 106, 1.44 \u00b7 106 and 7.2 \u00b7 105 corresponding to a return interval \u2206\u02dct on the underlying\ntimescale of 1, 5 and 10 seconds during 1 trading year.\nUsing these returns and an arbitrary starting price, the underlying price series \u02dcS (1) and \u02dcS (2) are calculated implying\na geometric Brownian motion with zero drift and a standard deviation of 10\u22123 per time step. To model the asyn-\nchronous trade processes, these prices are sampled independently using exponentially distributed waiting times with\naverage values typical for the stock market (see Fig. 1). In the following example, we choose the average waiting\ntimes as \u00b5(1) = 15 and \u00b5(2) = 25 (equivalent to seconds in this example), while the underlying time series were cor-\nrelated with c = 0.4. On the resulting \u201cmacroscopic\u201d time series, the return between two points in time (of the i-th\nstock) can be calculated as\nr(i)\n\u2206t(t) = S (i)(t + \u2206t) \u2212S (i)(t)\nS (i)(t)\n,\n(4)\nwhere S (i)(t) denotes the price at time t and \u2206t is the return interval. Between these return time series, we now calculate\nthe correlation coe\ufb03cient,\ncorr(r(i)\n\u2206t, r(j)\n\u2206t ) =\nD\nr(i)\n\u2206tr(j)\n\u2206t\nE\n\u2212\nD\nr(i)\n\u2206t\nE D\nr(j)\n\u2206t\nE\n\u03c3(i)\n\u2206t\u03c3(j)\n\u2206t\n,\n(5)\nwhere \u27e8. . .\u27e9denotes the mean value of a time series with length T and where \u03c3 refers to the standard deviation of the\nsame time series. We note that we refer to the whole time series of returns r(i)\n\u2206t when the argument (t) is omitted.\nWhen calculating the correlation of returns of the sampled time series \u02dcS using di\ufb00erent return intervals \u2206t, the\ncorrelation coe\ufb03cient scales down as shown in Fig 2. This behavior is very similar to the Epps e\ufb00ect in empirical\ndata. It occurs only because of the asynchrony of the trading times. As this behavior is already observed in this simple\nsetting, we are able to derive a method to compensate it, as the following demonstrates.\n2.2.",
    "chunk_index": 2,
    "start_char": 5314,
    "end_char": 8027,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "is omitted.\nWhen calculating the correlation of returns of the sampled time series \u02dcS using di\ufb00erent return intervals \u2206t, the\ncorrelation coe\ufb03cient scales down as shown in Fig 2. This behavior is very similar to the Epps e\ufb00ect in empirical\ndata. It occurs only because of the asynchrony of the trading times. As this behavior is already observed in this simple\nsetting, we are able to derive a method to compensate it, as the following demonstrates.\n2.2. Compensation\nThe basic idea of this approach is the following: Due to the asynchrony, each term of the correlation coe\ufb03cient\ncan be divided into a part which contributes to the correlation and a part which is uncorrelated and therefore lowers\nthe correlation coe\ufb03cient.\nAccording to the model assumption, the price change during \u2206t is based on price changes on an underlying\n\u201cmicroscopic\u201d timescale. Thus, the return can also be expressed as a sum of the underlying returns,\n3\n\nr(i)(t) =\nN(i)\n\u2206t(t)\nX\nj=0\n\u02dcr(i)(\u03b3(i)(t) + j\u2206\u02dct) .\n(6)\nHere \u02dcr(i)(ti) is the return related to S (t) on the underlying time scale of non-overlapping intervals \u2206\u02dct (e.g. 1 second)\ngiven by\n\u02dcr(i)(t + j\u2206\u02dct) =\n\u02dcS (t + (j + 1)\u2206\u02dct) \u2212\u02dcS (t + j\u2206\u02dct)\nS (t)\n.\n(7)\nThe quantity \u03b3(i)(t) in equation (6) represents the time of the last trade of the i-th stock at time t,\n\u03b3(i)(t) = max(t(i)\ntrade)\n\f\f\f\ft(i)\ntrade\u2264t .\n(8)\nWhen calculating the return for the interval [t, t + \u2206t] of two stocks, the actual price at t and t + \u2206t is generally in\nthe past, more precisely at \u03b3(1)(t), \u03b3(2)(t) and \u03b3(1)(t + \u2206t), \u03b3(2)(t + \u2206t). These trading times are distinct for each stock,\ntherefore only a fraction of the underlying prices processed by the return is correlated. The number of terms N(i)\n\u2206t of\nthe sum in equation (6) is given by\nN(i)\n\u2206t(t) = (\u03b3(i)(t + \u2206t) \u2212\u03b3(i)(t))\n\u2206\u02dct\n.\n(9)\nWe normalize the returns to zero mean and unit variance and indicate them as g and \u02dcg:\ng(i)\n\u2206t(t) =\nr(i)\n\u2206t(t) \u2212\u27e8r(i)\n\u2206t\u27e9\n\u03c3(i)\n\u2206t\n,\n\u02dcg(i)(t) = \u02dcr(i)(t) \u2212\u27e8\u02dcr(i)\u27e9\n\u02dc\u03c3(i)\n.\n(10)\nIn this context, the relation of the returns on both time scales in equation (6) changes to\ng(i)\n\u2206t(t) =\nr\n\u2206\u02dct\n\u2206t\nN(i)\n\u2206t(t)\nX\nj=0\n\u02dcg(i)(\u03b3(i)(t) + j\u2206\u02dct) \u2212\n\u27e8\u02dcr(i)\u27e9\n\u0010 \u2206t\n\u2206\u02dct \u2212N(i)\n\u2206t(t)\n\u0011\n\u03c3(i)\n\u2206t\n,\n(11)\nas worked out in appendix Appendix A. When using normalized returns, the correlation coe\ufb03cient of two return time\nseries r(1)\n\u2206t and r(2)\n\u2206t (see equation (5)) simpli\ufb01es to\ncorr(r(1)\n\u2206t , r(2)\n\u2206t ) = corr(g(1)\n\u2206t , g(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\ng(1)\n\u2206t (t j)g(2)\n\u2206t (t j) .\n(12)\nAs the mean value over T of the second term from equation (11) is equal to zero, we obtain in terms of the underlying\ntime series\ncorr(r(1)\n\u2206t , r(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN(1)\n\u2206t (tj)\nX\nk=0\n\u02dcg(1)(\u03b3(1)(t j) + k\u2206\u02dct)\nN(2)\n\u2206t (tj)\nX\nl=0\n\u02dcg(2)(\u03b3(2)(t j) + l\u2206\u02dct)\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2206\u02dct\n\u2206t .",
    "chunk_index": 3,
    "start_char": 7573,
    "end_char": 10296,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "r(2)\n\u2206t (see equation (5)) simpli\ufb01es to\ncorr(r(1)\n\u2206t , r(2)\n\u2206t ) = corr(g(1)\n\u2206t , g(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\ng(1)\n\u2206t (t j)g(2)\n\u2206t (t j) .\n(12)\nAs the mean value over T of the second term from equation (11) is equal to zero, we obtain in terms of the underlying\ntime series\ncorr(r(1)\n\u2206t , r(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN(1)\n\u2206t (tj)\nX\nk=0\n\u02dcg(1)(\u03b3(1)(t j) + k\u2206\u02dct)\nN(2)\n\u2206t (tj)\nX\nl=0\n\u02dcg(2)(\u03b3(2)(t j) + l\u2206\u02dct)\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2206\u02dct\n\u2206t .\n(13)\nAs illustrated in Fig. 3, only a subset of the underlying prices \u02dcS of two prices S share an overlapping time-interval.\nBecause of this \u201coverlap\u201d only a certain amount \u00afN\u2206t(t) of the underlying returns is correlated, namely\n\u00afN\u2206t(t) = \u2206to(t)\n\u2206\u02dct\n(14)\nwith \u2206to(t) being the time interval of the actual overlap,\n\u2206to(t) = min(\u03b3(1)(t + \u2206t), \u03b3(2)(t + \u2206t)) \u2212max(\u03b3(1)(t), \u03b3(2)(t)) .\n(15)\n4\n\nFigure 3: Illustration of the overlap \u2206to.\nEach sum can be split up into N(i)\n\u2206t \u2212\u00afN terms that are uncorrelated and \u00afN that are correlated. Thus, equation (13) can\nbe written as:\ncorr(r(1)\n\u2206t , r(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN(1)\n\u2206t (tj)\u2212\u00afN\u2206t(tj)\nX\nk= \u00afN\u2206t(tj)+1\n\u02dcg(1)(tk)\n| {z }\nasync.\n+\n\u00afN\u2206t(tj)\nX\n\u00afk=0\n\u02dcg(1)(t\u00afk)\n| {z }\nsync.\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u00d7\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN(2)\n\u2206t (tj)\u2212\u00afN\u2206t(tj)\nX\nl= \u00afN\u2206t(tj)+1\n\u02dcg(2)(tl)\n| {z }\nasync.\n+\n\u00afN\u2206t(tj)\nX\n\u00afl=0\n\u02dcg(2)(t\u00afl)\n| {z }\nsync.\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2206\u02dct\n\u2206t\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\n(16)\nwhere only the sums of synchronous returns are correlated among each other. In this notation, the underlying time\nseries is indexed as [\u02dcr(i)(t0), \u02dcr(i)(t1), . . . \u02dcr(i)(tN(i)\n\u2206t ))], where the returns from t0 to t \u00afN\u2206t are corresponding to the overlap.\nWhen expanding the product, the non-correlated returns converge to zero due to the outer average\ncorr(r(1)\n\u2206t , r(2)\n\u2206t )\n=\n1\nT\nT\nX\nj=0\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u00afN\u2206t(tj)\nX\nk=0\n\u02dcg(1)(tk)\u02dcg(2)(tk)\n| {z }\n\u00afN\u2206t(t)corrt j(\u20d7\u02dcr1,\u20d7\u02dcr2)\n+ . . .\n|{z}\n0\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2206\u02dct\n\u2206t\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n=\n1\nT\nT\nX\nj=0\ncorrtj(\u02dcg(1), \u02dcg(2))\n\u00afN\u2206t(t)\u2206\u02dct\n\u2206t\n=\n1\nT\nT\nX\nj=0\ncorrtj(\u02dcg(1), \u02dcg(2))\u2206to(t j)\n\u2206t\n,\n(17)\n5\n\n(a) \u2206t = 150\n(b) \u2206t = 450\n(c) \u2206t = 1500\n(d) Mean overlap\nFigure 4: Distribution of the overlaps for simulated return intervals \u2206t = 150 (a), 450 (b) and 1500 (c) data points (corresponding to 2.5, 7.5 and\n25 minutes). Towards larger return-intervals, the distribution sharpens, as well as its mean converges to 1 (d).\nwhere corrt represents the correlation of the underlying returns corresponding to the interval [t, t + \u2206t].\n\u2206to(t)/\u2206t is the fractional overlap of the corresponding return interval. The fractional overlap does not depend\non the actual timescale of the underlying time series. As equation (17) clearly shows, the correlation coe\ufb03cient\nof the synchronous part of the return time series is multiplied by the fractional overlap.",
    "chunk_index": 4,
    "start_char": 9871,
    "end_char": 12698,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "sharpens, as well as its mean converges to 1 (d).\nwhere corrt represents the correlation of the underlying returns corresponding to the interval [t, t + \u2206t].\n\u2206to(t)/\u2206t is the fractional overlap of the corresponding return interval. The fractional overlap does not depend\non the actual timescale of the underlying time series. As equation (17) clearly shows, the correlation coe\ufb03cient\nof the synchronous part of the return time series is multiplied by the fractional overlap. Hence, this e\ufb00ect can be\ncompensated by\ncorrcorrected(r(1)\n\u2206t , r(2)\n\u2206t ) = 1\nT\nT\nX\nj=0\ng(1)\n\u2206t (t j)g(2)\n\u2206t (t j)\n\u2206t\n\u2206to(t j) .\n(18)\nThe dashed line in Fig. 5 represents the asynchrony-compensated correlation within our simulation. It turns out that\nthere is a remaining e\ufb00ect that still causes a downscaling of the correlation coe\ufb03cient for very small return intervals.\nThis behavior occurs when the price of either of the stocks did not change during the return-interval and therefore\nthe corresponding return equals zero. Of course, this event becomes more probable on smaller return intervals \u2206t. It\ncorresponds to the small peak at \u2206to = 0 in Fig. 4(a). This remaining downscaling coincides with the cumulative\nestimator described by Hayashi and Yoshida [11]. It can also be expressed in the formalism used here. It reads\ncorr(r(1)\n\u2206t , r(2)\n\u2206t )\n\f\f\f\f(\u03b3(1)\n1 (t),\u03b3(1)(t+\u2206t))\u2227(\u03b3(2)(t),\u03b3(2)(t+\u2206t)) .\n(19)\nTherefore, when combining both estimators, and thus only regarding returns with overlapping time intervals, the\nremaining scaling behavior for very small returns can be compensated as well.\nAs displayed in Fig. 4, the overlap can also be larger than the actual return interval, implying that terms with\nsuch overlaps are corrected downwards. Therefore the compensation can amplify a speci\ufb01c term of the correlation\ncoe\ufb03cient as well as it can attenuate it.\n6\n\n(a) Noh\n(b) GARCH(1,1)\nFigure 5: Compensation of asynchrony e\ufb00ects within the model with di\ufb00erent approaches in the generation of the underlying time series. The\ndashed line represents the correlation coe\ufb03cient, which is corrected by the overlap. The solid line regards in addition only returns, in which time\nintervals trades occurred.\n3. Application to market data\nCertainly, many aspects contribute to the Epps e\ufb00ect. Our present aim is to quantify the part, which is caused by\nthe asynchrony of the time series.\nIt is di\ufb03cult to isolate the Epps E\ufb00ect on single stock pairs, as it can superimpose with other e\ufb00ects leading to\nother characteristics of the correlation coe\ufb03cient than expected according to the Epps e\ufb00ect. A common approach\non this topic is to pick the pairs of stocks, which show a distinctive Epps e\ufb00ect and focus the analysis to these pairs\n[3, 19, 12]. In the following, we would like to take a di\ufb00erent approach:\nWe classify two ensembles of stock pairs. After compensating the asynchrony e\ufb00ect for each pair, we build the\naverage for the ensemble. We also plot the error bars representing the double standard deviation 2\u03c3.",
    "chunk_index": 5,
    "start_char": 12224,
    "end_char": 15214,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "this topic is to pick the pairs of stocks, which show a distinctive Epps e\ufb00ect and focus the analysis to these pairs\n[3, 19, 12]. In the following, we would like to take a di\ufb00erent approach:\nWe classify two ensembles of stock pairs. After compensating the asynchrony e\ufb00ect for each pair, we build the\naverage for the ensemble. We also plot the error bars representing the double standard deviation 2\u03c3. By this method,\nwe can show the scope of the asynchrony model and identify regions, in which other e\ufb00ects dominate. All data was\nextracted from the NYSE\u2019s TAQ database for the year 2007 [27].\nThe \ufb01rst ensemble consists of stock pairs which provide the most stable correlation. Thereby we want to suppress\nthose e\ufb00ects which are caused by a change in the correlation during the period in which the correlation coe\ufb03cient is\ncalculated. This ensemble represents ideal test conditions for the asynchrony compensation. To identify those stock\npairs with a stable correlation, we calculate the correlation coe\ufb03cient of 30 daily returns. After shifting this window\nin 1-day intervals through the year, we calculate the variance of the obtained correlation coe\ufb03cients (varcorr). Then we\nidentify the \ufb01ve stocks providing the smallest variance for each Global Industry Classi\ufb01cation System (GICS) branch\nof the Standard & Poor\u2019s (S&P) 500 index. This results in an ensemble of 50 stocks as shown in table B.1, appendix\nAppendix B.\nAs the correlation structure of stocks can be non-stationary, we also evaluate the asynchrony compensation without\nthe restriction to stable correlations. For this purpose, we select a second ensemble consisting of 5 stock pairs of each\nGICS branch of the S&P 500 index, whose daily returns are providing the strongest correlation during the year\n2007. These stocks include highly non-stationary correlations as indicated in table B.2, appendix Appendix B (row\n\u201cvarcorr\u201d).\nFig. 6 shows the ensemble average of the correlation coe\ufb03cient and the asynchrony-compensated correlation\ncoe\ufb03cient for both ensembles in 2007 (250 trading days). Before averaging, the correlation coe\ufb03cients for each\nstock have been normalized to the value at a return interval \u2206t = 40 minutes.\nWhen looking at the whole ensemble we discover that the asynchrony has a pronounced impact on the Epps e\ufb00ect.\nThe asynchrony e\ufb00ect seems to be the dominating cause for the Epps e\ufb00ect on return intervals down to approximately\n10 minutes, where the remaining Epps e\ufb00ect is on average less than 3% of the correlation coe\ufb03cient\u2019s saturation value\nat large return intervals. For smaller return intervals, other e\ufb00ects dominate, e.g. a lag between the time series of two\n7\n\n(a) Ensemble of most stable correlations\n(b) Ensemble of highest correlations\nFigure 6: Asynchrony-compensated correlations of two ensembles. The data has been normalized to its value at 40 minutes. The error bars\nrepresent the double standard deviation.\n(a) D - XEL\n(b) AMGN - GENZ\nFigure 7: Asynchrony-compensated correlations between Dominion Resources, Inc. (D) - Xcel Energy Inc. (XEL) and Amgen Inc. (AMGN) -\nGenzyme Corp. (GENZ)\nstocks, as recent study indicates [19].",
    "chunk_index": 6,
    "start_char": 14813,
    "end_char": 17948,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "between the time series of two\n7\n\n(a) Ensemble of most stable correlations\n(b) Ensemble of highest correlations\nFigure 6: Asynchrony-compensated correlations of two ensembles. The data has been normalized to its value at 40 minutes. The error bars\nrepresent the double standard deviation.\n(a) D - XEL\n(b) AMGN - GENZ\nFigure 7: Asynchrony-compensated correlations between Dominion Resources, Inc. (D) - Xcel Energy Inc. (XEL) and Amgen Inc. (AMGN) -\nGenzyme Corp. (GENZ)\nstocks, as recent study indicates [19].\nHowever, the ensemble consists also of stocks which are very frequently traded, providing a very short average\nwaiting time which results in a fractional overlap \u2206to(t)/\u2206t close to unity. Evidently the presented compensation only\nhas a small impact on the correlation estimation of these stocks, as they are so frequently traded that their time series\ncan almost be described as continuous. Naturally the presented compensation works best for less frequently traded\nstocks, as they actually show an asynchronous behavior. Fig. 7 provides two examples of stock pairs for which the\nasynchrony of time series is a major e\ufb00ect. While Fig. 7(a) shows a \u201cclean\u201d Epps e\ufb00ect, Fig. 7(b) shows an Epps\ne\ufb00ect which is superimposed with other phenomena.\nOf course, within the statistical ensemble stock pairs can be found that either do not show an Epps e\ufb00ect or that\nare so infrequently traded that the assumption of an underlying timeline seems to be unreasonable. Even though the\nassumption of an underlying time series is a common and intuitive approach on this topic, it may not be valid for very\ninfrequently traded stocks.\nWhen looking at single stock pairs, it turns out that the asynchrony-compensation works well, if a distinguished\nEpps e\ufb00ect is found. In case of adopting the presented method as a black box model without looking at the scaling\n8\n\nbehavior of the correlation coe\ufb03cient, we believe that a return interval of 5 minutes represents a good lower bound\nfor the scope of this method.\n4. Conclusion\nWe presented a model for the scaling behavior of \ufb01nancial correlations due to the asynchrony of the time series.\nThis purely statistical e\ufb00ect can be compensated. Furthermore, we applied this compensation to market data under the\nassumption of an underlying time series with non-lagged correlations. We quanti\ufb01ed the in\ufb02uence of the asynchrony\non the overall decay of the correlation coe\ufb03cient towards small return intervals, which is known as the Epps e\ufb00ect.\nThe results clearly demonstrate that the asynchrony can have a huge impact on the Epps e\ufb00ect. It rather can be\nthe dominating cause for less frequently traded stocks. The main advantage of our method is that no parameters or\nadjustments are necessary, since it is based on the trading times only.\nIn our empirical study, the asynchrony-compensation allowed us to recover the correlation coe\ufb03cient for return\nintervals down to 10 minutes. At this return interval, the remaining Epps e\ufb00ect is on average less than 3% of the\ncorrelation coe\ufb03cient\u2019s saturation value at large return intervals.",
    "chunk_index": 7,
    "start_char": 17439,
    "end_char": 20506,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "dominating cause for less frequently traded stocks. The main advantage of our method is that no parameters or\nadjustments are necessary, since it is based on the trading times only.\nIn our empirical study, the asynchrony-compensation allowed us to recover the correlation coe\ufb03cient for return\nintervals down to 10 minutes. At this return interval, the remaining Epps e\ufb00ect is on average less than 3% of the\ncorrelation coe\ufb03cient\u2019s saturation value at large return intervals. We also demonstrated that the presented method\nholds for non-stationary correlated time series. The accurate calculation of correlations is of major importance for\nrisk management. To keep the estimation error small, a long time series of returns is required. Yet at the same time,\nthe time series should not reach too far into the past. The latter is important because the correlation structure can\nbe highly dynamic, as the dramatic events of autumn 2008 prove. Applying our method to intraday data allows to\nchoose smaller return intervals and hence provides improved statistical signi\ufb01cance of the correlations for the same\ntime horizon.\nAcknowledgements\nThe authors thank J. H\u00a8ammerling and V. Osipov for fruitful discussion. M.C.M. acknowledges \ufb01nancial support\nfrom Studienstiftung des deutschen Volkes.\nAppendix A. Relation between g(i)\n\u2206t and \u02dcg(i)\nWe de\ufb01ned the normalzed returns as\ng(i)\n\u2206t(t) =\nr(i)\n\u2206t(t) \u2212\u27e8r(i)\n\u2206t\u27e9\nq\nVar(r(i)\n\u2206t)\n(A.1)\n\u02dcg(i)(t) = \u02dcr(i)(t) \u2212\u27e8\u02dcr(i)\u27e9\np\nVar(\u02dcr(i))\n,\n(A.2)\nwhere Var(\u00b7 \u00b7 \u00b7) refers to the variance of a time series. Inserting the return, expressed through the underlying time\nseries,\nr(i)(t) =\nN(i)\n\u2206t(t)\nX\nj=0\n\u02dcr(i)(\u03b3(i)(t) + j\u2206\u02dct) ,\n(A.3)\nin equation (A.1), results in\ng(i)\n\u2206t(t)\n=\nN(i)\n\u2206t(t)\nP\nj=0\n\u0010\n\u02dcr(i)(\u03b3(i)(t) + j\u2206\u02dct)\n\u0011\n\u2212\u27e8r(i)\n\u2206t\u27e9\nq\nVar(r(i)\n\u2206t)\n(A.4)\n=\np\nVar(\u02dcr(i))\nN(i)\n\u2206t(t)\nP\nj=0\n\u0010\n\u02dcg(i)(\u03b3(i)(t) + j\u2206\u02dct)\n\u0011\n\u2212\u27e8r(i)\n\u2206t\u27e9+ N(i)\n\u2206t(t)\u27e8\u02dcr(i)\u27e9\nq\nVar(r(i)\n\u2206t)\n.\n(A.5)\n9\n\nIn equation (A.5), (A.2) was used to express the underlying returns \u02dcr(i).\nThe mean values and variance are additive, which leads to\nD\nr(i)\n\u2206t\nE\n=\nD\nN(i)\n\u2206t(t)\nE D\n\u02dcr(i)E\n,\nVar(r(i)\n\u2206t) =\nD\nN(i)\n\u2206t(t)\nE\nVar(\u02dcr(i)) .\n(A.6)\nTherefore, we obtain\ng(i)\n\u2206t(t) =\n1\nqD\nN(i)\n\u2206t(t)\nE\nN(i)\n\u2206t(t))\nX\nj=0\n\u02dcg(i)(\u03b3(i)(t) + j\u2206\u02dct) \u2212\n\u27e8\u02dcr(i)\u27e9\n\u0010D\nN(i)\n\u2206t(t)\nE\n\u2212N(i)\n\u2206t(t)\n\u0011\nq\nVar(r(i)\n\u2206t)\n.\n(A.7)\nAs the average time interval per return converges to \u2206t, the mean number of underlying price changes\nD\nN(i)\n\u2206t(t)\nE\nis given\nby \u2206t/\u2206\u02dct. Thus, we arrive at\ng(i)\n\u2206t(t) =\nr\n\u2206\u02dct\n\u2206t\nN(i)\n\u2206t(t))\nX\nj=0\n\u02dcg(i)(\u03b3(i)(t) + j\u2206\u02dct) \u2212\n\u27e8\u02dcr(i)\u27e9\n\u0010 \u2206t\n\u2206\u02dct \u2212N(i)\n\u2206t(t)\n\u0011\nq\nVar(r(i)\n\u2206t)\n,\n(A.8)\nwhich is equation (11).\n10\n\n11\nAppendix B. Stock ensembles\nTable B.1: Top 5 \ufb01ve stock pairs with the most stable correlation from each GICS branch of the S&P 500 index.\nGICS Branch\nStock 1\nStock 2\ncorr\nvarcorr\nSymbol\nName\nStock Exchange\nVolume\nSymbol\nName\nStock Exchange\nVolume\nConsumer Discretionary\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nSBUX\nStarbucks Corp.",
    "chunk_index": 8,
    "start_char": 20032,
    "end_char": 22925,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "j=0\n\u02dcg(i)(\u03b3(i)(t) + j\u2206\u02dct) \u2212\n\u27e8\u02dcr(i)\u27e9\n\u0010 \u2206t\n\u2206\u02dct \u2212N(i)\n\u2206t(t)\n\u0011\nq\nVar(r(i)\n\u2206t)\n,\n(A.8)\nwhich is equation (11).\n10\n\n11\nAppendix B. Stock ensembles\nTable B.1: Top 5 \ufb01ve stock pairs with the most stable correlation from each GICS branch of the S&P 500 index.\nGICS Branch\nStock 1\nStock 2\ncorr\nvarcorr\nSymbol\nName\nStock Exchange\nVolume\nSymbol\nName\nStock Exchange\nVolume\nConsumer Discretionary\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nSBUX\nStarbucks Corp.\nNASDAQ\n1287200\n0.80\n0.45\nAPOL\nApollo Group\nNASDAQ\n332200\nSHLD\nSears Holdings Corporation\nNASDAQ\n317900\n0.28\n0.57\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nSPLS\nStaples Inc.\nNASDAQ\n674500\n0.80\n0.62\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nCMCSA\nComcast Corp.\nNASDAQ\n910000\n0.69\n0.64\nEXPE\nExpedia Inc.\nNASDAQ\n883300\nSHLD\nSears Holdings Corporation\nNASDAQ\n317900\n0.43\n0.66\nConsumer Staples\nCOST\nCostco Co.\nNASDAQ\n237500\nPG\nProcter & Gamble\nNYSE\n1396077700\n0.05\n0.71\nCOST\nCostco Co.\nNASDAQ\n237500\nCVS\nCVS Caremark Corp.\nNYSE\n1454768200\n0.09\n0.77\nKO\nCoca Cola Co.\nNYSE\n1152559200\nCOST\nCostco Co.\nNASDAQ\n237500\n0.04\n0.79\nMO\nAltria Group Inc.\nNYSE\n1312801100\nCCE\nCoca-Cola Enterprises\nNYSE\n335449400\n0.29\n0.79\nCCE\nCoca-Cola Enterprises\nNYSE\n335449400\nKFT\nKraft Foods Inc-A\nNYSE\n1438368700\n0.32\n0.80\nEnergy\nEP\nEl Paso Corp.\nNYSE\n697103700\nSE\nSpectra Energy Corp.\nNYSE\n331523100\n0.34\n0.84\nCVX\nChevron Corp.\nNYSE\n1271849400\nSE\nSpectra Energy Corp.\nNYSE\n331523100\n0.26\n0.85\nHES\nHess Corporation\nNYSE\n427802700\nSE\nSpectra Energy Corp.\nNYSE\n331523100\n0.23\n0.85\nMUR\nMurphy Oil\nNYSE\n223916300\nSE\nSpectra Energy Corp.\nNYSE\n331523100\n0.25\n0.86\nSII\nSmith International\nNYSE\n370893400\nSE\nSpectra Energy Corp.\nNYSE\n331523100\n0.32\n0.86\nFinancials\nSCHW\nCharles Schwab\nNASDAQ\n1445500\nETFC\nE*Trade Financial Corp.\nNASDAQ\n1391800\n0.67\n0.44\nETFC\nE*Trade Financial Corp.\nNASDAQ\n1391800\nFITB\nFifth Third Bancorp\nNASDAQ\n218000\n0.29\n0.58\nSCHW\nCharles Schwab\nNASDAQ\n1445500\nFITB\nFifth Third Bancorp\nNASDAQ\n218000\n0.36\n0.58\nSCHW\nCharles Schwab\nNASDAQ\n1445500\nHCBK\nHudson City Bancorp\nNASDAQ\n686900\n0.54\n0.60\nACAS\nAmerican Capital Strategies Ltd\nNASDAQ\n207200\nSCHW\nCharles Schwab\nNASDAQ\n1445500\n0.50\n0.60\nHealth Care\nCELG\nCelgene Corp.\nNASDAQ\n619200\nESRX\nExpress Scripts\nNASDAQ\n998400\n0.65\n0.47\nAMGN\nAmgen\nNASDAQ\n813900\nCELG\nCelgene Corp.\nNASDAQ\n619200\n0.48\n0.50\nAMGN\nAmgen\nNASDAQ\n813900\nBIIB\nBIOGEN IDEC Inc.\nNASDAQ\n381800\n0.48\n0.52\nCELG\nCelgene Corp.\nNASDAQ\n619200\nTHC\nTenet Healthcare Corp.\nNYSE\n805228900\n-0.23\n0.53\nAMGN\nAmgen\nNASDAQ\n813900\nGENZ\nGenzyme Corp.\nNASDAQ\n242900\n0.57\n0.53\nIndustrials\nGE\nGeneral Electric\nNYSE\n4303823300\nLUV\nSouthwest Airlines\nNYSE\n862775700\n0.53\n0.73\nMMM\n3M Company\nNYSE\n549124400\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\n0.24\n0.73\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\nGWW\nGrainger (W.W.) Inc.\nNYSE\n95324400\n0.15\n0.73\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\nGR\nGoodrich Corporation\nNYSE\n144177800\n0.15\n0.75\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\nFLR\nFluor Corp. (New)\nNYSE\n171713100\n0.10\n0.76\nInformation Technology\nAAPL\nApple Inc.\nNASDAQ\n4627500\nINTC\nIntel Corp.\nNASDAQ\n5529100\n0.84\n0.19\nAAPL\nApple Inc.\nNASDAQ\n4627500\nCSCO\nCisco Systems\nNASDAQ\n4886800\n0.76\n0.24\nAAPL\nApple Inc.\nNASDAQ\n4627500\nYHOO\nYahoo Inc.\nNASDAQ\n2609700\n0.71\n0.25\nAAPL\nApple Inc.",
    "chunk_index": 9,
    "start_char": 22489,
    "end_char": 25671,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "Cooper Industries Ltd.\nNYSE\n175911300\nGWW\nGrainger (W.W.) Inc.\nNYSE\n95324400\n0.15\n0.73\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\nGR\nGoodrich Corporation\nNYSE\n144177800\n0.15\n0.75\nCBE\nCooper Industries Ltd.\nNYSE\n175911300\nFLR\nFluor Corp. (New)\nNYSE\n171713100\n0.10\n0.76\nInformation Technology\nAAPL\nApple Inc.\nNASDAQ\n4627500\nINTC\nIntel Corp.\nNASDAQ\n5529100\n0.84\n0.19\nAAPL\nApple Inc.\nNASDAQ\n4627500\nCSCO\nCisco Systems\nNASDAQ\n4886800\n0.76\n0.24\nAAPL\nApple Inc.\nNASDAQ\n4627500\nYHOO\nYahoo Inc.\nNASDAQ\n2609700\n0.71\n0.25\nAAPL\nApple Inc.\nNASDAQ\n4627500\nORCL\nOracle Corp.\nNASDAQ\n1731900\n0.78\n0.25\nAAPL\nApple Inc.\nNASDAQ\n4627500\nEBAY\neBay Inc.\nNASDAQ\n1056100\n0.73\n0.26\nMaterials\nMON\nMonsanto Co.\nNYSE\n479605700\nSEE\nSealed Air Corp.(New)\nNYSE\n126716200\n0.06\n0.52\nFCX\nFreeport-McMoran Cp & Gld\nNYSE\n1058215000\nSIAL\nSigma-Aldrich\nNASDAQ\n133800\n0.05\n0.54\nECL\nEcolab Inc.\nNYSE\n163404500\nSEE\nSealed Air Corp.(New)\nNYSE\n126716200\n0.26\n0.60\nATI\nAllegheny Technologies Inc\nNYSE\n269746100\nSEE\nSealed Air Corp.(New)\nNYSE\n126716200\n0.10\n0.63\nPX\nPraxair Inc.\nNYSE\n245761900\nSEE\nSealed Air Corp.(New)\nNYSE\n126716200\n0.12\n0.65\nTelecommunication Services\nQ\nQwest Communications Int\nNYSE\n1623807700\nS\nSprint Nextel Corp.\nNYSE\n2044634000\n0.52\n0.84\nQ\nQwest Communications Int\nNYSE\n1623807700\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.86\nS\nSprint Nextel Corp.\nNYSE\n2044634000\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.87\nAMT\nAmerican Tower Corp.\nNYSE\n387199400\nQ\nQwest Communications Int\nNYSE\n1623807700\n0.26\n0.97\nAMT\nAmerican Tower Corp.\nNYSE\n387199400\nWIN\nWindstream Corporation\nNYSE\n400634200\n0.10\n0.99\nUtilities\nDUK\nDuke Energy\nNYSE\n902519200\nDYN\nDynegy Inc.\nNYSE\n702035600\n0.54\n0.74\nCMS\nCMS Energy\nNYSE\n264225200\nDYN\nDynegy Inc.\nNYSE\n702035600\n0.48\n0.79\nCMS\nCMS Energy\nNYSE\n264225200\nDUK\nDuke Energy\nNYSE\n902519200\n0.39\n0.81\nAES\nAES Corp.\nNYSE\n556049300\nCMS\nCMS Energy\nNYSE\n264225200\n0.31\n0.84\nCNP\nCenterPoint Energy\nNYSE\n359757800\nDUK\nDuke Energy\nNYSE\n902519200\n0.33\n0.84\n\n12\nTable B.2: Top 5 \ufb01ve stock pairs with the highest correlation from each GICS branch of the S&P 500 index.\nGICS Branch\nStock 1\nStock 2\ncorr\nvarcorr\nSymbol\nName\nStock Exchange\nVolume\nSymbol\nName\nStock Exchange\nVolume\nConsumer Discretionary\nAPOL\nApollo Group\nNASDAQ\n332200\nSPLS\nStaples Inc.\nNASDAQ\n674500\n0.77\n1.08\nBBBY\nBed Bath & Beyond\nNASDAQ\n233000\nSPLS\nStaples Inc.\nNASDAQ\n674500\n0.79\n1.08\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nSPLS\nStaples Inc.\nNASDAQ\n674500\n0.80\n0.62\nAMZN\nAmazon Corp.\nNASDAQ\n1215700\nSBUX\nStarbucks Corp.\nNASDAQ\n1287200\n0.80\n0.45\nSPLS\nStaples Inc.\nNASDAQ\n674500\nSBUX\nStarbucks Corp.\nNASDAQ\n1287200\n0.81\n0.69\nConsumer Staples\nKO\nCoca Cola Co.\nNYSE\n1152559200\nSLE\nSara Lee Corp.\nNYSE\n542934700\n0.42\n0.90\nSLE\nSara Lee Corp.\nNYSE\n542934700\nWMT\nWal-Mart Stores\nNYSE\n1992433400\n0.43\n1.02\nCVS\nCVS Caremark Corp.\nNYSE\n1454768200\nKFT\nKraft Foods Inc-A\nNYSE\n1438368700\n0.44\n0.98\nKFT\nKraft Foods Inc-A\nNYSE\n1438368700\nSLE\nSara Lee Corp.\nNYSE\n542934700\n0.48\n0.87\nCOST\nCostco Co.\nNASDAQ\n237500\nWFMI\nWhole Foods Market\nNASDAQ\n211400\n0.59\n1.09\nEnergy\nEP\nEl Paso Corp.\nNYSE\n697103700\nRDC\nRowan Cos.\nNYSE\n369346200\n0.39\n0.91\nCVX\nChevron Corp.\nNYSE\n1271849400\nXOM\nExxon Mobil Corp.\nNYSE\n2798325600\n0.41\n1.00\nXOM\nExxon Mobil Corp.",
    "chunk_index": 10,
    "start_char": 25146,
    "end_char": 28334,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "Lee Corp.\nNYSE\n542934700\nWMT\nWal-Mart Stores\nNYSE\n1992433400\n0.43\n1.02\nCVS\nCVS Caremark Corp.\nNYSE\n1454768200\nKFT\nKraft Foods Inc-A\nNYSE\n1438368700\n0.44\n0.98\nKFT\nKraft Foods Inc-A\nNYSE\n1438368700\nSLE\nSara Lee Corp.\nNYSE\n542934700\n0.48\n0.87\nCOST\nCostco Co.\nNASDAQ\n237500\nWFMI\nWhole Foods Market\nNASDAQ\n211400\n0.59\n1.09\nEnergy\nEP\nEl Paso Corp.\nNYSE\n697103700\nRDC\nRowan Cos.\nNYSE\n369346200\n0.39\n0.91\nCVX\nChevron Corp.\nNYSE\n1271849400\nXOM\nExxon Mobil Corp.\nNYSE\n2798325600\n0.41\n1.00\nXOM\nExxon Mobil Corp.\nNYSE\n2798325600\nHAL\nHalliburton Co.\nNYSE\n1701703200\n0.45\n1.18\nCVX\nChevron Corp.\nNYSE\n1271849400\nEP\nEl Paso Corp.\nNYSE\n697103700\n0.45\n0.88\nCOP\nConocoPhillips\nNYSE\n1382115600\nEP\nEl Paso Corp.\nNYSE\n697103700\n0.46\n0.94\nFinancials\nCINF\nCincinnati Financial\nNASDAQ\n55900\nTROW\nT. Rowe Price Group\nNASDAQ\n216400\n0.60\n1.74\nFITB\nFifth Third Bancorp\nNASDAQ\n218000\nHBAN\nHuntington Bancshares\nNASDAQ\n71600\n0.62\n2.01\nFITB\nFifth Third Bancorp\nNASDAQ\n218000\nTROW\nT. Rowe Price Group\nNASDAQ\n216400\n0.63\n1.34\nSCHW\nCharles Schwab\nNASDAQ\n1445500\nETFC\nE*Trade Financial Corp.\nNASDAQ\n1391800\n0.67\n0.44\nTROW\nT. Rowe Price Group\nNASDAQ\n216400\nZION\nZions Bancorp\nNASDAQ\n48500\n0.70\n1.40\nHealth Care\nBIIB\nBIOGEN IDEC Inc.\nNASDAQ\n381800\nPDCO\nPatterson Cos. Inc.\nNASDAQ\n106400\n0.60\n1.15\nBIIB\nBIOGEN IDEC Inc.\nNASDAQ\n381800\nGENZ\nGenzyme Corp.\nNASDAQ\n242900\n0.62\n0.85\nGENZ\nGenzyme Corp.\nNASDAQ\n242900\nGILD\nGilead Sciences\nNASDAQ\n275100\n0.64\n0.91\nCELG\nCelgene Corp.\nNASDAQ\n619200\nESRX\nExpress Scripts\nNASDAQ\n998400\n0.65\n0.47\nBSX\nBoston Scienti\ufb01c\nNYSE\n1205569800\nTHC\nTenet Healthcare Corp.\nNYSE\n805228900\n0.68\n0.60\nIndustrials\nGE\nGeneral Electric\nNYSE\n4303823300\nLUV\nSouthwest Airlines\nNYSE\n862775700\n0.53\n0.73\nCTAS\nCintas Corporation\nNASDAQ\n48500\nEXPD\nExpeditors Int\u2019l\nNASDAQ\n215600\n0.54\n1.48\nEXPD\nExpeditors Int\u2019l\nNASDAQ\n215600\nMNST\nMonster Worldwide\nNASDAQ\n196700\n0.55\n1.20\nCHRW\nC.H. Robinson Worldwide\nNASDAQ\n112400\nEXPD\nExpeditors Int\u2019l\nNASDAQ\n215600\n0.56\n1.46\nMNST\nMonster Worldwide\nNASDAQ\n196700\nPCAR\nPACCAR Inc.\nNASDAQ\n164400\n0.56\n1.23\nInformation Technology\nDELL\nDell Inc.\nNASDAQ\n909400\nORCL\nOracle Corp.\nNASDAQ\n1731900\n0.79\n0.47\nCSCO\nCisco Systems\nNASDAQ\n4886800\nDELL\nDell Inc.\nNASDAQ\n909400\n0.79\n0.41\nINTC\nIntel Corp.\nNASDAQ\n5529100\nORCL\nOracle Corp.\nNASDAQ\n1731900\n0.82\n0.29\nCSCO\nCisco Systems\nNASDAQ\n4886800\nORCL\nOracle Corp.\nNASDAQ\n1731900\n0.83\n0.34\nAAPL\nApple Inc.\nNASDAQ\n4627500\nINTC\nIntel Corp.\nNASDAQ\n5529100\n0.84\n0.19\nMaterials\nDD\nDu Pont (E.I.)\nNYSE\n716944300\nFCX\nFreeport-McMoran Cp & Gld\nNYSE\n1058215000\n0.36\n1.02\nFCX\nFreeport-McMoran Cp & Gld\nNYSE\n1058215000\nMON\nMonsanto Co.\nNYSE\n479605700\n0.36\n0.85\nAPD\nAir Products & Chemicals\nNYSE\n187953500\nBLL\nBall Corp.\nNYSE\n119560600\n0.36\n0.88\nECL\nEcolab Inc.\nNYSE\n163404500\nNEM\nNewmont Mining Corp. (Hldg. Co.)\nNYSE\n958900000\n0.37\n0.93\nFCX\nFreeport-McMoran Cp & Gld\nNYSE\n1058215000\nNEM\nNewmont Mining Corp. (Hldg. Co.)\nNYSE\n958900000\n0.43\n1.06\nTelecommunication Services\nT\nAT&T Inc.\nNYSE\n2663617200\nQ\nQwest Communications Int\nNYSE\n1623807700\n0.45\n1.01\nS\nSprint Nextel Corp.\nNYSE\n2044634000\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.87\nQ\nQwest Communications Int\nNYSE\n1623807700\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.86\nT\nAT&T Inc.",
    "chunk_index": 11,
    "start_char": 27834,
    "end_char": 31015,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "0.88\nECL\nEcolab Inc.\nNYSE\n163404500\nNEM\nNewmont Mining Corp. (Hldg. Co.)\nNYSE\n958900000\n0.37\n0.93\nFCX\nFreeport-McMoran Cp & Gld\nNYSE\n1058215000\nNEM\nNewmont Mining Corp. (Hldg. Co.)\nNYSE\n958900000\n0.43\n1.06\nTelecommunication Services\nT\nAT&T Inc.\nNYSE\n2663617200\nQ\nQwest Communications Int\nNYSE\n1623807700\n0.45\n1.01\nS\nSprint Nextel Corp.\nNYSE\n2044634000\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.87\nQ\nQwest Communications Int\nNYSE\n1623807700\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.49\n0.86\nT\nAT&T Inc.\nNYSE\n2663617200\nVZ\nVerizon Communications\nNYSE\n1472335800\n0.50\n1.05\nQ\nQwest Communications Int\nNYSE\n1623807700\nS\nSprint Nextel Corp.\nNYSE\n2044634000\n0.52\n0.84\nUtilities\nDUK\nDuke Energy\nNYSE\n902519200\nTE\nTECO Energy\nNYSE\n177983100\n0.35\n0.87\nD\nDominion Resources\nNYSE\n321656100\nXEL\nXcel Energy Inc\nNYSE\n337262500\n0.36\n0.92\nCMS\nCMS Energy\nNYSE\n264225200\nDUK\nDuke Energy\nNYSE\n902519200\n0.39\n0.81\nCMS\nCMS Energy\nNYSE\n264225200\nDYN\nDynegy Inc.\nNYSE\n702035600\n0.48\n0.79\nDUK\nDuke Energy\nNYSE\n902519200\nDYN\nDynegy Inc.\nNYSE\n702035600\n0.54\n0.74\n\nReferences\n[1] T. W. Epps, Comovements in stock prices in the very short run, Journal of the American Statistical Association 74 (366) (1979) 291\u2013298.\nURL http://www.jstor.org/stable/2286325\n[2] G. Bonanno, F. Lillo, R. N. Mantegna, High-frequency cross-correlation in a set of stocks, Quantitative Finance 1 (1) (2001) 96\u2013104.\n[3] J. Kwapien,\nS. Drozdz,\nJ. Speth,\nTime scales involved in emergent market coherence, Physica A 337 (1-2) (2004) 231 \u2013 242.\ndoi:10.1016/j.physa.2004.01.050.\nURL http://www.sciencedirect.com/science/article/B6TVG-4BRJY2K-1/2/d141755cb92c5d1b33cead1bb57b0bed\n[4] M. Tumminello, T. D. Matteo, T. Aste, R. Mantegna, Correlation based networks of equity returns sampled at di\ufb00erent time horizons, The\nEuropean Physical Journal B 55 (2) (2007) 209\u2013217. doi:10.1140/epjb/e2006-00414-4.\nURL http://dx.doi.org/10.1140/epjb/e2006-00414-4\n[5] A. A. Zebedee, M. Kasch-Haroutounian, A closer look at co-movements among stock returns, Journal of Economics and Business 61 (4)\n(2009) 279 \u2013 294. doi:10.1016/j.jeconbus.2008.11.001.\nURL http://www.sciencedirect.com/science/article/B6V7T-4TX794N-1/2/26edc1f881adc4aad35511cb584cb8d2\n[6] M. Lundin, M. Dacorogna, U. A. M\u00a8uller, Financial Markets Tick By Tick, Wiley, 1998, Ch. Correlation of High Frequency Financial Time\nSeries.\n[7] J. Muthuswamy, S. Sarkar, A. Low, E. Terry, Time variation in the correlation structure of exchange rates: high-frequency analyses, Journal\nof Futures Markets 21 (2) (2001) 127\u2013144.\n[8] R. Sch\u00a8afer, M. Sj\u00a8olin, A. Sundin, M. Wolanski, T. Guhr, Credit risk\u2013a structural model with jumps and correlations, Physica A: Statistical\nMechanics and its Applications 383 (2) (2007) 533 \u2013 569. doi:10.1016/j.physa.2007.04.053.\nURL http://www.sciencedirect.com/science/article/B6TVG-4NKJ0FV-4/2/b5717970f8ab98dedb2034f3c64ab7af\n[9] R.\nSch\u00a8afer,\nN.\nNilsson,\nT.\nGuhr,\nPower mapping with dynamical adjustment for improved portfolio optimization,\nQuantitative\nFinancedoi:10.1080/14697680902748498.\nURL http://www.informaworld.com/10.1080/14697680902748498\n[10] J.-P. Onnela, A. Chakraborti, K. Kaski, J. Kert\u00b4esz, A. Kanto, Dynamics of market correlations: Taxonomy and portfolio analysis, Physical\nReview E 68 (5) (2003) 056110. doi:10.1103/PhysRevE.68.056110.\n[11] T. Hayashi, N. Yoshida, On covariance estimation of non-synchronously observed di\ufb00usion processes, Bernoulli 11 (2) (2005) 359\u2013379.\n[12] V. Voev, A. Lunde, Integrated Covariance Estimation using High-frequency Data in the Presence of Noise, Journal of Financial Econometrics\n5 (1) (2007) 68\u2013104. arXiv:http://jfec.oxfordjournals.org/cgi/reprint/5/1/68.pdf, doi:10.1093/jjfinec/nbl011.\nURL http://jfec.oxfordjournals.org/cgi/content/abstract/5/1/68\n[13] J. E. Gri\ufb03n, R. C. Oomen, Covariance measurement in the presence of non-synchronous trading and market microstructure noise, Journal of\nEconometrics In Press, Corrected Proof.",
    "chunk_index": 12,
    "start_char": 30505,
    "end_char": 34428,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "Physical\nReview E 68 (5) (2003) 056110. doi:10.1103/PhysRevE.68.056110.\n[11] T. Hayashi, N. Yoshida, On covariance estimation of non-synchronously observed di\ufb00usion processes, Bernoulli 11 (2) (2005) 359\u2013379.\n[12] V. Voev, A. Lunde, Integrated Covariance Estimation using High-frequency Data in the Presence of Noise, Journal of Financial Econometrics\n5 (1) (2007) 68\u2013104. arXiv:http://jfec.oxfordjournals.org/cgi/reprint/5/1/68.pdf, doi:10.1093/jjfinec/nbl011.\nURL http://jfec.oxfordjournals.org/cgi/content/abstract/5/1/68\n[13] J. E. Gri\ufb03n, R. C. Oomen, Covariance measurement in the presence of non-synchronous trading and market microstructure noise, Journal of\nEconometrics In Press, Corrected Proof.\n[14] R. A. Edelson, J. H. Krolik, The discrete correlation function - A new method for analyzing unevenly sampled variability data, Astrophysical\nJournal 333 (1988) 646\u2013659. doi:10.1086/166773.\n[15] F. Corsi, F. Audrino, Realized Correlation Tick-By-Tick, SSRN eLibrary.\nURL http://ssrn.com/paper=957997\n[16] L.\nZhang,\nEstimating\ncovariation:\nEpps\ne\ufb00ect,\nmicrostructure\nnoise,\nJournal\nof\nEconometrics\nIn\nPress.\ndoi:10.1016/j.jeconom.2010.03.012.\n[17] O. E. Barndor\ufb00-Nielsen, P. R. Hansen, A. Lunde, N. Shephard, Multivariate realised kernels: consistent positive semi-de\ufb01nite estimators of the covariation of equi\nEconomics Papers 2008-W10, Economics Group, Nu\ufb03eld College, University of Oxford (2008).\nURL http://ideas.repec.org/p/nuf/econwp/0810.html\n[18] R. Ren`o, A closer look at the epps-e\ufb00ect, International Journal of Theoretical and Applied Finance 6 (1) (2003) 87\u2013102.\n[19] B. T\u00b4oth, J. Kert\u00b4esz, The epps e\ufb00ect revisited, Quantitative Finance 9 (7) (2009) 793\u2013802. doi:10.1080/14697680802595668.\n[20] O. E. Barndor\ufb00-Nielsen, N. Shephard, Econometric analysis of realised covariation: High frequency covariance, regression and correlation in \ufb01nancial economics\nEconomics Papers 2002-W13, Economics Group, Nu\ufb03eld College, University of Oxford (Nov. 2001).\nURL http://ideas.repec.org/p/nuf/econwp/0213.html\n[21] W. F. Sharpe, Capital asset prices: A theory of market equilibrium under conditions of risk, The Journal of Finance 19 (3) (1964) 425\u2013442.\nURL http://www.jstor.org/stable/2977928\n[22] J. D. Noh, Model for correlations in stock markets, Physical Review E 61 (5) (2000) 5981\u20135982. doi:10.1103/PhysRevE.61.5981.\n[23] P. Gopikrishnan, V. Plerou, L. A. Nunes Amaral, M. Meyer, H. E. Stanley, Scaling of the distribution of \ufb02uctuations of \ufb01nancial market\nindices, Physical Review E 60 (5) (1999) 5305\u20135316. doi:10.1103/PhysRevE.60.5305.\n[24] J. Voit, From brownian motion to operational risk: Statistical physics and \ufb01nancial markets, Physica A: Statistical Mechanics and its Appli-\ncations 321 (1-2) (2003) 286 \u2013 299. doi:10.1016/S0378-4371(02)01783-1.\nURL http://www.sciencedirect.com/science/article/B6TVG-47CJ9HW-D/2/f8f724aa49a6da1bdbd15ed143ef7e42\n[25] R. F. Engle, Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom in\ufb02ation, Econometrica 50 (4)\n(1982) 987\u20131007.\nURL http://www.jstor.org/stable/1912773\n[26] R. F. Engle, Garch 101: The use of arch/garch models in applied econometrics, Journal of Economic Perspectives 15 (4) (2001) 157\u2013168.\nURL http://ideas.repec.org/a/aea/jecper/v15y2001i4p157-168.html\n[27] New York Stock Exchange, Monthly TAQ DVD, License #400570 (2007).\n13",
    "chunk_index": 13,
    "start_char": 33723,
    "end_char": 37073,
    "paper_title": "Compensating asynchrony effects in the calculation",
    "paper_category": "q-fin.ST",
    "paper_filename": "Compensating_asynchrony_effects_in_the_calculation.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Compensating_asynchrony_effects_in_the_calculation.pdf"
  },
  {
    "text": "Noname manuscript No.\n(will be inserted by the editor)\nCryptocurrency market structure: connecting\nemotions and economics\nTomaso Aste\nMarch 5, 2019\nAbstract We study the dependency and causality structure of the cryptocur-\nrency market investigating collective movements of both prices and social sen-\ntiment related to almost two thousand cryptocurrencies traded during the\n\ufb01rst six months of 2018. This is the \ufb01rst study of the whole cryptocurrency\nmarket structure. It introduces several rigorous innovative methodologies ap-\nplicable to this and to several other complex systems where a large number of\nvariables interact in a non-linear way, which is a distinctive feature of the dig-\nital economy. The analysis of the dependency structure reveals that prices are\nsigni\ufb01cantly correlated with sentiment. The major, most capitalised cryptocur-\nrencies, such as bitcoin, have a central role in the price correlation network but\nonly a marginal role in the sentiment network and in the network describing\nthe interactions between the two. The study of the causality structure reveals\na causality network that is consistently related with the correlation structures\nand shows that both prices cause sentiment and sentiment cause prices across\ncurrencies with the latter being stronger in size but smaller in number of signi-\n\ufb01cative interactions. Overall our study uncovers a complex and rich structure\nof interrelations where prices and sentiment in\ufb02uence each other both instan-\ntaneously and with lead-lag causal relations. A major \ufb01nding is that minor\ncurrencies, with small capitalisation, play a crucial role in shaping the overall\ndependency and causality structure. Despite the high level of noise and the\nshort time-series we veri\ufb01ed that these networks are signi\ufb01cant with all links\nstatistically validated and with a structural organisation consistently repro-\nduced across all networks.\nKeywords Cryptocurrencies \u00b7 Dependency \u00b7 Causality \u00b7 Networks\nDepartment of Computer Science, UCL, London, UK. \u00b7 UCL Centre for Blockchain Tech-\nnologies, UCL, London, UK. \u00b7 Systemic Risk Centre, London School of Economics, London\nUK. E-mail: t.aste@ucl.ac.uk\narXiv:1903.00472v1 [q-fin.ST] 3 Mar 2019\n\n2\nTomaso Aste\n1 Introduction\nDuring the last two years we have witnessed the creation of a large number\nof cryptocurrencies. This burst has been mainly fueled by the opportunity\ngenerated by the ICO mechanism used by companies as a new channel to\nfund innovation. Furthermore, this burst follows the surge of new business\nmodels based on blockchain and associated digital tokens and crypto-money.\nThe most dynamic period in the cryptocurrencies market has been, so far,\nthe beginning of 2018 on which this study is focusing. At the time of writing\n(September 2018) the cryptocurrency market capitalization is \ufb02oating around\n200 billion USD down from 800 billion USD reached in January 2018 [1]. This\nmarket comprises thousands of currencies with only a few with signi\ufb01cant\ncapitalization. In particular \ufb01ve currencies, namely, Bitcoin (BTC), Bitcoin\nCash (BCH), Ethereum (ETH), Litecoin (LTC) and Ripple (XTC) have been\ndominating the market during the last few years with a share of capitalization\nconsistently above 70%.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3224,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "cryptocurrency market capitalization is \ufb02oating around\n200 billion USD down from 800 billion USD reached in January 2018 [1]. This\nmarket comprises thousands of currencies with only a few with signi\ufb01cant\ncapitalization. In particular \ufb01ve currencies, namely, Bitcoin (BTC), Bitcoin\nCash (BCH), Ethereum (ETH), Litecoin (LTC) and Ripple (XTC) have been\ndominating the market during the last few years with a share of capitalization\nconsistently above 70%. Overall, there are 15 currencies with capitalization\nover 1 billion USD, more than 60 with capitalization over 100 million USD\nand about 800 with capitalization over 1 million USD. This is a new and\nconfused market characterized by large volatilities, by quick increases in the\nvalue of some currencies at the time of their release and, often, a rapid decrease\nof the value afterwards until failure. This is a market strongly echoed in social\nmedia with great expectations, quick swifts of sentiment, strong beliefs and\nharsh disputes.\nIn the literature, there have been some studies of correlations in cryptocur-\nrency markets highlighting the non-normal statistics of correlations between\nprice \ufb02uctuations [2] and their relations with \ufb01at currencies [3]. Social media\nand Twitter sentiment signals have been used to attempt nowcasting and fore-\ncasting for some of these currencies [4,5]. The main focus, so far, has been on\nBitcoin with little published research on other cryptocurrencies.\nIn this paper we investigate how cryptocurrency prices collectively behave\nand how the price behaviour is related with the sentiment behaviour expressed\nthrough Twitter and StockTwits [6] messages that refer explicitly to the re-\nlated currency. We ask if this market has a characteristic structure, we enquire\nwhere the major cryptocurrencies are located within this structure and we in-\nvestigate the role of minor cryptocurrencies in shaping this structure. We study\nthe in\ufb02uence of social sentiment and its interplay with prices. We do this by\nlooking at the entire market (1944 cryptocurrencies recorded during the \ufb01rst\nsix months of 2018) instead of concentrating on a few \u2018important\u2019 currencies\nonly. We intentionally study the whole market even if most of the capitaliza-\ntion is retained by a few currencies and most of the other currencies play a\nmarginal economic role. From a naive perspective, a-priori one would had ex-\npected to observe minor currencies being driven by the behaviour of the major\nones in a similar way as it happens for the dynamics of stock prices that tend to\ncluster around the leading \ufb01rms of the relative sector [7,8,9]. Surprisingly, we\nshall uncover instead that this is not happening in the cryptocurrency market.\nIndeed, in this work we uncover signals revealing that these marginal curren-\n\nCryptocurrency market structure: connecting emotions and economics\n3\ncies play a statistically signi\ufb01cant role in the collective dynamics of prices and\ntheir interplay with social sentiment. Therefore they should not be excluded\na-priori from the investigation and their role with respect the major currencies\nmust be studied in detail.",
    "chunk_index": 1,
    "start_char": 2771,
    "end_char": 5888,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "in the cryptocurrency market.\nIndeed, in this work we uncover signals revealing that these marginal curren-\n\nCryptocurrency market structure: connecting emotions and economics\n3\ncies play a statistically signi\ufb01cant role in the collective dynamics of prices and\ntheir interplay with social sentiment. Therefore they should not be excluded\na-priori from the investigation and their role with respect the major currencies\nmust be studied in detail. This opens new challenges for what concerns invest-\nment strategies and risk management which must handle very large number\nof variables and cannot be limited to the study of a few in\ufb02uential factors.\nIn this market, both prices and sentiment data are noisy with large volatil-\nity; for this reason we quantify dependency and causality mainly using rank\nstatistics and topology reducing in this way the e\ufb00ect of noisy outliers. We\npay a special attention to statistically validate dependency and causality links\nby using non-parametric permutation tests and by assessing the e\ufb00ect of the\nvalidation threshold on the resulting structure. We also cross-test results by\ncomparing the overall structural properties of the networks discarding the\nnull-hypotersis that they might be the expression of random spurious links.\nOur study uncovers a complex structure of interrelations where prices and\nsentiments in\ufb02uence each other both within a given currency and across cur-\nrencies. To our knowledge, this is the \ufb01rst attempt to understand dependency\nand causality structure in this market.\nThe structure of the cryptocurrency market as unveiled in this work is\nunavoidably speci\ufb01c to the period investigated, which has been a very special\nand dramatic period. In this respect, this paper presents a unique picture of\na very interesting period of the cryptocurrency market. Despite the fact that\nalready at the time of \ufb01nishing the revision of this paper the cryptocurrency\nmarket has changed signi\ufb01cantly, nonetheless some aspects such as the intrinsic\nnonlinearity in the interactions and the role of \u2018minor\u2019 variables on the whole\nsystem will rest signi\ufb01cant for this market as well as for other systems in the\ndigital economy. Furthermore, this paper contributes to the study of these\nsystems by introducing several general and rigorous methodologies to handle\ndependency and causality in these noisy and non linear systems composed\nby a large number of variables and often supported by a small number of\nobservations. These novel methodologies have broad applicability to the study\nof the digital economy and complex systems in general.\nThe paper is organised as follows. In section 2 we describe the dataset. Sec-\ntion 3 describes the methodology adopted for quantifying dependency, causal-\nity, their representation into networks and the statistical validation procedure.\nResults are presented in Section 4 where the properties of dependency and\ncausality networks for both sentiment and prices and their interplay are de-\nscribed in details. Section 5 provides a detailed discussion of the results with\nspecial attention at their statistical signi\ufb01cance. Conclusions and perspectives\nare outlined in Section 6.\n2 Data\nPrices and Twitter sentiment data of 1944 cryptocurrencies traded during the\nperiod from January 2018 (02/01/2018) to the middle of of June (14/06/2018)",
    "chunk_index": 2,
    "start_char": 5443,
    "end_char": 8761,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "dependency, causal-\nity, their representation into networks and the statistical validation procedure.\nResults are presented in Section 4 where the properties of dependency and\ncausality networks for both sentiment and prices and their interplay are de-\nscribed in details. Section 5 provides a detailed discussion of the results with\nspecial attention at their statistical signi\ufb01cance. Conclusions and perspectives\nare outlined in Section 6.\n2 Data\nPrices and Twitter sentiment data of 1944 cryptocurrencies traded during the\nperiod from January 2018 (02/01/2018) to the middle of of June (14/06/2018)\n\n4\nTomaso Aste\nare analyzed. In the dataset, four major currencies, namely BTC, LTC, ETH\nand XRP had records starting earlier, respectively from: 01/09/2014, 01/09/2014,\n07/08/2015 and 21/01/2015. The number of currencies simultaneously present\nat any time during the period Jan-June 2018 is reported in Fig.1. This num-\nber is not constant because new currencies are introduced over time and other\nfail and cease to be traded in the market. Often they do not disappear but\ntheir capitalisation become negligible and the price become constant and they\nare therefore excluded from our database. The largest number of currencies\ncontemporarily present were 1301 as recorded at the end of January 2018.\nThen numbers gradually decreased to 471 at the end of the observation pe-\nriod. The peak at the end of January 2018 re\ufb02ects the popularity of ICOs that\nindeed peaked in that period. Prices have been obtained from Cryptocompare\n[10] whereas sentiment is provided by PsychSignal [11]. The sentiment signal\nis computed from natural language processing of Twitter and StockTwits [6]\nmessages that refer explicitly to the related currency. Messages are classi\ufb01ed as\npositive, negative or unclassi\ufb01ed depending on the words contained and their\ncontext. The signal we analyse is the number of messages in each category,\nreferred to as volume. In this work we consider the relative changes in positive\nand negative volumes only; we treat them as separate signals and we ignore\nthe unclassi\ufb01ed volumes. Original data are hourly, though in the following an-\nalytics we transformed them into a daily signal by aggregating prices reporting\nthe average daily price and by aggregating volumes reporting the total daily\nvolume. This aggregation process reduces noise. Similar results are obtained\nwith di\ufb00erent aggregation criteria.\n3 Methodology\nWe investigated collective movements of currency prices and currency senti-\nment by computing Kendall cross-correlations [12] and non-parametric transfer\nentropy [13,14] of daily log-returns, log Price(t)\u2212log Price(t\u22121) (di\ufb00erences\nof the logarithm of the price between a day and the previous), and daily\nchanges of the logarithm of the number of messages classi\ufb01ed positive or neg-\native, log(Number of messages with positive sentiment on day t)\u2212log(Number\nof messages with positive sentiment on day t\u22121). The choice of the log-returns\nfor prices is standard in \ufb01nancial literature [15]. Di\ufb00erencing makes the series\nstationary and the logarithm reduces e\ufb00ects of non-normal variations. In con-\ntrast, the choice of log variation of sentiment volume is here mainly motivated\nby the convenience of treating both variables in the same way.",
    "chunk_index": 3,
    "start_char": 8160,
    "end_char": 11432,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "of the number of messages classi\ufb01ed positive or neg-\native, log(Number of messages with positive sentiment on day t)\u2212log(Number\nof messages with positive sentiment on day t\u22121). The choice of the log-returns\nfor prices is standard in \ufb01nancial literature [15]. Di\ufb00erencing makes the series\nstationary and the logarithm reduces e\ufb00ects of non-normal variations. In con-\ntrast, the choice of log variation of sentiment volume is here mainly motivated\nby the convenience of treating both variables in the same way. Test results\nshow that the use of the volume-variations instead of its log-variations gives\noverall similar outcomes.\nWe estimated dependency structure by computing Kendall\u2019s \u03c4 correlation\ncoe\ufb03cients [12]. We veri\ufb01ed that comparable results are obtained by using\nPearson or Spearman correlations. Nonetheless, Kendall correlation are a more\nappropriate analytics tool for the kind of data we are investigating in this\nwork. Indeed, the time-series are short and the statistics of both sentiment and\n\nCryptocurrency market structure: connecting emotions and economics\n5\nJan\nFeb\nMar\nApr\nMay\nJun\n0\n500\n1000\n1500\nFig. 1 Number of currencies simultaneously present during the period Jan-Jun 2018.\nprices log-variations are non-normal, making a rank estimate more reliable to\nestablish dependency than the Pearson\u2019s counterpart [12,16,17].\nCorrelations were computed between pairs of variables by using all available\ndays where both variables had observations. We consider only correlations be-\ntween pairs of variables with more than 20 common observations. We validated\nnon-parametrically correlations by using a permutation test that compares the\nobserved correlation coe\ufb03cients with a null (non-correlated) hypothesis gener-\nated by randomly shu\ufb04ing time entries in the series. Observed correlations are\nconsidered \u2018valid\u2019 only if they deviate from the mean of the random ones by at\nleast three standard deviations (i.e. Z score larger than 3 [18]). Note that this\nvalidation criteria is non-parametric and therefore robust also in the present\ncase where correlations do not follow the ststistical distribution assumed in\nstandard tests [19].\nThe dependency structure was analyzed in terms of its topological proper-\nties (the validated links structure). For this purpose, we de\ufb01ne the network\u2019s\nadjacency matrix Ai,j as a matrix with Ai,j = 1 when the corresponding corre-\nlation has Z > 3 and it is computed from more than 20 observations; Ai,j = 0\notherwise.\nWe computed all combinations of correlations within and across the va-\niables: i) cross correlations of log-price returns; ii) cross correlations of log-\nvolume sentiment changes (for both positive and negative sentiment); iii) the\ncombined cross correlations between price and sentiment log changes (for pos-\nitive sentiment only).\n\n6\nTomaso Aste\nWe also investigated weighted betweenness-centrality and closeness mea-\nsures [20] for each node in the validated correlation networks. The weight of an\nedge (i, j) between currency \u2018i\u2019 and currency \u2018j\u2019 was associated to the relative\ncorrelation \u03c4i,j as wi,j = 1 \u2212\u03c4 2\ni,j. Therefore uncorrelated nodes are connected\nwith edges with cost equal to 1 and perfectly correlated or anti-correlated\nnodes have zero-cost connection.",
    "chunk_index": 4,
    "start_char": 10924,
    "end_char": 14165,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "sentiment only).\n\n6\nTomaso Aste\nWe also investigated weighted betweenness-centrality and closeness mea-\nsures [20] for each node in the validated correlation networks. The weight of an\nedge (i, j) between currency \u2018i\u2019 and currency \u2018j\u2019 was associated to the relative\ncorrelation \u03c4i,j as wi,j = 1 \u2212\u03c4 2\ni,j. Therefore uncorrelated nodes are connected\nwith edges with cost equal to 1 and perfectly correlated or anti-correlated\nnodes have zero-cost connection.\nCausality was studied by estimating transfer entropy computed by means\nof a non-parametric histogram methodology, using 4 equally spaced bins (see in\n[14]). Transfer entropies were computed for log-price returns and log-volume\npositive sentiment changes. A validated transfer entropy network was con-\nstructed in an analogous way to the validated correlation networks by keeping\nlinks generated from time-series combinations longer than 40 days and keep-\ning transfer entropy permutation-test Z score larger than 3. Transfer entropy\nmeasures the reduction in uncertainty about the value of a given variable pro-\nvided by the knowledge of the previous values of another variable discounting\nfor the information from the past of the variable itself. In our case, we tested\nthe causal e\ufb00ect of positive sentiment on the next day prices and -conversely-\nthe causal e\ufb00ect of prices on next day positive sentiment across all currencies.\nWe also compared transfer entropy results with the Granger causality ap-\nproach that uses linear regression [21,22]. The outcomes of the two methods\nare overall consistent and here we report only the results for the non paramet-\nric method that obtains a larger number of validated causal links. It must be\nnoted that, in the linear case, when variables follow a multivariate normal dis-\ntribution, the transfer entropy method is identical to the well-known Granger\ncausality approach [23]. However, we are well aware that the dataset under\ninvestigation is not following a multivariate normal distribution and therefore\nthe non-parametric transfer entropy approach must be adopted. The fact that\nwe obtain a larger number of valid links with non-parametric transfer entropy\nreinforce the point that this system of variables must be properly described\nwith non-normal multivariate statistics. For the histogram approach we tested\ndi\ufb00erent binning observing that results are a\ufb00ected by the choice of the bins\nbut overall outcomes are consistent over a range of bins from 3 to 6.\nUnder normality assumptions a Z score larger than 3 would imply rejec-\ntion of null hypothesis with p-value below 0.13%. In this paper we use Z > 3\nas a threshold to eliminate noise from the correlations and we do not directly\nassociate this threshold on the Z-score with p-value null hypothesis rejection.\nIndeed, in our case, p-value is a\ufb00ected by the fact that statistics are not normal\nand samples are small. A precise testing of statistical signi\ufb01cance is beyond\nthe purposes of this paper however it is crucial to establish if the structures\nthat we uncover are re\ufb02ecting dependency and causalities among the variables\nor they are just picking randomly spurious interactions from a large number\nof possibilities on very noisy data.",
    "chunk_index": 5,
    "start_char": 13709,
    "end_char": 16912,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "we do not directly\nassociate this threshold on the Z-score with p-value null hypothesis rejection.\nIndeed, in our case, p-value is a\ufb00ected by the fact that statistics are not normal\nand samples are small. A precise testing of statistical signi\ufb01cance is beyond\nthe purposes of this paper however it is crucial to establish if the structures\nthat we uncover are re\ufb02ecting dependency and causalities among the variables\nor they are just picking randomly spurious interactions from a large number\nof possibilities on very noisy data. To this purpose we also tested validation at\nZ > 6 which, under normality assumptions, would imply rejection of null hy-\npotheses with p-value below 10\u22129. Outcomes from Z > 6 were consistent with\nthe analysis with Z > 3 but networks become extremely sparse to the point\nthat the transfer entropy network becomes largely disconnected into small\n\nCryptocurrency market structure: connecting emotions and economics\n7\n200\n400\n600\n800\n1000\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a)\nPrices\nBTC\nBCH\nETH\nLTC\nXPR\n0\n50\n100\n150\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b)\nPos. Sentiment\nNeg. Sentimet\nBTC\nBCH\nETH\nLTC\nXPR\nFig. 2 Complementary cumulative degree distribution (Probability(k > x) for the validated\nKendall cross correlation networks constructed from (a) the cross correlations of log-price\nreturns and (b) cross correlations of log-volume sentiment changes for both positive and\nnegative sentiments. The degrees of Bitcoin (BTC), Bitcoin Cash (BCH), Ethereum (ETH),\nLitecoin (LTC) and Ripple (XTC) are indicated explicitly with symbols.\n0\n1\n2\n3\n4\n5\n10-4\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a)\nPrices\nPos. Sentiment\nNeg. Sentimet\nBTC\nBCH\nETH\nLTC\nXPR\n0\n1000\n2000\n3000\n4000\n5000\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b)\nPrices\nPos. Sentiment\nNeg. Sentimet\nBTC\nBCH\nETH\nLTC\nXPR\nFig. 3 Closeness and betweenness-centrality complementary cumulative probability distri-\nbutions computed over the validated networks using weights wi,j = 1 \u2212\u03c4 2\ni,j.\nclusters and isolated nodes. We therefore also looked at similarity between\nthe various networks using the network from cross-correlation of log-price re-\nturns as a structure-template. The hypothesis we tested in this case was that\nsigni\ufb01cant structural similarity being incompatible with random networks.\n4 Results\n4.1 Price-Price & Sentiment-Sentiment cross-correlation validated networks\nWe \ufb01rst computed the validated networks from cross correlation of: 1) log-\nprices; 2) positive sentiment log-volume variations; 3) negative sentiment log-\nvolume variations. These are symmetric matrices of size 1944\u00d71944 with ones\non the diagonal. We observed predominately positive correlations with average\ncorrelation between log-prices variations being equal to 0.40, average correla-\ntion between positive sentiment log-volume variations being equal to 0.18 and\n\n8\nTomaso Aste\naverage correlation between the negative sentiment log-volume variations be-\ning equal to 0.22.\nWe computed the degree distribution by considering for each currency i\nthe number of other currencies j with which it shares a statistically validated\ncorrelation (ki = P\nj Ai,j). The valid correlation networks are sparse with\nthe network from price log-returns correlations having 15% of valid links and\naverage degree of 300.7.",
    "chunk_index": 6,
    "start_char": 16383,
    "end_char": 19584,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "0.40, average correla-\ntion between positive sentiment log-volume variations being equal to 0.18 and\n\n8\nTomaso Aste\naverage correlation between the negative sentiment log-volume variations be-\ning equal to 0.22.\nWe computed the degree distribution by considering for each currency i\nthe number of other currencies j with which it shares a statistically validated\ncorrelation (ki = P\nj Ai,j). The valid correlation networks are sparse with\nthe network from price log-returns correlations having 15% of valid links and\naverage degree of 300.7. In contrast, the positive and the negative sentiment\nvolume networks have respectively average degrees equal to 16.3 and 10.7. All\nnetworks have one connected giant component, a few small clusters and sev-\neral isolated nodes. The sizes of the giant components are respectively 1216,\n730 and 564 for price, positive and negative sentiment networks. Results for\nthe complementary cumulative degree distributions (Probability(ki > x)) are\nreported in Figs.2(a,b) for the three networks. In the \ufb01gures the degrees of\nBitcoin (BTC), Bitcoin Cash (BCH), Ethereum (ETH), Litecoin (LTC) and\nRipple (XTC) are indicated with symbols. A summary of the results for the\nmajor currencies is reported in Table 1. We notice that in the price network\nthese major cryptocurrencies have high degrees between 800 and 900 ranking\nin the top 10% of highly connected nodes being therefore hubs within the\nconnected component. Conversely, these currencies have relatively low degrees\nin the sentiment networks ranking below 50% in the positive sentiment net-\nwork and just above 50% in the negative sentiment network with number of\nconnections between 10 and 50.\nIn order to better understand the relative positioning within the cryptocur-\nrency market also with respect to the weighting of the correlations, we com-\nputed closeness and centrality distributions. These weighted measures, com-\nputed over the validated networks, are reported in Fig.3. We observe that for\nthe closeness the relative ranking of the \ufb01ve major cryptocurrencies is similar\nto the ones observed for the degree distribution; conversely the betweenness-\ncentrality places all major cryptocurrencies into medium/peripheral rankings.\n4.2 Price-Sentiment validated correlation network\nFrom now on we consider only positive volume sentiment. This choice is to sim-\nplify computation and description of the results. We investigated the Kendall\ncross correlations between log variation of positive sentiment volume and log\nvariations of price. This is an asymmetric 1944 \u00d7 1944 matrix representing a\nbipartite undirected network.\nThe diagonal elements of this matrix are the correlations between positive\nsentiment and price for each currency. Among the \ufb01ve major cryptocurrencies\nwe observe correlations on the diagonal of: 0.09 BTC, 0.07 BCH, 0.11 ETH,\n0.10 LTC and 0.05 XPR. Except for BCH and XPR they are all statistically\nvalidated with Z > 3 and series length over 20 points (BCH and XPR have\ninstead Z = 1.1 and 1.7 respectively). Overall, only 1% of currency log-price\nvariations have a valid correlation with their own log positive sentiment volume",
    "chunk_index": 7,
    "start_char": 19043,
    "end_char": 22189,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "on the diagonal of: 0.09 BTC, 0.07 BCH, 0.11 ETH,\n0.10 LTC and 0.05 XPR. Except for BCH and XPR they are all statistically\nvalidated with Z > 3 and series length over 20 points (BCH and XPR have\ninstead Z = 1.1 and 1.7 respectively). Overall, only 1% of currency log-price\nvariations have a valid correlation with their own log positive sentiment volume\n\nCryptocurrency market structure: connecting emotions and economics\n9\n10\n20\n30\n40\n50\n60\n70\n80\n0\n0.2\n0.4\n0.6\n0.8\n1\nImpacted\nImpacting\nBTC\nBCH\nETH\nLTC\nXPR\nFig. 4 In-degree and out-degree complementary cumulative distributions for the validated\nKendall cross correlation network between log variations of price of one currency and log\nvariation of positive sentiment volume of another. The \u2018impacted\u2019 distribution is counting\nthe number of valid links with other currencies whose positive sentiment is a\ufb00ected by the\ncurrency price. The \u2018impacting\u2019 distribution is counting the number of valid links with other\ncurrencies whose price is a\ufb00ected by the currency positive sentiment.\nvariations; they have mostly positive correlations but there are a few with\nnegative valid correlations as well.\nThe o\ufb00-diagonal elements, \u03c4i,j i \u0338= j, of this matrix are non-symmetric (\n\u03c4i,j \u0338= \u03c4j,i). They represent respectively: \u03c4i,j the correlation of positive sen-\ntiment of currency i with price of currency j; \u03c4j,i the correlation of positive\nsentiment of currency j with price of currency i. Here we must distinguish\ntwo kinds of degrees: 1) \u2018impacting\u2019 degree which is the sum of the valid\nentries over the columns (Igi = P\nj Ai,j); 2) \u2018impacted\u2019 degree which is the\nsum of the valid elements over the rows (Idj = P\ni Ai,j). Note that, in the\nliterature, these degrees are commonly referred as in-degree and out-degree\n[20]; however in our case this underlying implicit representation of the graph\nas a directed graph can be misleading implying some sort of causality that is\nnot measured here (it will be measured with Transfer Entropy as reported in\nthe next session). The \u2018impacting\u2019 degree of a given currency i is counting the\nnumber of valid links with other currencies j whose price is a\ufb00ected by the cur-\nrency positive sentiment. Conversely \u2018impacted\u2019 degree of a given currency i is\ncounting the number of valid links with other currencies j whose sentiment is\na\ufb00ected by the currency price. It results that this o\ufb00-diagonal matrix has 0.2%\n\n10\nTomaso Aste\nvalidated entries. The average degree is 3.1 for both impacting and impacted\ndegrees. The degree distributions are reported in Fig.4. We observe that the\ndistribution of the impacting degree has fatter tails than the one of the im-\npacted degree indicating that large variations of sentiment of a given currency\nare more in\ufb02uential on other currency price variations than large changes in\ncurrency price to other currency sentiment. Given that the average degree is\nthe same for both distributions we have -conversely- that small variations of\nsentiment of a given currency is more in\ufb02uential to other currency prices vari-\nations than small changes in currency price to other currency sentiment.",
    "chunk_index": 8,
    "start_char": 21836,
    "end_char": 24939,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "reported in Fig.4. We observe that the\ndistribution of the impacting degree has fatter tails than the one of the im-\npacted degree indicating that large variations of sentiment of a given currency\nare more in\ufb02uential on other currency price variations than large changes in\ncurrency price to other currency sentiment. Given that the average degree is\nthe same for both distributions we have -conversely- that small variations of\nsentiment of a given currency is more in\ufb02uential to other currency prices vari-\nations than small changes in currency price to other currency sentiment. In\nparticular we observe that changes in Bitcoin sentiment are correlated above\nvalidation threshold with changes in prices of almost eighty other currencies\nwhereas changes in Bitcoin price have valid correlation links to only ten other\ncurrency sentiment changes. A summary of the results for the major currencies\nis reported in left columns of Table 1.\nWe must stress that correlation is not causality and from the previous\nresults we cannot conclude what is the cause and what is the e\ufb00ect. For this\npurpose we must use other kinds of measures, such as -for instance- transfer\nentropy, as we shall proceed to the next section.\n4.3 Price-Sentiment transfer entropy causality network\nIn order to quantify causal relations between sentiment and price in the cryp-\ntocurrency market, we computed non parametric transfer entropy between log\nvariation of positive sentiment volume and log variations of price and vice\nversa. These are two 1944 \u00d7 1944 asymmetric matrices representing bipartite\ndirected networks.\nThe diagonals of these matrices report respectively the causal in\ufb02uence of\nsentiment over price and the causal in\ufb02uence of price over sentiment for each\ncurrency. As for the correlations we retain only the valid entries (over 40 com-\nmon observations and Z > 3). We observed that the overall information \ufb02ow\n(di\ufb00erence between the transfer entropy between sentiment to price and price\nto sentiment) is positive indicating for each currency that more information\nis transferred from past price to future sentiment than the contrary. However,\nonly about 2% of currencies have valid causality relations with 19 currencies\nhaving stronger causal in\ufb02uence of price over sentiment and, conversely, other\n11 currencies with stronger causal in\ufb02uence of sentiment over price. Inter-\nestingly, none of the \ufb01ve major currencies has valid internal price-sentiment\ncausality in either directions.\nThe o\ufb00-diagonal elements estimate the causal in\ufb02uence between sentiment\nin currency i on price of currency j and, conversely, the causal in\ufb02uence be-\ntween price in currency i on sentiment of currency j. These matrices are sparse\nwith only about 0.3% valid entries (about 10,000 causality links). Here we ob-\nserved that the overall information \ufb02ow is in the direction sentiment to price\nindicating that the past sentiment of other currencies in\ufb02uences the future\nprice of a given currency more than the e\ufb00ect of past prices over future sen-\n\nCryptocurrency market structure: connecting emotions and economics\n11\nPrice valid cross correlation degree\nPositive sentiment valid cross correlation degree\nNegative sentiment valid cross correlation degree",
    "chunk_index": 9,
    "start_char": 24358,
    "end_char": 27584,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "matrices are sparse\nwith only about 0.3% valid entries (about 10,000 causality links). Here we ob-\nserved that the overall information \ufb02ow is in the direction sentiment to price\nindicating that the past sentiment of other currencies in\ufb02uences the future\nprice of a given currency more than the e\ufb00ect of past prices over future sen-\n\nCryptocurrency market structure: connecting emotions and economics\n11\nPrice valid cross correlation degree\nPositive sentiment valid cross correlation degree\nNegative sentiment valid cross correlation degree\nPositive sentiment impacting other currencies prices\nPrice impacted by other currencies positive sentiment\nPrice causing other currencies positive sentiment\nOther currencies prices causing positive sentiment\nPositive sentiment causing other currencies prices\nOther currencies positive sentiment causing price\nZ > 3\nBTC\n894\n19\n51\n76\n10\n11\n10\n15\n8\nBCH\n864\n23\n47\n3\n3\n2\n4\n13\n13\nETH\n902\n9\n9\n11\n7\n7\n27\n6\n8\nLTC\n874\n8\n18\n4\n6\n10\n17\n17\n22\nXPR\n837\n18\n8\n8\n5\n2\n16\n6\n11\nZ > 6\nBTC\n542\n2\n2\n1\n1\n0\n0\n1\n0\nBCH\n497\n1\n2\n0\n0\n0\n0\n0\n0\nETH\n535\n1\n2\n0\n0\n0\n0\n0\n0\nLTC\n484\n1\n1\n0\n0\n0\n1\n0\n0\nXPR\n507\n0\n1\n0\n0\n0\n0\n0\n0\nTable 1\nSummary of results for the \ufb01ve major currencies. From left, the \ufb01rst column\nreports the Z validation threshold. THe following reports the currency tickers. Then the\nfollowing three columns report the degree in the valid cross correlation networks for prices,\npositive sentiment and negative sentiment. The following two columns report respectively the\nimpacting and impacted degree for the positive sentiment - price valid correlation network.\nFinally, the last four columns report degrees in the valid transfer entropy network.\ntiment. Conversely if we count the number of validated causality links we\nobserve 13,179 causality links for prices causing sentiment and instead 10,352\nfor sentiment causing prices. The price causing sentiment network has average\ndegree 6.8 and it has one giant component with 1023 elements. Similarly, the\nsentiment causing price network has average degree 5.3 and one giant compo-\nnent with 1018 elements. The degree distributions of the causality networks\nare shown in Fig.5. As in the previous case, we report two distributions: the\n\u2018impacting\u2019 and the \u2018impacted\u2019, the \ufb01rst being the number of all other cur-\nrencies that act a valid causality over a given currency, the latter being the\nnumber of all other currencies that react with valid causality from a given cur-\nrency. These two degrees are computed for both the Price causing Sentiment\nand the Sentiment causing Price networks. We observe that the \ufb01ve major\n\n12\nTomaso Aste\n0\n10\n20\n30\n40\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a)\nS->P\nP->S\nBTC\nBCH\nETH\nLTC\nXPR\n0\n10\n20\n30\n40\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b)\nS->P\nP->S\nBTC\nBCH\nETH\nLTC\nXPR\nFig.",
    "chunk_index": 10,
    "start_char": 27045,
    "end_char": 29787,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "react with valid causality from a given cur-\nrency. These two degrees are computed for both the Price causing Sentiment\nand the Sentiment causing Price networks. We observe that the \ufb01ve major\n\n12\nTomaso Aste\n0\n10\n20\n30\n40\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a)\nS->P\nP->S\nBTC\nBCH\nETH\nLTC\nXPR\n0\n10\n20\n30\n40\n50\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b)\nS->P\nP->S\nBTC\nBCH\nETH\nLTC\nXPR\nFig. 5 Complementary cumulative degree distributions for the validated transfer entropy\nnetwork. (a) \u2019impacting\u2019 distribution: number of other currencies in\ufb02uenced by a given cur-\nrency. (b) \u2019impacted\u2019 distribution: number of other currencies in\ufb02uencing a given currency.\nThe plots report both the validated transfer entropy network for prices causing sentiment\nand the network for sentiment causing prices.\ncurrencies are spread in a central region of the ranking with respect to the\nother currencies, with Bitcoin sentiment being among the most impactful on\nother currency prices but with Bitcoin price being the least impacted by other\ncurrency sentiment.\nSummary of the results for the major currencies is reported in the last\nthree columns of Table 1. We indeed can see that BTC positive sentiment is\ncausing prices in 15 other currencies whereas only 8 other currencies sentiment\nare causing BTC price. We also note that ETH positive sentiment is the most\nimpacted by other currencies prices and LTC price is caused by the largest\nnumber of other currencies positive sentiment. Finally, BCH causality is driven\nby sentiment much more than by prices.\nWe analyzed whether the relative position of a currency in the price net-\nwork has an e\ufb00ect on the relation between this currency and sentiment. To\nthis end we looked at the top 25% most central currencies in the price cross\ncorrelation network in terms of weighted betweenness centrality. We then com-\nputed the transfer entropies of price causing sentiment and sentiment causing\nprices for these currencies and compared the number of causal relations with\nthe ones for the bottom 25% most peripheral currencies in the price cross\ncorrelation network. Results show that central currencies have ten times more\ncausality links than the peripheral counterparts. Indeed, the top 20% central\ncurrencies account already for 50% of total causality links. Intriguingly, the\nsignal is larger for sentiment causing prices than for prices causing sentiment.\n4.4 Network signi\ufb01cance from the comparison between price and sentiment\nnetworks\nWe have analyzed very noisy data that follow non-normal distribution and\nwe tested millions of relations between variables. Spurious dependency and\ncausality relations are certainly present. What we must test is if the structural\n\nCryptocurrency market structure: connecting emotions and economics\n13\nproperties we unveiled are real to the system or only spurious consequences\nof noise and randomness. To this purpose we \ufb01rst tested di\ufb00erent levels of\nvalidation from Z > 2 to Z > 6 verifying that the results are consistent\nand persistent for di\ufb00erent validation thresholds. Some of these results for\nZ > 6 are reported in the bottom part of table 1.",
    "chunk_index": 11,
    "start_char": 29433,
    "end_char": 32519,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "causality relations are certainly present. What we must test is if the structural\n\nCryptocurrency market structure: connecting emotions and economics\n13\nproperties we unveiled are real to the system or only spurious consequences\nof noise and randomness. To this purpose we \ufb01rst tested di\ufb00erent levels of\nvalidation from Z > 2 to Z > 6 verifying that the results are consistent\nand persistent for di\ufb00erent validation thresholds. Some of these results for\nZ > 6 are reported in the bottom part of table 1. Note that, within normal\nstatistics assumptions Z > 6, would correspond to p-values below 10\u22129 and\nnonetheless we still retrieve some of the results previously reported especially\nfor the price cross correlation network that is still highly connected. However,\nwe also observed that at this threshold the transfer entropy network does\nnot have any longer a giant component with the larger cluster having only\n36 elements and average degree being 0.1. Overall, this analysis at large Z\nthresholds gives us some con\ufb01dence but still provides us with inconclusive\nanswers about the signi\ufb01cance of the results, indeed the non-normality of the\nstatistics can strongly a\ufb00ect the corresponding statistics of the Z-score with\nsizeable likelihood of spurious results even at this threshold levels.\nWe therefore decided to adopt a di\ufb00erent approach and, instead of trying\nto statistically validate each network, we cross-validate results by comparing\nmetrics from networks build from unrelated signals, namely, the price, the\npositive and the negative sentiment. We argue that if, for instance, the net-\nwork from sentiment correlations has signi\ufb01cantly similar properties with the\nnetwork from price correlations it is highly unlikely that the two represent\nrandom spurious correlations. We therefore compared the degree centrality\n(degree of each vertex) of the varius networks at di\ufb00erent validation thresh-\nolds. We used Superman correlation for the quanti\ufb01cation of the similarity\nbetween these measures. Results are reported in Tab.2 where we can see that\nthere are large and statistically signi\ufb01cant correlations (t-test p-values smaller\nthan 10\u221245) between all networks analyzed in this paper at all levels of valida-\ntion thresholding from Z > 3 to Z > 6. We note that similarities between the\ncorrelation networks tend to increase with thresholding value up to Z\u2217= 4 and\nthen decrease afterwards. Whereas the similarities with the combined Trans-\nfer Entropies network has maxima at Z\u2217= 3. The similarity increase with Z\u2217\nin the correlation networks is consequence of the reduction in the noise and\nthe decrease is instead the consequence of the reduction in statistics. In the\ntable, we did not include results from the sentiment-price network to avoid\nconfusion and also because they are less signi\ufb01cant given that the network is\nalready build from the two signals. Yet, results are well in line with the one\nreported in Table 2 with correlations ranging between 90% to 45%.\n5 Discussion\nOur \ufb01rst and most important comment concerning this work is that data are\nvery noisy. Price data have a slightly stronger signal than sentiment ones but\nin both cases noise is predominant.",
    "chunk_index": 12,
    "start_char": 32016,
    "end_char": 35197,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "table, we did not include results from the sentiment-price network to avoid\nconfusion and also because they are less signi\ufb01cant given that the network is\nalready build from the two signals. Yet, results are well in line with the one\nreported in Table 2 with correlations ranging between 90% to 45%.\n5 Discussion\nOur \ufb01rst and most important comment concerning this work is that data are\nvery noisy. Price data have a slightly stronger signal than sentiment ones but\nin both cases noise is predominant. Nonetheless, we observe the presence of\na structural organization both in the correlations and in the transfer entropy\nand we demonstrated that such a structure is not random.\n\n14\nTomaso Aste\nZ\u2217\nP-pS\nP-nS\npS-nS\nTESP-P\nTESP-pS\nTESP-nS\n3\n0.42\n0.43\n0.58\n0.80\n0.69\n0.57\n4\n0.63\n0.54\n0.72\n0.75\n0.61\n0.49\n5\n0.58\n0.50\n0.68\n0.61\n0.51\n0.41\n6\n0.49\n0.46\n0.60\n0.45\n0.38\n0.32\nTable 2\nSpearman correlations between degree centralities in the dependency and causal-\nity networks from prices and sentiment signals. Rows are di\ufb00erent levels of validation thresh-\nold with Z > Z\u2217. Columns are Spearman correlation coe\ufb03cients between degree centrality\nmeasures of di\ufb00erent networks. Speci\ufb01cally: P is the symbol for the prices network from\nKendall correlations; pS is the symbol for the positive sentiment network from Kendall cor-\nrelations; nS is the symbol for the negative sentiment network from Kendall correlations;\nTESP is the symbol for the combined Transfer Entropy causality networks between prices\nand sentiment. The combined transfer entropy degree centrality is the sum of all edges\nincoming in and outgoing from each vertex in the transfer entropy networks. Statistical val-\nidation of the correlation values (t-test) give p-values below 10\u221245 for all these correlations.\nConcerning the correlation analytics we have seen that price-price depen-\ndency have larger correlations but sentiment-sentiment and also sentiment-\nprices show valid and positive correlations. Not surprising, we observed that\nBitcoin and the other four major currencies have strong dependency ties with\nthe prices of a vast number of other currencies. More surprisingly, we observed\nthat, in contrast, in the sentiment dependency network these major cryp-\ntocurrencies are not highly connected. This is also re\ufb02ected in the closeness\nand centrality measures that see all major currencies in non-central positions\nin the network with exception only for the closeness measure for the price\nnetwork. The sentiment-price correlation network also re\ufb02ects mainly positive\ndependencies with major currencies having only average or just slightly above\naverage degrees with exception for the dependency between Bitcoin sentiment\nand other currency prices that reveal instead very strong dependency connec-\ntions.\nThe transfer entropy has a lower fraction of valid links. This is mainly due\nto the fact that this measure requires the estimate of a probability distribution\nbetween three variables which is hard to estimate well with the short time-\nseries we have. Nonetheless, we observe a sizable fraction of valid causality links\nwith most information \ufb02owing from prices to sentiment for each currency but\ninstead from sentiment to price when the cross-e\ufb00ect of a currency on another\nis considered.",
    "chunk_index": 13,
    "start_char": 34697,
    "end_char": 37956,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "currency prices that reveal instead very strong dependency connec-\ntions.\nThe transfer entropy has a lower fraction of valid links. This is mainly due\nto the fact that this measure requires the estimate of a probability distribution\nbetween three variables which is hard to estimate well with the short time-\nseries we have. Nonetheless, we observe a sizable fraction of valid causality links\nwith most information \ufb02owing from prices to sentiment for each currency but\ninstead from sentiment to price when the cross-e\ufb00ect of a currency on another\nis considered. Interestingly, in terms of number of valid links we observe a\nlarger number of causality links for prices causing sentiment than for sentiment\ncausing prices. This indicates that causality of sentiment over price carries a\nlarger amount of information but also a larger amount of noise and therefore\nit is validated only at higher transfer entropy values.\nThe comparison between causality of the central nodes in the prices net-\nwork with respect the peripheral ones for what concerns the e\ufb00ect of sentiment\nover prices and prices over sentiment shows that currencies that are central\nto the systems in term of price behavior are also the ones that most strongly\nin\ufb02uence the sentiment in the whole system. This is an interesting \ufb01nding also\n\nCryptocurrency market structure: connecting emotions and economics\n15\nin the light of the results in [24] that uncovered the great di\ufb00erence between\ncentral and peripheral vertices in terms of investment performances and risk.\nNote that the centre of the prices correlation network contains the \ufb01ve major\ncurrencies, however they are not the main responsible for the causality e\ufb00ect.\nWe already stressed that in this work we have investigated only valid de-\npendency and causality links giving us some con\ufb01dence that weak noisy links\nare removed. However, statistics is not normal and in our system we have al-\nmost four million possible relations between variables and some might turn out\nto be validated just as the e\ufb00ect of random \ufb02uctuations. We argued that the\nproof that, overall, results are robust and not reporting just incidental spurious\nrelations must be searched in the similarity of metrics of networks extracted\nwith di\ufb00erent methodologies (Kendall correlations or Transfer Entropy) from\ndi\ufb00erent signals (prices or sentiment). In this respect the strong correlations\nreported in Tab.2 are a good indication that these systems have a consistent\nstructural organization with prices and sentiments in\ufb02uencing each-other in a\nsigni\ufb01cant way.\n6 Conclusions\nThis study demonstrates that the current cryptocurrency market has a com-\nplex structure.\nMajor, highly capitalized cryptocurrencies and minor little\ncapitalized ones are interlocked into this complex structure with major cur-\nrencies playing central roles only for the price dependency network. Sentiment\nand prices are interconnected and they show both dependency and causality\nmainly between di\ufb00erent currencies.\nSocial sentiment plays a very important role in this market with Bitcoin\nsentiment correlating with other currencies prices even more than with its\nown price and with validated causal measures showing that sentiment is more\nin\ufb02uential on price than the contrary.",
    "chunk_index": 14,
    "start_char": 37395,
    "end_char": 40646,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "highly capitalized cryptocurrencies and minor little\ncapitalized ones are interlocked into this complex structure with major cur-\nrencies playing central roles only for the price dependency network. Sentiment\nand prices are interconnected and they show both dependency and causality\nmainly between di\ufb00erent currencies.\nSocial sentiment plays a very important role in this market with Bitcoin\nsentiment correlating with other currencies prices even more than with its\nown price and with validated causal measures showing that sentiment is more\nin\ufb02uential on price than the contrary.\nAn unexpected outcome of this research is that minor low-capitalised cur-\nrencies are playing a very important role in moving the market sentiment and\nconsequently are signi\ufb01cantly a\ufb00ecting prices also of the highly capitalised\ncurrencies. This is a fundamental di\ufb00erence from traditional markets where\nthe driving economic factors are typically re\ufb02ected into the dependency and\ncausality structure [7,8,9]. The fact that economically irrelevant variables can\nhave in\ufb02uence on the whole structure of the system is however a typical feature\nof complex systems where the system cannot be understood from the analysis\nof its parts in isolation [25]. This indicates that the study of cryptocurrencies\nand more generally of the digital economy require the development of tools\nbeyond traditional approaches with use of instruments from the science of\ncomplex systems.\nCryptocurrencies are increasingly traded and are becoming part of main-\nstream investment choices. From a risk-management and investment perspec-\ntives the present investigation unveil that the overall market dynamics is domi-\nnated by noise, large volatility and large failure rates. This is therefore a highly\n\n16\nTomaso Aste\nrisky domain where most of the traditional risk management and asset alloca-\ntion instruments are likely to be ine\ufb00ective. Complex system science [25] can\nguide us into the development of new tools for modelling, managing risk and\ndesign investment strategies for these markets and the new digital economy.\nThis paper is a \ufb01rst attempt to explore the very vast and intricate \ufb01eld\nof cryptocurrency market. Our e\ufb00orts have been mostly dedicated to perform\na statistically rigorous investigation of the whole market by using innovative\ntools such as network measures, non-linear quanti\ufb01cation of dependency and\ncausality and non-parametric validation techniques. The results are robust\ndespite the very challenging task to infer, from short time-series, non-linear\ninterrelations in a very large multivariate system.\nThese are extremely dynamical systems that change continuously. Our\nanalysis is limited to a short period of time and the system has already changed\nbetween the time when the system was analysed and the publication of this\npaper. This is an unavoidable reality in these system and the contribution of\nthis paper is not primarily about the actual speci\ufb01c properties of the cryp-\ntocurrencies market during the period investigated but some general facts,\nsuch as the in\ufb02uence of minor currencies on the whole system, that are likely\nto remain in the future and to be also characteristic of other systems. Further,\nan important contribution of this paper is the introduction of a set of rigorous\ninnovative methodologies for",
    "chunk_index": 15,
    "start_char": 40065,
    "end_char": 43374,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "This is an unavoidable reality in these system and the contribution of\nthis paper is not primarily about the actual speci\ufb01c properties of the cryp-\ntocurrencies market during the period investigated but some general facts,\nsuch as the in\ufb02uence of minor currencies on the whole system, that are likely\nto remain in the future and to be also characteristic of other systems. Further,\nan important contribution of this paper is the introduction of a set of rigorous\ninnovative methodologies for the study of systems composed of a very large set\nof variables with non-liner interactions and with small numbers of available\nobservations. This is a very general challenge common to most socio-economic\nand complex systems where the methods introduced with this paper can be\nconveniently adopted in the future.\nMuch more must be done in future. For instance, in the study of the inter-\nactions between prices and sentiment we neglected, for simplicity, the negative\nsentiment. It is however clear that this plays a very important role which ap-\npears to be not trivially related to the positive one. We also made many choices,\nstarting from the Z statistics validation threshold or the use of log-variation of\nsentiment volumes or the choice of considering all currencies and not only the\nfew with relevant market share. Di\ufb00erent choices produce di\ufb00erent results. In\nour investigations we veri\ufb01ed that the overall reported results are robust and\nthese are retrieved similarly by adopting di\ufb00erent choices. However, a more\nextensive and systematic study is necessary.\nAcknowledgmets\nMany thanks to the PsychSignal team for providing the sentiment data. We\nwish to thank Yuqing Long for his careful data preparation. Also thank to Zac\nKeskin for investigating transfer entropy in these systems within a parallel\nwork that helped clarifying several issues in this one as well. Many thanks to\nthe UCL-FCA group for help with discussions and proof reading. Finally we\nare grateful to EPSRC for funding the BARAC project (EP/P031730/1) and\nto EU for funding the FinTech project (H2020-ICT-2018-2 825215).\n\nCryptocurrency market structure: connecting emotions and economics\n17\nReferences\n1. https://coinmarketcap.com/.\n2. Konstantinos Gkillas, Stelios Bekiros, and Costas Siriopoulos. Extreme correlation in\ncryptocurrency markets. 2018.\n3. Beata Szetela, Grzegorz Mentel, and Stanis law Gedek. Dependency analysis between\nbitcoin and selected global currencies. Dynamic Econometric Models, 16(1):133\u2013144,\n2016.\n4. Young Bin Kim, Jun Gi Kim, Wook Kim, Jae Ho Im, Tae Hyeong Kim, Shin Jin Kang,\nand Chang Hun Kim. Predicting \ufb02uctuations in cryptocurrency transactions based on\nuser comments and replies. PloS one, 11(8):e0161197, 2016.\n5. Jermain Kaminski. Nowcasting the bitcoin market with twitter signals. arXiv preprint\narXiv:1406.7577, 2014.\n6. https://stocktwits.com/.\n7. Tomaso Aste, W Shaw, and Tiziana Di Matteo. Correlation structure and dynamics in\nvolatile markets. New Journal of Physics, 12(8):085009, 2010.\n8. Won-Min Song, T Di Matteo, and Tomaso Aste. Hierarchical information clustering by\nmeans of topologically embedded graphs. PLoS One, 7(3):e31929, 2012.\n9. Nicol\u00b4o Musmeci, Tomaso Aste, and Tiziana di Matteo.\nClustering and hierarchy of\n\ufb01nancial markets data: advantages of the dbht. CoRR, 2014.",
    "chunk_index": 16,
    "start_char": 42883,
    "end_char": 46187,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "Kaminski. Nowcasting the bitcoin market with twitter signals. arXiv preprint\narXiv:1406.7577, 2014.\n6. https://stocktwits.com/.\n7. Tomaso Aste, W Shaw, and Tiziana Di Matteo. Correlation structure and dynamics in\nvolatile markets. New Journal of Physics, 12(8):085009, 2010.\n8. Won-Min Song, T Di Matteo, and Tomaso Aste. Hierarchical information clustering by\nmeans of topologically embedded graphs. PLoS One, 7(3):e31929, 2012.\n9. Nicol\u00b4o Musmeci, Tomaso Aste, and Tiziana di Matteo.\nClustering and hierarchy of\n\ufb01nancial markets data: advantages of the dbht. CoRR, 2014.\n10. https://www.cryptocompare.com/.\n11. https://www.psychsignal.com, October 2015.\n12. Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81\u201393,\n1938.\n13. Thomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461,\n2000.\n14. Sachapon Tungsong, Fabio Caccioli, and Tomaso Aste. Relation between regional un-\ncertainty spillovers in the global banking system.\narXiv preprint arXiv:1702.05944,\n2017.\n15. John Y Campbell, Andrew W Lo, Archie Craig MacKinlay, et al. The econometrics of\n\ufb01nancial markets, volume 2. princeton University press Princeton, NJ, 1997.\n16. F Pozzi, T Aste, G Rotundo, and T Di Matteo. Dynamical correlations in \ufb01nancial sys-\ntems [6802-54]. In PROCEEDINGS-SPIE THE INTERNATIONAL SOCIETY FOR\nOPTICAL ENGINEERING, volume 6802, page 6802. International Society for Optical\nEngineering; 1999, 2008.\n17. Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste. Centrality and peripherality in\n\ufb01ltered graphs from dynamical \ufb01nancial correlations. Advances in Complex Systems,\n11(06):927\u2013950, 2008.\n18. Samuel S Wilks. Certain generalizations in the analysis of variance. Biometrika, pages\n471\u2013494, 1932.\n19. Maurice George Kendall et al. The advanced theory of statistics. The advanced theory\nof statistics., (2nd Ed), 1946.\n20. Mark EJ Newman. The mathematics of networks. The new palgrave encyclopedia of\neconomics, 2(2008):1\u201312, 2008.\n21. Clive Granger. Investigating causal relations by econometric models and cross-spectral\nmethods. Econometrica, 37(3):424\u201338, 1969.\n22. Clive WJ Granger. Testing for causality: a personal viewpoint. Journal of Economic\nDynamics and control, 2:329\u2013352, 1980.\n23. Lionel Barnett, Adam B. Barrett, and Anil K. Seth. Granger causality and transfer\nentropy are equivalent for gaussian variables. Phys. Rev. Lett., 103:238701, Dec 2009.\n24. Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste. Spread of risk across \ufb01nancial\nmarkets: better to invest in the peripheries. Scienti\ufb01c reports, 3, 2013.\n25. Tomaso Aste and Tiziana Di Matteo. Introduction to complex and econophysics sys-\ntems: A navigation map. In Complex physical, biophysical and econophysical systems,\npages 1\u201335. 2010.",
    "chunk_index": 17,
    "start_char": 45615,
    "end_char": 48368,
    "paper_title": "Cryptocurrency market structure connecting emotion",
    "paper_category": "q-fin.ST",
    "paper_filename": "Cryptocurrency_market_structure_connecting_emotion.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Cryptocurrency_market_structure_connecting_emotion.pdf"
  },
  {
    "text": "Dual-CLVSA: a Novel Deep Learning Approach to\nPredict Financial Markets with Sentiment\nMeasurements\nJia Wang\u2217, Hongwei Zhu\u2020, Jiancheng Shen\u2021, Yu Cao\u2217, Benyuan Liu\u2217\n\u2217Department of Computer Science\n\u2020Department of Operations and Information Systems\nUniversity of Massachusetts Lowell\nEmail: {jwang, ycao, bliu}@cs.uml.edu, harry zhu@uml.edu\n\u2021Dongwu Business School\nSoochow University\nEmail: jcshen@suda.edu.cn\nAbstract\u2014It is a challenging task to predict \ufb01nancial markets.\nThe complexity of this task is mainly due to the interaction\nbetween \ufb01nancial markets and market participants, who are\nnot able to keep rational all the time, and often affected\nby emotions such as fear and ecstasy. Based on the state-of-\nthe-art approach particularly for \ufb01nancial market predictions,\na hybrid convolutional LSTM Based variational sequence-to-\nsequence model with attention (CLVSA), we propose a novel\ndeep learning approach, named dual-CLVSA, to predict \ufb01nancial\nmarket movement with both trading data and the correspond-\ning social sentiment measurements, each through a separate\nsequence-to-sequence channel. We evaluate the performance of\nour approach with backtesting on historical trading data of SPDR\nSP 500 Trust ETF over eight years. The experiment results show\nthat dual-CLVSA can effectively fuse the two types of data, and\nverify that sentiment measurements are not only informative\nfor \ufb01nancial market predictions, but they also contain extra\npro\ufb01table features to boost the performance of our predicting\nsystem.\nIndex Terms\u2014Deep learning, Finance, TRMI.\nI. INTRODUCTION\nPredicting \ufb01nancial markets is always challenging. The\nmain difference between \ufb01nancial markets and other natural\nsequential events (e.g. DNA Sequences) is that, the evolution\nof \ufb01nancial markets is caused by the collective behavior of\nmarket participants rather than being governed by law of\nnature. The adaptive nature of \ufb01nancial markets makes their\nmovement more complicated and dif\ufb01cult to predict as market\nparticipants are not able to be rational all the time. Once\nmarket participants are dominated by their emotions, such\nas fear, upset, ecstasy, and frustration, they inevitably cannot\nhelp overreacting or even making wrong decisions. Behavioral\neconomists demonstrate that inef\ufb01ciency of \ufb01nancial markets\nresults from the spread of emotional responses among market\nparticipants, systematically biasing trading behaviors. As the\ngroup of market participants with the same emotion expands,\ntheir biased behaviors create trends of \ufb01nancial markets, which\nsubsequently force the market price to move away from the\ntrue value.\nHow to capture effective latent features from trading data is\nthe key to build robust predicting systems for \ufb01nancial mar-\nkets. Some research, such as [1]\u2013[3], use machine learning al-\ngorithms (e.g. SVM, Nearest Neighborhood, and Feed-forward\nnetworks) to extract latent features from technical indicators.\nWhile technical indicators have been widely used by market\nparticipants, these methods may inevitably introduce human\nbiases into models. Other popular sources for extracting latent\nfeatures include market-related texts and information, such as\nreports, news, and tweets. Although classic economic theories\nbelieve that prices re\ufb02ect all information, the sentiment data is\nstill informative for traders due to a basic fact that people have\nemotions, and they take actions in markets. Moreover, many\nstudies, such as [4]\u2013[6] have demonstrated that a person\u2019s\narousal level impacts decision-making .",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3511,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "been widely used by market\nparticipants, these methods may inevitably introduce human\nbiases into models. Other popular sources for extracting latent\nfeatures include market-related texts and information, such as\nreports, news, and tweets. Although classic economic theories\nbelieve that prices re\ufb02ect all information, the sentiment data is\nstill informative for traders due to a basic fact that people have\nemotions, and they take actions in markets. Moreover, many\nstudies, such as [4]\u2013[6] have demonstrated that a person\u2019s\narousal level impacts decision-making .\nTherefore, if sentiment data can be obtained quickly, we\nwill probably attain signals of the upcoming trend of \ufb01nancial\nmarkets. In this paper, we use Thomson Reuters MarketPsych\nIndices (TRMI) [7] to investigate whether sentiment data\nprovide signals that are more directional than random price\nmovements. TRMI utilizes two groups of data sources to mea-\nsure sentiment, namely, news and social media. The feed data\nconsist of three types: a social media feed, a news media feed,\nand an aggregated feed of combined social and news media\ncontent. We proceed our research with the following three\nsteps: 1. Verify the informativeness of TRMI data. We choose\nrecurrent neural network with LSTM units as the baseline\nmodel (LSTMs), and compare the expeirmental results on\nthe three following datasets to examine the informativeness of\nTRMI data: historical trading data only, historical trading data\nwith technical indicators, historical trading data with TRMI\ndata. 2. Building upon the state-of-the-art experimental results\nof CLVSA on futures market predictions [8], we evaluate this\napproach on historical SPDR SP500 Trust ETF (SPY) trading\ndata. Our experimental results show that CLVSA still achieves\nthe best performance for historical SPY trading data, compared\nto baseline methods, such as LSTMs. We thus use it as the\narXiv:2202.03158v1 [q-fin.ST] 27 Jan 2022\n\nbaseline method of the 3rd step. 3. Design an effective method\nto fuse historical trading data and TRMI data based on the\napproach that is veri\ufb01ed by the previous step. The intrinsic\ncharacteristics of historical trading data and TRMI data are so\ndifferent that it does not work to directly fuse them at the input,\nwhich is veri\ufb01ed by the \ufb01rst- and second-step experiment\nwith SPY historical trading data with technical indicators and\nTRMI data. We design a fusion strategy, called dual-CLVSA,\nwhich applies two parallel channels of sequence-to-sequence\nframework for TRMI data and historical trading data to capture\ntheir distinctive features, and then combine the features to take\nadvantage of the two different sources of information.\nWe summarize our contributions as follows:\n1) Although there is complicated and implicit relevance\nbetween TRMI data and \ufb01nancial trading data, the\ndifferent nature between the two types of data disturb\nthem to fuse together with a simple manner. This paper\nexplores fusion approaches to train TRMI data and raw\n\ufb01nancial trading data together.\n2) We train our model using 8-year trading data of SPY\nwith the corresponding TRMI data. Our experimental re-\nsults show that our fusion model, dual-CLVSA, achieves\nthe best performance on both \ufb01nancial and machine\nlearning",
    "chunk_index": 1,
    "start_char": 2946,
    "end_char": 6185,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "complicated and implicit relevance\nbetween TRMI data and \ufb01nancial trading data, the\ndifferent nature between the two types of data disturb\nthem to fuse together with a simple manner. This paper\nexplores fusion approaches to train TRMI data and raw\n\ufb01nancial trading data together.\n2) We train our model using 8-year trading data of SPY\nwith the corresponding TRMI data. Our experimental re-\nsults show that our fusion model, dual-CLVSA, achieves\nthe best performance on both \ufb01nancial and machine\nlearning criteria, which also veri\ufb01es that TRMI data\ncontains extra informative features which can boost the\nperformance of prediction systems.\nThe remainder of the paper is organized as follows. Re-\nlated work on \ufb01nancial market prediction with deep learning\nmethods is presented in Section II. The methodology of our\nexploration of predicting \ufb01nancial markets with sentiment\nmeasurements is presented in Section III. The data preprosess-\ning and experimental setup and results are described in Section\nIV. Two case studies are presented in Section V, followed by\nconcluding remarks in Section VI.\nII. RELATED WORK\nAlthough traditional predicting approaches such as technical\nanalysis/indicators have existed for over hundreds of years,\nautomated trading systems based on pattern recognition and\nmachine learning have become popular since the 1990s. Var-\nious algorithms, such as SVM, nearest-neighbour, decision\ntrees, and feed-forward neural networks, have been applied to\npredict stocks, foreign exchange, and commodity futures mar-\nkets [1]\u2013[3], [9]. All the aforementioned work use technical\nindicators as input features. Since 2010s, more research start\nto utilize the power of deep learning algorithms to predict\n\ufb01nancial markets. [10], [11] use deep convolutional neural\nnetworks to capture potential trading features from \ufb01nancial\nevents and \ufb01nancial trading data, respectively. [12] proposes\na variant of LSTM enhanced by discrete fourier transform to\ndiscover Multi-Frequency Trading Patterns. [13] proposes an\napproach based on reinforcement learning to model automated\ndata-centric decision makers in quantitative \ufb01nance.\nBinding the local feature extraction ability of deep convo-\nlutional neural networks with the temporal features retention\nof LSTM, convolutional LSTM proposed by [14] has been\napplied in many \ufb01elds such as weather forecasting [14], image\ncompression [15], and general algorithmic tasks (e.g. binary\naddition) [16]. The sequence-to-sequence framework proposed\nby [17] achieves a signi\ufb01cantly success in neural machine\ntranslation tasks, enhanced subsequently by inter-attention [18]\nand self-attention [19]. [20], [21] propose variational auto-\nencoder (VAE) that uses the encoder to form the approximate\nposterior, then trains the generative decoder to approximate\nthe inputs of the encoder with variational lower bound and\nKLD. SRM [22], [23] extends the basic idea of VAE into\nrecurrent networks, using backward recurrent neural networks\nas the approximate posterior instead.\nSome apporaches, such as [10], [24], [25], use natural lan-\nguage processing approaches to extract latent features within\nmarket-related texts and information, such as reports, news,\nand tweets. However, to the best of our knowledge, our\nresearch is among the \ufb01rst attempts to extract latent feature\nwithin sentiment measurements (e.g., Thomson Reuters Mar-\nketPsych Indices, a.k.a TRMI) with deep learning approaches.",
    "chunk_index": 2,
    "start_char": 5682,
    "end_char": 9109,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "basic idea of VAE into\nrecurrent networks, using backward recurrent neural networks\nas the approximate posterior instead.\nSome apporaches, such as [10], [24], [25], use natural lan-\nguage processing approaches to extract latent features within\nmarket-related texts and information, such as reports, news,\nand tweets. However, to the best of our knowledge, our\nresearch is among the \ufb01rst attempts to extract latent feature\nwithin sentiment measurements (e.g., Thomson Reuters Mar-\nketPsych Indices, a.k.a TRMI) with deep learning approaches.\nTRMI use natural language processing approaches to process\nsentiment-laden content in text, scoring content that pertains\nto speci\ufb01c companies, currencies, commodities, and countries.\nAs the background of TRMI, varying levels of stress have\nbeen shown to map to cognitive performance in an inverse-\nU curve called the Yerkes-Dodson Law [4], [5]. When stress\nlevels are very high, complex problem-solving performance\ndrops and reliance on pre-existing habits increases [6]. On the\nother hand, low stress levels also lead to subpar performance in\ncomplex decision-making environments due to inattention and\nslow reaction. Thus decision-makers typically perform with\noptimal cognition when arousal is in the middle of its range.\nIII. METHODOLOGY\nA. Introduction to Thomson Reuters MarketPsych Indices\nThomson Reuters MarketPsych Indices (TRMI) measure the\nsentiment of market participants by distilling a massive col-\nlection of news and social media content through an extensive\nnatural language processing framework. The indices consider\ndifferent emotions (optimism, confusion, urgency etc.), as well\nas \ufb01nancial terms (interest rate, mergers etc.).\nTRMI have two groups of data sources: news and social\nmedia. The feed data consist of three types: a social media\nfeed, a news media feed, and an aggregated feed of combined\nsocial and news media content. TRMI use natural language\nprocessing approaches to process sentiment-laden content in\ntext, scoring content that pertains to speci\ufb01c companies,\ncurrencies, commodities, and countries. The entire content\nset includes over 2 million articles and posts daily from\npremium news wires, internet news sources, and social media.\nIn our research, we focus on two types of TRMI: companies\nand equity index TRMI indices, and energy and material\ncommodity TRMI indices. Each TRMI index consists of a\ncombination of variables (PsychVars), such as AccountingBad,\n\nConv-LSTM\nwtih Self-attn\nConv-LSTM\nwtih Self-attn\nInter-attn\nHistorical Trading\nData\nFully Connected\nLayers\nHistorical Sentiment\nData\nSingle-channel CLVSA\nEncoder\nDecoder\nFig. 1: The architecture of CLV SA2. In this approach, we fuse historical sentiment data and trading data at the input, and\nour experimental results show that this fusion method does not work.\nAccountingGood, Ambiguity, and Anger. Formally:\nBUZZ(a) =\nX\nc\u2208C(a),p\u2208P\n|PsychV arc,p|,\nWhere Buzz(a) denotes the sum of the absolute values of\nall TRMI-contributing PsychVars. P denotes the set of all\nPsychVars underlying any TRMI of the asset class, C(a)\ndenotes the set of all elements of asset a. For example, if\na is SP500, then C(a) represents the stocks of the 500 large\ncompanies in SP500. Each TRMI is then computed as a ratio\nof the sum of all relevant PsychVars to the Buzz.",
    "chunk_index": 3,
    "start_char": 8569,
    "end_char": 11864,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "c\u2208C(a),p\u2208P\n|PsychV arc,p|,\nWhere Buzz(a) denotes the sum of the absolute values of\nall TRMI-contributing PsychVars. P denotes the set of all\nPsychVars underlying any TRMI of the asset class, C(a)\ndenotes the set of all elements of asset a. For example, if\na is SP500, then C(a) represents the stocks of the 500 large\ncompanies in SP500. Each TRMI is then computed as a ratio\nof the sum of all relevant PsychVars to the Buzz. We de\ufb01ne\na function to indicate whether a PsychVar p \u2208P is additive,\nsubtractive, or irrelevant to a TRMI. Formally,\nI(t, p) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n+1, if additive,\n\u22121, if subtractive,\n0, if irrelevant,\nTRMIt(a) =\nP\nc\u2208C(a),p\u2208P (t)(I(t, p) \u2217PsychV arc,p)\nBuzz(a)\n,\nwhere TRMIt(a) denotes the t \u2212th TRMI for asset a.\nB. Experimental Plan\nThe main goal of our research is to verify our hypothesis\nthat sentiment data can provide extra informative features to\n\ufb01nancial markets predictions. We thus design a three-step\nexperimental plan based on the state-of-the-art model, CLVSA,\nwith modi\ufb01cations as minimum as possible.\n1) Verify the informativeness of TRMI data. We choose\nrecurrent neural network with Long Short Term Memory\n(LSTM) units as the baseline model (LSTMs), and\nuse four different datasets to train LSTMs, including\nSPDR SP500 Trust ETF (SPY) historical trading data\nonly, SPY historical trading data with technical indica-\ntors, SPY historical trading data with the correspond-\ning TRMI data, and SPY historical trading data with\ntechnical indicators and the corresponding TRMI data.\nWe follow the methods in [8] to generate technical\nindicators.\n2) Identify a high performance baseline model with his-\ntorical SPY trading data. In the previous research, our\nexperimental results verify that CLVSA outperforms\nLSTMs. If we could reproduce similar results for SPY\nhistorical trading data, CLVSA would qualify as the\nbaseline model for the experiments with TRMI data.\n3) Explore effective methods to fuse historical trading\ndata and TRMI data for \ufb01nancial market prediction.\nThe intrinsic mechanisms of historical trading data and\nTRMI data are so different that it does not work to\ndirectly fuse them. We design a novel fusion method,\nnamed dual-CLVSA to address this problem.\nC. dual-CLVSA: the fusion method\nThe base approach of our fusion method, CLVSA, is\na hybrid model consisting of convolutional LSTM units,\nsequence-to-sequence framework with attention mechanism,\nand stochastic recurrent networks, schematically shown in Fig-\nure 1. The encoder and decoder of the sequence-to-sequence\nframework take 2-D frames of historical trading data of two\nconsecutive days as input, respectively. The inter-attention\nmodule highlights parts of the \ufb01rst one of two consecutive days\nas the context of the second day. The convolutional LSTM\nunits of the encoder and decoder process 2-D data frames in\ntwo steps: i) Convolutional kernels capture local features, ii)\nBased on the local features, LSTM networks capture temporal\nfeatures with gated recurrent networks. In each layer of the\nencoder and decoder, a self-attention module is utilized to\nhighlight parts of the sequence of daily data frames.",
    "chunk_index": 4,
    "start_char": 11440,
    "end_char": 14559,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "inter-attention\nmodule highlights parts of the \ufb01rst one of two consecutive days\nas the context of the second day. The convolutional LSTM\nunits of the encoder and decoder process 2-D data frames in\ntwo steps: i) Convolutional kernels capture local features, ii)\nBased on the local features, LSTM networks capture temporal\nfeatures with gated recurrent networks. In each layer of the\nencoder and decoder, a self-attention module is utilized to\nhighlight parts of the sequence of daily data frames.\nFrom the above description, we can see that convolutional\nkernels play a fundamental role in CLVSA. The convolutional\nkernels operate directly on input data, so the other parts,\nsuch as LSTM units and attention layers, work based on\nlocal features extracted by convolutional kernels. However,\nas demonstrated in [11], Cross-Data-Type 1-D Convolution\n(CDT 1-D Convolution) is applied as convolutional kernels\nin CLVSA to accommodate the characteristics of historical\ntrading data, which is comprised of \ufb01ve elements: Open,\nHigh, Low, Close prices, and Volume. However, there is a\nprerequisite to use CDT 1-D Convolution, that is, all elements\nshould have strong relevance with each other (e.g. prices and\nvolume under \ufb01nancial markets) so they can share parameters.\nOur experimental results show that the performance of CLVSA\nwith a direct fusion of TRMI data and historical SPY trading\n\ndata (CLV SA2 in Table I) degrades by 18.5%, and 1.01 for\naverage annual return (AAR), Sharpe ratio (SR), respectively,\ncompared to CLVSA with historical SPY trading data only\n(CLV SA1).\nTo solve this problem, we propose a dual-CLVSA model to\nfuse TRMI data and historical trading data. The architecture\nof dual-CLVSA is illustrated in Figure 2. The basic idea is\nthat, we assign two separate sequence-to-sequence framework\nto TRMI data and historical trading data, respectively. The\ntwo channels are not fused until outputs of decoders from the\ntwo channels are concatenated and fed into fully connected\nlayers. On one hand, two separate channels avoid mix-up\non convolutions as they have different characteristics; on the\nother hand, the two channels are combined after individual\nsequence-to-sequence framework, guaranteeing that the two\nindependent sets of features are processed with the same\nweight in the fully connected layers. We do not apply another\nset of Kullback-Leibler divergence (KLD) for the channel of\nTRMI data because of the sporadic characteristic of sentiment\ndata.\nIV. EXPERIMENTAL SETUP AND RESULTS\nA. Preprocessing TRMI Data\nWe bind TRMI data to raw trading data of the corresponding\nsecurities with the same time stamps. That means, we treat\nTRMI as sentiment \u201cindicators\u201d, and expect these sentiment\nindicators to provide the information that is not contained in\nprice movement and trading volume. Speci\ufb01cally,\n1) The datasets we used in this paper include two parts: (1)\nhistorical trading records of a commodity futures WTI\nCrude Oil (CL), and an exchange-traded fund, SPDR\nS&P 500 Trust ETF (SPY). Both of these securities\ninclude the following seven attributes: date, time, open\nprice, high price, low price, close price, and trading vol-\nume;",
    "chunk_index": 5,
    "start_char": 14064,
    "end_char": 17221,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "these sentiment\nindicators to provide the information that is not contained in\nprice movement and trading volume. Speci\ufb01cally,\n1) The datasets we used in this paper include two parts: (1)\nhistorical trading records of a commodity futures WTI\nCrude Oil (CL), and an exchange-traded fund, SPDR\nS&P 500 Trust ETF (SPY). Both of these securities\ninclude the following seven attributes: date, time, open\nprice, high price, low price, close price, and trading vol-\nume; (2) The corresponding TRMI data, from which we\nchoose the following \ufb01ve common indices as sentiment\nfeatures: buzz, sentiment, optimism, fear, and joy. For the\nmodels that contain convolutional kernels, we follow the\npreprocessing strategy in [11]; For others that do not\ncontain convolutional kernels, we aggregate historical\ntrading records into half-hour data points, and normalize\nthe TRMI data weighted by the Buzz,\nTRMIT (a) =\nP\ni\u2208T (Buzzi \u2217TRMIi(a))\nP\ni\u2208T Buzzi\n,\nwhere T denotes the desired time interval, which is half\nhour in our research, i denotes the time stamps within\nT, a denotes the type of TRMI (e.g., joy, fear). After\nthe aggregation, we also bind the two types of data with\ntime stamps.\n2) We guarantee that datasets with and without senti-\nment measurement are aligned in time for the purpose\nof meaningful comparisons. After the alignment, the\ndatasets contain both historical trading and sentiment\ndata. We then pick the corresponding \ufb01elds according\nto experimental setup. Therefore, the binding procedure\nMAP\nAAR\nSR\nDJA\nYJA\nLSTM1\ns\n42.0%\n\u221219.9%\n\u22121.45\n\u22120.12%\n\u221221.3%\nLSTM2\ns\n46.0%\n34.2%\n1.75\n0.24%\n43.6%\nLSTM3\ns\n45.5%\n32.8%\n1.27\n0.24%\n41.9%\nLSTM4\ns\n45.7%\n28.9%\n1.08\n0.22%\n37.2%\nCLV SA1\n46.1%\n48.0%\n2.11\n0.24%\n57.9%\nCLV SA2\n45.4%\n29.5%\n1.10\n0.14%\n35.6%\ndual-CLVSA\n50.7%\n50.7%\n50.7%\n57.3%\n57.3%\n57.3%\n3.01\n3.01\n3.01\n0.29%\n0.29%\n0.29%\n65.0%\n65.0%\n65.0%\nTABLE I: Experimental results of SPY. MAP, AAR, SR, DJA,\nYJA denote the mean average precision, average annual return,\nand Sharpe ratio, Daily Jensen Alpha, and Yearly Jensen Alpha\nrespectively.\nis similar to sentiment data \u201cright joining\u201d historical\ntrading data with timestamps. We inevitably need to\nadd paddings into sentiment data when TRMI data\nare missing in some parts of the data. It is a normal\noperation because of the impulsing characteristic of sen-\ntiment data, however, too much padding will harm the\nprediction performance. More details will be described\nin Section V-B.\nB. Experimental results of LSTMs\nThe baseline method at the \ufb01rst step aims to verify the\ninformativeness of sentiment data. We use recurrent neural\nnetwork with LSTM units (LSTMs) to train and test the\nfollowing four types of datasets, SPY historical trading data,\nSPY historical trading data with technical indicators, SPY\nhistorical trading data with sentiment data, SPY historical\ntrading data with technical indicators and sentiment data.\nWe named the above four experimental sessions as LSTM 1\ns\nto LSTM 4\ns , respectively. Table I shows their experimental\nresults.\nSince LSTMs is designed for temporal feature extrac-\ntion, which lacks the capability of local feature extraction.",
    "chunk_index": 6,
    "start_char": 16758,
    "end_char": 19868,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "with LSTM units (LSTMs) to train and test the\nfollowing four types of datasets, SPY historical trading data,\nSPY historical trading data with technical indicators, SPY\nhistorical trading data with sentiment data, SPY historical\ntrading data with technical indicators and sentiment data.\nWe named the above four experimental sessions as LSTM 1\ns\nto LSTM 4\ns , respectively. Table I shows their experimental\nresults.\nSince LSTMs is designed for temporal feature extrac-\ntion, which lacks the capability of local feature extraction.\nConsequently, the experimental results of LSTM 1\ns shows\nsevere losses, -19.9% average annual return (AAR). LSTM 2\ns ,\nhowever, stays positive and achieves an AAR of 34.2%.\nThe signi\ufb01cant difference between the above two experiments\ndemonstrates that technical indicators provide informative\nlocal features to LSTMs.\nThe experimental results of LSTM 3\ns show a positive AAR\nof 32.8% as well. Although it works slightly worse than\nthe experiment with technical indicators, the performance is\nsigni\ufb01cantly better than the experiment with historical SPY\ntrading data only (LSTM 1\ns ). This result veri\ufb01es that TRMI\ndata is able to provide informative features as technical indi-\ncators.\nThe experiments of LSTM 4\ns show interesting results. Com-\npared to the aforementioned two experiments, the AAR drops\nto 28.9%, indicating that technical indicators and TRMI data\ncan not be fused directly although both of them contain\ninformative features. We also observe similar results in the\nexperiments of CLVSA with SPY historical trading data and\nTRMI data, which is demonstrated in the next section.\n\nConv-LSTM \nwtih Self-attn\nConv-LSTM\nwtih Self-attn\nConv-LSTM\nwtih Self-attn\nConv-LSTM\nwtih Self-attn\nInter-attn\nInter-attn\nConcatenation\nHistorical Sentiment\nData\nHistorical Trading\nData\nFully Connected\nLayers\nDual-CLVSA\nEncoder\nDecoder\nEncoder\nDecoder\nFig. 2: The architecture of dual-CLVSA. We add another sequence-to-sequence framework to train historical sentiment data,\ncompared to the original CLVSA. We concatenate the outputs of the two sequence-to-sequence framework before projection\nlayers. We do not apply another set of Kullback-Leibler divergence (KLD) for the channel of sentiment data because of the\nimpulsive characteristic of sentiment data.\nFig. 3: The cumulative and monthly return of SPY by dual-CLVSA with historical SPY trading data and TRMI data.\nWhile the experiments show the informativeness of TRMI\ndata, the mediocre performance of LSTMs with either TRMI\ndata or the mixed data indicates that LSTMs may not be the\noptimal framework to take advantages of TRMI data.\nC. Experimental results of CLVSA\nThe baseline method of the second step aims to reproduce\nthe experimental results as described in [8] with SPY historical\ntrading data. In [8], CLVSA achieves the best performance\namong the \ufb01ve models for all the datasets of six futures.\nWe thus test the performance of CLVSA on SPY historical\ntrading data, named CLV SA1, as shown in Table I. CLV SA1\nachieves an AAR of 48.0% over the same time period,\noutperforming all the previous experiments with LSTMs. This\nresult veri\ufb01es the superior performances of CLVSA, and thus\nwe choose CLVSA to be the base model for the 3rd-step\nexperiments.",
    "chunk_index": 7,
    "start_char": 19339,
    "end_char": 22578,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "historical\ntrading data. In [8], CLVSA achieves the best performance\namong the \ufb01ve models for all the datasets of six futures.\nWe thus test the performance of CLVSA on SPY historical\ntrading data, named CLV SA1, as shown in Table I. CLV SA1\nachieves an AAR of 48.0% over the same time period,\noutperforming all the previous experiments with LSTMs. This\nresult veri\ufb01es the superior performances of CLVSA, and thus\nwe choose CLVSA to be the base model for the 3rd-step\nexperiments.\nWe also investigate the performance of CLVSA with a\ndirect fusion of TRMI data and historical SPY trading data,\nnamed CLV SA2. We treat TRMI data as alternative \u201ctech-\nnical indicators\u201d, in other words, TRMI data is fed into the\nconvolutional kernels of CLVSA along with historical trading\ndata. Similar to LSTM 4\ns , CLV SA2 underperforms CLV SA2\nfor AAR by 18.5%, which con\ufb01rms again that it does not work\nto fuse historical trading data and TRMI data directly at the\ninput.\nTo sum up, the \ufb01rst-step experiments verify that TRMI data\nis able to provide informative features for price movement\nprediction, while they also indicate that we can not simply\n\ncombine TRMI data and historical trading data; the second-\nstep experiments yield similar results to the ones in our\nprevious research, demonstrating again that CLVSA outper-\nforms the singular models such as LSTMs. Meanwhile, the\nresults also show that we need a better fusion strategy to take\nadvantage of TRMI data.\nD. Experimental results of dual-CLVSA\nFigure 3 shows the experimental results of dual-CLVSA\nwith the SPY TRMI data and historical trading data. The cu-\nmulative return of SPY remains positive for all the months, and\neventually achieves 380%. The monthly return stay positive for\n57 out of 68 months, and not a single month suffers a negative\nreturn below -5%. Daul-CLVSA also exceeds the baseline\nmethods. Compared to LSTMs, dual-CLVSA surpasses them\nfor mean average precision (MAP), average annual return\n(AAR), Sharpe ratio (SR), Daily Jensen Alpha (DJA), and\nYearly Jensen Alpha (YJA) by up to 7.3%, 77.2%, 4.46,\n0.41%, and 86.3%, respectively. Compared to CLVSA, dual-\nCLVSA outperforms it for MAP, AAR, SR, DJA, and YJA\nby 1.9%, 24.1%, 1.30, 0.15%, and 7.1%, respectively. Our\nexperimental results verify our hypothesis that with appropri-\nate approach to fusing TRMI data and historical trading data,\nTRMI data provides extra informative features and thus boost\nthe performance of the predictions and \ufb01nancial returns.\nWe explore more about how TRMI data works in dual-\nCLVSA with the following two cases.\nV. CASE ANALYSIS\nA. Informativeness of TRMI data in bull and bear markets\nPeople usually become more emotional when \ufb01nancial\nmarkets enter into a bull or bear market. We thus look into\ntwo particular time periods, one in a bull market (from May\n2003 to March 2004) and the other in a bear market (from\nMarch 2002 to July 2002), to investigate the effectiveness of\nTRMI data for predicting the market movement.",
    "chunk_index": 8,
    "start_char": 22099,
    "end_char": 25076,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "following two cases.\nV. CASE ANALYSIS\nA. Informativeness of TRMI data in bull and bear markets\nPeople usually become more emotional when \ufb01nancial\nmarkets enter into a bull or bear market. We thus look into\ntwo particular time periods, one in a bull market (from May\n2003 to March 2004) and the other in a bear market (from\nMarch 2002 to July 2002), to investigate the effectiveness of\nTRMI data for predicting the market movement.\nOur experimental results show that TRMIs are informative\nfor \ufb01nancial markets prediction. Compared to CLV SA1 with\nonly SPY historical trading data in the bull market, dual-\nCLVSA captures 104 more trading opportunities, yields 29%\nhigher Pro\ufb01table to Unpro\ufb01table Trades ratio, and achieves\nhigher monthly return, Sharpe ratio, and daily Jensen alpha\nby 46.0%, 2.84, and 0.19%, respectively, as shown in Table\nII. In the bear market, dual-CLVSA captures 52 more trad-\ning opportunities yields 6% higher Pro\ufb01table to Unpro\ufb01table\nTrades, and achieves higher monthly return, Sharpe ratio, and\ndaily Jensen alpha by 12.5%, 1.62,and 0.11%, as show in Table\nIII.\nB. Frequencies of TRMI Data\nWe also evaluate the performance of dual-CLVSA with\ncrude oil futures (CL) TRMI data and historical trading\ndata, as shown in Table IV. Although it achieves average\nannual return (AAR) of 81.2% , dual-CLVSA underperforms\nCLV SA1 for mean average precision (MAP), AAR, Sharpe\nTC\nWT/LT\nMR\nSR\nDJA\nCLV SA1\n277\n154/123\n29.3%\n1.58\n0.01%\ndual-CLVSA\n381\n381\n381\n231/150\n231/150\n231/150\n75.3%\n75.3%\n75.3%\n4.42\n4.42\n4.42\n0.20%\n0.20%\n0.20%\nTABLE II: Case analysis A: comparison between CLV SA1\nand dual-CLVSA over the bull market period (from May\n2003 to March 2004). TC: trade count, WT/LT: pro\ufb01table\ntrades/unpro\ufb01table trades, MR: monthly return, SR: Sharpe\nratio, DJA: daily Jensen alpha. The extra features from TRMI\ndata makes dual-CLVSA capture 104 more trading opportuni-\nties and yield 29% higher ratio of Pro\ufb01table to Unpro\ufb01table\nTrades, outperforming CLV SA1 for MR, SR, and DJA by\n46.0%, 2.84, and 0.19%, respectively.\nTC\nWT/LT\nMR\nSR\nDJA\nCLV SA1\n149\n83/66\n107.3%\n4.78\n0.89%\ndual-CLVSA\n201\n201\n201\n114/87\n114/87\n114/87\n129.8%\n129.8%\n129.8%\n6.40\n6.40\n6.40\n1.00%\n1.00%\n1.00%\nTABLE III: Case analysis A: comparison between CLV SA1\nand dual-CLVSA over the bear market period (from March\n2002 to July 2002). TC: trade count, WT/LT: pro\ufb01table\ntrades/unpro\ufb01table trades, MR: monthly return, SR: Sharpe\nratio, DJA: daily Jensen alpha. The extra features from TRMI\ndata makes dual-CLVSA capture 52 more trading opportunities\nand 6% higher ratio of Pro\ufb01table to Unpro\ufb01table Trades, and\noutperforms CLV SA1 for MR, SR, and DJA by 12.5%, 1.62,\nand 0.11%, respectively.\nratio (SR), daily Jensen alpha, and yearly Jensen Alpha by\n2.5%, 31.8%, 0.98, 0.12%, and 28.5% respectively.\nTo understand why the distinctly different results come out\nbetween SPY and CL, we look deeper into TRMI data of\nSPY and CL. Since Social buzz is the weight measurement of\nTRMI data, it re\ufb02ects how active social media are at different\nmoments.",
    "chunk_index": 9,
    "start_char": 24646,
    "end_char": 27670,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "outperforms CLV SA1 for MR, SR, and DJA by 12.5%, 1.62,\nand 0.11%, respectively.\nratio (SR), daily Jensen alpha, and yearly Jensen Alpha by\n2.5%, 31.8%, 0.98, 0.12%, and 28.5% respectively.\nTo understand why the distinctly different results come out\nbetween SPY and CL, we look deeper into TRMI data of\nSPY and CL. Since Social buzz is the weight measurement of\nTRMI data, it re\ufb02ects how active social media are at different\nmoments. We plot hourly Social buzz boxplots of CL over\neight years and SPY over seven years in Figure 4. While\nCL\u2019s and SPY\u2019s Social buzz share a similar characteristic that\nthe values in the morning are at a daily low, the Social buzz\ndistribution of CL has distinct differences from that of SPY: i)\nThe values of CL Social buzz range from 0 to 8, much lower\nthan 0-300 for SPY. ii) The median of CL Social buzz in all\nhours are extremely close to zero, which indicates that almost\nhalf of minutely datapoints of CL TRMI data are empty.\nWe also investigate the calendar-month Social buzz dis-\ntributions of CL and SPY (Figure 5). We observe that the\ncalendar-month Social buzz distribution of CL display an\nimmerse variability over time. In some months, such as July\n2012, social media were completely quiet; while in December\n2014 and December 2015, the two months when crude oil\nprices plummeted, Social buzz has a high third quartile and\nmaximum value, even though the \ufb01rst quartile and median\nvalues are still very low. From the above analysis, we can\nsee that CL TRMI data is extremely sparse and volatile.\nCompared to SP500, crude oil futures are much less popular\namong individual investors, and people discuss about crude oil\nin social media more sporadically triggered by major events\nrather than regularly for SP500 which receives a broader\n\nMAP\nAAR\nSR\nDJA\nYJA\nCLV SA1\n49.7%\n49.7%\n49.7%\n113.0%\n113.0%\n113.0%\n3.99\n3.99\n3.99\n0.55%\n0.55%\n0.55%\n107.4%\n107.4%\n107.4%\ndual-CLVSA\n47.2%\n81.2%\n3.01\n0.43%\n78.9%\nTABLE IV: Case analysis B: comparison between CLV SA1\nand dual-CLVSA on CL datasets. The overly sparse TRMI data\nmakes dual-CLVSA underperforms CLV SA1 for MAP, AAR,\nSR, DJA, YJA by 2.5%, 31.8%, 0.98, 0.12%, and 28.5%,\nrespectively.\n(a) Hourly CL Social buzz boxplots.\n(b) Hourly SPY Social buzz boxplots.\nFig. 4: the Comparisons of Hourly Social buzz between CL\nand SPY.\ninterest. The above facts are probably the main reason why\nthe characteristic of CL TRMI data is signi\ufb01cantly different\nfrom SPY\u2019s. The sparsity and volatility of CL TRMI data\ninevitably result in the poor performance of dual-CLVSA.\nSpeci\ufb01cally, The overly sparse CL TRMI data makes the\nsecond channel of sequence-of-sequence framework not able\nto provide informative features. In other words, the outputs\nof the second channel may be zero matrices for most of the\ntime, which pollutes the outputs of the \ufb01rst channel after\nconcatenations and thus drag down the overall performance\nof dual-CLVSA.\nVI.",
    "chunk_index": 10,
    "start_char": 27237,
    "end_char": 30148,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "and volatility of CL TRMI data\ninevitably result in the poor performance of dual-CLVSA.\nSpeci\ufb01cally, The overly sparse CL TRMI data makes the\nsecond channel of sequence-of-sequence framework not able\nto provide informative features. In other words, the outputs\nof the second channel may be zero matrices for most of the\ntime, which pollutes the outputs of the \ufb01rst channel after\nconcatenations and thus drag down the overall performance\nof dual-CLVSA.\nVI. CONCLUSION\nIn this paper, we introduce TRMI data to investigate whether\nor not the sentiment data provides signals that can help predict\n\ufb01nancial market movements. Our main contribution is that,\nbased on the state-of-the-art deep learning approach, CLVSA,\nwe design a dual-channel method, named dual-CLVSA, to\nfuse TRMI data and historical trading data. Our experimental\nresults show that dual-CLVSA outperforms CLVSA by 9.3%\nfor average annual return and 0.91 for Sharpe ratio on SPDR\nS&P 500 ETF Trust. These results indicate that, sentiment data\ndoes not only provide informative features to our prediction\nsystems, but also contains extra informative features which\nprices and volume do not contain.\nACKNOWLEDGMENT\nThe authors wish to thank Richard Peterson, Managing\nDirector at MarketPsych, for providing the authors with the\nproprietary Thomson Reuters MarketPsych Indices (TRMI)\ndata.\nREFERENCES\n[1] K.-j. Kim, \u201cFinancial time series forecasting using support vector\nmachines,\u201d Neurocomputing, vol. 55, no. 1, pp. 307\u2013319, 2003.\n[2] F. Fern\u00b4andez-Rodr\u00b4\u0131guez, S. Sosvilla-Rivero, and J. Andrada-Felix,\n\u201cTechnical analysis in foreign exchange markets: evidence from the\nems,\u201d Applied Financial Economics, vol. 13, no. 2, pp. 113\u2013122, 2003.\n[3] M. Dixon, D. Klabjan, and J. H. Bang, \u201cClassi\ufb01cation-based \ufb01nancial\nmarkets prediction using deep neural networks,\u201d Algorithmic Finance,\nno. Preprint, pp. 1\u201311, 2016.\n[4] R. M. Yerkes, J. D. Dodson, et al., \u201cThe relation of strength of stimulus\nto rapidity of habit-formation,\u201d Punishment: Issues and experiments,\npp. 27\u201341, 1908.\n[5] D. O. Hebb, \u201cDrives and the cns (conceptual nervous system).,\u201d Psy-\nchological review, vol. 62, no. 4, p. 243, 1955.\n[6] L. Schwabe and O. T. Wolf, \u201cStress prompts habit behavior in humans,\u201d\nJournal of Neuroscience, vol. 29, no. 22, pp. 7191\u20137198, 2009.\n[7] R. L. Peterson, Trading on sentiment: The power of minds over markets.\nJohn Wiley & Sons, 2016.\n[8] J. Wang, T. Sun, B. Liu, Y. Cao, and H. Zhu, \u201cClvsa: A convolutional\nlstm based variational sequence-to-sequence model with attention for\npredicting trends of \ufb01nancial markets,\u201d in Proceedings of the Twenty-\nEighth International Joint Conference on Arti\ufb01cial Intelligence, IJCAI-\n19, pp. 3705\u20133711, International Joint Conferences on Arti\ufb01cial Intelli-\ngence Organization, 7 2019.\n[9] M. D. Rechenthin, \u201cMachine-learning classi\ufb01cation techniques for the\nanalysis and prediction of high-frequency stock direction,\u201d 2014.\n[10] X. Ding, Y. Zhang, T. Liu, and J. Duan, \u201cDeep learning for event-driven\nstock prediction.,\u201d in Ijcai, pp. 2327\u20132333, 2015.\n[11] J. Wang, T. Sun, B. Liu, Y. Cao, and D. Wang, \u201cFinancial markets\nprediction with deep learning,\u201d in 2018 17th IEEE International Con-\nference on Machine Learning and Applications (ICMLA), pp. 97\u2013104,\nIEEE, 2018.\n[12] L. Zhang, C. Aggarwal, and G.-J.",
    "chunk_index": 11,
    "start_char": 29693,
    "end_char": 32994,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "D. Rechenthin, \u201cMachine-learning classi\ufb01cation techniques for the\nanalysis and prediction of high-frequency stock direction,\u201d 2014.\n[10] X. Ding, Y. Zhang, T. Liu, and J. Duan, \u201cDeep learning for event-driven\nstock prediction.,\u201d in Ijcai, pp. 2327\u20132333, 2015.\n[11] J. Wang, T. Sun, B. Liu, Y. Cao, and D. Wang, \u201cFinancial markets\nprediction with deep learning,\u201d in 2018 17th IEEE International Con-\nference on Machine Learning and Applications (ICMLA), pp. 97\u2013104,\nIEEE, 2018.\n[12] L. Zhang, C. Aggarwal, and G.-J. Qi, \u201cStock price prediction via\ndiscovering multi-frequency trading patterns,\u201d in Proceedings of the 23rd\nACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pp. 2141\u20132149, ACM, 2017.\n[13] V. Bacoyannis, V. Glukhov, T. Jin, J. Kochems, and D. R. Song,\n\u201cIdiosyncrasies and challenges of data driven learning in electronic\ntrading,\u201d arXiv preprint arXiv:1811.09549, 2018.\n[14] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-\nc. Woo, \u201cConvolutional lstm network: A machine learning approach for\nprecipitation nowcasting,\u201d in Advances in neural information processing\nsystems, pp. 802\u2013810, 2015.\n[15] G. Toderici, S. M. O\u2019Malley, S. J. Hwang, D. Vincent, D. Minnen,\nS. Baluja, M. Covell, and R. Sukthankar, \u201cVariable rate image compres-\nsion with recurrent neural networks,\u201d arXiv preprint arXiv:1511.06085,\n2015.\n[16] \u0141. Kaiser and I. Sutskever, \u201cNeural gpus learn algorithms,\u201d arXiv\npreprint arXiv:1511.08228, 2015.\n[17] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning\nwith neural networks,\u201d in Advances in neural information processing\nsystems, pp. 3104\u20133112, 2014.\n\n(a) Calendar-month CL Social buzz boxplots from 2010 to 2017.\n(b) Calendar-month SPY Social buzz boxplots from 2001 to 2006.\nFig. 5: the Comparisons of Monthly Social buzz between CL over eight years and SPY over seven years.\n[18] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by\njointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473,\n2014.\n[19] J. Cheng, L. Dong, and M. Lapata, \u201cLong short-term memory-networks\nfor machine reading,\u201d arXiv preprint arXiv:1601.06733, 2016.\n[20] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv\npreprint arXiv:1312.6114, 2013.\n[21] D. J. Rezende, S. Mohamed, and D. Wierstra, \u201cStochastic backprop-\nagation and approximate inference in deep generative models,\u201d arXiv\npreprint arXiv:1401.4082, 2014.\n[22] J. Bayer and C. Osendorfer, \u201cLearning stochastic recurrent networks,\u201d\narXiv preprint arXiv:1411.7610, 2014.\n[23] A. G. A. P. Goyal, A. Sordoni, M.-A. C\u02c6ot\u00b4e, N. R. Ke, and Y. Bengio, \u201cZ-\nforcing: Training stochastic recurrent networks,\u201d in Advances in neural\ninformation processing systems, pp. 6713\u20136723, 2017.\n[24] T. Sun, J. Wang, P. Zhang, Y. Cao, B. Liu, and D. Wang, \u201cPredicting\nstock price returns using microblog sentiment for chinese stock market,\u201d\nin 2017 3rd International Conference on Big Data Computing and\nCommunications (BIGCOM), pp. 87\u201396, IEEE, 2017.\n[25] Y. Xu and S. B. Cohen, \u201cStock movement prediction from tweets and\nhistorical prices,\u201d in Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\nvol. 1, pp. 1970\u20131979, 2018.",
    "chunk_index": 12,
    "start_char": 32480,
    "end_char": 35722,
    "paper_title": "Dual-CLVSA a Novel Deep Learning Approach to Predi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Dual-CLVSA_a_Novel_Deep_Learning_Approach_to_Predi.pdf"
  },
  {
    "text": "arXiv:0903.1531v1 [q-fin.ST] 9 Mar 2009\nInference on multivariate ARCH processes\nwith large sizes\nGilles Zumbach\nRiskMetrics Group\nAv. des Morgines 12\n1213 Petit-Lancy\nGeneva, Switzerland\ngilles.zumbach@riskmetrics.com\nkeywords: Multivariate GARCH processes, covariance matrix, white noise residuals\nOctober 28, 2018\nAbstract\nThe covariance matrix is formulated in the framework of a linear multivariate ARCH\nprocess with long memory, where the natural cross product structure of the covariance is\ngeneralized by adding two linear terms with their respective parameter. The residuals of\nthe linear ARCH process are computed using historical data and the (inverse square root\nof the) covariance matrix. Simple measure of qualities assessing the independence and unit\nmagnitude of the residual distributions are proposed. The salient properties of the computed\nresiduals are studied for three data sets of size 54, 55 and 330. Both new terms introduced\nin the covariance help in producing uncorrelated residuals, but the residual magnitudes are\nvery di\ufb00erent from unity. The large sizes of the inferred residuals are due to the limited\ninformation that can be extracted from the empirical data when the number of time series\nis large, and denotes a fundamental limitation to the inference that can be achieved.\n1\n\n1\nIntroduction\nThe construction of multivariate models for \ufb01nancial time series is an important but di\ufb03cult\ntopic. Aiming at practical applications on todays portfolios, the number of time series N should\nbe large, from hundred(s) to thousand(s). In this context, a parsimonious approach is crucial,\nand the estimation of the model parameters should be very e\ufb03cient. On the other hand, a\nrealistic model should capture the heteroskedasticity and fat tails observed in the \ufb01nancial\ntime series. These requirements impose a minimum complexity on the model, and essentially a\nGARCH type structure in order to capture the volatility clustering. A time dependent volatility\nis the key quantity for the heteroskedasticity, and the covariance matrix is the main object that\nneeds to be studied in a multivariate context.\nFor multivariate GARCH, essentially two directions have been explored so far. First, the exten-\nsions of the univariate I-GARCH process lead to a simple exponential weighted moving average\n(EWMA) scheme, with one parameter that characterizes the decay of the exponential memory.\nThis parameter can be set a priory using a reasonable univariate estimation. This approach\nis used extensively in risk evaluation, following the original RiskMetrics methodology. In the\nsame line, the covariance matrix can be a simple (equal weights) product of the time series\nin a given window, typically of one or a few years. Such an approach is used extensively in\nportfolio allocations. The second direction that is found in the literature follows a multivari-\nate GARCH structure, with a greater \ufb02exibility in the structure of the equations, but with a\nnumber of parameters that grows with N, typically as N 2 or N 4 (see e.g. a recent review in\n[Silvennoinen and Ter\u00a8asvirta, 2008] and the references therein, or some of the original papers\n[Bollerslev et al., 1988, Engle and Kroner, 1995]).",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3205,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "used extensively in\nportfolio allocations. The second direction that is found in the literature follows a multivari-\nate GARCH structure, with a greater \ufb02exibility in the structure of the equations, but with a\nnumber of parameters that grows with N, typically as N 2 or N 4 (see e.g. a recent review in\n[Silvennoinen and Ter\u00a8asvirta, 2008] and the references therein, or some of the original papers\n[Bollerslev et al., 1988, Engle and Kroner, 1995]). Even though these models could be better\nfor capturing the structure of the empirical data, their complexity is way too large for practical\napplications with N large.\nProgress have also be made recently with respect to univariate processes, in particular in \ufb01nding\nprocesses that achieve a balance between accuracy, simplicity and robustness. The concepts\nneeded to reach these goals are the long memory for the volatility and the linear structure for\nthe covariance [Zumbach, 2004]. An important stylized fact is that the lagged correlation for the\nvolatility decays slowly (as a logarithm of the lag) for all \ufb01nancial time series[Zumbach, 2006],\nbut certainly not exponentially fast. This is the long memory, observed empirically as volatility\nclusters at many time scales. Processes with many time scales are needed to correctly model\nthis long memory for the volatility. The second concept is related to the linear versus a\ufb03ne\nstructure of the variance equations. If an a\ufb03ne term is included in the equations, it \ufb01xes the\nmean volatility, but this introduces two more parameters. The key observation is that for a\nvolatility forecast, a linear structure is su\ufb03cient. Moreover, the long memory introduces a non\ntrivial term structure in the volatility forecast (similar to the mean reversion of GARCH(1,1)).\nThis simpli\ufb01cation of the process allows us to dispose of the mean volatility parameter, a quantity\nthat is clearly series dependent. This approach has been used to build a better risk methodology\nin [Zumbach, 2006].\nThis paper explores the multivariate extensions of the (univariate) linear long memory GARCH\nprocess developed in [Zumbach, 2004]. It can be seen as a better version of the EWMA applied\nto multivariate time series.\nThe model is very parsimonious as it introduces only two new\nparameters, essentially related to the shrinkage of the correlation and to the regularization of\nthe smallest eigenvalues in the covariance matrix. In a process setup, the covariance is used to\ntransform the realized returns into the innovations (or residuals), which should be uncorrelated\n2\n\nwhite noises by hypothesis. Measures of quality are introduced that quantify how good is the\n(inverse square root) covariance at producing white noise innovations.\nIn [Zumbach, 2008], the time dependent covariance and correlation matrices are analyzed in\ndetail.\nThe extensive empirical investigations on three data sets allow to better understand\nthe speci\ufb01city of the multivariate problem, in particular the role of the largest eigenvalues and\neigenvectors, as well as the small and possibly null eigenvalues. This last point is particularly\ncritical for model inferences as the inverse square root of the covariance is needed.",
    "chunk_index": 1,
    "start_char": 2755,
    "end_char": 5933,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "(inverse square root) covariance at producing white noise innovations.\nIn [Zumbach, 2008], the time dependent covariance and correlation matrices are analyzed in\ndetail.\nThe extensive empirical investigations on three data sets allow to better understand\nthe speci\ufb01city of the multivariate problem, in particular the role of the largest eigenvalues and\neigenvectors, as well as the small and possibly null eigenvalues. This last point is particularly\ncritical for model inferences as the inverse square root of the covariance is needed. For our\npurpose, the key property of the covariance is that the eigenvalues of the covariance matrix\ndecrease exponentially fast toward zero. The accumulation of very small eigenvalues is creating\nproblems when computing the inverse volatility, even when the covariance is mathematicaly\nwell behaved.\nA very similar problem appears in portfolio optimization in a mean-variance\nscheme, as the inverse of the covariance determines the optimal allocations. In both cases, a\nproper regularization should be introduced; this issue is investigated thoroughly in the empirical\nsection.\nThe structure of this paper is the following. The next section introduces the relevant theoretical\nmaterial, and in particular the multivariate extensions of the linear long memory process. The\nsample empirical correlations for the returns is presented in Section 4, and contrasted in Sec-\ntion 5 with the sample correlation for the residuals. Section 6 presents measures related to the\nwhitening of the innovations. The main kernels used to evaluate the covariance matrix (equal\nweights, exponential, long memory, long memory + regularization) are compared in Section 7.\nFinally, Section 8 compares the cut-o\ufb00that are commonly used in portfolio optimization versus\nthe regularization introduced in this paper, before the conclusions.\n2\nGeneral framework\nThe largest quantitative correlations across assets are between the returns \u03c1(r, r), the volatilities\n\u03c1(r2, r2) and the lagged volatilities \u03c1(L[r2], r2) (see Section 7). Consequently, we need to select\na time series process that can capture these e\ufb00ects.\nThe model is built upon the (linear)\nunivariate long memory ARCH process developed in [Zumbach, 2004].\nThanks to its good\nvolatility forecast and analytical tractability, this process has been used to develop a better risk\nmethodology in [Zumbach, 2006]. In particular, the process is very parsimonious as it contains\nonly three parameters, with one parameter that characterizes the decay of the volatility lagged\ncorrelation, and two parameters that are simple cut-o\ufb00. An extensive (univariate) empirical\nanalysis in [Zumbach, 2006] shows that the same parameter values can be used for all \ufb01nancial\ntime series. If we accept these parameter values, we are left with a univariate process that does\nnot contain any free parameters.\nAs the process is quadratic (for the return), the extension to a multivariate setting is straight-\nforward. The \ufb01nancial universe is made of N time series, indexed by \u03b1 and \u03b2. The price p,\nmapped price x, return r are column vectors with N components, while the covariance matrix\n\u03a3e\ufb00has a size N \u00d7 N. We assume that the daily data generating process (DGP), with a daily\ntime increment \u03b4t, is given by\nx(t + \u03b4t)\n=\nx(t) + r(t + \u03b4t)\n(1)\nr(t + \u03b4t)\n=\n\u03a3e\ufb00(t)1/2 \u03b5(t + \u03b4t).\n(2)\n3",
    "chunk_index": 2,
    "start_char": 5397,
    "end_char": 8726,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "that does\nnot contain any free parameters.\nAs the process is quadratic (for the return), the extension to a multivariate setting is straight-\nforward. The \ufb01nancial universe is made of N time series, indexed by \u03b1 and \u03b2. The price p,\nmapped price x, return r are column vectors with N components, while the covariance matrix\n\u03a3e\ufb00has a size N \u00d7 N. We assume that the daily data generating process (DGP), with a daily\ntime increment \u03b4t, is given by\nx(t + \u03b4t)\n=\nx(t) + r(t + \u03b4t)\n(1)\nr(t + \u03b4t)\n=\n\u03a3e\ufb00(t)1/2 \u03b5(t + \u03b4t).\n(2)\n3\n\nThe mapped price x is x(t) = ln(p(t)) for stock, stock indexes, FX and commodities. For interest\nrates, x corresponds to the rate at a \ufb01xed time to maturity1. The return vector r is the return\nfor the time horizon \u03b4t = 1 day. The covariance matrix \u03a3e\ufb00is the e\ufb00ective variance/covariance\nover the next time period \u03b4t. The covariance should capture the heteroskedasticity of \ufb01nancial\nreturns observed empirically, as well as the correlations across time series.\nThe residual \u03b5\n(or innovation) is a white noise with a distribution p(\u03b5\u03b1). The usual hypothesis is that the\ninnovations are independent and with unit variance\nE\n\u0002\n\u03b5\u03b1(t) \u03b5\u03b2(t\u2032)\n\u0003\n= \u03b4\u03b1,\u03b2 \u03b4t, t\u2032.\n(3)\nThe residual distribution p(\u03b5\u03b1) is left unspeci\ufb01ed for (most of) this study; if needed, a Student\ndistribution with 5 degrees of freedom gives a good description of the (univariate) empirical\ndistribution (again, for all \ufb01nancial time series), as shown in [Zumbach, 2006].\nIn principle, a drift \u00b5 = E [ r(t + \u03b4t) | \u2126(t)] can be included in (2). As the empirical analysis\nshows, this term is small in magnitude, and is particularly di\ufb03cult to estimate reliably. Moreover,\nits magnitude for a horizon of one day is small compared to the covariance. Therefore, we chose\nto neglect it for this study and to concentrate on the covariance matrix.\nIn general, we consider the class of (linear) processes where the covariance matrix \u03a3e\ufb00is given\nby a bilinear function of the past returns. The most straightforward extension of the univariate\nprocess is given by a \u201ccross product\u201d of the return vector\n\u03a3e\ufb00(t) =\nimax\nX\ni=0\n\u03bb(i) r(t \u2212i \u03b4t) r\u2032(t \u2212i \u03b4t)\n(4)\nwith r a column vector and r\u2032 its transpose. The weight for the past returns \u03bb(i) obeys the sum\nrule P\ni \u03bb(i) = 1. Common choices for the weights are equal weights (i.e. a rectangular window\nwith equal weights), exponential weights (i.e. equivalent to an exponential moving average),\nand long memory weights.\nFor the long memory process, the weights decay logarithmically\nslowly, with \u03bb(i) \u22431\u2212log(i\u03b4t)/ log(\u03c40). The recursion equations used to de\ufb01ne the \u03bb(i) and the\nparameter values are given in [Zumbach, 2006]. A process with long memory weights reproduces\nthe long memory observed in the empirical lagged correlation of the volatility [Zumbach, 2004],\nwith \u03c40 of the order of six years.",
    "chunk_index": 3,
    "start_char": 8211,
    "end_char": 11020,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "an exponential moving average),\nand long memory weights.\nFor the long memory process, the weights decay logarithmically\nslowly, with \u03bb(i) \u22431\u2212log(i\u03b4t)/ log(\u03c40). The recursion equations used to de\ufb01ne the \u03bb(i) and the\nparameter values are given in [Zumbach, 2006]. A process with long memory weights reproduces\nthe long memory observed in the empirical lagged correlation of the volatility [Zumbach, 2004],\nwith \u03c40 of the order of six years.\nFor a volatility forecast based on (4), the long memory\nweights deliver consistently a better forecast than the other typical choices (equal weights or\nexponential). For these reasons, the long memory weights are investigated in detail in this work.\nIn the class of linear equations between covariance and return squares, other extensions of the\nunivariate process can be written.\nAn interesting \ufb01rst extension is similar to the shrinkage\nof the covariance matrix [Ledoit and Wolf, 2004a, Ledoit and Wolf, 2004b], but applied on the\ncorrelation. A simple shrinkage of the correlation matrix, with the shrinkage parameter \u03b3, is\n\u03c1(\u03b3) = (1 \u2212\u03b3)\u03c1 + \u03b3 IN.\n(5)\nwhere \u03c1 is the correlation matrix corresponding to \u03a3e\ufb00\n\u03c1\u03b1,\u03b2 =\n\u03a3e\ufb00,\u03b1,\u03b2\np\u03a3e\ufb00,\u03b1,\u03b1\u03a3e\ufb00,\u03b2,\u03b2\n.\n(6)\n1More precisely, for the interest rate R, the mapped price is x = log(1 + R/R0) with R0 = 4%. This mapping\ndecouples the volatility from R, see [Zumbach, 2006]. This mapping introduces a small correction on the returns\nfor interest rates.\n4\n\nThe rationale for using a shrinkage is to allow more \ufb02uctuations for the return across assets than\nwhat the historical correlation structure would impose. The natural prior for the correlation\nis the identity, corresponding to the condition imposed on the residuals E [ \u03b5\u03b1\u03b5\u03b2 ] = \u03b4\u03b1,\u03b2. The\ncorresponding equation for the shrinkage of the covariance matrix is\n\u03a3e\ufb00(\u03b3) = (1 \u2212\u03b3)\u03a3e\ufb00+ \u03b3 \u03a3e\ufb00|diag\n(7)\nwhere \u03a3e\ufb00|diag is the diagonal part of \u03a3e\ufb00. Essentially, this equation shrinks only the o\ufb00-diagonal\npart by 1 \u2212\u03b3, whereas the diagonal part is given by the volatility of the respective time series.\nA second interesting extension consists in shrinking the spectrum of the covariance toward a\nmultiple of the identity matrix\n\u03a3e\ufb00(\u03b3, \u03be) = (1 \u2212\u03be)\u03a3e\ufb00(\u03b3) + \u03be\n\n\u03c32\u000b\nIN.\n(8)\nand with the mean variance across all assets de\ufb01ned by\n\n\u03c32\u000b\n= 1\nN tr (\u03a3e\ufb00) .\n(9)\nThe covariance has been de\ufb01ned so as to preserve the mean variance across assets\ntr (\u03a3e\ufb00(\u03b3, \u03be)) =\n\n\u03c32\u000b\n(10)\nfor all values of \u03b3 and \u03be. It is easy to check that, if e\u03b1 is an eigenvalue of \u03a3e\ufb00(\u03b3), the corresponding\neigenvalue of \u03a3e\ufb00(\u03b3, \u03be) is (1 \u2212\u03be)e\u03b1 + \u03be\n\n\u03c32\u000b\n. In particular, the addition of the identity matrix\nchanges the spectrum of the covariance by setting the minimal eigenvalue at \u03be\n\n\u03c32\u000b\n.\nThis\nmodi\ufb01cation is very similar to the regularization of the (inverse) covariance given below in (16),\nand therefore we call \u03be the regularization parameter as it allows to compute a well de\ufb01ned\ninverse for \u03a3e\ufb00. The intuition for this term is to have the same number of (signi\ufb01cant) sources of\nrandomness as of time series.",
    "chunk_index": 4,
    "start_char": 10582,
    "end_char": 13567,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "of \u03a3e\ufb00(\u03b3, \u03be) is (1 \u2212\u03be)e\u03b1 + \u03be\n\n\u03c32\u000b\n. In particular, the addition of the identity matrix\nchanges the spectrum of the covariance by setting the minimal eigenvalue at \u03be\n\n\u03c32\u000b\n.\nThis\nmodi\ufb01cation is very similar to the regularization of the (inverse) covariance given below in (16),\nand therefore we call \u03be the regularization parameter as it allows to compute a well de\ufb01ned\ninverse for \u03a3e\ufb00. The intuition for this term is to have the same number of (signi\ufb01cant) sources of\nrandomness as of time series. In contrast, zero eigenvalues project out the corresponding source\nof randomness, and very small eigenvalues act similarly in practice. The addition of the identity\nmatrix introduces a minimal level of \ufb02uctuations corresponding to each source of randomness,\nthat would be missing otherwise. As the parameters for the long memory kernel are \ufb01xed (see\n[Zumbach, 2006]), the process de\ufb01ned with the covariance (8) has only the parameters \u03b3 and \u03be\nthat need be studied empirically. Moreover, for N = 1 the dependency on \u03b3 and \u03be disappears\nand the variance reduces to the univariate case.\nAs emphasized above, the covariances (4), (7) and (8) are linear in the return squares. It is easy\nto introduce an a\ufb03ne mean variance term in the process, for example with\n\u03a3e\ufb00, a\ufb03ne = w\u221e\u03a3\u221e+ (1 \u2212w\u221e)\u03a3e\ufb00(\u03b3, \u03be).\n(11)\nThe mean volatility, measured on an in\ufb01nite time scale, is \ufb01xed by the N \u00d7 N matrix \u03a3\u221e,\nwhereas w\u221eis the corresponding scalar coupling constant. Such a process has N(N + 1)/2 + 1\nparameters corresponding to the mean term and coupling, plus the possible parameters in the\nlinear covariance. To make contact with the more familiar univariate GARCH(1,1) process, the\nalgebraic structure of the linear covariance \u03a3e\ufb00(\u03b3, \u03be) corresponds to the I-GARCH(1) process,\nwhereas (11) corresponds to the GARCH(1,1) process with w\u221e\u03a3\u221ethe additive constant that\n\ufb01xes the mean volatility (often denoted \u03b10 or \u03c9 in the GARCH(1,1) literature). A detailed\ndiscussion of the linear and a\ufb03ne process equations is given in [Zumbach, 2004]. For our purpose,\nwe are interested in the inference about \u03b5 at the scale of one day, whereas the crossover time to\nthe asymptotic regime \ufb01xed by the mean volatility is of the order of several years (a multiple of\n5\n\nthe longest time scale included in the linear part). Therefore, it is perfectly justi\ufb01ed to consider\nonly the linear part in a study focused at one day, and therefore to reduce drastically the number\nof parameters in the model.\nIf a Gaussian random walk is the zero order model for the price behavior, the model above\ncan be viewed as the next approximation. Essentially, the process described by (2) with the\ncovariance (8) or (11) is the simplest process that captures the major features of multivariate\n\ufb01nancial time series, namely the basic random walk of the prices, the volatility clustering (or\nheteroskedasticity), the return correlations, and the fat tail character of the price changes and\nof the innovations (with a Student distribution for p(\u03b5\u03b1)).",
    "chunk_index": 5,
    "start_char": 13072,
    "end_char": 16055,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "is the zero order model for the price behavior, the model above\ncan be viewed as the next approximation. Essentially, the process described by (2) with the\ncovariance (8) or (11) is the simplest process that captures the major features of multivariate\n\ufb01nancial time series, namely the basic random walk of the prices, the volatility clustering (or\nheteroskedasticity), the return correlations, and the fat tail character of the price changes and\nof the innovations (with a Student distribution for p(\u03b5\u03b1)).\nMoreover, because of its simple\nmathematical structure, this basic model can be improved in many ways to capture \ufb01ner e\ufb00ects.\nIt is therefore also a good stepping stone for a more detailled description of the \ufb01nancial time\nseries.\nThe equation (2) is formulated as a process. Using historical data, we want to validate this\nmodel. Inference about this simple process requires to invert (2), namely\n\u03b5(t + \u03b4t) = \u03a3\u22121/2\ne\ufb00\n(t) r(t + \u03b4t).\n(12)\nThe residuals \u03b5 can then be used to compute a log-likelihood, for example. As the covariance\nmatrix is symmetric, the spectral decomposition is given by\n\u03a3e\ufb00=\nN\nX\n\u03b1=1\ne\u03b1 v\u03b1v\u2032\n\u03b1\n(13)\nwhere the eigenvalues e\u03b1 = e\u03b1(t) and eigenvectors v\u03b1 = v\u03b1(t) are time dependent, and the\neigenvectors v\u03b1 are orthogonal.\nFor convenience, the eigenvalues are ordered by decreasing\nvalues such that e\u03b2 < e\u03b1 for \u03b2 > \u03b1. Provided that the eigenvalues are strictly positive, the\ninverse square root covariance, or inverse volatility, is\n\u03a3\u22121/2\ne\ufb00\n=\nN\nX\n\u03b1=1\n1\n\u221ae\u03b1\nv\u03b1v\u2032\n\u03b1.\n(14)\nFor \u03a3e\ufb00(\u03b3 = 0, \u03be = 0) and for systems of practical interest, the covariance matrix is singular\nand some eigenvalues are null. This occurs always when the number of time series N is larger\nthan the memory length imax used to compute the covariance. In such case, the number of\nstrictly positive eigenvalues is given by Npos = min(N, imax). For many practical applications,\nthe memory length is of the order of one to two years (imax = 260 to imax = 520), whereas the\nnumber of time series can be of the order of thousand(s). However, even for systems that are non\nsingular according to the mathematical criterion, the eigenvalues of the covariance matrix decay\nexponentially toward zero, leading to very large values in the inverse volatility. Clearly, this\nwill impact the computed residuals. Therefore, except for systems of very small dimensions, the\ncomputation of the inverse volatility at \u03b3 = \u03be = 0 needs to be regularized, even when N < imax.\nWith a singular covariance, several schemes can be used to de\ufb01ne an inverse volatility with an\nappropriate cut-o\ufb00. A \ufb01rst possibility is to use only the leading eigenvalues, namely to invert\nthe covariance in the leading k subspace\n\u03a3\u22121/2\ne\ufb00\n=\nk\nX\n\u03b1=1\n1\n\u221ae\u03b1\nv\u03b1v\u2032\n\u03b1\n(15)\n6\n\nThe \u201ccut-o\ufb00parameter\u201d k is chosen so that ek > 0. We call this scheme \u201cprojected\u201d.",
    "chunk_index": 6,
    "start_char": 15550,
    "end_char": 18348,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "a singular covariance, several schemes can be used to de\ufb01ne an inverse volatility with an\nappropriate cut-o\ufb00. A \ufb01rst possibility is to use only the leading eigenvalues, namely to invert\nthe covariance in the leading k subspace\n\u03a3\u22121/2\ne\ufb00\n=\nk\nX\n\u03b1=1\n1\n\u221ae\u03b1\nv\u03b1v\u2032\n\u03b1\n(15)\n6\n\nThe \u201ccut-o\ufb00parameter\u201d k is chosen so that ek > 0. We call this scheme \u201cprojected\u201d. A second\npossibility is to complement the previous operator so that it has full rank\n\u03a3\u22121/2\ne\ufb00\n=\nk\nX\n\u03b1=1\n1\n\u221ae\u03b1\nv\u03b1v\u2032\n\u03b1 +\n1\n\u221aek+1\nN\nX\n\u03b1=k+1\nv\u03b1v\u2032\n\u03b1\n(16)\nand we call this scheme \u201cfull rank\u201d. A multiplicative constant can be inserted in front of both\nde\ufb01nitions so as to preserve the trace. In practice, the optimal rank k should be chosen large\nenough (see below), and this normalization constant is essentially irrelevant.\nThe singularity related to the inverse of a covariance matrix appears in many places in \ufb01nance.\nA common example is the computation of the most e\ufb03cient portfolio in a mean-variance scheme\nand the related de\ufb01nition of an e\ufb03cient frontier. Other examples are the computations of the\nfactor loadings in a factor model, the inference in a multivariate process as explained above,\nor the computation of conditional Gaussian distribution in a multivariate setting. Depending\non the application, the choice of the cut-o\ufb00can di\ufb00er, with the \u201cprojected\u201d de\ufb01nition being\npossibly the most widely used. As the empirical analysis below shows, the \u201cfull rank\u201d scheme is\nbetter. Yet, the regularization through a modi\ufb01cation of the covariance by using \u03a3e\ufb00(\u03b3, \u03be) turns\nout to be the most e\ufb03cient method.\nFinally, we want to investigate how e\ufb00ective is \u03a3e\ufb00(\u03b3, \u03be) at whitening the residuals. To this\npurpose, we compare the sample correlation for the returns and for the residuals. The empirical\naverage of y is denoted \u27e8y\u27e9, computed over the sample from 1.1.2000 to 1.1.2008. The covariance\nmatrix between the vectors y and z is denoted \u03a3(y, z). The correlation matrix between the\nvectors y and z is denoted \u03c1(y, z), and evaluated with the usual Pearson formula. The lag one\noperator is L[y] with value L[y](t) = y(t\u22121). Up to lag one, all the N \u00d7N correlation matrices\nare:\n\u2022 \u03c1(r, r): The contemporaneous return correlation.\n\u2022 \u03c1(r2, r2): The contemporaneous volatility correlation.\n\u2022 \u03c1(r, r2): The contemporaneous return/volatility correlation.\n\u2022 \u03c1(L[r], r): The lagged return correlation.\n\u2022 \u03c1(L[r2], r2):\nThe lagged volatility correlation.\nThis correlation characterizes the het-\neroskedasticity, but here also across time series.\n\u2022 \u03c1(L[r], r2): The in\ufb02uence of the return moves on the subsequent volatility. For stocks, this\nis commonly called the \u201cleverage e\ufb00ect\u201d.\n\u2022 \u03c1(L[r2], r): The in\ufb02uence of the volatility on the subsequent returns.\nThe correlations matrices for the residuals \u03b5 are computed similarly, but also depend on the\nregularization used to compute \u03a3\u22121/2\ne\ufb00\n.\nFor the returns, all these correlations are very interesting to study as they summarize the infor-\nmation about the market structures and its dynamics.",
    "chunk_index": 7,
    "start_char": 17999,
    "end_char": 20980,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "the return moves on the subsequent volatility. For stocks, this\nis commonly called the \u201cleverage e\ufb00ect\u201d.\n\u2022 \u03c1(L[r2], r): The in\ufb02uence of the volatility on the subsequent returns.\nThe correlations matrices for the residuals \u03b5 are computed similarly, but also depend on the\nregularization used to compute \u03a3\u22121/2\ne\ufb00\n.\nFor the returns, all these correlations are very interesting to study as they summarize the infor-\nmation about the market structures and its dynamics. For the residuals, if they are e\ufb00ectively\niid, the correlation matrices should be either the identity matrix (for \u03c1(r, r) and \u03c1(r2, r2)) or\nzero (for all the other correlations). Simple overall measures of these relationships are given by\nq2 =\n1\nN(N \u22121)\nX\n\u03b1\u0338=\u03b2\n\u03c12\n\u03b1,\u03b2\n(17)\n7\n\nfor \u03c1(r, r) and \u03c1(r2, r2), and\nq2 =\n1\nN 2\nX\n\u03b1,\u03b2\n\u03c12\n\u03b1,\u03b2\n(18)\nfor the other correlation matrices. Essentially, these whitening qualities q(y, z) = q(\u03c1(y, z))\nmeasure the independence of the pairs of residuals. The unit variance of the residuals E\n\u0002\n\u03b52\n\u03b1\n\u0003\n=\n1 should still be tested, and a simple measure is given by\nq2(\u03b52) = 1\nN\nX\n\u03b1\n\u0000\n\u03b52\n\u03b1\n\u000b\n\u22121\n\u00012 .\n(19)\nEmpirically, the variances E\n\u0002\n\u03b52\n\u03b1\n\u0003\nhave a similar behavior for all \u03b1, and an informative quantity\nis the mean residual variance\n1\nN\nX\n\u03b1\n\n\u03b52\n\u03b1\n\u000b\n.\n(20)\n3\nThe data sets\nThe three data sets we are using have been presented in [Zumbach, 2008]. Essentially, they are\nthe ICM data set (International Capital Market) that covers majors asset classes and world\ngeographical areas with N = 340, the G10 data set covers the largest economies (European,\nJapan, and USA) with N = 55, and the USA data set focuses on the American economy\nwith N = 54. All time series contain daily prices from January 1, 1999 to the January 1, 2008\n(9 years), corresponding to a length of 2515 days. The in-sample investigations are done from\n1.1.2000 to 1.1.2008, and one year (imax = 260 business day) is used to evaluate the e\ufb00ective\ncovariance \u03a3e\ufb00.\n4\nThe sample correlations for the returns\nFrom the seven correlations between return, volatility and their lag one time series (see the end\nof Section 2), the largest correlation is \u03c1(r, r), while the correlation between volatility \u03c1(r2, r2) is\nthe second largest. The overall return correlation is presented on Figure 1 for the ICM data set.\nEssentially, this \ufb01gure measures the dependencies in today \ufb01nancial world, and several features\nare worth noticing on a qualitative basis.\n1. By and large, there is one cluster of connected markets, and then disconnected areas\n(disconnected countries are in Asia, central America, most of South America, Central\nAfrica, the former Soviet Union). The cluster contains all developed economies from Asia,\nNorth America, Europe and South Africa.\n2. In the FX sector, there is a strongly correlated cluster around the Euro (red patch between\nindex 53 to 70), related to another cluster containing Canada, Australia, Singapore and\nJapan (index 33 to 38).",
    "chunk_index": 8,
    "start_char": 20516,
    "end_char": 23428,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "features\nare worth noticing on a qualitative basis.\n1. By and large, there is one cluster of connected markets, and then disconnected areas\n(disconnected countries are in Asia, central America, most of South America, Central\nAfrica, the former Soviet Union). The cluster contains all developed economies from Asia,\nNorth America, Europe and South Africa.\n2. In the FX sector, there is a strongly correlated cluster around the Euro (red patch between\nindex 53 to 70), related to another cluster containing Canada, Australia, Singapore and\nJapan (index 33 to 38). The rest of the Asiatic currencies are less correlated.\n3. In the stock index sector, the correlations are overall fairly large, with three clusters\ncorresponding respectively to Paci\ufb01c/Asia (index 98 to 111), North America (112 to 116)\nand Euroland (123 to 142).\n8\n\n \n \n50\n100\n150\n200\n250\n300\n50\n100\n150\n200\n250\n300\n\u2212100\n\u221280\n\u221260\n\u221240\n\u221220\n 0\n 20\n 40\n 60\n 80\n100\nFigure 1: The correlation \u03c1(r, r) for the returns for the ICM data set.\n4. The interest rates show a strong di\ufb00erence between short maturity at 1 day (index 150 to\n165) and 1 month (index 166 to 207) and long maturity at 1 year (index 208 to 245) and\n10 years (index 246 to 276). The short maturities are essentially disconnected from all\nother time series, and have only small correlations with 1 year interest rates, except for a\nlarger correlation with the same yield curve at 1 year, visible as an o\ufb00diagonal line.\n5. In the commodities sector, the metals (spot and future, index 1 to 15) behave very similarly\nto the FX, while the gas futures (index 16 to 19) have no correlations except with some\nAmerican stocks in the energy and petroleum business.\n6. The correlation between European currencies and European stock indexes is negative.\nThis indicates that when the dollar gets stronger (i.e. the FX rate USD/EUR decreases),\nthe stock indexes increase. This is probably due both to the prices of European stocks\nappearing cheaper to US investors, as well as American stocks appearing more expensive\nto European investors.\n7. The IR with long maturities show clear positive correlations with stock indexes. These\ncorrelations indicate moves in the same directions for both asset classes, and go somewhat\n9\n\nagainst the view of alternating cycle between bonds and equities. The time scales are\nhowever very di\ufb00erent, as the correlations between daily price changes are computed in\nthis paper, whereas the business cycles appear at a time scale of years and for the prices.\n8. The IR with long maturities show clear negative correlations with FX. This is probably\ndue to larger (smaller) US long term IR making the dollar stronger (weaker), coupled with\nthe strong positive correlations for long terms IR.\n5\nThe sample correlations for the residuals\n \n \n50\n100\n150\n200\n250\n300\n50\n100\n150\n200\n250\n300\n\u2212100\n\u221280\n\u221260\n\u221240\n\u221220\n 0\n 20\n 40\n 60\n 80\n100\nFigure 2: For the ICM data set, the correlation \u03c1(\u03b5, \u03b5) for the residuals at \u03b3 = \u03be = 0 and cut-o\ufb00\nparameter k = 91.",
    "chunk_index": 9,
    "start_char": 22867,
    "end_char": 25853,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "the prices.\n8. The IR with long maturities show clear negative correlations with FX. This is probably\ndue to larger (smaller) US long term IR making the dollar stronger (weaker), coupled with\nthe strong positive correlations for long terms IR.\n5\nThe sample correlations for the residuals\n \n \n50\n100\n150\n200\n250\n300\n50\n100\n150\n200\n250\n300\n\u2212100\n\u221280\n\u221260\n\u221240\n\u221220\n 0\n 20\n 40\n 60\n 80\n100\nFigure 2: For the ICM data set, the correlation \u03c1(\u03b5, \u03b5) for the residuals at \u03b3 = \u03be = 0 and cut-o\ufb00\nparameter k = 91.\nThe residuals \u03b5(t) are computed according to (12), with the e\ufb00ective volatility \u03a3e\ufb00(\u03b3, \u03be) given\nby (8). The inverse volatility is given by (14). When \u03be = 0, a cut-o\ufb00k should used in the inverse\nvolatility in order to avoid numerical over\ufb02ows, as in (15) or (16). The empirical covariances\nand correlations for the residuals are computed similarly as for the returns, on the same sample\n(1.1.2000 to 1.1.2008).\n10\n\nIn order to understand intuitively the statistical properties of the residuals, the correlation \u03c1(\u03b5, \u03b5)\nfor the residuals can be displayed, similarly to the Figure 1 for the returns. The interpretation\nis easier when taking \u03b3 = \u03be = 0, with a cut-o\ufb00parameter k < imax. The Figure 2 corresponds\nto k = 91, with an inverse volatility computed with (16). Despite the fairly large number of\neigenvalues included in the inverse volatility, structures are still clearly visible, remnant of the\ncorrelation for the returns. This \ufb01gure makes clear that computing the inverse volatility using\nonly the leading eigenspace for k small is leaving most of the return structures in the residuals.\nOn the other hand, noise in the background is also visible in the form of random speckles.\nThe same \ufb01gure plotted for increasing cut-o\ufb00k shows slowly disappearing structures while the\nbackground noise is increasing. This trade o\ufb00between washing out the structures while keeping\nthe noise small leads to an optimal choice for the cut-o\ufb00k, or for the regularization \u03be. The\nother important criterion for the statistical properties of the residuals is that they have a unit\nvariance. The in\ufb02uence of the regularization parameter on the residual variance is large, with\nan decreasing variance for an increasing regularization parameter \u03be. The problem is therefore\nto \ufb01nd a priori a regularization parameter that produces a unit variance for the residuals.\n6\nWhitening of the residuals\nThe overall measures q for the \u201cwhitening\u201d of the residuals are given by (17), (18) and (19).\nThey are plotted on Figures 3, 4 and 5 for the ICM, G10 and USA data sets. The horizontal\naxises give the regularization parameter \u03be and the black curves correspond to no shrinkage\n\u03b3 = 0. The curves with the color changing from black to blue correspond to the increasing\nshrinkage parameter \u03b3 = 0.0, 0.05, 0.1, 0.2, 0.4. The black horizontal line gives the value of the\ncorresponding quantity for the returns, while the pair of horizontal red lines corresponds to",
    "chunk_index": 10,
    "start_char": 25356,
    "end_char": 28287,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "plotted on Figures 3, 4 and 5 for the ICM, G10 and USA data sets. The horizontal\naxises give the regularization parameter \u03be and the black curves correspond to no shrinkage\n\u03b3 = 0. The curves with the color changing from black to blue correspond to the increasing\nshrinkage parameter \u03b3 = 0.0, 0.05, 0.1, 0.2, 0.4. The black horizontal line gives the value of the\ncorresponding quantity for the returns, while the pair of horizontal red lines corresponds to\nthe 5% and 95% quantiles of the distribution of the whitening quality for uncorrelated Student\ninnovations (see text below). Essentially, the empirical whitening qualities should lies between\nthese two extremes. On the horizontal axis for the regularization parameter \u03be, the point at 10\u22125\ncorresponds to \u03be = 0 (that would be otherwise at \u2212\u221eon a logarithmic axis). For the ICM data\nset, the inverse volatility is computed with a \ufb02oor on the spectrum of 10\u221212 in order to avoid\nnumerical over\ufb02ows.\nFor the ICM data set, the spectrum of the covariance is given on the top left panel in Figure 3.\nFor this panel, the black curves correspond to \u03b3 = 0, for di\ufb00erent values of \u03be. The e\ufb00ect of\nthe regularization can be clearly seen, with a spectrum that goes from singular for \u03be = 0 (with\nzero eigenvalues for rank larger than 260) to a constant spectrum given by the mean volatility\n(\n\n\u03c32\u000b\n\u22430.1) for \u03be = 1. The spectrums for increasing shrinkage \u03b3 are drawn with colors that go\nfrom black for \u03b3 = 0 to blue for \u03b3 = 0.4. The shrinkage e\ufb00ect on the small eigenvalues is very\nclear, with a less singular spectrum.\nThe mean size of the residuals are shown on the top right panel. There is no particular feature\naround a unit variance, and instead the dominant feature is the very strong increase of the\nresidual size for decreasing values of the regularization \u03be. Increasing the shrinkage parameter \u03b3\nalleviates the problem, but there is no plateau around\n\n\u03b52\u000b\n= 1.\nThe whitening qualities of the residuals are displayed on the four other panels. The largest\ncorrelations are between the contemporaneous quantities \u03c1(r, r) and \u03c1(r2, r2); the corresponding\nwhitening qualities are plotted in the center panels. The next largest correlation is \u03c1(L[r2], r2),\ncorresponding to the heteroskedasticity, and is displayed in the bottom right panel. For these 3\n11\n\nmeasures of quality, the best results are achieved for parameters in the range \u03b3 \u2208[0.05, 01] and\n\u03be \u2208[10\u22123, 10\u22122]. The four measures of quality related to the other correlations have a similar\nbehavior, but with a smaller magnitude. The bottom right panel shows the whitening quality\nfor the magnitude of the residuals according to (19). For this measure, the optimal value for\nthe parameters are larger, with \u03b3 \u22430.2 and \u03be \u224310\u22121. With this data set, it seems di\ufb03cult to\nhave optimal parameter values according to all the whitening qualities.",
    "chunk_index": 11,
    "start_char": 27833,
    "end_char": 30675,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "four measures of quality related to the other correlations have a similar\nbehavior, but with a smaller magnitude. The bottom right panel shows the whitening quality\nfor the magnitude of the residuals according to (19). For this measure, the optimal value for\nthe parameters are larger, with \u03b3 \u22430.2 and \u03be \u224310\u22121. With this data set, it seems di\ufb03cult to\nhave optimal parameter values according to all the whitening qualities.\nIn order to obtain con\ufb01dence boundaries around the hypothesis of no correlation for the resid-\nuals, we have used Monte Carlo simulations. Independent residuals are drawn from a Student\ndistribution with \ufb01ve degrees of freeedom, and the measure of quality computed for the same\nnumber of time series and sample length. This procedure is repeated 1000 times, and the 5% and\n95% quantiles are extracted from the distribution for the measure of quality. Both values are\nplotted as horizontal red lines. For all the measures of quality, the empirical values are clearly\nabove the 95% con\ufb01dence limits. This points to the misspeci\ufb01cation of the covariance \u03a3e\ufb00(\u03b3, \u03be),\nregardless of the parameter values. Despite this negative result, the most subtantial part of the\ndependencies are removed by the covariance. For example, for the measure q(\u03b5, \u03b5) (center left\npanel), the measure of quality for the returns is slightly below 13%, for the residuals they are in\nthe 3% range, while perfectly uncorrelated residuals have a value around 2%. Clearly, the less\nsatisfactory quantitative results are for the magnitude of the residuals, with empirical values all\nabove 0.7, while the perfectly uncorelated 95% quantile is at 0.07. Let us emphasize that it is\nonly when computing the con\ufb01dence bounds that a distributional assumption is made.\nThe same quantities but for the G10 and USA data sets are given respectively on Figures 4 and\n5. Despite the fact that these data sets relate to non singular covariance matrices and are quite\ndi\ufb00erent in their compositions, very similar patterns emerge compared to the ICM data set. For\nboth data sets, the covariance at \u03b3 = \u03be = 0 is non singular and the inverse volatility can be\ncomputed without special care. The whitening qualities at \u03b3 = \u03be = 0 are quite diverse, with\nq(\u03b5, \u03b5) close to an optimal value while q(L[\u03b52], \u03b52) is at the same level of q(L[r2], r2), or with a\nmagnitude for the residuals very di\ufb00erent from 1. Clearly, adding shrinkage and regularization\nproduces better overall measures of quality, with optimal values around \u03b3 \u2208[0.05, 0.1] and\n\u03be \u2208[10\u22123, 10\u22122]. The model with \u03b3 = \u03be = 0 is clearly misspeci\ufb01ed, while adding shrinkage and\nregularization improves the situation but is still not perfect.\nInterestingly, the optimal values for the parameters are very similar for the three data sets,\npointing to a general feature of the underlying data generating process. The model at \u03b3 = \u03be = 0\nis likely misspeci\ufb01ed, and a process closer to the empirical data generating process is obtained\nwith value around \u03b3 \u22430.05 and \u03be \u224310\u22123.",
    "chunk_index": 12,
    "start_char": 30253,
    "end_char": 33247,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "10\u22122]. The model with \u03b3 = \u03be = 0 is clearly misspeci\ufb01ed, while adding shrinkage and\nregularization improves the situation but is still not perfect.\nInterestingly, the optimal values for the parameters are very similar for the three data sets,\npointing to a general feature of the underlying data generating process. The model at \u03b3 = \u03be = 0\nis likely misspeci\ufb01ed, and a process closer to the empirical data generating process is obtained\nwith value around \u03b3 \u22430.05 and \u03be \u224310\u22123. With these values, the residuals are the closest to\nuncorrelated, for the three data sets. An intuitive picture for this result can be build as follows.\nWith this regularization, the bottom of the covariance spectrum is at 10\u22123 \n\u03c32\u000b\n, leading to an\ninverse volatility in the range 32/\np\n\u27e8\u03c32\u27e9. The \ufb02uctuations of the returns in the corresponding\ndirection get multiplied by this large factor, leading to large residuals. With an increasing matrix\nsize, the number of very small covariance eigenvalues increases with N, in turn increasing the\nresidual sizes.\nIn principle, the large residual problem can be solved by using large enough\nshrinkage and regularization parameters, but then the covariance is modi\ufb01ed too much and\nleaves dependencies between residuals.\nTherefore, one cannot \ufb01nd optimal values for the parameters that would both lead to small\nresidual dependencies and to unit variances.\nThe later issue is essentially due the the fast\ndecrease toward zero of the spectrum and to the possible zero eigenvalues. Indeed, this is the\nsignature of the limited information that can be extracted from such a multivariate system.\n12\n\nCorresponding to a given kernel \u03bb(i), there is an e\ufb00ective number of time series beyond which\nthe multivariate information gets very tenuous, as signaled by the very small or null eigenvalues.\nThe obvious solution would consist in using a memory kernel that allow for more information\nto be harvested, but there are fundamental limitations in this direction. First, the long memory\nkernel is optimal at capturing the dynamics of the univariate volatility clustering, and using\nanother kernel leads to worst (univariate) decorrelation of the residual. Then, a rectangular\nkernel such that imax > N is not always possible due to the limited availability of historical\ndata. Moreover, such a solution would clearly miss the short term dynamics of the volatility.\nTherefore, there is a fundamental limitation to the empirical multivariate information that can\nbe extracted from a set of time series. This limitation leads to small eigenvalues, and to the large\nvariance for the residuals. Intuitively, the large residuals \u201cmake up\u201d for the small eigenvalues,\nso that \ufb02uctuations along all possible directions do occur. The regularization by \u03be > 0 is such\nthat all sources of randomness \u03b5 contribute to the \ufb02uctuations of the returns. Notice that this is\nfundamentally di\ufb00erent from a misspeci\ufb01cation, as the process could be speci\ufb01ed correctly, but\nour inference power is limited for N large.\n13",
    "chunk_index": 13,
    "start_char": 32774,
    "end_char": 35775,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "the large residuals \u201cmake up\u201d for the small eigenvalues,\nso that \ufb02uctuations along all possible directions do occur. The regularization by \u03be > 0 is such\nthat all sources of randomness \u03b5 contribute to the \ufb02uctuations of the returns. Notice that this is\nfundamentally di\ufb00erent from a misspeci\ufb01cation, as the process could be speci\ufb01ed correctly, but\nour inference power is limited for N large.\n13\n\n0\n50\n100\n150\n200\n250\n300\n10\n\u22126\n10\n\u22124\n10\n\u22122\n10\n0\n10\n2\nrank\neigenvalue\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\n<stdDev>\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n2\n4\n6\n8\n10\n12\n14\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n1\n2\n3\n4\n5\n6\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\nwhitening quality\nFigure 3:\nFor the ICM data set, the most important whitening measures, as function of the\nregularization parameter \u03be, and shrinkage parameter \u03b3 = 0.0 (black), 0.05, 0.1, 0.2, 0.4 (blue).\nUpper left panel: the spectrum as function of the eigenvalue rank. Upper right panel: the mean\nmagnitude of the residual \u27e8\u03b5\u27e9de\ufb01ned by (20). Center left, center right and bottom left panels\ndisplay respectively the whitening quality q(\u03b5, \u03b5), q(\u03b52, \u03b52) and q(L[\u03b52], \u03b52). Bottom right: the\nwhitening quality q(\u03b52) for the unit variance of the residuals.\n14\n\n0\n10\n20\n30\n40\n50\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\nrank\neigenvalue\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\n<stdDev>\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\n25\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n1\n2\n3\n4\n5\n6\n7\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\nwhitening quality\nFigure 4: As for Figure 3, but for the G10 data set.\n15\n\n0\n10\n20\n30\n40\n50\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\nrank\neigenvalue\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\n<stdDev>\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\n25",
    "chunk_index": 14,
    "start_char": 35382,
    "end_char": 37682,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "4\nRegularization parameter\nwhitening quality\nFigure 4: As for Figure 3, but for the G10 data set.\n15\n\n0\n10\n20\n30\n40\n50\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\nrank\neigenvalue\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\n<stdDev>\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\n25\n30\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n5\n10\n15\n20\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n2\n4\n6\n8\n10\nRegularization parameter\nwhitening quality\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nRegularization parameter\nwhitening quality\nFigure 5: As for Figure 3, but for the USA data set.\n16\n\n7\nComparing di\ufb00erent covariance kernels\nThe performances of di\ufb00erent kernels at producing i.i.d. residuals is investigated in table 7. All\nare evaluated using 260 days of history, with shapes that are equal weights, exponential with\ndecay 0.94, long memory, and long memory with shrinkage and regularization. The performances\nof the \ufb01rst three kernels are somewhat similar, while the added parameters related to shrinkage\nand regularization makes the fourth kernel often the best. Yet, there is no absolute ranking. The\nmost salient feature is the di\ufb03culty in ful\ufb01lling the criterion\n\n\u03b52\u000b\n= 1 with increasing size N,\nwith the shrinkage and regularization in the fourth kernel helping signi\ufb01cantly. In this respect,\nthe exponential kernel shows the worst performance, which can be understood as follow. Due\nto the the fast decay of the kernel, the amount of multivariate information harvested by the\nexponential kernel is the smallest, leading to the fastest decays toward zero in the spectrum. In\nturn, these small eigenvalues lead to the largest inverse volatilities, and therefore to the largest\nresidual sizes. In contrast, the long memory kernel leads to residuals 6 times smaller, with a\nsomewhat similar overall shape.\nIt would then be tempting to evaluate the correlations with a much longer kernel, for example\nwith equal weights (corresponding to the usual formula for the correlation). Yet, [Zumbach, 2008]\n\ufb01nds that the correlation has a clear dynamics, with long memory. Using a long kernel with\nequal weights will wash out this structure, therefore missing the short term correlation moves\n(this will also produces bad volatility estimators). This points to a fundamental limitation of\nthe information that can be extracted from a multivariate system, when the dynamics of the\ncorrelation has also to be captured correctly. This limitation explains the strong similarities\nof the key properties for the three data sets, despite that their sizes vary by a factor six, and\ndespite that two of them are non degenerate. Essentially, the exponential decay toward zero\nof the eigenvalues (without regularization) is shared by all data sets, and this decay makes the\nhistory cut-o\ufb00imax mostly irrelevant.",
    "chunk_index": 15,
    "start_char": 37368,
    "end_char": 40282,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "produces bad volatility estimators). This points to a fundamental limitation of\nthe information that can be extracted from a multivariate system, when the dynamics of the\ncorrelation has also to be captured correctly. This limitation explains the strong similarities\nof the key properties for the three data sets, despite that their sizes vary by a factor six, and\ndespite that two of them are non degenerate. Essentially, the exponential decay toward zero\nof the eigenvalues (without regularization) is shared by all data sets, and this decay makes the\nhistory cut-o\ufb00imax mostly irrelevant. The exponential decay of the spectrum also explains why\nthe shrinkage and regularization are e\ufb00ective at producing residuals with the desired properties.\n17\n\ndata set\nwhitening quality\nreturn\nEqual Weights\nExponential\nLong memory\nLM + regularization\nwhite noise\nICM\n\u03c1(\u03b5, \u03b5)\n12.6\n3.3\n3.1\n2.7\n3.4\n2.2\n\u03c1(\u03b52, \u03b52)\n8.9\n10.6\n5.9\n7.4\n4.1\n2.2\n\u03c1(\u03b5, \u03b52)\n3.5\n4.0\n3.0\n3.4\n3.0\n2.3\n\u03c1(L[\u03b5], \u03b5)\n4.8\n2.5\n2.4\n2.4\n2.5\n2.2\n\u03c1(L[\u03b52], \u03b5)\n2.8\n2.8\n2.5\n2.6\n2.4\n2.2\n\u03c1(L[\u03b5], \u03b52)\n3.1\n2.8\n2.5\n2.6\n2.4\n2.2\n\u03c1(L[\u03b52], \u03b52)\n5.1\n5.6\n3.9\n4.2\n2.9\n2.2\n\n\u03b52\u000b\n= 1\n105\n691\n129\n2.4\n0.060\nG10\n\u03c1(\u03b5, \u03b5)\n25.0\n2.0\n4.2\n2.6\n4.2\n2.2\n\u03c1(\u03b52, \u03b52)\n18.8\n8.6\n15.4\n10.7\n7.4\n2.2\n\u03c1(\u03b5, \u03b52)\n5.2\n5.0\n6.0\n5.2\n4.6\n2.7\n\u03c1(L[\u03b5], \u03b5)\n6.3\n3.6\n3.0\n3.2\n3.5\n2.2\n\u03c1(L[\u03b52], \u03b5)\n3.2\n3.1\n3.4\n3.2\n2.7\n2.2\n\u03c1(L[\u03b5], \u03b52)\n3.8\n3.3\n3.4\n3.3\n2.7\n2.2\n\u03c1(L[\u03b52], \u03b52)\n6.5\n6.1\n7.4\n6.5\n3.8\n2.2\n\n\u03b52\u000b\n= 1\n0.67\n15.3\n2.5\n0.64\n0.059\nUSA\n\u03c1(\u03b5, \u03b5)\n27.8\n2.5\n3.4\n2.4\n3.4\n2.2\n\u03c1(\u03b52, \u03b52)\n18.2\n6.3\n11.2\n7.1\n6.3\n2.2\n\u03c1(\u03b5, \u03b52)\n5.1\n4.4\n4.8\n4.4\n4.4\n2.7\n\u03c1(L[\u03b5], \u03b5)\n3.6\n2.7\n2.5\n2.5\n2.5\n2.2\n\u03c1(L[\u03b52], \u03b5)\n3.6\n2.7\n2.6\n2.6\n2.5\n2.2\n\u03c1(L[\u03b5], \u03b52)\n3.9\n2.8\n2.6\n2.6\n2.4\n2.2\n\u03c1(L[\u03b52], \u03b52)\n9.3\n4.6\n4.5\n4.3\n3.2\n2.2\n\n\u03b52\u000b\n= 1\n0.48\n10.2\n2.1\n0.94\n0.059\nTable 1: The whitening qualities for the three data sets, for di\ufb00erent kernel shapes \u03bb(i). The\ncolumn \u201creturn\u201d gives the empirical values for the whitening qualities for the returns, while\nthe column \u201cwhite noise\u201d gives the average values for an uncorrelated Student white noise.\nThe column \u201cLM + regularization\u201d corresponds to a long memory covariance with parameters\n\u03b3 = 0.05 and \u03be = 0.01. For the ICM data set, the equal weights, exponential and long memory\nkernels are regularized using \u03be = 0.0001.\n18\n\n8\n\u201cProjected\u201d and \u201cfull rank\u201d Regularization\n10\n0\n10\n1\n10\n2\n0\n5\n10\n15\n20\n25\n30\nRegularization rank\nwhitening quality\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\nreturn\n10\n0\n10\n1\n0\n10\n20\n30\n40\n50\nRegularization rank\nwhitening quality\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\nreturn\nFigure 6: The whitening quality q(\u03b5, \u03b5), for the ICM (left) and G10 (right) data sets.",
    "chunk_index": 16,
    "start_char": 39691,
    "end_char": 42383,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "2\n0\n5\n10\n15\n20\n25\n30\nRegularization rank\nwhitening quality\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\nreturn\n10\n0\n10\n1\n0\n10\n20\n30\n40\n50\nRegularization rank\nwhitening quality\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\nreturn\nFigure 6: The whitening quality q(\u03b5, \u03b5), for the ICM (left) and G10 (right) data sets. The\nregularized quality is computed with \u03b3 = 0.05 (red) and \u03b3 = 0.1 (blue) and the regularization\nparameter \u03be is mapped to a \u201cplausible\u201d equivalent rank k by using the mean spectrum.\nIn order to compute the inverse volatility, the most widely used method is to use the cross product\ncovariance \u03a3e\ufb00(\u03b3 = 0, \u03be = 0), and to regularize the inverse volatility by using the \u201cprojected\u201d\ninverse (15) or the \u201cfull rank\u201d inverse (16). It is interesting to contrast these two approaches\nwith the regularization of the covariance with \u03b3 and \u03be.\nThe Figure 6 shows the whitening\nquality q(\u03b5, \u03b5). The data corresponding to \u03a3e\ufb00(\u03b3, \u03be) are mapped into an equivalent projector\nrank by using the mean spectrum of the covariance at \u03b3 = 0, \u03be = 0. These curves show clearly\nthat a \u201dprojected\u201d regularization (black line) is not good for small ranks. This \ufb01nding goes\nclearly against a common practice consisting in using only the leading eigenvalues/eigenvectors\nfor computing the inverse of the covariance.\nThe \u201cfull rank\u201d regularization is clearly better\nthan the \u201cprojected\u201d scheme, for all choices of rank. For the regularization on the covariance\n(red curves), the similarity of the \u201cfull rank\u201d and the \u201cregularized\u201d quality is due to the similar\nmodi\ufb01cation of the spectrum made by both schemes. Overall, the regularized method using\n\u03a3e\ufb00(\u03b3, \u03be) is slightly superior.\n10\n0\n10\n1\n10\n2\n0\n0.5\n1\n1.5\n2\nRegularization rank\nq\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\n10\n0\n10\n1\n0\n0.5\n1\n1.5\n2\nRegularization rank\nq\n \n \nresidual: projected\nresidual: full rank\nresidual: regularized, 0.05\nresidual: regularized, 0.10\nFigure 7: The whitening quality q(\u03b52) for the norm of the residuals according to (19), for the\nICM (left) and G10 (right) data sets. The regularized qualities are computed with \u03b3 = 0.05 (red)\nand \u03b3 = 0.1 (blue) and the regularization parameter \u03be is mapped to a \u201cplausible\u201d equivalent\nrank k by using the mean spectrum.\n19\n\nThe Figure 7 compares the measure of quality for the norm of the residuals. For this quality\nmeasure, the regularization of the spectrum performs better, for the \u201cprojected\u201d and the \u201cfull\nrank\u201d schemes. However, the best measure occurs for a quite narrow range of projector ranks,\nand it is not clear how to select a priori the best rank. As a \ufb01rst rule of thumb, a regularization\nparameter k between 30% to 60% of the rank of the spectrum seems to give good results.",
    "chunk_index": 17,
    "start_char": 41959,
    "end_char": 44820,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "spectrum performs better, for the \u201cprojected\u201d and the \u201cfull\nrank\u201d schemes. However, the best measure occurs for a quite narrow range of projector ranks,\nand it is not clear how to select a priori the best rank. As a \ufb01rst rule of thumb, a regularization\nparameter k between 30% to 60% of the rank of the spectrum seems to give good results.\n9\nConclusion\nIn a multivariate process setting, the covariance de\ufb01ned by a simple cross product as in (4) is\nmisspeci\ufb01ed, and a shrinking and a regularization term as in (8) lead to better properties for\nthe residuals. The spectrum of the covariance is then always non singular, and the computed\nresiduals have statistical properties that are closer from being uncorrelated with unit variances.\nYet, it is di\ufb03cult to obtain residuals both independent and with a unit variance. With a given\ncovariance kernel \u03bb(i), the number of very small or null eigenvalues is increasing for increasing\nsize N, leading to very large values in the inverse volatility. When multiplied by historical re-\nturns, the large inverse volatility creates large residuals, and the residual variances are increasing\nwith N (or at \ufb01xed N, are increasing with a decreasing cuto\ufb00as \u03be or 1/k). This di\ufb03culty is fun-\ndamentally rooted in the limited information that can be extracted from N time series, leading\nto small eigenvalues in the covariance matrix. The directions corresponding to the small eigen-\nvalues are suppressed in the process, e\ufb00ectively reducing the number of sources of randomness.\nBut in an inference computation, return \ufb02uctuations do occurs along the suppressed directions,\nand very large innovations are needed to compensate for the small eigenvalues. Essentially, the\nlimited information contained in the covariance matrix leads to a fundamental limitation on the\ninference that can be achieved in a large multivariate system.\nThis structural limitation has been diagnosed because the structure and parameters for the\ncovariance have been inferred from a univariate linear process that has already the correct\n\u201cuniversal\u201d time structure and has no parameters (i.e. a long memory with multiple time scales\ninstead of the more common I-GARCH(1,1) structure that has one exponential time scale).\nThe multivariate extension is minimal, with the two multivariate parameters \u03b3 and \u03be left to be\nstudied.\nOne way around the limitation would be to build a process with fewer souces of randomness\nthan the system size N. For this solution to be e\ufb00ective, the most relevant directions in the\ncovariance must be stable. For a given rank corresponding to the number of sources of ran-\ndomness, the practical criterion is that the projector on the leading subspace should be stable.\n[Zumbach, 2008] shows that the projectors are instead largely \ufb02uctuating, with a long memory\ncorrelations. Clearly, a simple approach with a \ufb01xed projector is missing part of the dynamics,\nand a model for the joint dynamics of the eigenvalues and eigenvectors is much more complex.\nOur approach contrasts sharply with most multivariate GARCH extensions that contain of\nthe order N 2 or N 4 parameters [Silvennoinen and Ter\u00a8asvirta, 2008].",
    "chunk_index": 18,
    "start_char": 44481,
    "end_char": 47622,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "ran-\ndomness, the practical criterion is that the projector on the leading subspace should be stable.\n[Zumbach, 2008] shows that the projectors are instead largely \ufb02uctuating, with a long memory\ncorrelations. Clearly, a simple approach with a \ufb01xed projector is missing part of the dynamics,\nand a model for the joint dynamics of the eigenvalues and eigenvectors is much more complex.\nOur approach contrasts sharply with most multivariate GARCH extensions that contain of\nthe order N 2 or N 4 parameters [Silvennoinen and Ter\u00a8asvirta, 2008]. The most parsimonious\nmultivariate GARCH processes have a number of parameters of order N, and even this smallest\nnumber is already too large for actual portfolios. For this reason, the multivariate extension\nof GARCH have been mostly con\ufb01ned to the academic literature. Beside, for a multivariate\na\ufb03ne GARCH process, the standard procedure consists in \ufb01nding the optimal parameters with\na log-likelihood maximization, assuming a given distribution for the residuals. Following the\n20\n\nmodel hypothesis, the residual distribution is set to have a unit variance in this computation.\nAfter optimization, the empirical residuals have a variance close to one, in sharp distinction with\nthe present results. Indeed, the optimized parameters are \u201cabsorbing\u201d the problems related to\nthe evaluation of the large inverse volatility. The \ufb01rst e\ufb00ect is that the e\ufb00ective covariance, with\nthe optimal log-likelihood parameters, becomes too large with respect to the volatility of the\nreturns. The second e\ufb00ect is that the parameters acquire an unwanted dependency in N and\nthat their optimal values cannot be interpreted in economic terms (say like a characteristic time\nfor the lagged correlation). As a consequence, the optimal values cannot be checked for their\nplausibilities, and they cannot be transported from one universe to another.\nAs emphasized in this paper, the covariance \u03a3e\ufb00(\u03b3, \u03be) is bilinear in the returns. This implies that\nvolatility forecasts can be evaluated, namely all the quadratures can be computed analytically.\nThe volatility forecast at t for the time t + n \u03b4t takes the form\nE [ \u03a3e\ufb00;\u03b1,\u03b2(t + n \u03b4t) | \u2126(t)] =\nX\n\u03b1\u2032,\u03b2\u2032\nimax\nX\ni=0\ne\u03bb\u03b1,\u03b2;\u03b1\u2032,\u03b2\u2032(n, i) r\u03b1\u2032(t \u2212i \u03b4t) r\u03b2\u2032(t \u2212i \u03b4t).\n(21)\nThe equation for the weights e\u03bb\u03b1,\u03b2;\u03b1\u2032,\u03b2\u2032(n, i) is a recursion equation in n that can be implemented\nnumerically. Therefore, the multivariate volatility forecast can be evaluated, regardless of the\nforecast horizon t + n \u03b4t, for all values of \u03b3 and \u03be.\nThe present paper is partly set in a process framework, in order to justify the de\ufb01nition of\nthe covariance.\nThe de\ufb01nition is however fairly general, and many shapes can be used for\nthe weights \u03bb(i), including the common equal weights and exponential weights. As found in\n[Zumbach, 2008], regardless of the kernel, the spectrum decreases exponentially fast toward zero\n(the pace for the decay is depending on the kernel). Therefore, a regularization of a covariance\nmatrix computed with a given kernel \u2013say for example a uniform kernel\u2013 can also be achieved\nby adding a shrinkage and a regularization term.",
    "chunk_index": 19,
    "start_char": 47082,
    "end_char": 50173,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "general, and many shapes can be used for\nthe weights \u03bb(i), including the common equal weights and exponential weights. As found in\n[Zumbach, 2008], regardless of the kernel, the spectrum decreases exponentially fast toward zero\n(the pace for the decay is depending on the kernel). Therefore, a regularization of a covariance\nmatrix computed with a given kernel \u2013say for example a uniform kernel\u2013 can also be achieved\nby adding a shrinkage and a regularization term. Further studies along this line, extending the\nworks of [Ledoit and Wolf, 2004a], could lead to more robust results when computing an inverse\nvariance in portfolio optimizations.\nReferences\n[Bollerslev et al., 1988] Bollerslev, T., Engle, R., and Wooldridge, J. (1988). A capital asset\npricing model with time-varying covariances. The journal of Political Economy, 96:116\u2013131.\n[Engle and Kroner, 1995] Engle, R. F. and Kroner, K. F. (1995). Multivariate simulataneous\ngeneralized ARCH. Economic Theory, 11:122\u2013150.\n[Ledoit and Wolf, 2004a] Ledoit, O. and Wolf, M. (2004a). Honey, i shrunk the sample covari-\nance matrix. Journal of Portfolio Management, 4(30):110\u2013119.\n[Ledoit and Wolf, 2004b] Ledoit, O. and Wolf, M. (2004b). Improved estimation of the covari-\nance matrix of stock returns with an application to portfolio selection. Journal of Empirical\nFinance, 10:603\u2013621.\n[Silvennoinen and Ter\u00a8asvirta, 2008] Silvennoinen, A. and Ter\u00a8asvirta, T. (2008).\nMultivariate\nGARCH models.\nSpringer.\nTo appear in T.G. Andersen, R.A. Davies, J.-P. Kreiss and\nT. Mikosch eds, Handbook of Financial Time Series.\n21\n\n[Zumbach, 2004] Zumbach, G. (2004).\nVolatility processes and volatility forecast with long\nmemory. Quantitative Finance, 4:70\u201386.\n[Zumbach, 2006] Zumbach, G. (2006). The riskmetrics 2006 methodology. Technical report,\nRiskMetrics Group. available at: http://www.riskmetrics.com/publications/techdoc.html.\n[Zumbach, 2008] Zumbach, G. (2008). The empirical properties of large covariance matrices.\nTechnical report, RiskMetrics Group.\n22",
    "chunk_index": 20,
    "start_char": 49708,
    "end_char": 51718,
    "paper_title": "Inference on multivariate ARCH processes with larg",
    "paper_category": "q-fin.ST",
    "paper_filename": "Inference_on_multivariate_ARCH_processes_with_larg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Inference_on_multivariate_ARCH_processes_with_larg.pdf"
  },
  {
    "text": "Optimal bidding in hourly and quarter-hourly\nelectricity price auctions: trading large volumes of\npower with market impact and transaction costs\nMicha l Narajewski\nUniversity of Duisburg-Essen\nand\nFlorian Ziel\nUniversity of Duisburg-Essen\nFebruary 10, 2022\nAbstract\nThis paper addresses the question of how much to bid to maximize the pro\ufb01t when\ntrading in two electricity markets: the hourly Day-Ahead Auction and the quarter-\nhourly Intraday Auction. For optimal coordinated bidding many price scenarios are\nexamined, the own non-linear market impact is estimated by considering empirical\nsupply and demand curves, and a number of trading strategies is used. Addition-\nally, we provide theoretical results for risk neutral agents. The application study is\nconducted using the German market data, but the presented methods can be easily\nutilized with other two consecutive auctions. This paper contributes to the existing\nliterature by evaluating the costs of electricity trading, i.e. the price impact and the\ntransaction costs. The empirical results for the German EPEX market show that it is\nfar more pro\ufb01table to minimize the price impact rather than maximize the arbitrage.\nKeywords: electricity trading, coordinated bidding, day-ahead market, electricity price\nforecasting, intraday market, portfolio optimization, auction curves, market impact, risk\naverse\n1\narXiv:2104.14204v3 [q-fin.ST] 9 Feb 2022\n\n1\nIntroduction and motivation\nSince the deregulation of the electricity markets the energy exchanges like the European\nEnergy Exchange (EEX) have created many trading possibilities to account for various\nmarket challenges. The electricity trading in Europe consists of futures, spot and balancing\nmarkets. Here, we deal with the biggest and the most important one \u2013 the spot market.\nA brief description of the German electricity spot market can be seen in Figure 1, for\nmore details on the German market see e.g.\nViehmann [1].\nWe present the German\nspot electricity market as this is the biggest one in Europe, and we will also perform our\nempirical study based on the data from this market.\nThe diversity of the trading possibilities in the market has raised new very important\nquestions and challenges, e.g. when and how to trade the electricity in order to maximize\nthe expected gain. In a perfectly e\ufb03cient market with risk neutral agents this problem\nwould become irrelevant as the expected gain should be the same, disregarding the market\npart and product type that one would use to trade the electricity. However, this is not the\ncase due to the fact that the market participants do not possess the full information, and\nthey are highly dependent on the quality of their forecasts. Additionally, a very important\nrole is being played by the own price impact of the market participants, especially for the\nlarge ones. Furthermore, some market agents may be not perfectly risk neutral but may\nbe risk averse.\nThe following paper raises the issue considering two European auction-based spot\nmarkets: the hourly EPEX Day-Ahead Auction (DA) and the quarter-hourly Intraday\nAuction (IA), as they are currently used in Germany, Netherlands, Belgium and Austria.\nThe DA market is the main spot market and often serves as a reference price [1].",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3251,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "important\nrole is being played by the own price impact of the market participants, especially for the\nlarge ones. Furthermore, some market agents may be not perfectly risk neutral but may\nbe risk averse.\nThe following paper raises the issue considering two European auction-based spot\nmarkets: the hourly EPEX Day-Ahead Auction (DA) and the quarter-hourly Intraday\nAuction (IA), as they are currently used in Germany, Netherlands, Belgium and Austria.\nThe DA market is the main spot market and often serves as a reference price [1]. On\nthe other hand, the IA was introduced in the purpose of balancing the ramping e\ufb00ects\nof demand and power generation [2\u20134]. Let us note that there are countries with other\nsettings as e.g. France and Great Britain who use half-hourly Intraday Auctions. After a\nslight adjustment, the presented analysis can be also applied to these markets.\nd \u22121,\n12:00\nDay-Ahead\nAuction\nd \u22121,\n15:00\nIntraday\nAuction\nHourly Intraday Continuous\nd \u22121,\n16:00\nQuarter-Hourly\nIntraday Continuous\nIC market\ncloses\nd,\nh \u221230 min\nIC control\nzones close\nd,\nh \u22125 min\nDelivery\nd, h\nFigure 1: The daily routine of the German spot electricity market. d, h correspond to the\nday and hour of the delivery, respectively.\n2\n\nIn the analysis, we assume that a market participant wants to trade volume of electric-\nity in a given hour, and they split it between the two markets, ignoring all other trading\npossibilities, as well as not speculating against the balancing market. Limiting ourselves\nonly to the two auctions is a simpli\ufb01cation to some extent, but we discuss in the paper\nthat it could be also generalized for usage with other markets and with a higher number\nof them. Additionally, we put emphasis on large trades and the price impact they make\nto the auctions. The existing studies have mostly disregarded this problem [5\u20139] or used\nsimpli\ufb01ed settings as e.g. linear impact assumption [10, 11]. Due to this novelty, we de-\ncided to start with a smaller setting for a better understanding of the problem. Moreover,\nestimation of the impact in the continuous and balancing markets is very complex [12]\nand deserves a separate study. We also assume that the market player places bids that are\nunlimited in prices. That is to say, they bid the minimum price on the supply side and\nthe maximum price on the demand side. Both unlimited bidding [13] and price-volume\nbidding have previously been used in the literature [5\u20138, 10, 11].\nEven though we assume some restrictions, we take into account other major features\nin the markets.\nAs mentioned, we do consider the non-linear market impact and the\ntransaction costs that the trader must account for. Moreover, we assume multiple trading\nstrategies like minimization of the transaction costs, risk neutral and risk averse agents.\nFor the latter one we utilize arbitrary, but well-known in the literature and practice risk\nfunctions such as the mean-variance utility, the value-at-risk (VaR) and the expected\nshortfall, also known as the conditional value-at-risk (CVaR).",
    "chunk_index": 1,
    "start_char": 2719,
    "end_char": 5745,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "other major features\nin the markets.\nAs mentioned, we do consider the non-linear market impact and the\ntransaction costs that the trader must account for. Moreover, we assume multiple trading\nstrategies like minimization of the transaction costs, risk neutral and risk averse agents.\nFor the latter one we utilize arbitrary, but well-known in the literature and practice risk\nfunctions such as the mean-variance utility, the value-at-risk (VaR) and the expected\nshortfall, also known as the conditional value-at-risk (CVaR).\nThe portfolio optimization approach to the trading of the produced electricity has\nalready been taken into consideration in the literature. A signi\ufb01cant amount of the existing\npapers consider the setting with futures market, spot market, and bilateral contracts [14\u2013\n17]. The authors utilize the modern portfolio theory and do not estimate the own price\nimpact, assuming that the market participant is a price-taker. Another stream of the\nliterature name the problem an o\ufb00ering strategy, and they concern the spot day-ahead and\nintraday markets as well as the balancing one [18\u201323]. These studies are similar to our one,\nbut their main downside is the fact that they assume the market player to be a price-taker.\nThis assumption automatically makes these studies inapplicable for market players that\ntrade medium-sized or large volumes and impact the price with their bids signi\ufb01cantly.\nAn important part of the literature compares the coordinated and sequential bidding,\nespecially for storages [5\u20137, 10, 11, 13].\nThe authors consider a multi-market setting,\nhowever they also simplify the market impact issue. A detailed review of coordinated\nbidding literature was prepared by Aasg\u02daard et al. [24].\nTo the best of our knowledge, the problem of portfolio optimization in auction-based\n3\n\nspot markets with market impact and trading costs has not been addressed in the liter-\nature so far. Kath and Ziel [12] investigate the optimal order execution in the intraday\ncontinuous market accounting for the market impact. The work that is the closest to\nour setting is the paper of Ay\u00b4on et al. [25] who investigate the optimal bidding curves\nin day-ahead and intraday electricity markets. The authors, however, consider a \ufb02exible\ndemand setting and again assume not to make any price impact in the market. A big part\nof the literature concerning the optimal trading problem or the bidding behaviour in the\nspot markets, focuses only on one part of the market [2, 26\u201328]. These papers investigate\nmultiple aspects of the trading in the intraday continuous market. Also the renewable\nenergy forecasting plays a crucial role in the decision process for optimal power trading\n[29, 30].\nAn important factor in the strategy optimization for large volumes is the price impact\nestimation. We approach the problem using the aggregated curves data. Bidding in the\nauction-based markets causes shifts in the demand or supply curves. We use the fact to\ncalculate the non-linear price impact of the market participant\u2019s own bids. Here we also\nneed a forecast for the curves. Multiple papers look into the issue [31\u201335].",
    "chunk_index": 2,
    "start_char": 5221,
    "end_char": 8349,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "power trading\n[29, 30].\nAn important factor in the strategy optimization for large volumes is the price impact\nestimation. We approach the problem using the aggregated curves data. Bidding in the\nauction-based markets causes shifts in the demand or supply curves. We use the fact to\ncalculate the non-linear price impact of the market participant\u2019s own bids. Here we also\nneed a forecast for the curves. Multiple papers look into the issue [31\u201335]. The models\nare, however, very time-consuming as they estimate the full supply and demand curves.\nThe resulting intersection of the curves, can be regarded as an electricity price forecast.\nHowever, usually this is not as accurate as electricity price forecasting (EPF) models\nthat are designed only for the purpose providing accurate price predictions. To avoid the\naforementioned problem we actually consider a modelling approach that does not need\nto have curve forecast that provide accurate price predictions, but only gives reasonable\ncurve forecast for the neighbourhood of the expected price. It is then compared to using\nthe perfect forecast of both curves and prices to see the possible gain of having better,\nmore sophisticated models. As mentioned, we need suitable EPF models for our trading\napproach, see [32, 36, 37] for reviews. However, as the objective of this study is not to\ndevelop a new EPF model we chose to use the two well-known models often called the\nnaive and the expert.\nNow, let us summarize the major contributions of the manuscript:\n1. It is the \ufb01rst work concerning the electricity trading of large volumes with market\nimpact in the auction-based markets.\n2. The \ufb01rst manuscript which considers transaction costs that may vary across the\nconsidered markets.\n3. We provide an extensive analysis and discussion of the price formation and trading\nproblem in European price auctions.\n4\n\n4. The paper presents theoretical results on optimal bidding for risk neutral agents\nunder linear market impact and transactions.\n5. The trading setting is thoroughly examined with all the issues considered including\nrisk averse agents, and the possible extensions and generalization are discussed.\n6. The predictive performance of the utilized methods are compared in a forecasting\nstudy for di\ufb00erent market players (e.g. wind and solar power traders or retailers\nthat just buy electricity), multiple trading strategies, and forecasts\u2019 qualities.\n7. We provide insights on importance of the overall price impact reduction and evidence\nof irrelevance of the arbitrage between the DA and IA markets which indicates\nmarket e\ufb03ciency.\n8. The importance of this research is emphasized by the fact of launching intraday\nauctions in further European countries [38].\nThe remainder of this manuscript has the following structure. Section 2 describes the\nprice formation in European price auctions. The trading setting and objective in the day-\nahead and intraday auctions are discussed in Section 3. Trading strategies are described\nin Section 4. Section 5 presents the models used for the forecasting of the electricity prices\nand market impact using auction curve predictions. The application including the data\ndescription, evaluation measures and results is presented in Section 6.",
    "chunk_index": 3,
    "start_char": 7901,
    "end_char": 11141,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "launching intraday\nauctions in further European countries [38].\nThe remainder of this manuscript has the following structure. Section 2 describes the\nprice formation in European price auctions. The trading setting and objective in the day-\nahead and intraday auctions are discussed in Section 3. Trading strategies are described\nin Section 4. Section 5 presents the models used for the forecasting of the electricity prices\nand market impact using auction curve predictions. The application including the data\ndescription, evaluation measures and results is presented in Section 6. Section 7 discusses\nthe limitations and generalizations of the study where we focus on potential relaxation\nof the assumptions. Finally, Section 8 concludes the paper. In the Appendix we present\nthe important abbreviations, the notation used in Sections 2-5, the evaluation of the EPF\nmodels and additional \ufb01gures.\n2\nPrice formation in European price auctions\nWe consider two consecutive auctions for the day-ahead (DA) power market and the intra-\nday opening auction (IA) in Germany. The former one o\ufb00ers trading of hourly products,\nthe latter one trading of quarter-hourly products. For a selected delivery time point for\nday d and hour h we have in total \ufb01ve products traded in \ufb01ve corresponding auctions.\nAll auctions \ufb01nd the market clearing price by matching supply and demand such that\nwelfare (consumer and producer rent) is maximized. For the day-ahead this is based on\nthe EUPHEMIA algorithm which incorporates the market coupling of the region to allow\ncross-border trading and increase of the overall welfare.\nOn all markets we have non-negative volume bids BS(p) and BD(p) for p \u2208P with P\nas potential price grid on the considered auction on the supply and demand side. Here\n5\n\nD represent ask/buy/demand/purchase and S represent bid/sell/supply/sale.\nFurther,\nBS(p) and BD(p) are aggregates of all bids at price p, so if multiple market participants\nbid volumes at p they are aggregated in BS(p). On both markets we have a minimal bid\nprice increment of 0.1 EUR/MWh. For the considered markets we have pmin,DA = \u2212500,\npmin,IA = \u22123000 and pmax,DA = pmax,IA = 3000. Figure 2 shows an example of the bids.\nUsing the bids BS and BD on the supply and demand side we can compute the supply\nand demand curves AS and AD by aggregation. More precisely, the aggregated curves\nAS and AD are then de\ufb01ned by a linear interpolation of all aggregated bids at all bidden\nprices PS and PD. This is for the supply curve AS(p) = P\nx\u2208PS\u2229(\u2212\u221e,p] BS(x) for p \u2208PS\nand for the demand curve AD(p) = P\nx\u2208PD\u2229[p,\u221e) BD(x) for p \u2208PD. By construction, it is\nclear that the curves are strictly monotonic. Moreover, their inverse (AS)\u22121 and (AD)\u22121\ncan be regarded as continuous supply and demand curves. The unique intersection of AS\nand AD (resp. their graphs) or their inverse yields the market clearing volume and price\n(V \u2217, P \u2217).1 Finally, denote AS\ni and AD\ni for i \u2208{0, . . . , 4} the curves in the corresponding\nmarkets (0 = DA, 1, .",
    "chunk_index": 4,
    "start_char": 10560,
    "end_char": 13562,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "demand curve AD(p) = P\nx\u2208PD\u2229[p,\u221e) BD(x) for p \u2208PD. By construction, it is\nclear that the curves are strictly monotonic. Moreover, their inverse (AS)\u22121 and (AD)\u22121\ncan be regarded as continuous supply and demand curves. The unique intersection of AS\nand AD (resp. their graphs) or their inverse yields the market clearing volume and price\n(V \u2217, P \u2217).1 Finally, denote AS\ni and AD\ni for i \u2208{0, . . . , 4} the curves in the corresponding\nmarkets (0 = DA, 1, . . . , 4 = IA), analogue BS\ni and BD\ni , PS\ni and PD\ni . Further, let AS, AD,\nBS, BD, PS, PD, pmin and pmax be the corresponding vectors, e.g. AS = (AS\n0, . . . , AS\n4)\u2032.\nAn example of the curves is presented in Figure 3. For reporting purpose the exchange\nrounds to a 0.01 EUR/MWh price increment, and volumes to 0.1 MW. Further, note\nthat especially for the day-ahead market clearing results of the intersection (V \u2217, P \u2217) does\nnot always equal exactly the reported market clearing volume and price of the exchange.\n1Theoretically, it may happen that the graphs have no intersection. This happens if either AS(pmax) <\nAD(pmax) or AD(pmin) < AS(pmin). In those extreme event scenarios, we de\ufb01ne the intersections (V \u2217, P \u2217)\nby (AS(pmax), pmax) and (AD(pmin), pmin). Note that in the considered German market, any of those events\nnever happened since 2010.\n500\n0\n500\n1000\n1500\n2000\n2500\n3000\n104\n103\n102\n101\n0\n101\n102\n103\n104\n105\nDay-Ahead Auction bids\n3000\n2000\n1000\n0\n1000\n2000\n3000\nIntraday Auction bids\ndemand\nsupply\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrice (EUR/MWh)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVolume (MW)\nFigure 2: An example of the bids BS and BD in the German market with delivery on\n01.06.2017. The delivery periods are 12:00 to 13:00 for the DA and 12:00 to 12:15 for the\nIA. The demand bids are plotted as negative for better comparability and the volumes are\ngiven in a symmetric logarithmic scale\n6\n\n30\n35\n40\n45\n50\n500\n0\n1000\n2000\n3000\nDay-Ahead Auction curves\ndemand\nsupply\nintersection\n0\n2\n4\n6\n3000\n1500\n0\n1500\n3000\nIntraday Auction curves\n30\n32\n34\n36\n38\n40\n42\n44\n46\n0\n20\n40\n60\n80\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n20\n40\n60\n80\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVolume (GW)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrice (EUR/MWh)\nFigure 3: An example of the supply and demand curves (AS)\u22121 and (AD)\u22121 in the German\nmarket with delivery on 01.06.2017. The delivery periods are 12:00 to 13:00 for the DA\nand 12:00 to 12:15 for the IA. The bottom plots are the zoomed-in versions of the top ones\nThere are sometimes small deviations, which we ignore within this paper.2\n3\nTrading/Bidding in DA and IA markets\n3.1\nSetting\nTo simplify the trading problem, we consider a market player which only submits unlimited\nbids, often referred as volume bids.",
    "chunk_index": 5,
    "start_char": 13107,
    "end_char": 15776,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "Figure 3: An example of the supply and demand curves (AS)\u22121 and (AD)\u22121 in the German\nmarket with delivery on 01.06.2017. The delivery periods are 12:00 to 13:00 for the DA\nand 12:00 to 12:15 for the IA. The bottom plots are the zoomed-in versions of the top ones\nThere are sometimes small deviations, which we ignore within this paper.2\n3\nTrading/Bidding in DA and IA markets\n3.1\nSetting\nTo simplify the trading problem, we consider a market player which only submits unlimited\nbids, often referred as volume bids. Thus, we want to trade volumes v = (v1, v2, v3, v4)\u2032 in\nMWh in the corresponding market, this could be the planned wind power to be generated\nin the four quarter hours of the considered hour. The trading problem is to \ufb01nd bids\nb = (b0, b1, . . . , b4)\u2032 in MW for the \ufb01ve markets. We use the convention that the signs of\nb indicate the market side. Thus, bi > 0 are sell bids which shift the supply curve and\nbi < 0 are bids on the demand side. Obviously, the market participant may in\ufb02uence only\ntheir own bids b. Therefore, we introduce the notation AS\nb, AD\nb , BS\nb, BD\nb which re\ufb02ect\nthe agent\u2019s bidding behaviour. Note that the sets of bidded prices PS and PD are not\nimpacted by b as we assume that even without the agent\u2019s market impact there is at least\none further unlimited bid on the relevant market side.\nThe intersections of AS\nb(p) and demand AD\nb (p) de\ufb01ne the market clearing volumes and\n2The deviation results mainly due to the handling of multiple and complex orders at the money solving\nthe market clearing optimization problem, for more details see e.g. [31].\n7\n\nprices (V \u2217\nb , P \u2217\nb ) which also depend on the own bid b. Moreover, we want to remind that\nthe markets have a sequential order, i.e. \ufb01rst the DA auction is realized, and then the IA\nauctions, see Figure 1. Thus, only b0 impacts the DA price, whereas next to b1, . . . , b4 also\nb0 may have an in\ufb02uence on the IA auctions. This is because other market participants\nmay react on the IA auctions due to b0-in\ufb02uenced DA auction results. We model this\nimpact in Section 5.2.\nOf fundamental importance are situations without own market impact, i.e. b = 0. As\nit is relevant for us in further analysis, we summarize some characteristics. Obviously it\nholds for arbitrary unlimited bids b that BS\n0(pmin) = BS\nb(pmin) \u2212b+ and BD\n0 (pmax) =\nBD\nb (pmax) \u2212b\u2212where b+ and b\u2212are the element-wise positive and negative part of b.\nFurther, it holds that BS\n0(p) = BS\nb(p) for p > pmin and BD\n0 (p) = BD\nb (p) for p < pmax.\nIn conclusion, we receive\n(AS\n0)\u22121(z) = (AS\nb)\u22121(z \u2212b+) and (AD\n0 )\u22121(z) = (AD\nb )\u22121(z \u2212b\u2212).",
    "chunk_index": 6,
    "start_char": 15262,
    "end_char": 17862,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "holds for arbitrary unlimited bids b that BS\n0(pmin) = BS\nb(pmin) \u2212b+ and BD\n0 (pmax) =\nBD\nb (pmax) \u2212b\u2212where b+ and b\u2212are the element-wise positive and negative part of b.\nFurther, it holds that BS\n0(p) = BS\nb(p) for p > pmin and BD\n0 (p) = BD\nb (p) for p < pmax.\nIn conclusion, we receive\n(AS\n0)\u22121(z) = (AS\nb)\u22121(z \u2212b+) and (AD\n0 )\u22121(z) = (AD\nb )\u22121(z \u2212b\u2212).\n(1)\n3.2\nTrading objective\nNow, the trader has the gain of\nG(b; v) = (P \u2217\nb )\u2032(s \u2299b)\n|\n{z\n}\ntrading revenue\n\u2212\u03c4 \u2032(s \u2299b|\u00b7|)\n|\n{z\n}\ntransaction costs\n\u2212((v \u2212S\u2032b\n| {z }\nimbalance\n)|\u00b7|)\u2032R\n|\n{z\n}\nimbalance penalty\n(2)\n=\n4\nX\ni=0\nP \u2217\nb,isibi \u2212\n4\nX\ni=0\n\u03c4isi|bi| +\n4\nX\nj=1\n\f\f\f\f\fvj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f Rj\n(3)\nwhere \u2299is the element-wise multiplication (also known as Hadamard product), \u03c4 is a\ntransaction cost vector, S = (Si,j) = (14, I4)\u2032 is a 5x4 dimensional summation matrix and\ns = S\u203214/4 = (s0, . . . , s4)\u2032 = (1, .25, .25, .25, .25)\u2032 a summation vector which transfers MW\nto MWh for the corresponding markets (it contains the length of the delivery product pe-\nriod for each auction). R = (R1, . . . , R4) is the imbalance penalty price and z|\u00b7| a element-\nwise absolute value, i.e. z|\u00b7| = z+ + z\u2212. The imbalance price is a cross-control area uni-\nform balancing energy price (in German named REBAP: Regelzonen\u00a8ubergreifender Ein-\nheitlicher BilanzAusgleichsnergiePreis). In practice \u03c4 = (\u03c40, . . . , \u03c44)\u2032 = (\u03c4DA, \u03c4IA, . . . , \u03c4IA)\u2032\nsatis\ufb01es \u03c4DA \u2264\u03c4IA, thus trading in the hourly day-ahead market is not more expensive\nthan trading the same volume in the intraday opening auction. Nowadays, the EPEX\nnegotiate with all market participants its own trading fees.\nNote that strict market regulations require that market participants have to avoid\nsystem imbalance. Thus, they have to satisfy the linear imbalance constraint\nv \u2212S\u2032b = 0\n(4)\n8\n\nas we are not allowed to speculate against the imbalance price. Thus, under constraint\n(4) the gain equation (2) simpli\ufb01es to\nG(b) = (P \u2217\nb )\u2032(s \u2299b) \u2212\u03c4 \u2032(s \u2299b|\u00b7|) =\n4\nX\ni=0\nP \u2217\nb,isibi \u2212\n4\nX\ni=0\n\u03c4isi|bi|.\n(5)\nG does not depend on v any more, but v is contained in the constraint (4).\nIn practice, we want to maximize G with respect to b.\nHowever, one of the key\nchallenges is to describe adequately the price P \u2217\nb . This is a multivariate random variable\nwhich depends on the own bidding impact due to the bid b. To simplify this task we\nconsider the \u2019no bidding\u2019 situation with b = 0 as baseline. Hence, we de\ufb01ne \u2206b = P \u2217\nb \u2212P \u2217\n0\nas the price impact due to the trading of volume b. The hope is that we can easier access\n\u2206b and P \u2217\n0 than P \u2217\nb .",
    "chunk_index": 7,
    "start_char": 17506,
    "end_char": 20045,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "key\nchallenges is to describe adequately the price P \u2217\nb . This is a multivariate random variable\nwhich depends on the own bidding impact due to the bid b. To simplify this task we\nconsider the \u2019no bidding\u2019 situation with b = 0 as baseline. Hence, we de\ufb01ne \u2206b = P \u2217\nb \u2212P \u2217\n0\nas the price impact due to the trading of volume b. The hope is that we can easier access\n\u2206b and P \u2217\n0 than P \u2217\nb .\nWe rewrite G to\nG(b) = (P \u2217\n0 + \u2206b)\u2032(s \u2299b) \u2212\u03c4 \u2032(s \u2299b|\u00b7|) =\n4\nX\ni=0\n(P \u2217\n0,i + \u2206b,i)sibi \u2212\n4\nX\ni=0\n\u03c4isi|bi|.\n(6)\nNote that \u2206b is a highly non-linear function in b as the supply and demand curves are\nnon-linear what can be observed in Figure 3. Note that even under \u2206b = 0 the remaining\nequation is non-linear in b, as the absolute value is a non-linear function.\nNow, let us assume that the market participant wants to maximize a risk functional\nR(G). Typically, this could be R(G) = E[G] or \u00b5-\u03c3-utility R(G) = E[G] \u2212\u03b3Var[G], but\nexpected shortfall (CVaR) or value-at-risk (VaR) measures are plausible options as well.\nThus, the maximization problem is\nbopt =\narg max\nb\u2208R5 with v=S\u2032b\nR(G(b)).\n(7)\nNote that choosing non-linear risk measures does not seriously increase the complexity of\nthe trading problem. Thus, even in this relatively simple setting we are facing a non-linear\noptimization problem due to the non-linearity of G. This holds even if we choose R as a\nlinear functional, e.g. R = E.\nThe linear imbalance constraint (4) allows us to simplify the optimization problem (7)\nsigni\ufb01cantly. As v \u2212S\u2032b = 0 yields immediately that bi = vi \u2212b0 for i > 0, that is to say\neb = (2b0, v)\u2032 \u2212b01 = (b0, v1 \u2212b0, . . . , v4 \u2212b0)\u2032,\n(8)\nand we highlight that eb is a linear function in b0 under the imbalance constraint (4). We\nreceive the one-dimensional optimization problem\nbopt = (2bopt\n0 , v)\u2032 \u2212bopt\n0 1 with bopt\n0\n= arg max\nb0\u2208R\nR(G(ev + b0ceb))\n(9)\n9\n\nFor eb the latter term in (6) are the transaction costs T (b0) which can be simpli\ufb01ed to\nT (b0) = \u03c4 \u2032 \u0010\ns \u2299eb|\u00b7|\u0011\n= \u03c40|b0| + 1\n4\n4\nX\ni=1\n\u03c4i|vi \u2212b0|.\n(10)\nIn addition, we want to present a decomposition of G(eb) into four interpretable com-\nponents. Disentangling the DA and IA part by remembering the de\ufb01nitions of eb and s\ngives\nG(eb) =\n\u0010\nP \u2217\n0,0 + \u2206eb,0\n\u0011\nb0 + 1\n4\n4\nX\ni=1\n\u0010\nP \u2217\n0,i + \u2206eb,i\n\u0011\n(vi \u2212b0) \u2212T (b0)\n= 1\n4\n4\nX\ni=1\nP \u2217\n0,ivi\n|\n{z\n}\nIA revenue\n+\n \nP \u2217\n0,0 \u22121\n4\n4\nX\ni=1\nP \u2217\n0,i\n!\nb0\n|\n{z\n}\nDA-IA arbitrage\n+ \u2206eb,0b0 + 1\n4\n4\nX\ni=1\n\u2206eb,i(vi \u2212b0)\n|\n{z\n}\nDA&IA market impact\n\u2212\nT (b0).",
    "chunk_index": 8,
    "start_char": 19655,
    "end_char": 22090,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "by remembering the de\ufb01nitions of eb and s\ngives\nG(eb) =\n\u0010\nP \u2217\n0,0 + \u2206eb,0\n\u0011\nb0 + 1\n4\n4\nX\ni=1\n\u0010\nP \u2217\n0,i + \u2206eb,i\n\u0011\n(vi \u2212b0) \u2212T (b0)\n= 1\n4\n4\nX\ni=1\nP \u2217\n0,ivi\n|\n{z\n}\nIA revenue\n+\n \nP \u2217\n0,0 \u22121\n4\n4\nX\ni=1\nP \u2217\n0,i\n!\nb0\n|\n{z\n}\nDA-IA arbitrage\n+ \u2206eb,0b0 + 1\n4\n4\nX\ni=1\n\u2206eb,i(vi \u2212b0)\n|\n{z\n}\nDA&IA market impact\n\u2212\nT (b0).\n| {z }\nTransaction costs\n(11)\nThe four interpretable components are: a revenue term, an arbitrage term, a market\nimpact term and the transactions costs T . We will interpret them in more detail in the\nnext section. Here, we only want to point out that the IA revenue term does not depend\non the bid b0. However, in practice it usually contributes the most to the gain G, but it\ncannot be in\ufb02uenced by a trader.\n4\nTrading strategies\n4.1\nIntraday Auction only\nA straightforward strategy is to bid the volume v only in the IA market.\nbIA-only = (0, v) = (0, v1, . . . , v4)\n(12)\nObviously, IA-only seems odd if \u03c4DA < \u03c4IA holds. Moreover, we observe that the DA\nauctions have much larger volumes than the IA auctions. However, a simple counterpart\nstrategy DA-only that bids only at the DA auction and zero volume at the IA auctions\nis only possible if v is constant, i.e. v1 = . . . = v4. Once we start having ramps in at\nleast one asset, we may face ramps in the accepted bids as well. Under the imbalance\nconstraint (4) this imbalance is removed. In our setting this forces us to bid at the IA\nauction in such situations.\n4.2\nMinimal transaction costs\nFrom our point of view the intuitive DA-only counterpart to IA-only is the bidding strat-\negy that minimizes transaction costs under the \u03c4DA \u2264\u03c4IA assumption. Intuitively this\n10\n\napproach trades as much volume in the cheaper (from the transaction cost point of view)\nDA market, and balances the remaining power in the IA auction. Now, we derive the min-\nimal transaction cost strategy. The transaction costs T (b0) in (10) is a convex, piecewise\nlinear function. Thus, there exists a minimum. The minimizer of T is the \u03c4 \u2299s-weighted\nmedian of (0, v)\u2032 which we de\ufb01ne as the TC-min strategy\nbTC-min = ebTC-min = (bTC-min,0, v1 \u2212bTC-min,0, . . . , v4 \u2212bTC-min,0).\n(13)\nWe see that the minimal transaction costs strategy depends on the transaction costs,\nso in fact on \u03c4DA and \u03c4IA. For example, it is easy to derive that if we have v with vi \u2264vi+1\nand v1 \u22650 then the optimal volume bTC-min,0 is\nbTC-min,0 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0,\nif \u03c4DA > \u03c4IA\nv1,\nif \u03c4DA \u2264\u03c4IA < 2\u03c4DA\nv2,\nif 2\u03c4DA \u2264\u03c4IA\n(14)\nwhere the limiting cases \u03c4IA = \u03c4DA and \u03c4IA = 2\u03c4DA are not unique.",
    "chunk_index": 9,
    "start_char": 21783,
    "end_char": 24297,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "We see that the minimal transaction costs strategy depends on the transaction costs,\nso in fact on \u03c4DA and \u03c4IA. For example, it is easy to derive that if we have v with vi \u2264vi+1\nand v1 \u22650 then the optimal volume bTC-min,0 is\nbTC-min,0 =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0,\nif \u03c4DA > \u03c4IA\nv1,\nif \u03c4DA \u2264\u03c4IA < 2\u03c4DA\nv2,\nif 2\u03c4DA \u2264\u03c4IA\n(14)\nwhere the limiting cases \u03c4IA = \u03c4DA and \u03c4IA = 2\u03c4DA are not unique. Then, any element in\n[0, v1] and [v1, v2] is optimal, respectively. The \ufb01rst case in (14) is not realistic in practice.\nFor the remaining cases Figure 4 illustrates the optimal bidding strategy for selected\nvolume settings v. Those remaining cases in (14) are realistic and may occur in practice.\nA trader with relatively cheap IA transaction trading costs that faces case 2 should trade\naccording to the minimum transaction cost strategy only the lowest production among the\nfour quarter hours. This would avoid any buy transactions in the IA market.\n4.3\nOptimal expected pro\ufb01t trading\nIn this section, we analyse the optimal strategy for a risk neutral trader which maximizes\nthe objective (7) with respect to the expectation of R = E. In this case, for equation (11)\nwe receive\nE[G(eb)] = 1\n4\n4\nX\ni=1\nE[P \u2217\n0,i]vi\n|\n{z\n}\nExpected IA revenue\n+\n \nE[P \u2217\n0,0] \u22121\n4\n4\nX\ni=1\nE[P \u2217\n0,i]\n!\nb0\n|\n{z\n}\nExpected DA-IA arbitrage\n+ E[\u2206eb,0]b0 + 1\n4\n4\nX\ni=1\nE[\u2206eb,i](vi \u2212b0)\n|\n{z\n}\nExpected DA&IA market impact\n\u2212\nT (b0).\n| {z }\nTransaction costs\n(15)\nThis decomposition into four interpretable components corresponds to the expectations in\n(11). Only the transactions costs T as studied in Section 4.2 remain untouched.\nThe second term in (15) is the expected arbitrage opportunity between the day-ahead\nand the intraday auctions. If the price di\ufb00erence for trading 1 MWh M(P \u2217\n0) = E[P \u2217\n0,0] \u2212\n11\n\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(a) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (2, 3, 5, 8)\u2032\n-2\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(b) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (3, \u22122, 8, 5)\u2032\n-4\n-2\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(c) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (3, \u22122, 8, \u22125)\u2032\n-8\n-6\n-4\n-2\n0\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(d) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (\u22123, \u22122, \u22128, \u22125)\u2032\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(e) 2\u03c4DA \u2264\u03c4IA,\nv",
    "chunk_index": 10,
    "start_char": 23908,
    "end_char": 26268,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "Q4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(c) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (3, \u22122, 8, \u22125)\u2032\n-8\n-6\n-4\n-2\n0\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(d) \u03c4DA < \u03c4IA < 2\u03c4DA,\nv = (\u22123, \u22122, \u22128, \u22125)\u2032\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(e) 2\u03c4DA \u2264\u03c4IA,\nv = (2, 3, 5, 8)\u2032\n-2\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(f) 2\u03c4DA \u2264\u03c4IA,\nv = (3, \u22122, 8, 5)\u2032\n-4\n-2\n0\n2\n4\n6\n8\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(g) 2\u03c4DA \u2264\u03c4IA,\nv = (3, \u22122, 8, \u22125)\u2032\n-8\n-6\n-4\n-2\n0\nDelivery Period\nVolume\nQ1\nQ2\nQ3\nQ4\nDA sell\nDA buy\nIA sell\nIA buy\nv\n(h) 2\u03c4DA \u2264\u03c4IA,\nv = (\u22123, \u22122, \u22128, \u22125)\u2032\nFigure 4: Illustration of optimal transaction cost minimal strategies b for di\ufb00erent trans-\naction costs \u03c4DA and \u03c4IA, and target trade volume v.\n1\n4\nP4\ni=1 E[P \u2217\n0,i] is large, there are large arbitrage opportunities. In e\ufb03cient markets the\nmarket e\ufb03ciency assumption\nM(P \u2217\n0) = 0 \u21d4E[P \u2217\nb,0] = 1\n4\n4\nX\ni=1\nE[P \u2217\nb,i]\n(16)\nholds. Thus, if (16) holds the trader cannot expect any improvement of the expected gain\nE[G] from the second term. The third term in (15) represents the market impact due to\nthe trading. If v is close to 0 then we do not expect a big impact. However, for large\nvolumes this should be relevant.\nThe expected market impact E[\u2206eb,i] in (15) depends non-linearly on b0. The non-\nlinear behaviour results from the non-linearity of the auction curves (see e.g. Figure 3).\nIt cannot be analysed in more detail without making further assumptions. Therefore, we\nintroduce the linear expected market impact assumption\nE[\u2206eb] = a \u2299eb\n(17)\nwhere a = (a0, . . . , a4) is the expected linear impact to analyze a mathematically tractable\nspecial case. Note that a is the expected linear impact and may depend on the prices P \u2217\n0,\n12\n\nbE\nbE-Me\ufb00\nbE-LinImp\nbE-LinImpMe\ufb00\nbE-NoImp\nbE-NoImpMe\ufb00= bTC-min (13)\nMarket e\ufb03ciency (16)\nLinear market impact (17)\nMarket e\ufb03ciency (16)\nNo market impact (18)\nMarket e\ufb03ciency (16)\nFigure 5: Special cases of analyzed models for a risk neutral trader R = E.\nbecause \u2206eb may depend on P \u2217\n0.\nIn addition to (17), we may also consider the no expected market impact assumption\ngiven by\nE[\u2206eb] = 0.\n(18)\nChoosing a = 0 in the linear market impact case (17) leads to the no market impact\nassumption. It is clear, that the no expected market impact assumption is not realistic\nwhen trading large volumes, but it helps us understanding the optimal trading behaviour\nof rational agents that trade small volumes.",
    "chunk_index": 11,
    "start_char": 25973,
    "end_char": 28478,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "risk neutral trader R = E.\nbecause \u2206eb may depend on P \u2217\n0.\nIn addition to (17), we may also consider the no expected market impact assumption\ngiven by\nE[\u2206eb] = 0.\n(18)\nChoosing a = 0 in the linear market impact case (17) leads to the no market impact\nassumption. It is clear, that the no expected market impact assumption is not realistic\nwhen trading large volumes, but it helps us understanding the optimal trading behaviour\nof rational agents that trade small volumes.\nThe three assumptions (16), (17) and (18) open multiple combination options that\nlead all to di\ufb00erent special cases that can be analyzed. They are visualized in Figure 5.\nAs discussed, the bE and bE-Me\ufb00have the non-linear impact part which does not allow us\nto draw further analytical conclusions and has to be solved numerically. We will analyze\nthe remaining solutions of Figure 5 in the next paragraphs.\nUnder the no market impact assumption (18) the expected gain equation (15) simpli\ufb01es\nto\nE[G(eb)] = 1\n4\n4\nX\ni=1\nE[P \u2217\n0,i]vi +\n \nE[P \u2217\n0,0] \u22121\n4\n4\nX\ni=1\nE[P \u2217\n0,i]\n!\n|\n{z\n}\n=M(P \u2217\n0 )\nb0 \u2212T (b0).\n(19)\nThe \ufb01rst term is the trading revenue of volume v in the IA markets. The second term\ncharacterizes the arbitrage opportunity of the DA and IA markets.\nIt is linear in b0.\nThe corresponding slope M(P \u2217\n0) represents the expected price relationship between the\nDA and IA markets. Thus, if T (b0) would be zero we would choose either b0 \u2192\u221eor\nb0 \u2192\u2212\u221edepending on sign of M(P \u2217\n0). Anyway, if the second term in (19) is zero then\n13\n\nthe optimum depends only on the transaction costs T (b0). This is the case if b0 = 0 or\nthe market e\ufb03ciency assumption (16) holds. Thus, we have the following theorem.\nTheorem 1. If the no expected market impact assumption (18) with a = 0 and the market\ne\ufb03ciency assumption (16) hold then the optimal trading strategy of a risk neutral trader\n(i.e. R = E) bE-NoImp is the minimal transaction cost strategy bTC-min.\nNow, let us discuss the more general minimum bE-NoImp which is characterized by\nbE-NoImp,0 of (19). For the a = 0 case, (19) is concave and piecewise linear as \u2212T is\nconcave and piecewise linear. Thus, if a global minimum bE-NoImp,0 exists, it is at one of\nthe 5 corners of the graph of T which are in (0, v)\u2032. Hence, in practice we can simply\nevaluate the function in all elements of (0, v)\u2032 such as max((0, v)\u2032) + 1 or min((0, v)\u2032) \u22121.\nIf the optimum is at the increased maximum or decreased minimum then there is no global\noptimum, otherwise the minimum correspond to the global minimum. In this case, we can\nexpress bE-NoImp,0 as an augmented weighted median.\nNow, we assume the linear expected market impact assumption (17) for (15) to analyze\nthe corresponding solution bE-LinImp.",
    "chunk_index": 12,
    "start_char": 28006,
    "end_char": 30714,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "simply\nevaluate the function in all elements of (0, v)\u2032 such as max((0, v)\u2032) + 1 or min((0, v)\u2032) \u22121.\nIf the optimum is at the increased maximum or decreased minimum then there is no global\noptimum, otherwise the minimum correspond to the global minimum. In this case, we can\nexpress bE-NoImp,0 as an augmented weighted median.\nNow, we assume the linear expected market impact assumption (17) for (15) to analyze\nthe corresponding solution bE-LinImp. It holds for the expected market impact that\nE[\u2206eb,0]b0 + 1\n4\n4\nX\ni=1\nE[\u2206eb,i](vi \u2212b0) = a\u20321b2\n0 +\n4\nX\ni=1\nai(v2\ni + 2vib0).\n(20)\nThen we receive for (15):\nE[G(eb)] = 1\n4\n4\nX\ni=1\n\u0000E[P \u2217\n0,i] + aivi\n\u0001\nvi +\n \nE[P \u2217\n0,0] \u22121\n4\n4\nX\ni=1\nE[P \u2217\n0,i] + 2aivi\n!\nb0 + a\u20321b2\n0\n|\n{z\n}\n=Q(b0;P \u2217\n0 ,v,a)=Q0(P \u2217\n0 ,v,a)+Q1(P \u2217\n0 ,v,a)b0+Q2(a)b2\n0\n\u2212T (b0)\n(21)\nwhere we introduce the quadratic polynomial Q(b0) = Q0 +Q1b0 +Q2b2\n0 for the \ufb01rst three\nterms. The coe\ufb03cient a\u20321 in front of the quadratic term in (21) is always non-positive.\nAs the transaction costs T (b0) are a piecewise linear function the quadratic term a\u20321b2\n0\nwill always dominate for b0 \u2192\u00b1\u221eif a\u20321 < 0. Thus, we have a unique maximum if there\nis linear, non-zero market impact.\nThis maximum has an explicit solution. It may be computed by evaluating all maxima\nof the piecewise quadratic functions choosing the corresponding maximum. Therefore,\nremember that T (b0) is a piecewise linear function with at most 6 di\ufb00erent slopes. Thus,\nwe have to compute at most 6 solutions of quadratic functions to receive the optimum.\nWe want to highlight that the maximum is not necessarily the maximum of Q or \u2212T .\nFigure 6 illustrates two possible cases. In the \ufb01rst example, the optimum is exactly at an\nelement of the (0, v)\u2032, namely at 2. In the second example, the optimum is not an element\nof ev.\n14\n\n-1.0\n-0.8\n-0.6\n-0.4\n-0.2\n0.0\nb0\nf(b0)\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFunctions f\nNeg. transaction costs \u2212T\nQuadratic term Q\nQ \u2212T\n(a) (Q0, Q1, Q2) = (\u22120.2, 0.15, \u22120.05)\n-1.0\n-0.8\n-0.6\n-0.4\n-0.2\n0.0\nb0\nf(b0)\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFunctions f\nNeg. transaction costs \u2212T\nQuadratic term Q\nQ \u2212T\n(b) (Q0, Q1, Q2) = (\u22120.2, 0.05, \u22120.05)\nFigure 6: Illustration of equation (21) for v = (2, 3, 5, 8)\u2032, \u03c4DA = 0.04 and \u03c4IA = 0.1 for\ndi\ufb00erent values of (Q0, Q1, Q2) with dashed lines that highlight the maxima values.\nFurther, we want to remark that bE-LinImpMe\ufb00has to be computed in the same way as\nbE-LinImp. There is no structural simpli\ufb01cation possible. Finally, note that the linear mar-\nket impact coe\ufb03cients a are unknown in practice and have to be estimated. Realistically,\nit should depend on the price P \u2217\n0 as well.",
    "chunk_index": 13,
    "start_char": 30265,
    "end_char": 32878,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "of equation (21) for v = (2, 3, 5, 8)\u2032, \u03c4DA = 0.04 and \u03c4IA = 0.1 for\ndi\ufb00erent values of (Q0, Q1, Q2) with dashed lines that highlight the maxima values.\nFurther, we want to remark that bE-LinImpMe\ufb00has to be computed in the same way as\nbE-LinImp. There is no structural simpli\ufb01cation possible. Finally, note that the linear mar-\nket impact coe\ufb03cients a are unknown in practice and have to be estimated. Realistically,\nit should depend on the price P \u2217\n0 as well. So that the market impact in spiky price regions\nis larger than in common market situations.\n4.4\nRisk-Averse strategies\nWe also consider numerical solutions of several risk averse agents. In detail, we consider:\nmean-variance utility, value-at-risk (VaR) and expected shortfall (CVaR). These risk mea-\nsures are well-known both in practice and in the literature [14\u201319]. In the mean-variance\nutility, we maximize the following risk function\nR(G) = E[G] \u2212\u03b3Var[G]\n(22)\nto estimate the optimal bidding vector bE- Var -U. Here, a very important feature is the\nrisk aversion parameter \u03b3. In our analysis we set arbitrarily \u03b3 = 0.25. The second risk\naverse function that we use to optimize the bidding vector is the value-at-risk (VaR)\nR(G) = VaR\u03b1(G) = inf{x \u2208R : FG(x) > \u03b1} = Q\u03b1(G)\n(23)\nwhich with the risk aversion parameter \u03b1 can be interpreted as an \u03b1-quantile of the\npredicted gain. The last utilized risk measure is the expected shortfall, also known as\nconditional value-at-risk (CVaR) with the following formula\nR(G) = CVaR\u03b1(G) = \u22121\n\u03b1\nZ \u03b1\n0\nVaR\u03b3(G)d\u03b3.\n(24)\n15\n\nIn the case of CVaR also the \u03b1 parameter takes the role of risk aversion parameter. Both\nfor VaR and CVaR we assume arbitrarily that \u03b1 = 0.05.\n5\nForecasting models for the price and market impact\nDue to the non-linearity of the purchase and sale curves, it is pretty hard to model directly\nthe impacted prices P \u2217\nb . Instead, we decided to model separately the not impacted price\nvector P \u2217\n0 and the price impact \u2206b due to the trading of volume b. In this section, we\ndescribe the utilized models.\n5.1\nPrice models\nLet us remind that P \u2217\n0 = (P \u2217\n0,0, P \u2217\n0,1, . . . , P \u2217\n0,4) and thus P \u2217\n0,d,h =\n\u0010\nP \u2217,DA\n0,d,h , P \u2217,IAq1\n0,d,h , . . . , P \u2217,IAq4\n0,d,h\n\u0011\n.\nFor the scenario optimization we need many price trajectories, and we obtain them by\nforecasting the expected prices and by bootstrapping then the in-sample errors. The \ufb01rst\nexpected price model that we consider is the well-known and widely utilized [36, 39, 40]\nthe naive model. It predicts todays prices by prices of yesterday on Tuesday, Wednesday,\nThursday and Friday, and the last week\u2019s prices on other weekdays.",
    "chunk_index": 14,
    "start_char": 32417,
    "end_char": 35022,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "the expected prices and by bootstrapping then the in-sample errors. The \ufb01rst\nexpected price model that we consider is the well-known and widely utilized [36, 39, 40]\nthe naive model. It predicts todays prices by prices of yesterday on Tuesday, Wednesday,\nThursday and Friday, and the last week\u2019s prices on other weekdays. Its formula is as\nfollows\nE\n\u0000P \u2217\n0,d,h\n\u0001\n=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nP \u2217\n0,d\u22127,h,\nDoWk\nd,h = 1 for k = 1, 6, 7,\nP \u2217\n0,d\u22121,h,\notherwise,\n(25)\nwhere d and h indicate the day and the hour of the delivery, and DoWk\nd,h is the day-of-\nthe-week dummy.\nThe second considered model is the autoregressive with exogenous variables estimated\nusing the ordinary least-squares, in the literature often called the expert model [36, 40\u2013\n45]. It is given by\nE\n\u0000P \u2217\n0,d,h\n\u0001\n= \u03b2h,1 \u2299P \u2217\n0,d\u22121,h + \u03b2h,2 \u2299P \u2217\n0,d\u22122,h + \u03b2h,3 \u2299P \u2217\n0,d\u22127,h\n|\n{z\n}\nautoregressive e\ufb00ects\n+\n\u03b2h,4 \u2299P \u2217\n0,d\u22121,24\n|\n{z\n}\nyesterday\u2019s last hour price\n+ \u03b2h,5 \u2299P \u2217\n0,d\u22121,min + \u03b2h,6 \u2299P \u2217\n0,d\u22121,max\n|\n{z\n}\nnon-linear e\ufb00ects\n+\n7\nX\ni=1\n\u03b2h,6+i \u2299DoWi\nd,h\n|\n{z\n}\nweekday dummies\n+ \u03b2h,14 \u2299Loadd,h + \u03b2h,15 \u2299Solard,h + \u03b2h,16 \u2299WindOnd,h + \u03b2h,17 \u2299WindO\ufb00d,h\n|\n{z\n}\nday-ahead forecasts of electricity generation/consumption\n+ \u03b2h,18 \u2299EUAd\u22122\n|\n{z\n}\nCO2e price\n+ \u03b2h,19 \u2299Coald\u22122 + \u03b2h,20 \u2299Gasd\u22122 + \u03b2h,21 \u2299Oild\u22122\n|\n{z\n}\nmost recent fuel prices\n,\n(26)\nwhere \u2299is the element-wise multiplication. The model is estimated separately for each\nof the 5 markets, however for convenience we use the vector notation. The regressors\n16\n\nconsidered in the model are not di\ufb00erent to the ones utilized in the broad EPF literature.\nWe use autoregressive e\ufb00ects of lag 1, 2 and 7, the last hour\u2019s price of the previous day,\nthe element-wise minimum and maximum price of the previous day, and the weekday\ndummies. Additionally, we use the day-ahead forecasts of electricity load, solar, wind\nonshore, and wind o\ufb00shore production. Each of the vectors contains the total forecasted\npower (MW) in the given time interval (hour or quarter-hour). We also feed the model\nwith the EUA (European Union Allowance) price which represents emission costs and the\nfuel prices: API2 coal, TTF natural gas and Brent oil. Here we use the settle price lagged\nby 2 as at the time of forecasting for day d, which is around 11:30 on day d \u22121, the settle\nprice for day d \u22121 is not yet available.\nThe price trajectories are obtained using the bootstrap method Efron [46] which deliv-\ners very satisfying results [37, 47]. One could use more complicated probabilistic models,\nbut as we already mentioned in this manuscript, it is out of scope of our research. Thus,\nwe receive the trajectories by adding the in-sample bootstrapped errors to the forecasted\nexpected price\nb\nP \u2217,m\n0,d,h =\n\\\nE\n\u0010\nP \u2217\n0,d,h\n\u0011\n+ b\u03b5m\nd,h for m = 1, . .",
    "chunk_index": 15,
    "start_char": 34701,
    "end_char": 37425,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "available.\nThe price trajectories are obtained using the bootstrap method Efron [46] which deliv-\ners very satisfying results [37, 47]. One could use more complicated probabilistic models,\nbut as we already mentioned in this manuscript, it is out of scope of our research. Thus,\nwe receive the trajectories by adding the in-sample bootstrapped errors to the forecasted\nexpected price\nb\nP \u2217,m\n0,d,h =\n\\\nE\n\u0010\nP \u2217\n0,d,h\n\u0011\n+ b\u03b5m\nd,h for m = 1, . . . , M\n(27)\nwhere b\u03b5m\nd,h are drawn with replacement in-sample residuals for day d and hour h, i.e. we\nsample from the set of b\u03b5j,h = P \u2217\n0,j,h\u2212b\nP \u2217\n0,j,h for j = 1, . . . , D. M is the number of predicted\ntrajectories and D is the number of in-sample days. Naturally M > D is possible.\n5.2\nMarket impact models\nThe market impact is modelled using the aggregated supply AS\n0(p) and demand AD\n0 (p)\ncurves. Here we make use of the fact that bidding in the auction-based markets causes\nshifts in the respective curves. The curves are naturally unavailable at the time of fore-\ncasting, and thus we need to model them. The modelling and forecasting of the bidding\ncurves has been already approached in the literature [31\u201335], but the models are rather\ncomplicated and time-consuming to estimate. Moreover, the obtained forecast of clearing\nprice P \u2217\n0 is not that accurate as the one obtained using e.g. the expert model. Hence,\nwe model the curves using a functional simple moving average of the aggregated curves.\nFor the recent K days, this is\nb\nAS\n0,d,h(p) = 1\nK\nK\nX\nk=1\nAS\n0,d\u2212k,h(p)\n(28)\nand\nb\nAD\n0,d,h(p) = 1\nK\nK\nX\nk=1\nAS\n0,d\u2212k,h(p).\n(29)\n17\n\n20\n25\n30\n35\n40\n45\n50\n55\n500\n0\n1000\n2000\n3000\nDay-Ahead Auction curves\ntrue demand\ntrue supply\navg. demand\navg. supply\npast demand\npast supply\n0\n2\n4\n6\n3000\n1500\n0\n1500\n3000\nIntraday Auction curves\n25\n30\n35\n40\n45\n50\n0\n20\n40\n60\n80\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n0\n20\n40\n60\n80\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVolume (GW)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrice (EUR/MWh)\nFigure 7: An example of the average curves\n\u0010\nb\nAS\n0,d,h\n\u0011\u22121\nand\n\u0010\nb\nAD\n0,d,h\n\u0011\u22121\nin the German\nmarket calculated using K = 28 days before 01.06.2017. The delivery periods are 12:00\nto 13:00 for the DA and 12:00 to 12:15 for the IA. The bottom plots are the zoomed-in\nversions of the top ones\nAn example of such modelling and forecasting of the aggregated curves is presented in\nFigure 7. We see that the model does not forecast the true curves perfectly, and especially\nthe location of the intersection of the forecasted curves may be very inaccurate. Still,\nremember we are only interested in the impacts which is essentially given by the shape of\nthe curves around the intersection.",
    "chunk_index": 16,
    "start_char": 36983,
    "end_char": 39607,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "and 12:00 to 12:15 for the IA. The bottom plots are the zoomed-in\nversions of the top ones\nAn example of such modelling and forecasting of the aggregated curves is presented in\nFigure 7. We see that the model does not forecast the true curves perfectly, and especially\nthe location of the intersection of the forecasted curves may be very inaccurate. Still,\nremember we are only interested in the impacts which is essentially given by the shape of\nthe curves around the intersection.\nMoreover, in general the forecasted price induced as intersection of b\nAS\n0,d,h and b\nAD\n0,d,h\ndoes not coincide with the forecasted price b\nP \u2217,m\n0,d,h of the considered price forecasting model,\ne.g. naive or expert. Therefore, we shift our curve predictions so that the resulting\nintersection coincides with b\nP \u2217,m\n0,d,h. More precisely, without loss of generality we will shift\nthe supply side curve b\nAS\n0,d,h. As b\nP \u2217,m\n0,d,h di\ufb00ers for all m = 1, . . . , M this curve shift will\ndepend on m as well. Thus, we de\ufb01ne the shift for the intersection adjustment by\nb\u03be\u2217,m\n0,d,h = b\nAD\n0,d,h\n\u0010\nb\nP \u2217,m\n0,d,h\n\u0011\n\u2212b\nAS\n0,d,h\n\u0010\nb\nP \u2217,m\n0,d,h\n\u0011\n.\n(30)\nLet us note that taking \u2212b\u03be\u2217,m\n0,d,h we could shift the demand curve and get the same result.\nNow, remember that we are interested in the estimated impact b\u2206m\nb,d,h = b\nP \u2217,m\nb,d,h\u2212b\nP \u2217,m\n0,d,h.\nThe b-impacted price b\nP \u2217,m\nb,d,h results from the intersection of the shifted b\nAS\nb,d,h with b\nAD\nb,d,h.\nTo compute this intersection we may \ufb01nd the root of\nb\nAS\nb,d,h (p) + b\u03be\u2217,m\n0,d,h \u2212b\nAD\nb,d,h (p) = b\nAS\n0,d,h (p) + b\u03be\u2217,m\n0,d,h \u2212b\nAD\n0,d,h (p) + b\n(31)\n18\n\nwhere the equality holds by (1) and the fact that b = b+ \u2212b\u2212. The non-linearity of the\nfunction makes it impossible to \ufb01nd the root of (31) in an analytical way. This leads to a\nneed of \ufb01nding it numerically. Using an optimizer is possible, but it is not very optimal\nsolution, as we require the solution for all m = 1, . . . , M and we later on have to optimize\nwith respect to b. This would result in using the inner optimization each iteration of the\nouter optimization with high computational costs. Therefore, we decided to consider\nb\nCd,h(p) = b\nAS\n0,d,h (p) \u2212b\nAD\n0,d,h (p)\n(32)\nand want to compute the intersection curve b\nC\u22121\nd,h.\nFor the calculation of b\nC\u22121\nd,h we take the full price grid and compute the volumes. For\nmodel (21) with linear market impact assumption, the linear market impact coe\ufb03cients a\nhave to be estimated as well. Obviously, this should be the slope of the intersection curves\nat the expected intersection as illustrated in Figure 8.",
    "chunk_index": 17,
    "start_char": 39124,
    "end_char": 41683,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "(p)\n(32)\nand want to compute the intersection curve b\nC\u22121\nd,h.\nFor the calculation of b\nC\u22121\nd,h we take the full price grid and compute the volumes. For\nmodel (21) with linear market impact assumption, the linear market impact coe\ufb03cients a\nhave to be estimated as well. Obviously, this should be the slope of the intersection curves\nat the expected intersection as illustrated in Figure 8.\nIn the application study, we estimate a by central di\ufb00erence of the inverse impact\ncurves b\nC\u22121\nd,h with incremental slope of average 5% of the market clearing volume of the\npast K days. Formally, this is\nbad,h = b\nC\u22121\nd,h\n \n1\nM\nM\nX\nm=1\nb\u03be\u2217,m\n0,d,h + \u03bd\n!\n\u2212b\nC\u22121\nd,h\n \n1\nM\nM\nX\nm=1\nb\u03be\u2217,m\n0,d,h \u2212\u03bd\n!\n15\n10\n5\n0\n5\n10\n15\n20\n500\n0\n1000\n2000\n3000\nDA impact estimation\n6\n4\n2\n0\n2\n4\n6\n3000\n1500\n0\n1500\n3000\nIA impact estimation\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n20\n0\n20\n40\n60\n80\nintersection curve\nlinear impact\nexpected intersection\n3\n2\n1\n0\n1\n2\n3\n20\n0\n20\n40\n60\n80\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVolume shift (GW)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImpacted price\nFigure 8: An example of the predicted intersection curves C\u22121\nd,h in the German market\nwith delivery on 01.06.2017. The delivery periods are 12:00 to 13:00 for the DA and 12:00\nto 12:15 for the IA. The bottom plots are the zoomed-in versions of the top ones. In\naddition, the estimated linear impact ba is visualized (red).\n19\n\nwith \u03bd =\n0.05\nK\nPK\nk=1 b\nAD\n0,d,h\n\u0012\n\\\nE\n\u0010\nP \u2217\n0,d,h\n\u0011\u0013\n.\nObviously, the 5% is an ad hoc choice, and\nmight be improved. However, our empirical study yield plausible results, see Figure 8.\nGiven the intersection curves, we can easily calculate the b\nP \u2217,m\nb,d,h for m = 1, . . . , M by\nevaluating b\nC\u22121\nd,h(b\u03be\u2217,m\n0,d,h + b), and thus also the b\u2206m\nb,d,h. The latter one is, however, not the\n\ufb01nal price impact. This is due to the fact that the DA and IA markets are in a sequential\norder. Therefore, the b0 bid in the DA market can also in\ufb02uence the prices in the IA\nmarket. We refer to it with the following impact adjustment\nb\u2206\u2217,m\nb,d,h = ( b\u2206m\nb0,d,h, b\u2206m\nb1,d,h + \u03b4 b\u2206m\nb0,d,h, . . . , b\u2206m\nb4,d,h + \u03b4 b\u2206m\nb0,d,h)\n(33)\nwhere \u03b4 \u22650 is a market e\ufb03ciency factor. If the two markets were ine\ufb03cient and fully\nindependent, we would use \u03b4 close to 0. However, in our study we assume a more realistic\nscenario of \u03b4 = 1. In other words, we assume that the markets are e\ufb03cient and a 1 EUR\nprice shift in the day-ahead auction results also in a 1 EUR price shift in the intraday\nauction.",
    "chunk_index": 18,
    "start_char": 41294,
    "end_char": 43725,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "+ \u03b4 b\u2206m\nb0,d,h)\n(33)\nwhere \u03b4 \u22650 is a market e\ufb03ciency factor. If the two markets were ine\ufb03cient and fully\nindependent, we would use \u03b4 close to 0. However, in our study we assume a more realistic\nscenario of \u03b4 = 1. In other words, we assume that the markets are e\ufb03cient and a 1 EUR\nprice shift in the day-ahead auction results also in a 1 EUR price shift in the intraday\nauction.\nIn order to evaluate the quality of the relatively simple curve forecasts, we consider\nalso a setting where we know the true curves in advance. This is naturally unrealistic,\nbut can help us understand the possible gain of using better curve forecasts. For the same\nreason we consider also an instance of a perfect price forecast what allows us to inspect\nthe highest possible and at the same time highly unreachable gain rate.\n6\nApplication: Forecasting and trading study\n6.1\nData and setting\nFor the purpose of application, we use the German market data from January 01, 2016 to\nDecember 31, 2020. We conduct a rolling window forecasting study, which is a standard\nprocedure in the EPF literature. The initial in-sample data consists of D = 730 days, i.e.\n2 years and the out-of-sample of 3 years, i.e. N = 1097 days.. Every day of the out-of-\nsample dates we simulate a realistic situation: we estimate the price models based on the\nmost recent D = 730 days, bootstrap the in-sample residuals to obtain M = 1000 price\ntrajectories, and forecast the aggregated curves using K = 28 last days. Based on them,\nwe optimize the assumed risk functions using the sequential least squares programming\n(SLSQP) algorithm implemented in scipy package in Python to derive the trading strategy\nbbd,h. For the transaction costs \u03c4DA = 0.05 EUR/MWh and \u03c4IA = 0.10 EUR/MWh are\nassumed.\nIn the optimization, multiple settings and volumes v are considered. We start with\nv = v14 which assume constant electricity generation or consumption over all hours with\n20\n\nv \u2208{1, 10, 100, 1000} both on the supply and demand sides. This allows us to observe\nthe impact of growing volumes. De\ufb01nitely more realistic are the v assumed to be 1% or\n5% of the day-ahead predicted German wind or solar generation and 1% or 5% of the\nday-ahead predicted German load. These portfolios are far more possible and of high\nconcern for practitioners.\nBasic summary statistics of the utilized v are presented in\nTable 1. We show only the 5% values as the 1% and constant are easy to derive. In total,\nwe consider 14 di\ufb00erent portfolios v and additionally we assume 2 settings concerning\nthe past participation in the market. In the \ufb01rst setting, we have a new market player\nthat bids the portfolio v as new in the market. In the second one, the market player\nis already in the market bidding the minimal transaction cost strategy, but they would\nlike to evaluate their current strategy.",
    "chunk_index": 19,
    "start_char": 43348,
    "end_char": 46170,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "as the 1% and constant are easy to derive. In total,\nwe consider 14 di\ufb00erent portfolios v and additionally we assume 2 settings concerning\nthe past participation in the market. In the \ufb01rst setting, we have a new market player\nthat bids the portfolio v as new in the market. In the second one, the market player\nis already in the market bidding the minimal transaction cost strategy, but they would\nlike to evaluate their current strategy. It means that in this setting the market player is\nrebidding the portfolio v in the market.\nIt is worth to mention the forecasting of P \u2217\n0 in both settings. That is to say, in the \ufb01rst\none the original historical price series are used as the market player is new in the market\nand did not impact the prices before with their own bids. It means that the original price\nseries are the P \u2217\n0. In the second setting however, the market player was already bidding\nthe v in the past and impacted the prices with their strategy. Here, the original price\nseries are the P \u2217\nbTC-min. Thus, for every v in the second setting we subtract from the prices\nthe impact of the trader caused by trading according to the bTC-min strategy. Then, we\nconduct the forecasting using the newly acquired arti\ufb01cial price series P \u2217\n0 which depends\non the past trading path.\nTo summarize, let us remind that we use 2 models for the price forecasting: the naive\nand expert and one model for the aggregated curves. Additionally, we use perfect forecasts\nfor prices and the curves. Then, we trade 14 various portfolios in 2 aforementioned settings.\nThe portfolios are traded using 10 strategies. Three of them require no optimization: IA-\nonly, TC-min, E-NoImp. The other seven: E-LinImp, E-LinImpMe\ufb00, E-Me\ufb00, E,\nE-Var-U, VaR and CVaR are optimized using the SLSQP algorithm.\nv\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n5% of wind\n551\n427\n15\n221\n429\n764\n2105\n5% of solar\n245\n372\n0\n0\n9\n394\n1624\n5% of load\n-2753\n472\n-3796\n-3151\n-2748\n-2368\n-1621\nTable 1: Basic summary statistics of selected hourly volumes v\u2032\nd,h14/4 (MWh). The values\nare derived using the data from January 01, 2016 to December 31, 2020.\n21\n\n6.2\nEvaluation\nAs the objective of this paper is not the electricity price forecasting, we present a detailed\nevaluation of the forecasting accuracy price models in Appendix B. Here, we only want to\nmention that for all accuracy measures the expert model shows clearly better predictive\naccuracy than the naive model. Thus, we expect the trading strategies based on the expert\nmodel to perform better than those based on the naive forecasting model.\nFor the evaluation of the bidding strategies bbd,h we calculate an actual gain\neG(bbd,h) = (P \u2217\n0 + \u2206bbd,h)\u2032(s \u2299bbd,h) \u2212\u03c4 \u2032(s \u2299bb|\u00b7|\nd,h)\n(34)\n=\n4\nX\ni=0\n(P \u2217\n0,i + \u2206bbd,h,i)sibbi,d,h \u2212\n4\nX\ni=0\n\u03c4isi|bbi,d,h|\n(35)\nand for convenience and a",
    "chunk_index": 20,
    "start_char": 45732,
    "end_char": 48525,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "the expert model shows clearly better predictive\naccuracy than the naive model. Thus, we expect the trading strategies based on the expert\nmodel to perform better than those based on the naive forecasting model.\nFor the evaluation of the bidding strategies bbd,h we calculate an actual gain\neG(bbd,h) = (P \u2217\n0 + \u2206bbd,h)\u2032(s \u2299bbd,h) \u2212\u03c4 \u2032(s \u2299bb|\u00b7|\nd,h)\n(34)\n=\n4\nX\ni=0\n(P \u2217\n0,i + \u2206bbd,h,i)sibbi,d,h \u2212\n4\nX\ni=0\n\u03c4isi|bbi,d,h|\n(35)\nand for convenience and a better comparability among various v we report the average\ngain in EUR/MWh\neG =\n1\n24N\n24\nX\nh=1\nN\nX\nd=1\neG(bbd,h)\nv\u2032\nd,h14/4.\n(36)\nTo draw statistically signi\ufb01cant conclusions, we perform additionally a two sample boot-\nstrap test to compare the performance of bbd,h obtained using di\ufb00erent models and strate-\ngies. Let A and B denote two strategies bbA\nd,h and bbB\nd,h. For each model pair, we com-\npute the p-value of two one-sided tests. In the \ufb01rst one we consider the null hypothesis\nH0 : E\n\u0010\neG\n\u0010\nbbA\nd,h\n\u0011\u0011\n> E\n\u0010\neG\n\u0010\nbbB\nd,h\n\u0011\u0011\n, and in the second the reverse H0 : E\n\u0010\neG\n\u0010\nbbA\nd,h\n\u0011\u0011\n\u2264\nE\n\u0010\neG\n\u0010\nbbB\nd,h\n\u0011\u0011\n.\nLet us note that such constructed evaluation measure of the bidding strategies may\nfavour the E strategy as it actually optimizes the expected gain. This, however, cannot be\navoided as the E-Var-U and CVaR are not elicitable [48] which means that they cannot\nbe evaluated in a one step decision approach. We could additionally evaluate the \u03b1 = 5%-\nquantile of the actual gain what is the optimization goal of the VaR strategy, but we do\nnot do so for the sake of brevity.\n6.3\nResults\nTables 2 and 3 present the average actual gain eG of all strategies b and portfolios v in both\nconsidered settings. The tables are split to two parts \u2013 the supply and the demand. In the\n\ufb01rst case, we want to maximize the price, in the second one we want to minimize it. We\nobserve that overall the TC-min benchmark performs pretty well and the IA-only very\nbad, especially for the bigger portfolios. This is caused mainly by much lower liquidity\nin the IA market and thus a much higher price impact caused by the volume. Similarly,\nthe E-NoImp strategy fails to deliver satisfying results for bigger volumes. However, it\n22\n\nperforms best for small volumes where the no market impact assumption is met. Replacing\nthe no market impact (18) with the linear market impact (17) assumption keeps the very\ngood performance for small volumes and improves it substantially for larger volumes. By\nnot making any additional assumption on the price impact, i.e. considering the E strategy,\nwe improve the performance for large volumes even more. However, the best strategy for\nlarge volumes is the one assuming the market e\ufb03ciency (16) \u2013 E-Me\ufb00. Moreover, it is\nas good for the naive forecasts as for the expert ones.",
    "chunk_index": 21,
    "start_char": 48076,
    "end_char": 50831,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "the no market impact (18) with the linear market impact (17) assumption keeps the very\ngood performance for small volumes and improves it substantially for larger volumes. By\nnot making any additional assumption on the price impact, i.e. considering the E strategy,\nwe improve the performance for large volumes even more. However, the best strategy for\nlarge volumes is the one assuming the market e\ufb03ciency (16) \u2013 E-Me\ufb00. Moreover, it is\nas good for the naive forecasts as for the expert ones. This is perfectly sensible as this\nstrategy focuses on minimizing the overall price impact and transaction costs and ignores\nthe possible gain from the market arbitrage. Figure 9 presents an evidence that it is far\nmore pro\ufb01table to minimize the price impact rather than maximize the arbitrage. Let us\nremark that a much bigger improvement of the gain may be observed in the setting of\nrebidding the portfolios than in the setting of a new market player when compared to the\nTC-min strategy. This shows that our study may be particularly interesting for already\nexisting market players. However, this also indicates that the assumption of \u03b4 = 1 in\nequation (33) should be veri\ufb01ed in future research.\nThe enormous di\ufb00erence in EPF performance between the naive and expert models\nSupply/Sell (the higher the price the better)\nDemand/Buy (the lower the price the better)\nModel\nStrategy\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nwind\n5% of\nwind\n1% of\nsolar\n5% of\nsolar\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nload\n5% of\nload\nIA-only\n37.20\n36.87\n33.68 -61.20\n21.78 -214.07\n26.69 -84.74\n37.49\n37.84\n40.94 117.18\n66.31\n1055.64\nTC-min\n37.43 37.34 36.78\n32.55\n29.73\n25.48 32.69\n29.38\n37.57\n37.67\n38.23\n42.82\n42.13\n63.10\nnaive\nE-NoImp\n37.46\n37.25\n35.41 -13.95\n26.21\n-85.85\n30.30 -24.90\n37.39\n37.62\n39.45\n79.09\n53.30\n558.32\nE-LinImp\n37.46\n37.28\n36.59\n32.56\n29.52\n25.41\n32.55\n29.33\n37.39\n37.60\n38.37\n42.70\n42.14\n61.99\nE-LinImpMe\ufb00\n37.43\n37.33\n36.74\n32.44\n29.70\n25.27\n32.61\n29.23\n37.57\n37.67\n38.22\n42.81\n42.13\n64.44\nE-Me\ufb00\n37.43\n37.33\n36.76\n32.66 29.74\n25.59\n32.65 29.43\n37.57\n37.66 38.21\n42.62\n42.03\n61.58\nE\n37.46\n37.29\n36.64\n32.63\n29.60\n25.54\n32.60\n29.41\n37.39\n37.59\n38.33\n42.67\n42.09\n61.63\nE-Var-U\n37.37\n37.19\n36.40\n32.51\n29.44\n25.38\n32.39\n29.29\n37.53\n37.66\n38.39\n99.25\n56.88\n414.28\nVaR\n37.44\n37.30\n36.65\n32.48\n29.58\n25.38\n32.56\n29.26\n37.44\n37.62\n38.32\n42.86\n42.21\n62.54\nCVaR\n37.44\n37.27\n36.63\n32.55\n29.60\n25.44\n32.57\n29.36\n37.41\n37.61\n38.38\n42.86\n42.21\n62.68\nexpert\nE-NoImp 37.50\n37.29\n35.47 -10.24\n25.88 -107.88\n31.30\n2.68\n37.35\n37.57\n39.37\n76.70\n51.83\n517.79\nE-LinImp\n37.50\n37.32\n36.67\n32.60\n29.64\n25.48\n32.65\n29.38\n37.34 37.53\n38.27\n42.71\n42.11\n63.76\nE-LinImpMe\ufb00\n37.44\n37.34\n36.73\n32.35\n29.68\n25.16\n32.58\n29.13\n37.57\n37.67\n38.24\n42.97\n42.22\n69.79\nE-Me\ufb00\n37.43\n37.34\n36.77 32.67\n29.74\n25.57\n32.65\n29.42\n37.57\n37.67\n38.21 42.60 42.02\n61.41\nE\n37.50\n37.32\n36.69\n32.66\n29.66\n25.55\n32.66\n29.41\n37.34\n37.53\n38.26\n42.63\n42.07\n61.36\nE-Var-U\n37.45\n37.31\n36.66\n32.51\n29.60\n25.29\n32.58\n29.30\n37.40\n37.57\n38.33",
    "chunk_index": 22,
    "start_char": 50339,
    "end_char": 53302,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "31.30\n2.68\n37.35\n37.57\n39.37\n76.70\n51.83\n517.79\nE-LinImp\n37.50\n37.32\n36.67\n32.60\n29.64\n25.48\n32.65\n29.38\n37.34 37.53\n38.27\n42.71\n42.11\n63.76\nE-LinImpMe\ufb00\n37.44\n37.34\n36.73\n32.35\n29.68\n25.16\n32.58\n29.13\n37.57\n37.67\n38.24\n42.97\n42.22\n69.79\nE-Me\ufb00\n37.43\n37.34\n36.77 32.67\n29.74\n25.57\n32.65\n29.42\n37.57\n37.67\n38.21 42.60 42.02\n61.41\nE\n37.50\n37.32\n36.69\n32.66\n29.66\n25.55\n32.66\n29.41\n37.34\n37.53\n38.26\n42.63\n42.07\n61.36\nE-Var-U\n37.45\n37.31\n36.66\n32.51\n29.60\n25.29\n32.58\n29.30\n37.40\n37.57\n38.33\n57.16\n44.96\n218.01\nVaR\n37.47\n37.33\n36.69\n32.52\n29.64\n25.37\n32.61\n29.29\n37.42\n37.58\n38.25\n42.74\n42.14\n62.18\nCVaR\n37.46\n37.31\n36.69\n32.58\n29.64\n25.42\n32.61\n29.37\n37.41\n37.56\n38.25\n42.74\n42.13\n61.99\nTable 2: Average actual gain eG (EUR/MWh) of the considered strategies as a new market\nplayer. Colour indicates the performance column-wise (the greener, the better). With\nbold, we depicted the best values in each column\n23\n\nSupply/Sell (the higher the price the better)\nDemand/Buy (the lower the price the better)\nModel\nStrategy\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nwind\n5% of\nwind\n1% of\nsolar\n5% of\nsolar\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nload\n5% of\nload\nIA-only\n37.22\n37.01\n34.55 -37.59\n23.69 -151.61\n28.13 -52.56\n37.47\n37.70\n40.08 100.42\n59.55\n989.44\nTC-min\n37.45\n37.45\n37.45\n37.45\n30.98\n30.98\n33.65\n33.66\n37.55\n37.55\n37.55\n37.55\n38.98\n38.97\nnaive\nE-NoImp\n37.48\n37.37\n36.13\n0.12\n27.50\n-67.09\n31.19 -17.62\n37.37\n37.49\n38.72\n68.95\n49.55\n647.03\nE-LinImp\n37.48\n37.40\n37.34\n37.79\n30.86\n31.20\n33.56\n33.80\n37.37\n37.47\n37.61\n37.18\n38.78\n38.05\nE-LinImpMe\ufb00\n37.45\n37.45 37.52\n37.80 31.11\n31.28\n33.69\n33.87\n37.55\n37.55 37.44\n37.14\n38.68\n38.13\nE-Me\ufb00\n37.45\n37.45\n37.52 37.91\n31.11\n31.40 33.71 33.93\n37.55\n37.54\n37.45 37.10 38.66\n38.04\nE\n37.48\n37.41\n37.39\n37.86\n30.95\n31.32\n33.63\n33.89\n37.37\n37.46\n37.56\n37.13\n38.71\n38.06\nE-Var-U\n37.39\n37.34\n37.23\n37.74\n30.89\n31.24\n33.50\n33.79\n37.51\n37.54\n37.70\n87.15\n51.92\n381.80\nVaR\n37.46\n37.42\n37.43\n37.70\n30.97\n31.17\n33.62\n33.75\n37.43\n37.48\n37.53\n37.28\n38.80\n38.41\nCVaR\n37.45\n37.40\n37.44\n37.81\n31.02\n31.30\n33.66\n33.88\n37.39\n37.47\n37.55\n37.23\n38.76\n38.62\nexpert\nE-NoImp 37.52\n37.41\n36.17\n1.84\n27.17\n-87.07\n31.95\n-8.77\n37.33\n37.45\n38.65\n67.48\n48.61\n671.17\nE-LinImp\n37.52\n37.44\n37.42\n37.84\n31.00\n31.31\n33.66\n33.87\n37.33 37.40\n37.50\n37.13\n38.71\n38.10\nE-LinImpMe\ufb00\n37.45 37.46\n37.52\n37.72\n31.10\n31.24\n33.69\n33.79\n37.55\n37.55\n37.45\n37.24\n38.73\n38.70\nE-Me\ufb00\n37.45\n37.46\n37.52\n37.91\n31.11\n31.38\n33.71\n33.92\n37.55\n37.55\n37.46\n37.10\n38.66\n37.99\nE\n37.52\n37.44\n37.44\n37.88\n31.02\n31.35\n33.67\n33.89\n37.33\n37.40\n37.49\n37.12\n38.69\n37.99\nE-Var-U\n37.47\n37.45\n37.46\n37.80\n31.04\n31.27\n33.66\n33.85\n37.38\n37.45\n37.62\n50.15\n41.41\n176.12\nVaR\n37.48\n37.45\n37.47\n37.76\n31.03\n31.22\n33.66\n33.80\n37.41\n37.45\n37.47\n37.21\n38.75\n38.25\nCVaR\n37.48\n37.45\n37.47\n37.82\n31.05\n31.32\n33.68\n33.87\n37.39\n37.43\n37.45\n37.17\n38.72\n38.11\nTable 3: Average actual gain eG (EUR/MWh) of the considered strategies as an existing\nmarket player rebidding their portfolio v. Colour indicates the performance row-wise (the\ngreener, the better).",
    "chunk_index": 23,
    "start_char": 52816,
    "end_char": 55805,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "37.33\n37.40\n37.49\n37.12\n38.69\n37.99\nE-Var-U\n37.47\n37.45\n37.46\n37.80\n31.04\n31.27\n33.66\n33.85\n37.38\n37.45\n37.62\n50.15\n41.41\n176.12\nVaR\n37.48\n37.45\n37.47\n37.76\n31.03\n31.22\n33.66\n33.80\n37.41\n37.45\n37.47\n37.21\n38.75\n38.25\nCVaR\n37.48\n37.45\n37.47\n37.82\n31.05\n31.32\n33.68\n33.87\n37.39\n37.43\n37.45\n37.17\n38.72\n38.11\nTable 3: Average actual gain eG (EUR/MWh) of the considered strategies as an existing\nmarket player rebidding their portfolio v. Colour indicates the performance row-wise (the\ngreener, the better). With bold, we depicted the best values in each row\ndoes not always mean much better results in terms of trading gain. The di\ufb00erence in gain\nbetween the best naive-based and expert-based strategies is often a few cents, and under\nthe market e\ufb03ciency assumption it disappears. If we, however, consider the di\ufb00erence\nin total actual gain, it becomes clear that better price forecasts are advantageous. For\nexample, if we consider the 5% of wind portfolio in the rebidding setting and the CVaR\nstrategy, the di\ufb00erence of 0.02 EUR/MWh in average may seem not very high. Taking\ninto account the total gain this seeming small di\ufb00erence translates into over 290000 EUR\nof additional revenue in favour of the expert model over the analysed 1097 days. The\nadditional revenue is more than 10 times greater if we compare it with the benchmark\nTC-min strategy.\nThe potential advantage of utilizing perfect curve and price forecasts can be observed\nby comparing Tables 2 and 3 with Tables 5 and 6 from Appendix C. Having an oracle\nforecast of the intersection curves C\u22121\nd,h brings further a few cents of additional gain, but\nthe impact of oracle price forecast is far higher for all portfolios. In the example of 5%\nof wind portfolio it is 0.02 EUR/MWh using oracle curve forecast and 0.50 EUR/MWh\nusing additionally oracle price forecast. Thus, it is likely much more rewarding to improve\nthe price models rather than the curve models, especially given the wide EPF literature.\n24\n\nTC-min\nexpert E-NoImp\nexpert E-Me\ufb00\nexpert E\nexpert CVaR\nv =1% of solar\n42.5M\n43M\n43.5M\n44M\n44.5M\n45M\nRevenue (EUR)\n44.3M\n358K\n-1.18M\n-76K\n-70.9K 43.36M\n44.3M\n497K\n-953K\n-2.61M\n44.3M\n317K\n-926K\n-222K -74.5K 43.42M\n44.3M\n410K\n-1.04M\n-239K -81.3K 43.38M\n44.3M\n348K\n-870K\n-340K -78.8K 43.39M\n-1.26M\n-3.56M\n-1.15M\n-1.27M\n-1.21M\nTC-min\nexpert E-NoImp\nexpert E-Me\ufb00\nexpert E\nexpert CVaR\nv =5% of wind\n400M\n425M\n450M\n475M\n500M\n525M\nRevenue (EUR)\n522M -2.73M\n-69.2M\n-494K -739K 449.1M\n522M\n1.4M\n-33.9M\n-1.75B\n522M -2.44M\n-54.2M\n-9.87M -802K 454.9M\n522M -2.15M\n-54.2M\n-10.7M -814K 454.4M\n522M -2.21M\n-52.8M\n-12.4M -817K 454M\n-69.7M\n-1.78B\n-64.1M\n-64.9M\n-65.3M\nTC-min\nexpert E-NoImp\nexpert E-Me\ufb00\nexpert E\nexpert CVaR\nv =1% of load\n\u2212560M\n\u2212540M\n\u2212520M\n\u2212500M\n\u2212480M\nRevenue (EUR)\n-518M -3.48M\n-42.7M\n-218K -734K -565M\n-518M 1.03M\n-22M\n-165M\n-704.6M\n-518M -3.2M\n-31.7M\n-6.77M -811K -560.4M\n-518M -2.7M\n-31M\n-8.41M -829K -560.8M\n-518M -2.67M\n-29M\n-10.8M -854K -561.2M\n-42.9M\n-187M\n-38.5M\n-39.4M\n-39.8M\nIA revenue\nDA-IA arbitrage\nDA market impact\nIA market impact",
    "chunk_index": 24,
    "start_char": 55302,
    "end_char": 58319,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "-2.15M\n-54.2M\n-10.7M -814K 454.4M\n522M -2.21M\n-52.8M\n-12.4M -817K 454M\n-69.7M\n-1.78B\n-64.1M\n-64.9M\n-65.3M\nTC-min\nexpert E-NoImp\nexpert E-Me\ufb00\nexpert E\nexpert CVaR\nv =1% of load\n\u2212560M\n\u2212540M\n\u2212520M\n\u2212500M\n\u2212480M\nRevenue (EUR)\n-518M -3.48M\n-42.7M\n-218K -734K -565M\n-518M 1.03M\n-22M\n-165M\n-704.6M\n-518M -3.2M\n-31.7M\n-6.77M -811K -560.4M\n-518M -2.7M\n-31M\n-8.41M -829K -560.8M\n-518M -2.67M\n-29M\n-10.8M -854K -561.2M\n-42.9M\n-187M\n-38.5M\n-39.4M\n-39.8M\nIA revenue\nDA-IA arbitrage\nDA market impact\nIA market impact\nTransaction costs\nFinal gain\nFigure 9: Actual gain decomposition as in (11) for selected portfolios v and selected\nstrategies in the setting of rebidding the portfolio. The impact bars of E-NoImp strategy\npush the \ufb01nal gain to very low values. Therefore, they are not reported for the sake of\nlegibility.\nFigure 10 shows the results of the signi\ufb01cance tests for selected portfolios in the rebid-\nding setting. The results for remaining portfolios can be found in Appendix C. Strategy\nE-Me\ufb00is in most cases signi\ufb01cantly better or not signi\ufb01cantly worse than the others.\nAs seen before, its performance is rather undistinguishable between both price models.\nThis means that in both settings the market participants could signi\ufb01cantly improve their\nrevenue. Figure 11 presents the average daily weight of b0 for selected portfolios in the\nrebidding setting. For better clarity, we plot the risk neutral and the risk averse strategies\nseparately. Analogous plots for remaining portfolios can be found in Appendix C. All\nstrategies that do not neglect the market impact tend to vary higher between the con-\nsidered markets when trading smaller portfolios. On the contrary, they put signi\ufb01cantly\nhigher weight to the more liquid DA market when trading bigger portfolios. Additionally,\nthe strategies assuming market e\ufb03ciency behave much smoother, i.e. they do not exhibit\nas high spikes as the other strategies. A structural change in the weights can be observed\nin the beginning of year 2020. The algorithms started then putting a much higher weight\n25\n\nto the DA market what was caused by a signi\ufb01cant decrease of the number of o\ufb00ers in the\nIA market. This lead to much higher impact on the prices of self bids in the IA market.\nSince the curves are forecasted using only K = 28 last days, the algorithms could adjust\ntheir behaviour relatively fast. This shows a robustness of the proposed strategies for\nchanging market conditions.\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(a) v = 1 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR",
    "chunk_index": 25,
    "start_char": 57819,
    "end_char": 60879,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "E-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(a) v = 1 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(b) v = 1000 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(c) v = 5% of wind\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(d) v = 1% of solar\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(e) v = 1000 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 1% of load\nFigure 10: Results of the eG mean inequality test for selected portfolios v in the setting of\nrebidding the portfolio. The plots present p-values \u2014 the closer they are to zero (\u2192dark\ngreen), the more signi\ufb01cant the di\ufb00erence is between gains of X-axis strategy (better) and\ngains of the Y-axis strategy (worse).\n26\n\n7\nDiscussion on limitations and generalizations\n7.1\nPrice-volume bids\nIn the study we considered only unlimited bids. Thus, we assumed that the market player\nbids always pmin on the supply side and pmax on the demand side. However, of no less\nimportance are the price-volume bids where the market participant sets a price limit on the\nbid. These could be particularly interesting for such electricity producers or consumers\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5",
    "chunk_index": 26,
    "start_char": 60398,
    "end_char": 63338,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "where the market participant sets a price limit on the\nbid. These could be particularly interesting for such electricity producers or consumers\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 11: The average daily weight of b0 in relation to the whole b strategy for selected\nportfolios v in the setting of rebidding the portfolio.\nThe naive-based strategies are\nexcluded for better clarity\n27\n\nthat can quite \ufb02exibly manage their production or consumption, e.g. hydropower and\nbattery storages, or natural gas, wind and solar power plants. In order to use such price-\nvolume bids, the setting requires a slight modi\ufb01cation.\nLet (b, plim) denote the pair of vectors of bids with its corresponding price limits.\nPlacing the limited bid in the market may induce a change in the potential price grids PS\nand PD as it might happen that BS\ni (plim,i) = 0 or BD\ni (plim,i) = 0 for i \u2208{0, . . . , 4}. Such\nbids result in changing the shape of the supply AS\nb(p) and demand AD\nb (p) curves, as they\nwould be shifted in p = plim. Thus, an adjustment of intersection curve estimation would\nbe needed. Finally, to ful\ufb01l the imbalance constraint (4) one may need to introduce also\na price-limited volume vector v. A model with multiple price-volume bids would increase\nthe complexity of the trading problem even further.\n7.2\nImbalance constraint\nOur results rely heavily on the imbalance constraint. If R = E and the imbalance con-\nstraint (4) does not hold, then we receive\nE[G(b; v)] = E[P \u2217\nb ]\u2032(s \u2299b) \u2212\u03c4 \u2032 \u0010\ns \u2299b|\u00b7|\u0011\n\u2212\n\u0010\u0000v \u2212S\u2032b\n\u0001|\u00b7|\u0011\u2032\nE[R].\n(37)\nIf the imbalance v \u2212S\u2032b gets small, the imbalance penalty term gets small as well. If the\nsign of expected imbalance price E[R] is not in favour for the trader, they should have an\nintrinsic motivation to have v \u2212S\u2032b close to zero. However, in general forcing v \u2212S\u2032b = 0\ndoes not lead to the global optimumm, even if R is independent of P \u2217\nb .\n7.3\nStochastic trading volume \u2013 imbalance uncertainty\nThe results for transaction cost minimal trading also hold for uncertain volumes to trade.",
    "chunk_index": 27,
    "start_char": 62853,
    "end_char": 65623,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "expected imbalance price E[R] is not in favour for the trader, they should have an\nintrinsic motivation to have v \u2212S\u2032b close to zero. However, in general forcing v \u2212S\u2032b = 0\ndoes not lead to the global optimumm, even if R is independent of P \u2217\nb .\n7.3\nStochastic trading volume \u2013 imbalance uncertainty\nThe results for transaction cost minimal trading also hold for uncertain volumes to trade.\nNote that the consideration of stochastic trading volumes should be the preferred choice\nif the trader faces uncertainty in production or consumption. For wind and solar power\ntraders this is a natural situation, due to the meteorologically driven uncertainty in the\nproduction at time of the auctions. If we trade stochastic volumes, denoted by the random\nvector V = (V1, . . . , V4), then even the minimal transaction cost solution depends on\nrisk R.\nIf we choose R = E the transaction cost minimal solution is\nbopt\n0 (V ) = arg min\nb0\u2208R\nE[T (b0; V )] = arg min\nb0\u2208R\n \n\u03c40|b0|\n4\nX\ni=1\n\u03c4isiE |Vi \u2212b0|\n!\n.\n(38)\nThis can be regarded as joint \u03c4 \u2299s-weighted median of (0, V )\u2032.\n28\n\nWe can also show the generalized version of Theorem 1: If we assume no market impact\n(18) then we receive for eb = (2b0, V )\u2032 \u2212b01\nE[G(eb)] = E\n\u0002\nP \u2217\n0,0b0\n\u0003\n+ 1\n4\n4\nX\ni=1\nE\n\u0002\nP \u2217\n0,i(Vi \u2212b0)\n\u0003\n\u2212E[T (b0)]\n(39)\nSimilarly as in Section 4.3, we disentangle the DA and IA component and get\nE[G(eb)] = E[P \u2217\n0,0]b0 + 1\n4\n4\nX\ni=1\nE[P \u2217\n0,i(Vi \u2212b0)] \u2212E[T (b0)]\n(40)\n= 1\n4\n4\nX\ni=1\n\u0000E[P \u2217\n0,i]E[Vi] + Cov[P \u2217\n0,i, Vi]\n\u0001\n+ M(P \u2217\n0)b0 \u2212E[T (b0)].\n(41)\nby using E[P \u2217\n0,i(Vi \u2212b0)] = E[P \u2217\n0,i]E[Vi] \u2212b0E[P \u2217\n0,i] + Cov[P \u2217\n0,i, Vi].\nAgain, the market\ne\ufb03ciency assumption M(P \u2217\n0) = 0 leads to Theorem 1. Also without market e\ufb03ciency\nassumption, the solution b0,E-NoImp has the same structure based on the transaction cost\nminimal solution as the E[P \u2217\n0,i]E[Vi] and Cov[P \u2217\n0,i, Vi] terms are not impacted by b0.\nThe problematic part of this result is that it only holds under the imbalance con-\nstraint (4). For non-deterministic random variables (no Dirac measures) V \u2212S\u2032b = 0 can\nnever be satis\ufb01ed. To maintain the results from Theorem 1 for random volumes V for\nR = E we have to require that the expected imbalance constraint\nE[(V \u2212S\u2032b)|\u00b7|)\u2032R] =\n4\nX\nj=1\nE\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f Rj\n#\n(42)\ndoes not depend on b0. Unfortunately, assuming that (42) does not depend on b0 is a\nnon-trivial assumption. However, it holds\nE\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f Rj\n#\n= E\n\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f E[Rj] + Cov\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f , Rj\n#\n.",
    "chunk_index": 28,
    "start_char": 65232,
    "end_char": 67744,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "require that the expected imbalance constraint\nE[(V \u2212S\u2032b)|\u00b7|)\u2032R] =\n4\nX\nj=1\nE\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f Rj\n#\n(42)\ndoes not depend on b0. Unfortunately, assuming that (42) does not depend on b0 is a\nnon-trivial assumption. However, it holds\nE\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f Rj\n#\n= E\n\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f E[Rj] + Cov\n\"\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f , Rj\n#\n.\nHence, if we bid b such that P4\ni=0 si,jbi = med(Vj) holds for the median volumes med(Vj)\nfor j = 1, . . . , 4 it follows that\nE\n\f\f\f\f\fVj \u2212\n4\nX\ni=0\nsi,jbi\n\f\f\f\f\f = 0\n(43)\nholds. Additionally, we require that the absolute imbalance volume |Vj \u2212P4\ni=0 si,jbi| has\na correlation with Rj that does not depend on our bids. Note that it is not required that\nthe imbalance volume is uncorrelated with the imbalance price. Summarizing, Theorem 1\nholds under the no impact and market e\ufb03ciency assumptions if Cor[|Vj \u2212P4\ni=0 si,jbi|, Rj]\ndoes not depend on b0 and vj = med(Vj) is chosen for trading.\nHowever, again this solution is not the global optimum to the trading problem E[G(b; V )]\neven under no market impact assumption. The general solution requires a deeper investi-\ngation of the imbalance price R.\n29\n\n7.4\nSequential impact of the markets\nWe have accounted for the price impact caused by the sequential order of the DA and\nIA markets with the introduction of \u03b4 \u22650 parameter in equation (33). The idea is that\nmarket participants can react in the IA market to the conditions that appeared in the\nprevious one. In this paper we assumed \u03b4 = 1, but the discrepancy in the results between\nthe two considered settings suggests that a deeper investigation of this problem may be\nneeded. We would rather suspect that this parameter is not equal for every quarter-hour\n(or delivery period in general).\nMoreover, it may also depend on the price level and\npossibly other factors. It is clear that its structure is not trivial and requires a thorough\nanalysis. Therefore, in this study we limited ourselves to the assumption of \u03b4 = 1, and we\nleave the deeper analysis for the future research.\n7.5\nBeyond DA und IA\nThe results of Theorem 1 without market impact can be generalized easily to other settings\nwith two consecutive trading options where the latter market allows trading on an all\nequally sized delivery periods. In such a setting, we only have to adjust the summation\nmatrix S, such that S = (S1, S2) and s = S\u20321/1\u2032S1 for the two markets. Our setting\nresults from choosing S1 = 14 and S2 = I4. Similarly, we can model the setting in France\nand Great Britain where we currently have half-hourly Intraday opening auction. Here,\nwe simply choose S1 = 12 and S2 = I2. If we consider e.g. the future market with the\nday-ahead base product and the day-ahead auction, then we can model this by choosing\nS1 = 124 and S2 = I24.",
    "chunk_index": 29,
    "start_char": 67373,
    "end_char": 70163,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "we only have to adjust the summation\nmatrix S, such that S = (S1, S2) and s = S\u20321/1\u2032S1 for the two markets. Our setting\nresults from choosing S1 = 14 and S2 = I4. Similarly, we can model the setting in France\nand Great Britain where we currently have half-hourly Intraday opening auction. Here,\nwe simply choose S1 = 12 and S2 = I2. If we consider e.g. the future market with the\nday-ahead base product and the day-ahead auction, then we can model this by choosing\nS1 = 124 and S2 = I24.\nIf we traded in the futures market the day-ahead base and\npeak product then S1 = (124, 1peak) would be required with 1peak = (07, 112, 05). The\ntheory remains basically the same, e.g. for b = (b1, b2)\u2032 the imbalance constraint (4) leads\nto v = S\u2032b = S1b1 + S2b2 and for invertible S2 it holds b2 = S\u22121\n2 (v \u2212S1b1).\nThis\nimplies b = (b\u2032\n1, (S\u22121\n2 (v \u2212S1b1))\u2032)\u2032. The transaction cost minimal solution can be derived\nin the same way by minimizing T (b1). Again, this is also the optimal solution under no\nmarket impact and the market e\ufb03ciency assumption which leads to a generalized version\nof Theorem 1. Note that for more than two trading options the results are not easy to\nobtain as the problem has to be solved recursively.\n8\nConclusion\nThe paper raised the issue of optimal bidding of various electricity portfolios between\ntwo auction-based markets. The analysis included the market impact estimation what is\n30\n\nnecessary for large market players. We considered also the transaction costs and provided\ntheoretical insights regarding the minimal transaction costs strategy. The latter is optimal\nfor risk neutral traders under the relatively plausible assumptions of market e\ufb03ciency and\nno-market impacts if the volumes to trade are small. Additionally, we considered various\nstrategies with no/linear/non-linear market impact assumption as well as with the (no)\nmarket e\ufb03ciency assumption. The conducted study contained a number of portfolios that\nmimic the majority of electricity market participants like wind and solar power producers\n\u2013 from small to large ones. The results proved that even though we used very basic models\nto forecast the prices and curves, we could signi\ufb01cantly improve the overall revenue for\nthe majority of considered portfolios. Also, the analysis of gain components showed that\nthe crucial part of gain maximization is the price impact minimization, especially for large\nvolumes. The possible market arbitrage and the transaction costs are of marginal size\ncompared to the impact.\nWe conducted an extensive analysis of all aspects of the raised problem. The price\nformation and trading in the European electricity auction markets together with possible\nextensions and challenges were discussed. The paper leaves many open questions for future\nresearch, and we believe it can be a solid foundation for that. Especially that the attention\nof researchers and practitioners may be brought to this topic additionally by the recent\nlaunch of intraday opening auctions in further European countries [38].",
    "chunk_index": 30,
    "start_char": 69676,
    "end_char": 72694,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "compared to the impact.\nWe conducted an extensive analysis of all aspects of the raised problem. The price\nformation and trading in the European electricity auction markets together with possible\nextensions and challenges were discussed. The paper leaves many open questions for future\nresearch, and we believe it can be a solid foundation for that. Especially that the attention\nof researchers and practitioners may be brought to this topic additionally by the recent\nlaunch of intraday opening auctions in further European countries [38].\nAcknowledgments\nThis research article was partially supported by the German Research Foundation (DFG,\nGermany) and the National Science Center (NCN, Poland) through BEETHOVEN grant\nno.\n2016/23/G/HS4/01005 (to FZ and MN), and the National Science Center (NCN,\nPoland) through MAESTRO grant No. 2018/30/A/HS4/00444 (to FZ)\nReferences\n[1] J. Viehmann. State of the German Short-Term Power Market. Zeitschrift f\u00a8ur En-\nergiewirtschaft, 41(2):87\u2013103, Jun 2017.\n[2] R. Kiesel and F. Paraschiv. Econometric analysis of 15-minute intraday electricity\nprices. Energy Economics, 64:77\u201390, 2017.\n[3] M. Kremer, R. Kiesel, and F. Paraschiv. Intraday electricity pricing of night contracts.\nEnergies, 13(17):4501, 2020.\n[4] M. Kremer, R. Kiesel, and F. Paraschiv. An econometric model for intraday electricity\ntrading. Philosophical Transactions of the Royal Society A, 379(2202):20190624, 2021.\n31\n\n[5] S.-E. Fleten and T. K. Kristo\ufb00ersen. Stochastic programming for optimizing bid-\nding strategies of a Nordic hydropower producer. European Journal of Operational\nResearch, 181(2):916\u2013928, 2007.\n[6] N. L\u00a8ohndorf, D. Wozabal, and S. Minner. Optimizing trading decisions for hydro\nstorage systems using approximate dual dynamic programming. Operations Research,\n61(4):810\u2013823, 2013.\n[7] N. L\u00a8ohndorf and D. Wozabal. The Value of Coordination in Multimarket Bidding of\nGrid Energy Storage. Submitted, 2020.\n[8] B. Finnah. Optimal bidding functions for renewable energies in sequential electricity\nmarkets. OR Spectrum, pages 1\u201327, 2021.\n[9] B. Finnah, J. G\u00a8onsch, and F. Ziel. Integrated day-ahead and intraday self-schedule\nbidding for energy storage systems using approximate dynamic programming. Euro-\npean Journal of Operational Research, 2021.\n[10] T. K. Boomsma, N. Juul, and S.-E. Fleten. Bidding in sequential electricity markets:\nThe Nordic case. European Journal of Operational Research, 238(3):797\u2013809, 2014.\n[11] H. Kongelf, K. Overrein, G. Kl\u00e6boe, and S.-E. Fleten. Portfolio size\u2019s e\ufb00ects on gains\nfrom coordinated bidding in electricity markets. Energy Systems, 10(3):567\u2013591, 2019.\n[12] C. Kath and F. Ziel. Optimal Order Execution in Intraday Markets: Minimizing\nCosts in Trade Trajectories. arXiv preprint arXiv:2009.07892, 2020.\n[13] J. H. Kim and W. B. Powell. Optimal energy commitments with storage and inter-\nmittent supply. Operations research, 59(6):1347\u20131360, 2011.\n[14] M. Liu and F. F. Wu. Portfolio optimization in electricity markets. Electric Power\nsystems research, 77(8):1000\u20131009, 2007.\n[15] R. C. Garcia, V. Gonz\u00b4alez, J. Contreras, and J. E. Custodio. Applying modern port-\nfolio theory for a dynamic energy portfolio allocation in electricity markets. Electric\nPower Systems Research, 150:11\u201323, 2017.\n[16] R. P. Odeh, D. Watts, and M. Negrete-Pincetic. Portfolio applications in electricity\nmarkets review: Private investor and manager perspective trends. Renewable and\nSustainable Energy Reviews, 81:192\u2013204, 2018.\n[17] E. Canelas, T. Pinto-Varela, and B. Sawik.",
    "chunk_index": 31,
    "start_char": 72154,
    "end_char": 75682,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "in electricity markets. Electric Power\nsystems research, 77(8):1000\u20131009, 2007.\n[15] R. C. Garcia, V. Gonz\u00b4alez, J. Contreras, and J. E. Custodio. Applying modern port-\nfolio theory for a dynamic energy portfolio allocation in electricity markets. Electric\nPower Systems Research, 150:11\u201323, 2017.\n[16] R. P. Odeh, D. Watts, and M. Negrete-Pincetic. Portfolio applications in electricity\nmarkets review: Private investor and manager perspective trends. Renewable and\nSustainable Energy Reviews, 81:192\u2013204, 2018.\n[17] E. Canelas, T. Pinto-Varela, and B. Sawik. Electricity portfolio optimization for large\nconsumers: Iberian electricity market case study. Energies, 13(9):2249, 2020.\n[18] T. Dai and W. Qiao. Optimal bidding strategy of a strategic wind power producer\nin the short-term market. IEEE Transactions on Sustainable Energy, 6(3):707\u2013719,\n2015.\n[19] L. Baringo and A. J. Conejo. O\ufb00ering strategy of wind-power producer: A multi-stage\n32\n\nrisk-constrained approach. IEEE Transactions on Power Systems, 31(2):1420\u20131429,\n2015.\n[20] N. Mazzi, J. Kazempour, and P. Pinson. Price-taker o\ufb00ering strategy in electricity\npay-as-bid markets. IEEE Transactions on Power Systems, 33(2):2175\u20132183, 2017.\n[21] C. Kath and F. Ziel.\nThe value of forecasts: Quantifying the economic gains of\naccurate quarter-hourly electricity price forecasts. Energy Economics, 76:411\u2013423,\n2018.\n[22] T. Rintam\u00a8aki, A. S. Siddiqui, and A. Salo. Strategic o\ufb00ering of a \ufb02exible producer in\nday-ahead and intraday power markets. European Journal of Operational Research,\n284(3):1136\u20131153, 2020.\n[23] D. Wozabal and G. Rameseder. Optimal bidding of a virtual power plant on the Span-\nish day-ahead and intraday market for electricity. European Journal of Operational\nResearch, 280(2):639\u2013655, 2020.\n[24] E. K. Aasg\u02daard, S.-E. Fleten, M. Kaut, K. Midthun, and G. A. Perez-Valdes. Hy-\ndropower bidding in a multi-market setting. Energy Systems, 10(3):543\u2013565, 2019.\n[25] X. Ay\u00b4on, M. \u00b4A. Moreno, and J. Usaola. Aggregators\u2019 optimal bidding strategy in\nsequential day-ahead and intraday electricity spot markets. Energies, 10(4):450, 2017.\n[26] M. Narajewski and F. Ziel. Estimation and Simulation of the Transaction Arrival\nProcess in Intraday Electricity Markets. Energies, 12(23):4518, 2019.\n[27] N. Graf von Luckner and R. Kiesel. Modeling market order arrivals on the intraday\nmarket for electricity deliveries in Germany with the Hawkes process. Available at\nSSRN, 2020.\n[28] S. Glas, R. Kiesel, S. Kolkmann, M. Kremer, N. G. von Luckner, L. Ostmeier, et al.\nIntraday renewable electricity trading: advanced modeling and numerical optimal\ncontrol. Journal of Mathematics in Industry, 10(1):3, 2020.\n[29] M. Kozlova, S.-E. Fleten, and V. Hagspiel. Optimal timing and capacity choice under\nthe rate-of-return renewable energy support. MethodsX, 7:100828, 2020.\n[30] W. Li and F. Paraschiv. Modelling the evolution of wind and solar power infeed\nforecasts. Journal of Commodity Markets, page 100189, 2021.\n[31] F. Ziel and R. Steinert. Electricity price forecasting using sale and purchase curves:\nThe X-Model. Energy Economics, 59:435\u2013454, 2016.\n[32] F. Ziel and R. Steinert. Probabilistic mid-and long-term electricity price forecasting.\nRenewable and Sustainable Energy Reviews, 94:251\u2013266, 2018.\n[33] G. Mestre, J. Portela, A. M. San Roque, and E. Alonso. Forecasting hourly supply\ncurves in the Italian Day-Ahead electricity market with a double-seasonal SARMAHX\n33",
    "chunk_index": 32,
    "start_char": 75122,
    "end_char": 78567,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "page 100189, 2021.\n[31] F. Ziel and R. Steinert. Electricity price forecasting using sale and purchase curves:\nThe X-Model. Energy Economics, 59:435\u2013454, 2016.\n[32] F. Ziel and R. Steinert. Probabilistic mid-and long-term electricity price forecasting.\nRenewable and Sustainable Energy Reviews, 94:251\u2013266, 2018.\n[33] G. Mestre, J. Portela, A. M. San Roque, and E. Alonso. Forecasting hourly supply\ncurves in the Italian Day-Ahead electricity market with a double-seasonal SARMAHX\n33\n\nmodel.\nInternational Journal of Electrical Power & Energy Systems, 121:106083,\n2020.\n[34] S. Kulakov. X-model: further development and possible modi\ufb01cations. Forecasting,\n2(1):20\u201335, 2020.\n[35] M. Soloviova and T. Vargiolu. E\ufb03cient representation of supply and demand curves\non day-ahead electricity markets. Journal of Energy Markets, 14, 2021.\n[36] R. Weron. Electricity price forecasting: A review of the state-of-the-art with a look\ninto the future. International journal of forecasting, 30(4):1030\u20131081, 2014.\n[37] J. Nowotarski and R. Weron.\nRecent advances in electricity price forecasting: A\nreview of probabilistic forecasting. Renewable and Sustainable Energy Reviews, 81:\n1548\u20131568, 2018.\n[38] EPEX SPOT and ECC successfully launch Intraday auctions in Austria, Belgium,\nFrance and the Netherlands.\nhttps://www.epexspot.com/en/news/epex-spot-\nand-ecc-successfully-launch-intraday-auctions-austria-belgium-france-\nand-netherlands. Accessed: 2021-03-18.\n[39] F. J. Nogales, J. Contreras, A. J. Conejo, and R. Esp\u00b4\u0131nola. Forecasting next-day\nelectricity prices by time series models. IEEE Transactions on power systems, 17(2):\n342\u2013348, 2002.\n[40] F. Ziel and R. Weron. Day-ahead electricity price forecasting with high-dimensional\nstructures: Univariate vs. multivariate modeling frameworks. Energy Economics, 70:\n396\u2013420, 2018.\n[41] K. Maciejowska, B. Uniejewski, and T. Sera\ufb01n. PCA Forecast Averaging\u2014Predicting\nDay-Ahead and Intraday Electricity Prices. Energies, 13(14):3530, 2020.\n[42] B. Uniejewski, R. Weron, and F. Ziel. Variance stabilizing transformations for elec-\ntricity spot price forecasting. IEEE Transactions on Power Systems, 33(2):2219\u20132229,\n2017.\n[43] B. Uniejewski and R. Weron. E\ufb03cient forecasting of electricity spot prices with expert\nand LASSO models. Energies, 11(8):2039, 2018.\n[44] M. Narajewski and F. Ziel. Econometric modelling and forecasting of intraday elec-\ntricity prices. Journal of Commodity Markets, 19:100107, 2020.\n[45] B. Uniejewski, G. Marcjasz, and R. Weron. Understanding intraday electricity mar-\nkets: Variable selection and very short-term price forecasting using LASSO. Interna-\ntional Journal of Forecasting, 35(4):1533\u20131547, 2019.\n[46] B. Efron. Bootstrap Methods: Another Look at the Jackknife. The Annals of Statis-\ntics, pages 1\u201326, 1979.\n34\n\n[47] B. Uniejewski, G. Marcjasz, and R. Weron.\nOn the importance of the long-term\nseasonal component in day-ahead electricity price forecasting: Part ii\u2014probabilistic\nforecasting. Energy Economics, 79:171\u2013182, 2019.\n[48] T. Gneiting. Making and evaluating point forecasts. Journal of the American Statis-\ntical Association, 106(494):746\u2013762, 2011.\n[49] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estima-\ntion. Journal of the American statistical Association, 102(477):359\u2013378, 2007.\n[50] M. Narajewski and F. Ziel. Ensemble forecasting for intraday electricity prices: Sim-\nulating trajectories. Applied Energy, 279:115801, 2020.\nAppendix A\nA.1\nAbbreviations\nCVaR\nConditional value-at-risk\nDA\nDay-Ahead Auction\nEEX\nEuropean Energy Exchange\nEPEX\nEuropean Power Exchange\nEPF\nElectricity price forecasting\nE- Var -U\nMean-variance utility\nIA\nIntraday Auction\nIA-only\nstrategy to bid only in the IA market\nLinImp\nstrategy assuming the linear price impact",
    "chunk_index": 33,
    "start_char": 78084,
    "end_char": 81858,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "proper scoring rules, prediction, and estima-\ntion. Journal of the American statistical Association, 102(477):359\u2013378, 2007.\n[50] M. Narajewski and F. Ziel. Ensemble forecasting for intraday electricity prices: Sim-\nulating trajectories. Applied Energy, 279:115801, 2020.\nAppendix A\nA.1\nAbbreviations\nCVaR\nConditional value-at-risk\nDA\nDay-Ahead Auction\nEEX\nEuropean Energy Exchange\nEPEX\nEuropean Power Exchange\nEPF\nElectricity price forecasting\nE- Var -U\nMean-variance utility\nIA\nIntraday Auction\nIA-only\nstrategy to bid only in the IA market\nLinImp\nstrategy assuming the linear price impact\nMe\ufb00\nstrategy assuming the market e\ufb03ciency\nNoImp\nstrategy assuming no price impact\nREBAP\nCross-control area uniform balancing energy price (abbreviation from German)\nTC-min\nstrategy to bid at minimum transaction cost\nVaR\nValue-at-risk\nA.2\nNotation used in Sections 2-5\n\u2299\nelement-wise multiplication (Hadamard product)\nz|\u00b7|\nelement-wise absolute value, i.e. z|\u00b7| = z+ + z\u2212\nAD/S\nb,i\naggregated demand/supply curve impacted by own bid b in the ith market\nb\nAD/S\n0,d,h\nestimator for the not impacted aggregated demand/supply curve for day d and hour h\nai\nexpected slope of the linear impact in the ith market\na\nvector of expected slopes of the linear impact\nbad,h\nestimator of the slope of the linear impact on day d and hour h\n\u03b1\nrisk aversion parameter of VaR and CVaR\n35\n\nBD/S\nb,i\nnon-negative demand/supply volume bids impacted by own bid b\nBD/S\nb\nvector of non-negative demand/supply volume bids impacted by own bid b\nbi\nbid in the ith market\nb\nvector of bids\nb+/\u2212\nelement-wise positive/negative part of b\neb\neb = (2b0, v)\u2032 \u2212b01 \u2013 vector of bids linear in b0\nb\nCd,h\nestimated intersection curve for day d and hour h\n\u2206b\nprice impact due to the trading of volume b\nb\u2206\u2217,m\nb,d,h\nestimated price impact of mth scenario due to the trading of volume b\n\u03b4\nmarket e\ufb03ciency factor\nb\u03b5m\nd,h\nmth drawn with replacement in-sample residual for day d and hour h\nG\ngain function\neG\nactual gain function\n\u03b3\nrisk aversion parameter of E- Var -U strategy\ni\ni \u2208{0, . . . , 4} is a market index with 0 = DA, 1, . . . , 4 = IA\nM(P \u2217\n0 )\nexpected price di\ufb00erence between the DA and the IA\nm\nm = 1, . . . , M is a bootstrapping index\nP \u2217\nb,i\nmarket clearing price impacted by bid b\nP \u2217\nb\nvector of market clearing prices impacted by bid b\nb\nP \u2217,m\n0,d,h\nvector mth forecasted prices for day d and hour h\nPD/S\ni\nprice grid in the considered auction\nPD/S\nvector of price grids\npmax\nmaximum price bid, pmax,DA = pmax,IA = 3000\npmax\nvector of maximum price bids\npmin\nminimum price bid, pmin,DA = \u2212500, pmin,IA = \u22123000\npmin\nvector of minimum price bids\nR\nrisk functional\nRj\nimbalance price in jth quarter-hour\nR\nimbalance (REBAP) price\nS\nS = (Si,j) = (14, I4)\u2032 is a 5x4 dimensional summation matrix\ns\ns = S\u203214/4 = (1, .25, .25, .25, .25)\u2032 is a summation",
    "chunk_index": 34,
    "start_char": 81267,
    "end_char": 84078,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "grid in the considered auction\nPD/S\nvector of price grids\npmax\nmaximum price bid, pmax,DA = pmax,IA = 3000\npmax\nvector of maximum price bids\npmin\nminimum price bid, pmin,DA = \u2212500, pmin,IA = \u22123000\npmin\nvector of minimum price bids\nR\nrisk functional\nRj\nimbalance price in jth quarter-hour\nR\nimbalance (REBAP) price\nS\nS = (Si,j) = (14, I4)\u2032 is a 5x4 dimensional summation matrix\ns\ns = S\u203214/4 = (1, .25, .25, .25, .25)\u2032 is a summation vector converting MW to MWh\nT\nsimpli\ufb01ed transaction cost function\n\u03c4i\ntransaction cost in the ith market\n\u03c4\nvector of transaction costs\nV \u2217\nb,i\nmarket clearing volume impacted by bid b in the ith market\nV \u2217\nb\nvector of market clearing volumes impacted by bid b\nvj\nvolume to be traded in jth quarter-hour with j \u2208{1, . . . , 4}\nv\nvector of volumes to be traded in the given hour\nb\u03be\u2217,m\n0,d,h\nmth shift for the intersection adjustment for day d and hour h\n36\n\nAppendix B\nWe present the values of RMSE, MAE and CRPS which are strictly proper scoring rules for\nthe mean, median and marginal distribution forecasts [49]. These measures are commonly\nused by the practitioners and researchers [50]. We report also the bias of the forecasted\nprice trajectories. The formulas are given by\nRMSE =\nv\nu\nu\nt\n1\n24 \u00b7 5N\n24\nX\nh=1\nN\nX\nd=1\n4\nX\ni=0\n \nP \u2217\n0,i,d,h \u22121\nM\nM\nX\nm=1\nbP \u2217,m\n0,i,d,h\n!2\n,\n(44)\nMAE =\n1\n24 \u00b7 5N\n24\nX\nh=1\nN\nX\nd=1\n4\nX\ni=0\n\f\f\fP \u2217\n0,i,d,h \u2212medm=1,...,M\n\u0010\nbP \u2217,m\n0,i,d,h\n\u0011\f\f\f\n(45)\nwhere bP \u2217,m\n0,i,d,h is the m-th simulation of P \u2217\n0,i,d,h and medm=1,...,M\n\u0010\nbP \u2217,m\n0,i,d,h\n\u0011\nis the median\nof M simulated bP \u2217,m\n0,i,d,h prices.\nWe approximate the CRPS using the pinball loss\nCRPSi,d,h = 1\nR\nX\n\u03c4\u2208r\nPB\u03c4\ni,d,h\n(46)\nfor a dense equidistant grid of probabilities r between 0 and 1 of size R, see e.g. [37]. In\nthis study, we consider r = {0.01, 0.02, . . . , 0.99} of size R = 99. PB\u03c4\ni,d,h is the pinball loss\nwith respect to probability \u03c4. Its formula is given by\nPB\u03c4\ni,d,h =\n\u0010\n\u03c4 \u22121{P \u2217\n0,i,d,h<Q\u03c4\nm=1,...,M( bP \u2217,m\n0,i,d,h)}\n\u0011 \u0010\nP \u2217\n0,i,d,h \u2212Q\u03c4\nm=1,...,M\n\u0010\nbP \u2217,m\n0,i,d,h\n\u0011\u0011\n(47)\nwhere Q\u03c4\nm=1,...,M\n\u0010\nbP \u2217,m\n0,i,d,h\n\u0011\nis the \u03c4-th quantile of M simulated bP \u2217,m\n0,i,d,h prices. To calculate\nthe overall CRPS value we use a simple average\nCRPS =\n1\n24 \u00b7 5N\n24\nX\nh=1\nN\nX\nd=1\n4\nX\ni=0\nCRPSi,d,h.\n(48)\nTable 4 shows the error measures of the two considered price models based on the\nwhole out-of-sample data. Let us recall that the out-of-sample consists of 3 years of data\n(years 2018 to 2020). We observe a huge di\ufb00erence in the performance of the two models.",
    "chunk_index": 35,
    "start_char": 83647,
    "end_char": 86126,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "\u2217,m\n0,i,d,h prices. To calculate\nthe overall CRPS value we use a simple average\nCRPS =\n1\n24 \u00b7 5N\n24\nX\nh=1\nN\nX\nd=1\n4\nX\ni=0\nCRPSi,d,h.\n(48)\nTable 4 shows the error measures of the two considered price models based on the\nwhole out-of-sample data. Let us recall that the out-of-sample consists of 3 years of data\n(years 2018 to 2020). We observe a huge di\ufb00erence in the performance of the two models.\nThe expert model reports lower errors, but is slightly more biased. Its performance is\nnaturally not a surprise and is inline with the EPF literature [36, 40, 42\u201345]. Let us note\nthat the forecasts could be easily improved using a higher number of regressors or more\nsophisticated estimation methods both for the point and probabilistic models.\nMAE\nRMSE\nCRPS\nbias\nnaive\n10.73\n16.64\n4.17\n0.08\nexpert\n6.00\n8.68\n2.22\n0.22\nTable 4: Error measures of the considered price models\n37\n\nAppendix C\nSupply/Sell (the higher the price the better)\nDemand/Buy (the lower the price the better)\nModel\nStrategy\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nwind\n5% of\nwind\n1% of\nsolar\n5% of\nsolar\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nload\n5% of\nload\nIA-only\n37.20\n36.87\n33.68 -61.20\n21.78 -214.07\n26.69\n-84.74\n37.49\n37.84\n40.94 117.18\n66.31\n1055.64\nTC-min\n37.43 37.34\n36.78\n32.55\n29.73\n25.48 32.69\n29.38\n37.57\n37.67\n38.23\n42.82\n42.13\n63.10\nnaive\nE-NoImp\n37.46\n37.25\n35.41 -13.95\n26.21\n-85.85\n30.30\n-24.90\n37.39\n37.62\n39.45\n79.09\n53.30\n558.32\nE-LinImp\n37.46\n37.28\n36.59\n32.29\n29.51\n24.46\n32.52\n29.12\n37.39\n37.60\n38.37\n42.98\n42.29\n66.25\nE-LinImpMe\ufb00\n37.43\n37.33\n36.73\n32.24\n29.68\n24.94\n32.58\n29.02\n37.57\n37.67\n38.24\n43.00\n42.22\n68.91\nE-Me\ufb00\n37.42\n37.31\n36.78\n32.68\n29.76\n25.63\n32.67\n29.43\n37.56\n37.64\n38.20\n42.59\n42.01\n61.20\nE\n37.46\n37.31\n36.69\n32.65\n29.66\n25.58\n32.63\n29.41\n37.39\n37.58\n38.26\n42.62\n42.04\n61.25\nE-Var-U\n37.37\n37.20\n36.49\n32.46\n29.48\n25.45\n32.41\n29.24\n37.53\n37.66\n38.55 101.01\n58.61\n362.07\nVaR\n37.44\n37.29\n36.65\n32.45\n29.62\n25.42\n32.54\n29.21\n37.44\n37.61\n38.32\n42.89\n42.20\n62.85\nCVaR\n37.43\n37.27\n36.65\n32.55\n29.64\n25.52\n32.57\n29.32\n37.41\n37.61\n38.34\n42.89\n42.19\n63.86\nexpert\nE-NoImp 37.50\n37.29\n35.47 -10.24\n25.88 -107.88\n31.30\n2.68\n37.35\n37.57\n39.37\n76.70\n51.83\n517.79\nE-LinImp\n37.50\n37.32\n36.67\n32.37\n29.62\n25.12\n32.64\n29.25\n37.35\n37.53\n38.27\n42.91\n42.22\n68.17\nE-LinImpMe\ufb00\n37.43\n37.34\n36.73\n32.27\n29.71\n25.09\n32.57\n29.03\n37.57\n37.67\n38.24\n43.03\n42.23\n71.24\nE-Me\ufb00\n37.43\n37.33 36.79 32.72 29.77\n25.67\n32.67\n29.43\n37.57\n37.66 38.20 42.55 41.99\n60.91\nE\n37.50\n37.34\n36.73\n32.70\n29.70\n25.63\n32.67\n29.42\n37.35 37.51\n38.22\n42.58\n42.03\n60.94\nE-Var-U\n37.45\n37.32\n36.69\n32.52\n29.68\n25.46\n32.56\n29.22\n37.40\n37.57\n38.32\n60.66\n45.93\n230.18\nVaR\n37.46\n37.33\n36.71\n32.53\n29.67\n25.47\n32.61\n29.24\n37.42\n37.58\n38.24\n42.75\n42.12\n62.17\nCVaR\n37.46\n37.32\n36.71\n32.59\n29.69\n25.55\n32.62\n29.33\n37.41\n37.56\n38.22\n42.73\n42.09\n61.99\nperfect forecast\n38.42\n38.23\n37.30\n33.01\n30.31\n26.04\n33.16\n29.81\n36.43\n36.64\n37.61\n42.24\n41.68\n59.39\nTable 5: Average",
    "chunk_index": 36,
    "start_char": 85729,
    "end_char": 88623,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "60.91\nE\n37.50\n37.34\n36.73\n32.70\n29.70\n25.63\n32.67\n29.42\n37.35 37.51\n38.22\n42.58\n42.03\n60.94\nE-Var-U\n37.45\n37.32\n36.69\n32.52\n29.68\n25.46\n32.56\n29.22\n37.40\n37.57\n38.32\n60.66\n45.93\n230.18\nVaR\n37.46\n37.33\n36.71\n32.53\n29.67\n25.47\n32.61\n29.24\n37.42\n37.58\n38.24\n42.75\n42.12\n62.17\nCVaR\n37.46\n37.32\n36.71\n32.59\n29.69\n25.55\n32.62\n29.33\n37.41\n37.56\n38.22\n42.73\n42.09\n61.99\nperfect forecast\n38.42\n38.23\n37.30\n33.01\n30.31\n26.04\n33.16\n29.81\n36.43\n36.64\n37.61\n42.24\n41.68\n59.39\nTable 5: Average actual gain eG (EUR/MWh) of the considered strategies as a new market\nplayer with an oracle forecast of intersection curves. Colour indicates the performance\nrow-wise (the greener, the better). With bold, we depicted the best values in each row.\n38\n\nSupply/Sell (the higher the price the better)\nDemand/Buy (the lower the price the better)\nModel\nStrategy\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nwind\n5% of\nwind\n1% of\nsolar\n5% of\nsolar\n1\nMW\n10\nMW\n100\nMW\n1000\nMW\n1% of\nload\n5% of\nload\nIA-only\n37.22\n37.01\n34.55 -37.59\n23.69 -151.61\n28.13 -52.56\n37.47\n37.70\n40.08 100.42\n59.55\n989.44\nTC-min\n37.45\n37.45\n37.45\n37.45\n30.98\n30.98\n33.65\n33.66\n37.55\n37.55\n37.55\n37.55\n38.98\n38.97\nnaive\nE-NoImp\n37.48\n37.37\n36.13\n0.12\n27.50\n-67.09\n31.19 -17.62\n37.37\n37.49\n38.72\n68.95\n49.55\n647.03\nE-LinImp\n37.48\n37.40\n37.33\n37.53\n30.84\n30.28\n33.52\n33.63\n37.37\n37.47\n37.61\n37.34\n38.89\n38.53\nE-LinImpMe\ufb00\n37.45\n37.45\n37.51\n37.65\n31.09\n30.89\n33.68\n33.73\n37.55\n37.54\n37.45\n37.26\n38.75\n38.61\nE-Me\ufb00\n37.44\n37.43\n37.50\n37.89\n31.08\n31.39\n33.70\n33.90\n37.54\n37.52\n37.47\n37.10\n38.67\n38.06\nE\n37.48\n37.43\n37.42\n37.85\n30.97\n31.32\n33.64\n33.87\n37.37\n37.46\n37.51\n37.12\n38.70\n38.07\nE-Var-U\n37.38\n37.34\n37.31\n37.70\n30.92\n31.28\n33.51\n33.76\n37.51\n37.54\n37.87\n88.43\n53.17\n331.24\nVaR\n37.46\n37.42\n37.41\n37.64\n30.97\n31.20\n33.60\n33.69\n37.42\n37.48\n37.54\n37.34\n38.83\n38.65\nCVaR\n37.45\n37.41\n37.43\n37.78\n31.01\n31.32\n33.63\n33.83\n37.39\n37.47\n37.54\n37.28\n38.75\n38.98\nexpert\nE-NoImp 37.52\n37.41\n36.17\n1.84\n27.17\n-87.07\n31.95\n-8.77\n37.33\n37.45\n38.65\n67.48\n48.61\n671.17\nE-LinImp\n37.52\n37.45\n37.41\n37.61\n30.96\n31.15\n33.63\n33.75\n37.33\n37.40\n37.51\n37.30\n38.80\n39.30\nE-LinImpMe\ufb00\n37.45 37.46 37.52\n37.62 31.12\n31.24\n33.67\n33.78\n37.55\n37.55\n37.45\n37.30\n38.74\n39.92\nE-Me\ufb00\n37.45\n37.45\n37.51 37.93\n31.09\n31.4 33.70 33.91\n37.55\n37.54\n37.48 37.08 38.67\n37.97\nE\n37.52\n37.46\n37.45\n37.89\n31.03\n31.37\n33.67\n33.88\n37.33 37.39\n37.48\n37.10\n38.68\n37.96\nE-Var-U\n37.47\n37.45\n37.48\n37.79\n31.08\n31.28\n33.65\n33.80\n37.38\n37.45\n37.63\n51.50\n41.93\n176.09\nVaR\n37.47\n37.45\n37.47\n37.69\n31.03\n31.21\n33.64\n33.72\n37.41\n37.46\n37.47\n37.26\n38.78\n38.42\nCVaR\n37.48\n37.45\n37.48\n37.78\n31.07\n31.31\n33.67\n33.82\n37.39\n37.43 37.44\n37.20\n38.72\n38.27\nperfect forecast\n38.44\n38.35\n38.09\n38.29\n31.73\n31.87\n34.23\n34.33\n36.41\n36.50\n36.81\n36.69\n38.26\n37.40\nTable 6: Average actual gain eG (EUR/MWh) of the considered strategies as an existing\nmarket player rebidding their portfolio v with an oracle forecast of intersection curves.\nColour indicates the performance row-wise (the greener, the better). With bold, we de-\npicted the best values in each row.\n39",
    "chunk_index": 37,
    "start_char": 88144,
    "end_char": 91159,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "33.72\n37.41\n37.46\n37.47\n37.26\n38.78\n38.42\nCVaR\n37.48\n37.45\n37.48\n37.78\n31.07\n31.31\n33.67\n33.82\n37.39\n37.43 37.44\n37.20\n38.72\n38.27\nperfect forecast\n38.44\n38.35\n38.09\n38.29\n31.73\n31.87\n34.23\n34.33\n36.41\n36.50\n36.81\n36.69\n38.26\n37.40\nTable 6: Average actual gain eG (EUR/MWh) of the considered strategies as an existing\nmarket player rebidding their portfolio v with an oracle forecast of intersection curves.\nColour indicates the performance row-wise (the greener, the better). With bold, we de-\npicted the best values in each row.\n39\n\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(a) v = 1 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(b) v = 1000 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(c) v = 5% of wind\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(d) v = 1% of solar\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(e) v = 1000 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 1% of load\nFigure 12: Results of the eG mean inequality test for remaining portfolios v in the setting\nof a new market player. The plots present p-values \u2014 the closer they are to zero (\u2192dark\ngreen), the more signi\ufb01cant the di\ufb00erence is between gains of X-axis strategy (better) and\ngains of the Y-axis strategy (worse).\n40",
    "chunk_index": 38,
    "start_char": 90626,
    "end_char": 93479,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "E-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 1% of load\nFigure 12: Results of the eG mean inequality test for remaining portfolios v in the setting\nof a new market player. The plots present p-values \u2014 the closer they are to zero (\u2192dark\ngreen), the more signi\ufb01cant the di\ufb00erence is between gains of X-axis strategy (better) and\ngains of the Y-axis strategy (worse).\n40\n\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(a) v = 10 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(b) v = 100 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(c) v = 10 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(d) v = 100 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(e) v = 1% of wind\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 5% of solar\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(g) v = 1 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00",
    "chunk_index": 39,
    "start_char": 93099,
    "end_char": 95966,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(g) v = 1 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(h) v = 5% of load\nFigure 13: Results of the eG mean inequality test for remaining portfolios v in the setting\nof a new market player. For details on interpretation see Figure 12.\n41\n\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(a) v = 10 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(b) v = 100 MW (sell)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(c) v = 10 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(d) v = 100 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(e) v = 1% of wind\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 5% of solar\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%",
    "chunk_index": 40,
    "start_char": 95500,
    "end_char": 98359,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "E-Var-U\nVaR\nCVaR\nnaive\nexpert\n(e) v = 1% of wind\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(f) v = 5% of solar\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(g) v = 1 MW (buy)\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\np-value\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\nIA-only\nTC-min\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\nE\nE-Var-U\nVaR\nCVaR\nnaive\nexpert\n(h) v = 5% of load\nFigure 14: Results of the eG mean inequality test for remaining portfolios v in the setting\nof rebidding the portfolio. For details on interpretation see Figure 12.\n42\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 15: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of rebidding the portfolio.\nThe naive-based strategies are\nexcluded for better clarity\n43\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v",
    "chunk_index": 41,
    "start_char": 97943,
    "end_char": 100886,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "b0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 16: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of rebidding the portfolio.\nThe naive-based strategies are\nexcluded for better clarity\n44\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (buy)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (buy)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 17: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of rebidding the portfolio.\nThe naive-based strategies are\nexcluded for better clarity\n45\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load",
    "chunk_index": 42,
    "start_char": 100388,
    "end_char": 103372,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 18: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of a new market player. The naive-based strategies are excluded\nfor better clarity\n46\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of solar\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1% of wind\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 5% of load\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 19: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of a new market player. The naive-based strategies are excluded\nfor better clarity\n47\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 20: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of a new market player. The naive-based strategies are excluded\nfor better clarity\n48",
    "chunk_index": 43,
    "start_char": 102895,
    "end_char": 105859,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (sell)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (sell)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 20: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of a new market player. The naive-based strategies are excluded\nfor better clarity\n48\n\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (buy)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-NoImp\nE-LinImp\nE-LinImpMe\ufb00\nE-Me\ufb00\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 10 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 100 MW (buy)\n0.0\n0.5\n1.0\nb0\n2018\u221201\n2018\u221205\n2018\u221209\n2019\u221201\n2019\u221205\n2019\u221209\n2020\u221201\n2020\u221205\n2020\u221209\n2021\u221201\nTime, v = 1000 MW (buy)\n0.0\n0.5\n1.0\nb0\nIA-only\nTC-min\nE\nE-Var-U\nVaR\nCVaR\nFigure 21: The average daily weight of b0 in relation to the whole b strategy for remaining\nportfolios v in the setting of a new market player. The naive-based strategies are excluded\nfor better clarity\n49",
    "chunk_index": 44,
    "start_char": 105374,
    "end_char": 107092,
    "paper_title": "Optimal bidding in hourly and quarter-hourly elect",
    "paper_category": "q-fin.ST",
    "paper_filename": "Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Optimal_bidding_in_hourly_and_quarter-hourly_elect.pdf"
  },
  {
    "text": "Principal Component Analysis and Hidden Markov Model for\nForecasting Stock Returns\nEugene W. Park\nA thesis submitted in partial fulfillment\nof the requirements for the degree of\nMaster of Science\nCourant Institute of Mathematical Sciences\nNew York University\nMay, 2023\nAbstract\nThis paper presents a method for predicting stock returns using principal com-\nponent analysis (PCA) and the hidden Markov model (HMM), and tests the\nresults of trading stocks based on this approach. Principal component analysis\nis applied to the covariance matrix of stock returns for companies listed in the\nS&P 500 index, and interpreting principal components as factor returns, we ap-\nply the HMM model on them. Then we use the transition probability matrix\nand state conditional means to forecast the factors returns. Reverting the factor\nreturns forecasts to stock returns using eigenvectors, we obtain forecasts for the\nstock returns. We find that, with the right hyperparameters, our model yields a\nstrategy that outperforms the buy-and-hold strategy in terms of the annualized\nSharpe ratio.\nKeywords:\nPrincipal component analysis, factor model, hidden Markov\nmodel, stock market, forecasting\n1\nIntroduction\nStock market forecasting has been a prolonged practice of interest for people in various\nfields of discipline, and thus, various approaches have been used to tackle the problem\nfrom classical time series analysis and using factor models to high frequency trading\nand using deep learning techniques.\nA widely used method to analyze time series data, the hidden Markov model has\nbeen a popular method to analyze financial markets. [1], [2], [4], and [5] have used the\nHMM on its own to make predictions of stock prices, while others have combined the\nHMM with other methods such as the long short term memory model [3] and fuzzy\nlogic [6]. The usage of the hidden Markov model on stock price prediction is more\nextensive than what has been cited, but within the author\u2019s knowledge, there seems\nto be more attention on using HMM as a preliminary step than combining techniques\nthat refine the input for the HMM.\nIn this paper, we present a method that applies principal component analysis (PCA)\nas a form of a factor model to preprocess our data and uses the HMM on the prepro-\ncessed data to forecast of stock returns. Then, we test the accuracy of this model\nby computing the directional accuracy of the forecasts, and evaluate the long-term,\nrisk-adjusted returns of various trading strategies based on this model.\nThe rest of this paper is organized as follows. In section 2, we give a brief overview\nof the PCA as a factor model, and in section 3, briefly introduce the HMM. Then, we\n1\narXiv:2307.00459v1 [q-fin.ST] 2 Jul 2023\n\nexplain our model in detail in section 4, and discuss the implementation and results\nof our model as trading strategies in section 5. We conclude the paper in section 6 by\nmentioning some shortcomings of our paper and providing ways for improvement.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 2973,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "the PCA as a factor model, and in section 3, briefly introduce the HMM. Then, we\n1\narXiv:2307.00459v1 [q-fin.ST] 2 Jul 2023\n\nexplain our model in detail in section 4, and discuss the implementation and results\nof our model as trading strategies in section 5. We conclude the paper in section 6 by\nmentioning some shortcomings of our paper and providing ways for improvement.\n2\nBrief Introduction: Principal Component Analysis as Fac-\ntor Models\n2.1\nPrincipal Component Analysis\nGiven a dataset X \u2208Rt\u00d7n where the rows of X are de-meaned samples of the random\nvector \u02dcX \u2208Rn, principal component analysis is performed as follows:\n\u22c4Compute the covariance matrix of X:\nM = XTX \u2208Rn\u00d7n\n(1)\n\u22c4Compute the eigendecomposition of M, which exists by the spectral theorem for\nsymmetric matrices, such that\nM = EGET\nwhere E \u2208Rn\u00d7n is the column matrix of n orthonormal eigenvectors u1, . . . , un \u2208\nRn, and G \u2208Rn\u00d7n is a diagonal matrix of eigenvalues \u03bb1, . . . , \u03bbn.\nSince we are allowed to reorder the eigenvalues in descending order with the\neigenvectors ordered accordingly, we may assume that\n\u03bb1 > \u00b7 \u00b7 \u00b7 > \u03bbn\nEach \u03bbi denotes the amount of variance explained in M, and ui are the corre-\nsponding directions called the \u201dprincipal directions.\u201d\nNote that u1 is the direction of the highest variance in M and un is the direction of\nthe lowest variance.\nThe \u201dprincipal components\u201d are then the dataset X in the directions of the principal\ndirection ui:\nwi =\n\uf8ee\n\uf8ef\uf8f0\nwi,1\n...\nwi,n\n\uf8f9\n\uf8fa\uf8fb= Xui\n2.2\nFactor Models\nThe factor model, introduced in the Arbitrage Pricing Theory (APT) in [8], states\nthat there exist explanatory variables (called \u201dfactors\u201d) that explain the systematic\nbehavior of asset returns. Hence,\nr = Bf + \u03f5\n(2)\nwhere\n\u22c4r \u2208Rn\u00d71 is a random vector of returns for n assets,\n\u22c4f \u2208Rk\u00d71 is a random vector of returns for the k factors with E(f) = \u00b5f and\n\u03c32(f) = F,\n2\n\n\u22c4B \u2208Rn\u00d7k consists of columns representing factor loadings for the k factors,\n\u22c4\u03f5 \u2208Rn\u00d71 is random noise for which we assume E(\u03f5) = 0, \u03c32(\u03f5) = D, a diagonal\nmatrix.\nFrom , 2 we get\nE(r) = B\u00b5f\n\u03c32(r) = \u03a3r = BFBT + D\n(3)\nwhere \u03a3r = E(rTr) \u2208Rn\u00d7n is the covariance matrix of r.\n2.3\nClassical Factor Models and PCA Factor Models\n\u2217Suppose we have a dataset X of n asset returns across t time (X \u2208Rt\u00d7n). The\nclassical, APT-type, factor models uses features of the assets to derive the k factors\nthat explain X. Hence, factors are exogenous to X. The Fama-French five factor model\n[9] and factor-based risk models built by MSCI are some of the popular examples of\nthe classic factor models.\nThe PCA factor models differ from the classical ones in that the factors are not\ndiscerned exogenously. The factors are data-driven (i.e.",
    "chunk_index": 1,
    "start_char": 2599,
    "end_char": 5267,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "features of the assets to derive the k factors\nthat explain X. Hence, factors are exogenous to X. The Fama-French five factor model\n[9] and factor-based risk models built by MSCI are some of the popular examples of\nthe classic factor models.\nThe PCA factor models differ from the classical ones in that the factors are not\ndiscerned exogenously. The factors are data-driven (i.e. inferred from the data matrix\nX) by applying the eigendecompoistion on the covariance matrix in equation 1:\nM = XTX = EGET\nThe PCA factor model sets the eigenvectors as factor loadings,\nE = B\n(4)\n, and the factor returns as the n principal components\nf = XE = [w1, . . . wn]\n(5)\nThen, we have\nF = fTf = (XE)T(XE)\n\u21d2EFET = E(XE)T(XE)ET = XTX\n\u21d2F = G\nAssuming the PCA model to be exact, hence leaving aside D in (3) (i.e. \u03f5 = 0 in (2)),\nM = XTX = EGET = BFBT + 0\n(6)\nThus, the PCA factor model defines n factors that explain the full covariance matrix\nof our dataset of returns, X, where each factor corresponds to X rotated in n indepen-\ndent direction ui \u2208E, i = 1, . . . , n, and the factor loadings are these n independent\ndirections. \u2020 The analogue to (2) is then\nXT = E(XE)T\n(7)\nwhere XT \u2208Rn\u00d7t and (XE)T \u2208Rn\u00d7t consists of t samples in the columns.\n\u2217This subsection is largely based on [7]\n\u2020Note that the independence of eigenvectors come from the spectral theorem of symmetric matrices,\nand that the independence of ui\u2019s suggests that f|k consists of independent column vectors\n3\n\nWe note that the although factors derived by PCA are less intuitive, the PCA offers\nthe benefit of discovering factors that not may be discovered exogenously.\n3\nBrief Introduction: Hidden Markov Model\nThe fundamental argument of the hidden Markov model (HMM) is that underlying the\nobserved sequence of time series data Y = {y1, . . . , yt}, there exists a Markov chain,\nZ = {z1, . . . , zt} that generates Y where each zi \u2208S = {1, ..., N}, the state space. The\nunderlying Markov chain has an initial distribution L = {L1, . . . , Ln} and a transition\nprobability matrix P = (pij)i,j\u2208S, where pij denotes the probability of zl transitioning\nfrom state i to j. Finally, assuming our observations to be in state space O, the emission\nprobability matrix, R = (rij)i\u2208S,j\u2208O, denotes the probability of observation given that\nwe are in a certain state: rij = P(Y = j|Z = i).\nHence, the parameters that define the HMM are:\n\u0398 = (L, P, R)\n(8)\nand we are mainly concerned with the following problems:\n1. Selecting the best model \u0398 given a range of model options, \u0398k and the sequence\nof observations Y :\nargmax\n\u0398k\nP(Y |\u0398k)\n2. Determining the most probable state sequence Z given Y and \u0398:\nargmax\nZ\nP(Z|Y, \u0398)\n3.",
    "chunk_index": 2,
    "start_char": 4888,
    "end_char": 7549,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "observation given that\nwe are in a certain state: rij = P(Y = j|Z = i).\nHence, the parameters that define the HMM are:\n\u0398 = (L, P, R)\n(8)\nand we are mainly concerned with the following problems:\n1. Selecting the best model \u0398 given a range of model options, \u0398k and the sequence\nof observations Y :\nargmax\n\u0398k\nP(Y |\u0398k)\n2. Determining the most probable state sequence Z given Y and \u0398:\nargmax\nZ\nP(Z|Y, \u0398)\n3. Estimating the parameters \u0398 given Y :\nargmax\n\u0398\nP(Y |\u0398)\nThe forward algorithm is an algorithm to solve the first problem, Viterbi algorithm for\nthe second, and Baum-Welch Algorithm or EM (expectation-maximization) algorithm\nfor the last.\u2021\n4\nPCA + HMM as a Forecasting Model\nIn this section, we discuss details for how we use PCA with HMM to develop a model\nthat forecasts asset returns.\nSuppose that we are working with a dataset, X, of returns for n assets through T\ntime periods such that the columns of X are time series data of return for a particular\nasset:\nX = (rij)i={1,...,T},j={1,...,n}\n, where rij is the return of company j at time i. Thus, our goal is to forecast the returns\non the n assets for the next period, \u02c6XT+1,n={1,...,n}. Furthermore, since the directional\naccuracy of our forecasts is crucial for our trading strategies, we are, in fact, primarily\nconcerned with:\nsign( \u02c6XT+1,n={1,...,n})\n(9)\n\u2021Refer to [10] for details of the algorithms\n4\n\n4.1\nImplementing PCA\nPCA only requires our dataset to be de-meaned, but we normalize X for each column\nand denote it as Y :\nY = X \u2212\u00b5x\n\u03c3X\n(10)\nWe apply PCA by computing the eigendecomposition of the covariance matrix of Y\nsuch that the eigenvalues are in decreasing order and the eigenvectors are ordered\naccordingly:\nH = Y TY = EGET, H, E, G \u2208Rn\u00d7n\ndiagG = {\u03bb1, . . . , \u03bbn}, \u03bb1 > \u00b7 \u00b7 \u00b7 > \u03bbn\nThen, as in (7), we have a full PCA factor model:\nY T = E(Y E)T\nWe assume that the covariance matrix H, or equivalently Y , contains some noise. Thus,\nwe want to de-noise our dataset before training the HMM.\nRecall that the eigenvalues suggest the percentage of variance (i.e. information)\nof H explained by rotating it in the direction of the corresponding eigenvectors. Sup-\npose the amount of noise in the covariance matrix is p%. Then, we take the first k\neigenvectors that explain at least (1 \u2212p)% of variance such that\n\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbk \u22651 \u2212p\n\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbk\u22121 < 1 \u2212p\nand compute the corresponding principal components, (w1, . . . , wk), which represent k\nof the n factor returns as in (5).\nThen, we extract \u2248p% \u00a7 of noise from H by restricting our set of eigenvectors, E,\nto the first k eigenvectors\nE|k = {u1, . . . , uk} \u2208Rn\u00d7k\nand computing k factor returns,\nf|k = {w1, .",
    "chunk_index": 3,
    "start_char": 7148,
    "end_char": 9781,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "that\n\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbk \u22651 \u2212p\n\u03bb1 + \u00b7 \u00b7 \u00b7 + \u03bbk\u22121 < 1 \u2212p\nand compute the corresponding principal components, (w1, . . . , wk), which represent k\nof the n factor returns as in (5).\nThen, we extract \u2248p% \u00a7 of noise from H by restricting our set of eigenvectors, E,\nto the first k eigenvectors\nE|k = {u1, . . . , uk} \u2208Rn\u00d7k\nand computing k factor returns,\nf|k = {w1, . . . , wk} = Y E|k =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ny1 \u00b7 u1\n. . .\ny1 \u00b7 uk\ny2 \u00b7 u1\n. . .\ny2 \u00b7 uk\n...\n...\n...\nyT \u00b7 u1\n. . .\nyT \u00b7 uk\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\u2208RT\u00d7k\n(11)\n, where yi \u2208Rk is the returns for the k factors in time i.\nNote that f|k consists\nof columns that represent Y in k independent directions with the most amount of\ninformation (i.e. f|k consists of independent column vectors).\nAssuming the full PCA factor model to contain some noise, we are essentially\nassuming that not all of the principal component factors defined by PCA are significant\nfactors; factors that explain small portions of variance of the covaraince matrix are just\nnoise. Hence, we have:\nY T = E(Y E)T = E|k(Y E|k)T + \u03f5k\n(12)\n\u00a7\u2248p% is due to the fact that the k eigenvectors does not necessarily, in fact, most often does not,\nexplain exactly (1 \u2212p%) of the matrix.\n5\n\nwhere \u03f5k is the error term that contains \u2248p% of information in H.\n4.2\nHMM on Factor Returns\nNow, we extract the noise from Y and apply the HMM to f|k, the time series of k\nfactor returns, to obtain a one-step-ahead forecast for these factor returns. Then, we\nrevert the forecast of the factors back to assets which will then be our forecast for the\nassets.\nBefore, we begin training the model, we assume the emission probability in each\nstate in the state space, S, to have have a Gaussian distribution. Hence, the parameters\nof our model are\n\u0398 = (L, P, R)\n\u21d2\u0398 = (L, P, \u00b5i, \u03c32\ni ), i = 1, . . . , N\n(13)\n, where N is the number of states in the state space S, \u00b5i is the mean of the observation\nsequence in state i, and \u03c32\ni is the variance of observation sequence in state i.\n4.2.1\nTraining & Model Selection\nTo determine the number of state space, N, we train the Gaussian HMM (13) for\neach j in [2,3,4,5,6,7,8], denoted as \u0398j, compute the log likelihood, ln(P(Y |\u0398j)), using\nthe forward algorithm, and compute Akaike information criterion (AIC)[11] for each\nmodel.\nAIC = \u22122 ln(ln(P(Y |\u0398j))) + 2j\nThen, we choose our model to be \u0398j, which yields the lowest AIC.\nIn the training step of each \u0398j, we use the Baum-Welch algorithm to estimate the\nparameters, and thus, choose the initial conditions for our parameters as follows:\nL0 = 1/j\n(14)\nP0 = 1/j2\n(15)\n\u00b5i,0 = E(o)i\n(16)\n\u03c32\ni,0 = var(o)i\n(17)\n, where o is the observation sequence, o = {o1, .",
    "chunk_index": 4,
    "start_char": 9423,
    "end_char": 12032,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "the Gaussian HMM (13) for\neach j in [2,3,4,5,6,7,8], denoted as \u0398j, compute the log likelihood, ln(P(Y |\u0398j)), using\nthe forward algorithm, and compute Akaike information criterion (AIC)[11] for each\nmodel.\nAIC = \u22122 ln(ln(P(Y |\u0398j))) + 2j\nThen, we choose our model to be \u0398j, which yields the lowest AIC.\nIn the training step of each \u0398j, we use the Baum-Welch algorithm to estimate the\nparameters, and thus, choose the initial conditions for our parameters as follows:\nL0 = 1/j\n(14)\nP0 = 1/j2\n(15)\n\u00b5i,0 = E(o)i\n(16)\n\u03c32\ni,0 = var(o)i\n(17)\n, where o is the observation sequence, o = {o1, . . . , oT}. Here, we are assuming that it\nis equally likely for the underlying Markov chain to start in any state (14) and that it\nis equally likely for the Markov chain to transit from one state to another (15). (16)\nsuggests that we assume the \u00b5i to be the sample mean of our observations for all states\ni = 1, . . . , j (i.e. same initial mean for all states), and (17) suggests that we assume\n\u03c32\ni to be the sample variance of our observations for all states i = 1, . . . , j (i.e. same\ninitial variance for all states).\nNote that since f|k consists of k time series of factor returns, our observation sequnce\nis k-dimensional.\no = {o1, . . . , oT}, where ot = {o1\nt, . . . , ok\nt }\nHence, \u03c32\ni is a k \u00d7 k covariance matrix. Since, the k factor returns are independent,\nhowever, we may assume the covariance matrix to be diagonal. Thus, in (17), \u03c32\ni,0 =\nvar(o)i is a k \u00d7 k diagonal covariance matrix, for state i, with sample variances of\no = {o1, . . . , ok} in the diagonal.\n6\n\n4.2.2\nForecasting\nNow, once we have found and trained our model, which we will denote again as \u0398, we\nuse the estimated parameters to forecast the returns of the k factors in the next time\nperiod, ( \u02c6\nf|k)T+1, as follows:\n1. We use the Viterbi algorithm to obtain the sequence of the underlying Markov\nchain:\nZ = {z1, . . . , zT}, zi \u2208S = {1, . . . , N}\n, where zT = i states that the underlying Markov chain is currently in state i.\n2. Then, we use the transition probability matrix, \u02c6P, and the state conditional\nmean, \u02c6\u00b5i, to obtain an estimate for (f|k)T+1:\n( \u02c6f|k)T+1 =\nN\nX\nj\u2208S\nPij \u02c6\u00b5j\n(18)\nSince \u02c6\u00b5j \u2208Rk, for all j = {1, . . . , N}, ( \u02c6f|k)T+1 \u2208Rk.\nThus, our forecast of the k factor returns in the next period depends on 1) how likely\nit is to transition from state i to j, and 2) estimated state conditional means of the k\nfactors.\nAlthough we may trade factors, especially if it were defined exogenously as in the\nclassical factor models, it is hard to trade endogenous factors defined by the PCA.",
    "chunk_index": 5,
    "start_char": 11448,
    "end_char": 14022,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "estimate for (f|k)T+1:\n( \u02c6f|k)T+1 =\nN\nX\nj\u2208S\nPij \u02c6\u00b5j\n(18)\nSince \u02c6\u00b5j \u2208Rk, for all j = {1, . . . , N}, ( \u02c6f|k)T+1 \u2208Rk.\nThus, our forecast of the k factor returns in the next period depends on 1) how likely\nit is to transition from state i to j, and 2) estimated state conditional means of the k\nfactors.\nAlthough we may trade factors, especially if it were defined exogenously as in the\nclassical factor models, it is hard to trade endogenous factors defined by the PCA.\nHence, we revert the forecasts for the factor returns to forecasts for the n assets. I.e.\nwe want:\n( \u02c6f|k)T+1 \u2192\u02c6YT+1\nRecall from (12) that\nY T = E(Y E)T = E(f)T\nwhere f = {w1, . . . , wn}. Since we only have an estimate of f|k for time T + 1, we first\nestimate fk+1, . . . , fn for T + 1. Recall that the best estimate is its mean. Thus, we\nhave, for j = k + 1, . . . , n,\n( \u02c6fj)T+1 = E((fj)T+1) = 1\nT\nT\nX\ni=0\n(Yi \u00b7 uj) = 1\nT\nT\nX\ni=0\n(Yi) \u00b7 uj = 0\n(19)\nwhere Yi \u2208Rn is the i-th row of Y , and the last equality follows from Y being normalized\nwith row mean equal to 0. Equivalently,\nE(( \u02c6\u03f5k)T+1) = E(\u02c6Y T\nT+1 \u2212E|k(\u02c6YT+1E|k)T) = 1\nT\nT\nX\nh=0\nY T\nh \u2212E|k( 1\nT\nT\nX\nh=0\nY T\nh )E|k)T = 0\nwhere Y T\nh are the h-th column of Y T.\nNow, we have\n\u02c6fT+1 = {( \u02c6f|k)T+1, 0, . . . , 0}\nand from (12), we have\n\u02c6Y T\nT+1 = E( \u02c6fT+1)T = E|k( \u02c6f|k)T\nT+1\n7\n\n\u21d2\u02c6YT+1 = ( \u02c6f|k)T+1E|T\nk\nwhere \u02c6YT+1 \u2208R1\u00d7n, ( \u02c6f|k)T+1 \u2208R1\u00d7k, and E|T\nk \u2208Rk\u00d7n. Thus, we obtain forecasts for\nthe n assets by multiplying the transpose of the k eigenvectors to the forecasts for the\nk factor returns.\nFinally, since Y is the normalized version of X (10),\n\u02c6XT+1 = \u03c3X(\u02c6yT+1) + \u00b5X\n(20)\n5\nImplementation\nWe use data for the weekly returns of companies listed in the S&P500 to train our\nmodel.\u00b6\u2016 The returns are calculated using the closing stock prices, and each return\nsequence comprise the columns of our dataset, denoted X. Hence, we have X \u2208RTxn,\nwhere T is the last period of the time span of our data and n is the number of assets.\nWe note that, as companies are newly listed and delisted from the S&P500 index,\nsome companies that are listed in the index at the time of data retrieval may not have\nbeen listed previously, and some may have been delisted during the time span of our\ndata. Thus, we are subject to survivor bias, but minimize this effect by ensuring that\nwe have at least 400 data points of company returns for each time period.",
    "chunk_index": 6,
    "start_char": 13555,
    "end_char": 15918,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "We note that, as companies are newly listed and delisted from the S&P500 index,\nsome companies that are listed in the index at the time of data retrieval may not have\nbeen listed previously, and some may have been delisted during the time span of our\ndata. Thus, we are subject to survivor bias, but minimize this effect by ensuring that\nwe have at least 400 data points of company returns for each time period. With at\nleast 80% of the index, we conjecture that the several companies we fail to capture only\nexplains a minimal amount of information/variance in the covariance matrix, XTX,\nwhich we implement the PCA on. In other words, we suspect that most of the in-\nformation from the companies we fail to capture have already been captured by the\neigenvectors corresponding to large eigenvalues, and that the additional information\nfrom those companies constructs eigenvectors with very small eigenvalues. Thus, we\nsuspect that they are eventually extracted during the process of denoising our covari-\nance matrix via PCA. Because this is only a conjecture, however, we do not know for\nsure how much we are affected by survivor bias, and thus, leaving room for improvement\nupon this paper.\n5.1\nModel Training\nEnsuring our dataset to include returns data for at least 400 companies listed in the\nS&P500 index for each t, we start our data from July 23, 2004. Using 10 years\u2019 worth\nof weekly data, we train our PCA+HMM model and forecast the weekly returns for\nthe next period. Then, rolling the 10 years window by one week\u2217\u2217, we forecast returns\nfor a total of 100 weeks (the week for which we make our last forecast is July 8, 2016,\nthus our data spans from July 23, 2004 to July 8,2016)\u2020\u2020. Note that, since we train the\nPCA+HMM for each time window, the total number of PCA factors, the total number\nof states in the HMM, and the model parameters may vary over time.\nSince PCA is essentially an eigendecomposition, We use the linalg module in the\nnumpy library in Python to implement PCA. To train the HMM model, we use the\ngaussianhmm module in the hmmlearn Python library.\nRecall from part 4.1 that we have to specify the hyperparameter, p, that specifies the\nminimum percentage of noise to be extracted during the dimension reduction process.\n\u00b6Data was obtained from finance.yahoo.com\n\u2016List obtained from https://en.wikipedia.org/wiki/List of S%26P 500 companies on April 28, 2023\n\u2217\u2217The exact time samples (rows of our dataset) for each window may slightly differ due to different\nnumbers of holidays and trading days every year.\nMoreover, due to the entering and exiting of\ncompanies from the index since the beginning of our dataset, the number of stocks (columns of our\ndataset) may also differ.\n\u2020\u2020Additional data is available, but we limit ourselves to 100 forecasts for computational convenience.\n8",
    "chunk_index": 7,
    "start_char": 15507,
    "end_char": 18318,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "slightly differ due to different\nnumbers of holidays and trading days every year.\nMoreover, due to the entering and exiting of\ncompanies from the index since the beginning of our dataset, the number of stocks (columns of our\ndataset) may also differ.\n\u2020\u2020Additional data is available, but we limit ourselves to 100 forecasts for computational convenience.\n8\n\nWe train our model with four choices of p:\np = {45%, 30%, 15%, 10%}\nwhich corresponds to keeping no less than 55%, 70%, 85%, and 90% of the information\nin Y , our normalzied return matrix.\n5.1.1\nModel Implications\nBefore, we test trading strategies based on our model, we make several observations of\nour trained model.\nFigure 1 shows the total number of PCA factors kept in the model throughout\ntime for the different choices of p. Notice that as we keep more noise in our model,\nthe number of PCA factors kept not only increases, but also changes more frequently.\nMoreover, there is also a more gradual change in the total number of PCA factors in\nthe market, such as from the first 20 periods to the period between the 20th and 60th\nweek in figures 1.a and 1.b. There is also a gradual shift in 1.c and 1.d from the first\n50 weeks to the next. This suggests that there may exist short-term PCA factors and\nlonger-term PCA factors.\n(a) P = 45%\n(b) P = 30%\n(c) P = 15%\n(d) P = 10%\nFigure 1: Number of PCA Factors\nNow, we observe the state-conditional means and variances calibrated from the\nHMM model. Recall that the HMM calibrates the means and variances for K PCA\nfactors for each state, and since we run the model for 100 periods, we have a series of\nstate-conditional means and variances as follows:\n\u00b5 = {\u00b51, . . . , \u00b5100}\n9\n\n\u03c32 = {\u03c32\n1, . . . , \u03c32\n100}\nwhere \u00b5t, \u03c32\nt \u2208RNt\u00d7kt. Note that Nt and kt has a time subscript since the total number\nof states and PCA factors vary.\nAt each time, t, we average \u00b5t, \u03c32\nt along the k factors to get a time series of state-\nconditional means and variances (of returns)\n\u02dc\u00b5 = {\u02dc\u00b51, . . . , \u02dc\u00b5100}\n\u02dc\u03c32 = {\u02dc\u03c32\n1, . . . , \u02dc\u03c32\n100}\nwhere \u02dc\u00b5t, \u02dc\u03c32\nt \u2208RNt. Moreover, since the total number of states are changing, we sort\n\u02dc\u00b5t in decreasing order with \u02dc\u03c32\nt having the corresponding order. Thus, we disregard the\nactual state numbers, such as rather we are in state 1 or state 2, and put meaning\non the states by ranking them according to the calibrated state-conditional means\naveraged across the kt PCA factors. For each p, we find the minimum number states\nthat existence throughout time and visualize the emission probability distributions in\nfigure 2.\n(a) P = 45%\n(b) P = 30%\n(c) P = 15%\n(d) P = 10%\nFigure 2: State-Conditional Emission Probability Distributions\nNote:x-axis in percentages.",
    "chunk_index": 8,
    "start_char": 17963,
    "end_char": 20650,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "numbers, such as rather we are in state 1 or state 2, and put meaning\non the states by ranking them according to the calibrated state-conditional means\naveraged across the kt PCA factors. For each p, we find the minimum number states\nthat existence throughout time and visualize the emission probability distributions in\nfigure 2.\n(a) P = 45%\n(b) P = 30%\n(c) P = 15%\n(d) P = 10%\nFigure 2: State-Conditional Emission Probability Distributions\nNote:x-axis in percentages. 3 refers to 300%.\nFrom figure 2, we can see that more noise being kept in our factor returns yields\nprobability distributions that are less spread across states, suggesting that the under-\nlying states lose significance and the HMM model becomes less useful. Furthermore,\nfigure 2.a, the model with 45% of noise extracted from the normalized return dataset,Y ,\nis intuitively interpretable. State 4 to seems to be \u201dbear market\u201d state, while state 0\nto can be viewed as the \u201dbull market\u201d state, and states 1,2, and 3 are the relatively\n10\n\nstable states of the market. We can also view state 4 as where investors have exces-\nsively fearful, state 0 as the state where they are excessively optimistic, states 1 and 3\nas states where they are moderately optimistic and fearful, and state 2 as where they\nneutral about the market. Thus, models with more spread-out probability distribu-\ntions provide more reliable and interpretable information about the market which may\nbe useful for task such as risk-management.\n5.2\nTrading Strategies\nNow we construct and test simple trading strategies based on our model.\n1. Our first trading strategy is solely based on the forecasts given by ??. We short\nthe assets for which \u02c6XT+1 predicts a negative return and long the ones for which\n\u02c6XT+1 predicts a positive return. We weight the n stocks equally (i.e. 1/n) and\ntrade at the closing time of the stock market at the last trading day of every\nweek.\n2. The second strategy depends on the forecasts of the normalized asset returns,\n\u02c6yT+1. Since\nY = X \u2212\u00b5X\n\u03c3X\n, we hypothesize that there is predictive trading signal in not the return itself,\nbut the excess return over the average.\nWhen testing our trading strategies, we assume the following:\n1. we are able to trade at exactly the closing time\n2. we can exactly match the n assets with equal weight in our portfolio\n3. and we do not incur any trading costs.\n5.3\nResults\nTo evaluate the performance of our trading strategies, we compare our trading strate-\ngies with the simple buy-and-hold strategy of all companies in the index. As measure-\nments of performance, we compute:\n1. the \u201dwinning probability\u201d of our forecasts (i.e. probability of correctly forecasting\nthe direction of the returns in the next period).\n2. the annualized Sharpe ratio:\nS = r \u2212rf\n\u03c3\n(21)\nwhere r is the asset return, rf is the return of the risk-free rate, and \u03c3 is the\nstandard deviation of asset.",
    "chunk_index": 9,
    "start_char": 20181,
    "end_char": 23064,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "we compare our trading strate-\ngies with the simple buy-and-hold strategy of all companies in the index. As measure-\nments of performance, we compute:\n1. the \u201dwinning probability\u201d of our forecasts (i.e. probability of correctly forecasting\nthe direction of the returns in the next period).\n2. the annualized Sharpe ratio:\nS = r \u2212rf\n\u03c3\n(21)\nwhere r is the asset return, rf is the return of the risk-free rate, and \u03c3 is the\nstandard deviation of asset. We will assume the risk-free rate to be zero.\nThe winning probabilities of the two trading strategies are shown in table 1. We\nobserve that strategy 1 clearly outperforms strategy 2 in forecasting the direction of\nthe returns. Yet, strategy 1 yield just above 50%, suggesting that significant profit\ncan be made only in a casino-style type of trading where the model trades constantly\nwithout going bankrupt. Hence, for this strategy to be profitable over the long-run,\nadditional features must be included such as the trade size.\nTable 2 shows the Sharpe ratios of the trading strategies, and we observe that the\nstrategy with the highest Sharpe ratio is strategy 2 with p = 15%. This outperforms\n11\n\nTable 1: Winning Probability\np%\nStrategy 1\nStrategy 2\n45%\n0.532\n0.490\n30%\n0.543\n0.504\n15%\n0.538\n0.511\n5%\n0.532\n0.490\nNote: Rounded to the nearest thou-\nsandth.\nthe buy-&-hold strategy by more than 50%. From figure 5, we observe that the strategy\nyields lower cumulative return than the buy-&-hold, but is more stable; it rides out two\nocassions of downturns, during late-2014 and early-2016, without much losses, contrary\nto the other strategies. It also has a winning probability of 51%, and thus, seems to\nbe useful in terms of risk management and as a stable component within a portfolio.\nWe also note that the Sharpre ratios depend heavily on the hyperparameter p. The\ndifference between the highest and lowest ratios of strategy is as high as 0.91. For\nstrategy 1, there seems to be a trend where the ratio falls as p decreases to 30%, but\nincreases again as more noise is extracted. This suggests the significance of finding the\nright hyperparameter p, which could yield significantly higher Sharpe ratios than the\nones presented here.\nTable 2: Annualized Sharpe Ratio\np%\nStrategy 1\nStrategy 2\n45%\n0.688\n0.45\n30%\n0.581\n0.581\n15%\n0.703\n1.36\n10%\n0.877\n0.726\nBuy-&-Hold\n0.828\nNote: Values are percentages.\n6\nConclusion\nWe have shown that with the right hyperparameter p, the PCA + HMM model provides\nan intuitive understanding of the market through the estimated emission probability\ndistributions of the states, and yields model-based trading strategies with forecast\naccuracy slightly above 50%. We have also seen that these strategies have the potential\nto outperform the market by finding the right p.\nThis work suggests that future\nresearch into finding the optimal p will be very useful. Moreover, we have made several\nassumptions which such as zero transaction costs and that we are able to trade at\nexactly the closing prices at the closing time.",
    "chunk_index": 10,
    "start_char": 22615,
    "end_char": 25624,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "of the market through the estimated emission probability\ndistributions of the states, and yields model-based trading strategies with forecast\naccuracy slightly above 50%. We have also seen that these strategies have the potential\nto outperform the market by finding the right p.\nThis work suggests that future\nresearch into finding the optimal p will be very useful. Moreover, we have made several\nassumptions which such as zero transaction costs and that we are able to trade at\nexactly the closing prices at the closing time. These assumptions are unrealistic and\nfails in practice. Hence, our paper could be improved by evaluating the performance\nunder realistic conditions, such as existence of slippage, transactions costs, and possibly\nprice impact as well (when we assume large trade size).\nAcknowledgement: We thank professors Kenneth Winston, Petter Kolm, and Jonathan\nGoodman for their valuable comments and suggestions throughout the progress of this work.\n12\n\nFigure 3: Return Sequences for Strategies 1 and 2 with p = 45%\nFigure 4: Return Sequences for Strategies 1 and 2 with p = 30%\n13\n\nFigure 5: Return Sequences for Strategies 1 and 2 with p = 15%\nFigure 6: Return Sequences for Strategies 1 and 2 with p = 10%\n14\n\nAppendix\nAppendix A.1. Proof of Forecast Equation 18:\nDenote rt, return at time t, as the rows of ft.\n( \u02c6f|k)T+1 = E(rT+1|ZT = i)\n=\nN\nX\nj\u2208S\nE(rT+1, ZT+1 = j|ZT = i)\n=\nN\nX\nj\u2208S\nE(rT+1|ZT+1 = j)P(ZT+1 = j|ZT = i)\n=\nN\nX\nj\u2208S\nPij \u02c6\u00b5j\n15\n\nReferences\n[1] N. Nyugen. \u201dHidden Markov Model for Stock Trading\u201d. International Journal of\nFinancial Studies, pages 6-36, 2018.\n[2] B. Dhingra, A. Gupta. \u201dStock Market Prediction Using Hidden Markov Models\u201d.\n2012 Students Conference on Engineering and Systems, pages 1-4, 2012.\n[3] J. Huo, M. Liu, Y. Wu, J. Wu. \u201dStock Market Trend Analysis Using Hidden Markov\nModel and Long Short Term Memory\u201d. eprint arXiv:2104.09700, 2012.\n[4] R. Hassan, B. Nath. \u201dStock market forecasting using hidden Markov model: a\nnew approach\u201d. 5th International Conference on Intelligent Systems Design and\nApplications, pages 192-196, 2005.\n[5] G. Kavitha, A. Udhayakumar, D. Nagarajan. \u201dStock Market Trend Analysis Using\nHidden Markov Models\u201d. International Journal of Computer Science and Informa-\ntion Security , 2013.\n[6] R. Hassan. \u201dA Combination of hidden Markov model and fuzzy model for stock\nmarket forecasting\u201d. Neurocomputing, pages 3439-3446, 2009.\n[7] K. J. Winston. Quantitative Risk and Portfolio Management: Theory and Practice.\nCambridge University Press, 2023 (forthcoming).\n[8] S. A. Ross. \u201dThe arbitrage theory of capital asset pricing\u201d. Journal of Economic\nTheory, pages 341-360, 1976.\n[9] E. F. Fama, K.R. French. \u201dA five-factor asset pricing model\u201d. Journal of Financial\nEconomics, pages 1-22, 2014.\n[10] T. Li, W. E, E. Vanden-Eijnde. Applied Stochastic Analysis. American Mathe-\nmatical Society, 2019.\n[11] A. Hirotugu. \u201dA new look at the statistical model identification\u201d. IEEE TRans-\nactions on Automatic Control, pages 716-723, 1974.\n16",
    "chunk_index": 11,
    "start_char": 25097,
    "end_char": 28104,
    "paper_title": "Principal Component Analysis and Hidden Markov Mod",
    "paper_category": "q-fin.ST",
    "paper_filename": "Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Principal_Component_Analysis_and_Hidden_Markov_Mod.pdf"
  },
  {
    "text": "Recurrent neural network based parameter estimation of\nHawkes model on high-frequency \ufb01nancial data\nKyungsub Lee\u2217\nApril 25, 2023\nAbstract\nThis study examines the use of a recurrent neural network for estimating the param-\neters of a Hawkes model based on high-frequency \ufb01nancial data, and subsequently, for\ncomputing volatility. Neural networks have shown promising results in various \ufb01elds, and\ninterest in \ufb01nance is also growing. Our approach demonstrates signi\ufb01cantly faster com-\nputational performance compared to traditional maximum likelihood estimation methods\nwhile yielding comparable accuracy in both simulation and empirical studies. Further-\nmore, we demonstrate the application of this method for real-time volatility measurement,\nenabling the continuous estimation of \ufb01nancial volatility as new price data keeps coming\nfrom the market.\n1\nIntroduction\nWe propose a real-time estimation and volatility measurement scheme. Speci\ufb01cally, we con-\ntinuously estimate the \ufb01nancial model parameters and volatility in real-time as new price\nprocess become available during the market operation. Our method uses a recurrent neural\nnetwork to estimate the parameters of a Hawkes model based on high-frequency \ufb01nancial\ndata, and these estimates are subsequently used to compute volatility. This approach ex-\nhibits signi\ufb01cantly faster computational performance compared to the traditional maximum\nlikelihood estimation (MLE) method.\nThe Hawkes process introduced by Hawkes (1971) is a type of point process used to model\nthe occurrence of events over time. It is frequently utilized in \ufb01nance to model the arrival of\ntrades or other \ufb01nancial events in the market; here, we use it to describe the \ufb02uctuations in\nthe price in the tick structure. As a two dimensional Hawkes process with symmetric kernel in\nour study, it is characterized by two key features: self-excitation and mutual-excitation (Bacry\net al., 2013). Self-excitation means that the occurrence of an event increases the likelihood\nof the same types of events occurring, while mutual-excitation implies that the occurrence of\none event can increase the likelihood occurrence of other types of events.\nFor the estimation, we use a long short-term memory (LSTM) network introduced by\nHochreiter and Schmidhuber (1997). This is a type of recurrent neural network that is capable\nof learning long-term dependencies in data. LSTMs can be used for a variety of tasks that\ninvolve sequential data, including language modeling, machine translation, speech recognition,\ntime series based on historical data, and \ufb01nancial analysis (Zhang et al., 2021; Ghosh et al.,\n\u2217Department of Statistics, Yeungnam University, Gyeongsan, Gyeongbuk 38541, Korea\n1\narXiv:2304.11883v1 [q-fin.ST] 24 Apr 2023\n\n2022).\nThey are particularly useful for tasks where capturing long-term dependencies is\nimportant, as the gating mechanism allows the network to retain important earlier information\nin the sequence while discarding irrelevant or outdated information.\nThus, our method uses a neural network for parameter estimation. Similar attempts have\nbeen made in recent years in various \ufb01elds (Wlas et al., 2008; Wang et al., 2022; Wei and\nJiang, 2022), but there is still limited research on using neural networks for estimation in\n\ufb01nancial time series. Speci\ufb01cally, we use a direct parameter estimation approach in which\nthe neural network is trained to directly predict the model parameters based on the observed\ndata.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3457,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "the network to retain important earlier information\nin the sequence while discarding irrelevant or outdated information.\nThus, our method uses a neural network for parameter estimation. Similar attempts have\nbeen made in recent years in various \ufb01elds (Wlas et al., 2008; Wang et al., 2022; Wei and\nJiang, 2022), but there is still limited research on using neural networks for estimation in\n\ufb01nancial time series. Speci\ufb01cally, we use a direct parameter estimation approach in which\nthe neural network is trained to directly predict the model parameters based on the observed\ndata. This method is an example of a small aspect of network-based parameter estimation\nand expected to be applied to more complex models.\n2\nMethod\n2.1\nPrice model\nFirst, we explain the stochastic model to describe high-frequency stock price movements. We\nmodel the up and down movements of the tick-level price process as a marked Hawkes process,\nwhich captures both the timing and size of the movements. This process is de\ufb01ned by the\nrandom measures,\nM(du \u00d7 dz) =\n\u0014M1(du \u00d7 dz1)\nM2(du \u00d7 dz2)\n\u0015\n(1)\nin the product space of time and jump size, R \u00d7 Ei, for i = 1, 2, where Ei = N \u00d7 {i} denotes\nthe space of mark (jump) sizes for up and down price movements, respectively. Each measure\nin Eq. (1) is associated with a sequence of Ei-valued random variables {Zi,n} in addition to\nthe sequence of random times {\u03c4i,n} for each i. That is,\nMi(du \u00d7 dzi) =\nX\nn\n\u03b4\u03c4i,n,Zi,n(du \u00d7 dzi)\nwith the Dirac measure \u03b4, which is de\ufb01ned as follows: for any time interval I and Ai \u2282Ei\n\u03b4\u03c4i,n,Zi,n(I \u00d7 Ai) =\n\u001a 1, if \u03c4i,n \u2208I and Zi,n \u2208Ai,\n0, otherwise.\nA vector of c`adl`ag counting processes is de\ufb01ned by\nNt =\n\u0014N1(t)\nN2(t)\n\u0015\n=\nZ\n(0,t]\u00d7E\nDg(z)M(du \u00d7 dz),\nDg(z) =\n\u0014z1\n0\n0\nz2\n\u0015\n,\nE = E1 \u222aE2\nwhich counts the number of events weighted by their size, that is,\nNi(t) = Ni((0, t]) =\nX\nn\nZi,n1{0<\u03c4i,n\u2264t} = # of \u03c4i,n \u2208(0, t],\nfor i = 1, 2.\nAssumption 1. The stochastic intensity \u03bbi for Ni is represented by the following:\n\u03bbt =\n\u0014\u03bb1(t)\n\u03bb2(t)\n\u0015\n= \u00b5 +\nZ\n(\u2212\u221e,t]\u00d7E\n\u03b1 \u25e6b(t \u2212u)M(du \u00d7 dz)\n(2)\n2\n\nwhere \u00b5 is a 2\u00d71 positive constant base intensity vector, \u03b1 is a positive 2\u00d72 constant matrix.\nh is a decay function matrix and \u25e6denotes the element-wise product. From the de\ufb01nition\nof Eq. (2), for simplicity, we assume that the future impact of an event on intensities is\nindependent of jump size, as the integrand of the equation does not contain the jump variable\nz. In addition, for further parsimony, we assume that:\n\u00b5 =\n\u0014\u00b5\n\u00b5\n\u0015\n,\n\u03b1 =\n\u0014\u03b11\n\u03b12\n\u03b12\n\u03b11\n\u0015\n,\nb(t) =\n\u0014e\u2212\u03b2t\ne\u2212\u03b2t\ne\u2212\u03b2t\ne\u2212\u03b2t\n\u0015\n.\n(3)\nHence, the set of parameters to be estimated is {\u00b5, \u03b11, \u03b12, \u03b2}. We also assume that the mark\nZi at time t is independent from the \u03c3-algebra generated by (Nj(s), \u03bbj(s))s<t for j = 1, 2.",
    "chunk_index": 1,
    "start_char": 2878,
    "end_char": 5581,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "an event on intensities is\nindependent of jump size, as the integrand of the equation does not contain the jump variable\nz. In addition, for further parsimony, we assume that:\n\u00b5 =\n\u0014\u00b5\n\u00b5\n\u0015\n,\n\u03b1 =\n\u0014\u03b11\n\u03b12\n\u03b12\n\u03b11\n\u0015\n,\nb(t) =\n\u0014e\u2212\u03b2t\ne\u2212\u03b2t\ne\u2212\u03b2t\ne\u2212\u03b2t\n\u0015\n.\n(3)\nHence, the set of parameters to be estimated is {\u00b5, \u03b11, \u03b12, \u03b2}. We also assume that the mark\nZi at time t is independent from the \u03c3-algebra generated by (Nj(s), \u03bbj(s))s<t for j = 1, 2. The\nintensity process is assumed to be stationary and that the spectral radius of |\nR \u221e\n0 \u03b1 \u25e6b(t)dt|\nis less than 1.\nUnder this assumption, the Hawkes volatility of price movements \u2013 the standard deviation\nof total up and down net movements \u2013 is represented by\nSD(N1(t) \u2212N2(t)) =\nr\nu\u22a4\nh\nT\n\b\nZ \u25e6B\n \n+ Z\n(2) \u25e6Dg(E[\u03bbt])\ni\nut,\nu =\n\u0014 1\n\u22121\n\u0015\n(4)\nwhere T is an operator such that T (M) = M+M\u22a4for a square matrix M and Dg(\u00b7) denotes\na diagonal matrix whose diagonal entry is composed of the argument. Furthermore,\nE[\u03bbt] = (\u03b2 \u2212\u03b1)\u22121\u03b2\u00b5,\n\u03b2 =\n\u0014\u03b2\n0\n0\n\u03b2\n\u0015\n(5)\nand\nE[\u03bbt\u03bb\u22a4\nt ] = (\u03b2 \u2212\u03b1)\u22121\n\u00121\n2\u03b1Dg(E[\u03bbt])\u03b1 + \u03b2\u00b5E[\u03bb\u22a4\nt ]\n\u0013\n(6)\nand\nB =\nn\nZ\n\u22a4\u25e6E[\u03bbt\u03bb\u22a4\nt ] + Dg(E[\u03bbt])\n\u0000\u03b1 \u25e6Z\n\u0001\u22a4\u2212Dg(Z)E[\u03bbt]E[\u03bbt]\u22a4o\n(\u03b2 \u2212\u03b1)\u22121\n(7)\nand by the mark independent assumption,\nZ =\n\u0014E[Z1]\nE[Z2]\nE[Z1]\nE[Z2]\n\u0015\n,\nZ =\n\u0014E[Z2\n1]\nE[Z2\n2]\nE[Z2\n1]\nE[Z2\n2]\n\u0015\n.\nTo calculate the volatility of price changes, rather than the number of movements, we multiply\nthe minimum tick size to Eq. (4). Further details can be found in Lee and Seo (2017) and\nLee (2022).\n2.2\nNetwork model\nNext, we construct a recurrent neural network for parameter estimation. The traditional\nmethod of estimating the parameters of a Hawkes process is MLE, which involves maximizing\nthe log-likelihood function of the model:\nL(T, \u03b8) =\n2\nX\ni=1\n Z\n(0,T]\u00d7E\nlog \u03bbi(u)Mi(du \u00d7 dzi) \u2212\nZ T\n0\n\u03bbi(u)du\n!\n3\n\nto estimate the parameters most likely to generate the observed data.\nIn contrast, neural network based parameter estimation involves training a neural network\nto predict the parameters of a Hawkes process based on input data. To do this, numerous\nsample paths of inter-arrival times and movement types (up or down) are generated, where\nthe parameters of each path are determined randomly. These sample paths are then used as\nfeature variables, and the associated true parameter values are used as the target variables.\nThe neural network is trained on these data. Once trained, it can be used to predict the\nparameter values of a new sample path of Hawkes process data. These predicted parame-\nter values can then be used to compute further complicated formulae, such as the Hawkes\nvolatility in Eq. (4), which is a measure of the variability of the process over time.",
    "chunk_index": 2,
    "start_char": 5151,
    "end_char": 7754,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "the associated true parameter values are used as the target variables.\nThe neural network is trained on these data. Once trained, it can be used to predict the\nparameter values of a new sample path of Hawkes process data. These predicted parame-\nter values can then be used to compute further complicated formulae, such as the Hawkes\nvolatility in Eq. (4), which is a measure of the variability of the process over time.\nHere, we use an LSTM model with three layers as the neural network. The LSTM network\nis known for its capability of retaining information over a long duration, making it appropriate\nfor tasks that require context comprehension or state preservation. The gates mechanism in\nthe LSTM architecture that regulates the \ufb02ow of information between memory cells and the\nnetwork enables the network to choose which information should be preserved or discarded.\nWe also tested gated recurrent unit networks (Cho et al., 2014); however, the LSTM per-\nformed slightly better in our problem. A thorough account of the network\u2019s implementation\nis presented in the following section.\n3\nSimulation result\nIn this simulation study, we generate a set of paths of Hawkes processes to create a training\ndataset for the neural network. The dataset comprises a su\ufb03cient quantity of synthetic data,\nwhich are utilized to train the network to predict the real parameters of the Hawkes process.\nThe real parameters for each Hawkes process are randomly selected to cover the entire range of\npossible values, with each path having distinct parameters. For the ranges of the parameters,\nwe use the ranges of the estimates obtained by \ufb01tting the past intraday price process of various\nstocks to the Hawkes model. Approximately 30 symbols of stocks, including AAPL, AMZN,\nC, FB, GOOG, IBM, MCD, MSFT, NVDA, and XOM from 2018 to 2019 are used.\nFor the estimation, we focus on high-frequency rather than ultra-high-frequency. More\nprecisely, the raw data are \ufb01ltered as follows. We observe the mid-price at intervals of \u2206t = 0.1\nseconds, noting any changes from the previously observed price. If a change is detected, we\nrecord the exact time of the change and the new price. If the price remains the same, we\nmove on to the next interval of 0.1 seconds and repeat the process. This method allows us to\n\ufb01lter out unnecessary movements, commonly referred to as microstructure noise, observed at\nultra-high frequencies.\nOnce the set of estimates is obtained, we generate Hawkes process paths of a 2,000-time\nstep.\nThese are then used for neural network training together with estimates as target\nvariables. This method yields tens of thousands of datasets, which are su\ufb03cient to construct\na comprehensive training set.\nThe implementation of the LSTM network model is as follows. The \ufb01rst layer consists\nof 12 units, which manage sequential data. These data are a two-dimensional input of time\nseries data that comprise inter-arrivals and types of movements (up or down). Up and down\nmovements are encoded as 1 and 2, respectively. The output of this layer is a sequence of\n12 length of vectors, where each vector is the output of the \ufb01rst layer at a given time step.\n4",
    "chunk_index": 3,
    "start_char": 7334,
    "end_char": 10489,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "training together with estimates as target\nvariables. This method yields tens of thousands of datasets, which are su\ufb03cient to construct\na comprehensive training set.\nThe implementation of the LSTM network model is as follows. The \ufb01rst layer consists\nof 12 units, which manage sequential data. These data are a two-dimensional input of time\nseries data that comprise inter-arrivals and types of movements (up or down). Up and down\nmovements are encoded as 1 and 2, respectively. The output of this layer is a sequence of\n12 length of vectors, where each vector is the output of the \ufb01rst layer at a given time step.\n4\n\nThus, if the original time series has a 2,000-time step, then the output of the \ufb01rst layer is a\n2, 000 \u00d7 12 matrix, which is the time step \u00d7 the number of units.\nThe second layer has 12 units and produces a single (not a sequence of) vector of length\n12. The \ufb01nal layer is a dense (fully connected) layer with four units, which produces output\nrepresenting the parameters in the Hawkes model, \u00b5, \u03b11, \u03b12 and \u03b2. If we extend the model\nfor more complexity, the number of units in the last dense layer will be adjusted accordingly.\nThis is because each unit in the last layer represents each parameter.\nAs pointed out by Mei and Eisner (2017), a natural extension of LSTM is deep LSTM,\nespecially for complex structured data such as high-frequency \ufb01nancial data, where the ef-\nfectiveness of multi-layering seems promising. However, we utilized a relatively parsimonious\nHawkes model and achieved su\ufb03cient performance without employing a large number of lay-\ners, so we used the LSTM model proposed above, which is relatively faster to train. If the data\nstructure and model become more complex, an extension to deep LSTM would be helpful.\nFor training, we use 75,000 sample data points; hence, the dataset for the neural network\u2019s\ninput has a shape of 75, 000 \u00d7 1, 000 \u00d7 2. The Adam (Kingma and Ba, 2014) optimizer is\nused for training. Generally, more than 300 epochs are used. For testing, we use 15,000 data\npoints that were not used for training. This is done to evaluate the model\u2019s ability to make\npredictions on these unseen data based on mean squared error (MSE). We then compare\nthe results with a traditional MLE. The computation times were measured using a typical\ncommercial PC. The result shows that the MLE has slightly better performance in terms of\nMSE; however, the neural network also shows a reasonable result. Meanwhile, the general\nnumerical method of MLE requires many iterations and is time consuming. However, a well-\ntrained neural network computes an estimate very quickly, which is less than a hundredth of\nthe time required by MLE.\nNeural network\nMLE\nMSE\n0.0513\n0.0417\nTime (sec)\n0.0120\n1.763\nTo understand the basic properties of the neural network estimator, we investigate its\nsampling distribution. To examine the sampling distribution, we generate 100,000 paths of\nlength 2,000 using the \ufb01xed parameter values of \u00b5 = 0.3, \u03b11 = 0.4, \u03b12 = 0.7, \u03b2 = 1.5.",
    "chunk_index": 4,
    "start_char": 9874,
    "end_char": 12871,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "a reasonable result. Meanwhile, the general\nnumerical method of MLE requires many iterations and is time consuming. However, a well-\ntrained neural network computes an estimate very quickly, which is less than a hundredth of\nthe time required by MLE.\nNeural network\nMLE\nMSE\n0.0513\n0.0417\nTime (sec)\n0.0120\n1.763\nTo understand the basic properties of the neural network estimator, we investigate its\nsampling distribution. To examine the sampling distribution, we generate 100,000 paths of\nlength 2,000 using the \ufb01xed parameter values of \u00b5 = 0.3, \u03b11 = 0.4, \u03b12 = 0.7, \u03b2 = 1.5. We then\ncompare the obtained sampling distributions using the neural network and MLE methods.\nOverall, MLE outperforms the neural network slightly. Even so, the general performance of\nneural networks is also quite good.\nParameter\nTrue\nNeural network\nMLE\nMean\nS.D.\nMean\nS.D.\n\u00b5\n0.3000\n0.3151\n0.0358\n0.3036\n0.0314\n\u03b11\n0.4000\n0.4429\n0.0661\n0.3988\n0.0500\n\u03b12\n0.7000\n0.5733\n0.0697\n0.7024\n0.0608\n\u03b2\n1.5000\n1.5736\n0.1364\n1.5078\n0.1145\nSpeci\ufb01cally, the aforementioned example compares the performance of the numerical op-\ntimizer (used in MLE) and neural network. Owing to the nature of simulation studies, the\nnumerical optimizer has several advantages. The outcome of a numerical optimizer is often\nin\ufb02uenced by its initial value. In the example provided above, because the true value of the\n5\n\nparameter is known, it was directly used as the initial value. This may have improved the per-\nformance compared to the result from a random initial value. As \ufb01nding a good initial value\nis sometimes challenging, a numerical optimizer may perform worse in real-world problems.\nIn addition, if the numerical optimizer exhibits unexpected behavior, such as failure to\nconverge appropriately, human engagement may be necessary, such as adjustments to the\ninitial value or retrying the procedure. As the complexity of the model and level of noise\npresent in the empirical data increase, the advantages of the numerical optimizer may decrease.\nIn such cases, further research may be required to determine whether the numerical optimizer\nstill outperforms the neural network.\n4\nEmpirical result\nThe approach in the previous section can be directly applied to empirical data. However,\nwe need to consider whether robust estimation can be made in situations where the empirical\ndata do not completely follow the Hawkes process. For example, in \ufb01ltered high-frequency\nprice process data, a subdue e\ufb00ect (where an event reduces intensity) can sometimes occur.\nThis can result in negative estimates for the parameter \u03b1, which violates the de\ufb01nition of the\nHawkes model. To address this, a more complex model should be used; however, as this falls\noutside the scope of this study, an alternative method is to use a softplus activation function\nlog(1+exp(x)) for the last layer in the neural network. This approach is similar to constraint\noptimization by ensuring positive estimates. Furthermore, instead of predicting \u03b2 directly,\nwe trained and predicted \u03b2 \u2212\u03b11 \u2212\u03b12. By using the softplus function, this method ensures\nthat the branching ratio condition of the Hawkes model is met.",
    "chunk_index": 5,
    "start_char": 12297,
    "end_char": 15426,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "should be used; however, as this falls\noutside the scope of this study, an alternative method is to use a softplus activation function\nlog(1+exp(x)) for the last layer in the neural network. This approach is similar to constraint\noptimization by ensuring positive estimates. Furthermore, instead of predicting \u03b2 directly,\nwe trained and predicted \u03b2 \u2212\u03b11 \u2212\u03b12. By using the softplus function, this method ensures\nthat the branching ratio condition of the Hawkes model is met.\nTo further increase robustness, a combination of empirical data and its maximum likeli-\nhood estimates as training data can be used, rather than relying solely on simulation data.\nThis approach accounts for the possibility of model mis-speci\ufb01cation; for instance, the ob-\nserved data may not perfectly align with the Hawkes process. By incorporating the MLE into\nthe training data, the neural network can better mimic the MLE of the Hawkes model. Thus,\nif the goal is to construct a neural network that closely approximates the MLE, even under\nthe possibility of model mis-speci\ufb01cation, this method can be e\ufb00ective.\nThe following section explains the step-by-step procedure. We select segments of observed\nintraday data of inter-arrivals and movement types. Each segment consists of a 2,000-time\nstep. These selected paths are used to \ufb01t the Hawkes model using MLE. The resulting dataset\nis then used to train the neural network, where the inter-arrivals and types of real data serve\nas feature variables and the maximum likelihood estimates are the target variables.\nFigure 1 illustrates the intraday dynamics of estimates of the Hawkes model on a speci\ufb01c\ndate. The data used for this illustration are out-of-sample that are not used for training.\nSpeci\ufb01cally, it was estimated using segments of data corresponding to every 2,000-time step.\nThis corresponds to a time horizon of approximately 10-20 minutes. To create a more con-\ntinuous graph, the time windows for estimation were moved forward slowly with su\ufb03cient\noverlap. The neural network shows very consistent results with MLE.\nFigure 2 presents the instantaneous intraday annualized Hawkes volatility calculated using\nMLE, a neural network, and nonparametric realized volatility as a benchmark by Andersen\net al. (2003). The realized volatility is calculated using the observed values at 1-second inter-\nvals for the price process of the period. All three measures have a similar trend throughout\n6\n\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nAAPL, 2020-01-10\nNeural network\nMLE\n(a) \u00b5\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nAAPL, 2020-01-10\nNeural network\nMLE\n(b) \u03b2\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n1\nAAPL, 2020-01-10\nNeural network\nMLE\n(c) \u03b11\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.1\n0.2\n0.3\n0.4\n2\nAAPL, 2020-01-10\nNeural network\nMLE\n(d) \u03b12\nFigure 1: Intraday estimates for the NBBO of AAPL using MLE and neural network\nthe day. Although it is not possible to present all the results examined, in some cases, MLE\nshowed unstable dynamics of volatility.",
    "chunk_index": 6,
    "start_char": 14954,
    "end_char": 18057,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "MLE\n(b) \u03b2\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n1\nAAPL, 2020-01-10\nNeural network\nMLE\n(c) \u03b11\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.1\n0.2\n0.3\n0.4\n2\nAAPL, 2020-01-10\nNeural network\nMLE\n(d) \u03b12\nFigure 1: Intraday estimates for the NBBO of AAPL using MLE and neural network\nthe day. Although it is not possible to present all the results examined, in some cases, MLE\nshowed unstable dynamics of volatility. This is likely due to the fact that the 2,000-time\nlength used for estimation is a relatively small sample size to estimate the parameters of the\nHawkes model.\n5\nConclusion\nThis study shows that a neural network can accurately estimate time series parameters, with\nan accuracy similar to MLE and much faster computation. While the example used here is\nfor calculating Hawkes volatility, but our proposed method can be applied to various \ufb01elds.\nIt can be particularly useful in cases where the model is complex and traditional estimation\nprocedures are challenging, such as modeling entire limit order book. Further research in this\narea is expected to be ongoing and diverse.\n7\n\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nVolatility\nAAPL, 2020-01-10\nNeural network\nMLE\nRealized volatility\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\n16:00\nTime\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nVolatility\nAAPL, 2020-01-13\nNeural network\nMLE\nRealized volatility\nFigure 2: Intraday annualized volatilities for the NBBO of AAPL using MLE and neural\nnetwork\nAcknowledgements\nThis work has supported by the National Research Foundation of Korea(NRF) grant funded\nby the Korea government(MSIT)(No. NRF-2021R1C1C1007692).\nReferences\nAndersen, T. G., T. Bollerslev, F. X. Diebold, and P. Labys (2003). Modeling and forecasting\nrealized volatility. Econometrica 71, 579\u2013625.\nBacry, E., S. Delattre, M. Ho\ufb00mann, and J.-F. Muzy (2013). Modelling microstructure noise\nwith mutually exciting point processes. Quantitative Finance 13, 65\u201377.\nCho, K., B. Van Merri\u00a8enboer, D. Bahdanau, and Y. Bengio (2014). On the properties of neural\nmachine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\nGhosh, P., A. Neufeld, and J. K. Sahoo (2022). Forecasting directional movements of stock\nprices for intraday trading using lstm and random forests. Finance Research Letters 46,\n102280.\nHawkes, A. G. (1971). Point spectra of some mutually exciting point processes. Journal of\nthe Royal Statistical Society. Series B (Methodological) 33, 438\u2013443.\nHochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural computation 9,\n1735\u20131780.\nKingma, D. P. and J. Ba (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nLee, K. (2022). Application of hawkes volatility in the observation of \ufb01ltered high-frequency\nprice process in tick structures. arXiv 2207.05939.\nLee, K. and B. K. Seo (2017).\nMarked Hawkes process modeling of price dynamics and\nvolatility estimation. Journal of Empirical Finance 40, 174\u2013200.\n8\n\nMei, H. and J. M. Eisner (2017). The neural Hawkes process: A neurally self-modulating\nmultivariate point process. Advances in Neural Information Processing Systems 30.\nWang, X., J. Feng, Q. Liu, Y. Li, and Y. Xu (2022).",
    "chunk_index": 7,
    "start_char": 17606,
    "end_char": 20856,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "Lee, K. (2022). Application of hawkes volatility in the observation of \ufb01ltered high-frequency\nprice process in tick structures. arXiv 2207.05939.\nLee, K. and B. K. Seo (2017).\nMarked Hawkes process modeling of price dynamics and\nvolatility estimation. Journal of Empirical Finance 40, 174\u2013200.\n8\n\nMei, H. and J. M. Eisner (2017). The neural Hawkes process: A neurally self-modulating\nmultivariate point process. Advances in Neural Information Processing Systems 30.\nWang, X., J. Feng, Q. Liu, Y. Li, and Y. Xu (2022).\nNeural network-based parameter\nestimation of stochastic di\ufb00erential equations driven by l\u00b4evy noise. Physica A: Statistical\nMechanics and its Applications 606, 128146.\nWei, Y. M. and Z. Jiang (2022). Estimating parameters of structural models using neural\nnetworks. USC Marshall School of Business Research Paper.\nWlas, M., Z. Krzeminski, and H. A. Toliyat (2008). Neural-network-based parameter estima-\ntions of induction motors. IEEE Transactions on Industrial Electronics 55, 1783\u20131794.\nZhang, Y., G. Chu, and D. Shen (2021). The role of investor attention in predicting stock\nprices: The long short-term memory networks perspective. Finance Research Letters 38,\n101484.\n9",
    "chunk_index": 8,
    "start_char": 20339,
    "end_char": 21532,
    "paper_title": "Recurrent neural network based parameter estimatio",
    "paper_category": "q-fin.ST",
    "paper_filename": "Recurrent_neural_network_based_parameter_estimatio.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Recurrent_neural_network_based_parameter_estimatio.pdf"
  },
  {
    "text": "SCALING, STABILITY AND DISTRIBUTION OF THE\nHIGH-FREQUENCY RETURNS OF THE IBEX35 INDEX\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nAbstract. In this paper we perform a statistical analysis of the high-frequency re-\nturns of the Ibex35 Madrid stock exchange index. We \ufb01nd that its probability distri-\nbution seems to be stable over di\ufb00erent time scales, a stylized fact observed in many\ndi\ufb00erent \ufb01nancial time series. However, an in-depth analysis of the data using max-\nimum likelihood estimation and di\ufb00erent goodness-of-\ufb01t tests rejects the L\u00b4evy-stable\nlaw as a plausible underlying probabilistic model. The analysis shows that the Nor-\nmal Inverse Gaussian distribution provides an overall \ufb01t for the data better than any\nof the other subclasses of the family of the Generalized Hyperbolic distributions and\ncertainly much better than the L\u00b4evy-stable laws. Furthermore, the right (resp. left)\ntail of the distribution seems to follow a power-law with exponent \u03b1 \u22484.60 (resp.\n\u03b1 \u22484.28). Finally, we present evidence that the observed stability is due to temporal\ncorrelations or non-stationarities of the data.\nKeywords: \ufb01nancial time series, high-frequency returns, generalized hyperbolic\ndistributions, L\u00b4evy-stable distributions, scaling laws, tail behaviour\n1. Introduction\nThe marginal distribution of returns of \ufb01nancial assets have been placed under\nscrutiny since the times of Bachelier [5], and the idea of treating log-returns as inde-\npendent identically distributed Gaussian random variables lies in the core of the most\nwell-known and celebrated \ufb01nancial models [9, 37] and so it is crucial for derivative\npricing and risk management. And even though this idea works \ufb01ne as a \ufb01rst approx-\nimation, it is well documented that empirical \ufb01nancial data drawn from very di\ufb00erent\nmarkets, time periods and instruments do not \ufb01t the Gaussian model [15, 20, 33, 38].\nThe empirical distributions of log-returns present tails heavier than Gaussian as well\nas many other non-trivial statistical properties collectivelly know as stylized facts [14]\nthat place the Gaussian hypothesis in jeopardy and point towards a possible universal\nbehavior of the underlying processes.\nOne of the most celebrated of these stylized facts is the scaling symmetry or stability\nof the distribution of log-returns, i.e. its invariance under aggregation up to rescaling.\nFor independent identically distributed random variables the Gaussian law is the only\ndistribution with \ufb01nite second moment that has this property, and that is why the\ncentral limit theorem singles it out as the limiting distribution of rescaled sums of\ni.i.d. random variables with \ufb01nite variance [11]. To observe stability in distributions\nother than Gaussian the requirement of a \ufb01nite second moment has to be dropped. This\nseminal idea was \ufb01rst expounded in \ufb01nance by Mandelbrot [33] who proposed the family\n1\narXiv:1208.0317v1 [q-fin.ST] 1 Aug 2012\n\n2\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nof L\u00b4evy-stable laws as an alternative to the Gaussian model of log-returns. One feature\nof these probability distributions is the divergence of their second moment caused by\nthe power-law behavior of its tails with characteristic exponent \u03b1 < 2.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3206,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "idea was \ufb01rst expounded in \ufb01nance by Mandelbrot [33] who proposed the family\n1\narXiv:1208.0317v1 [q-fin.ST] 1 Aug 2012\n\n2\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nof L\u00b4evy-stable laws as an alternative to the Gaussian model of log-returns. One feature\nof these probability distributions is the divergence of their second moment caused by\nthe power-law behavior of its tails with characteristic exponent \u03b1 < 2. Considering\na \ufb01nancial market as a complex system of interacting agents in which prices are the\noutcome of many independent individual decisions, according to the generalized central\nlimit theorem its limiting distribution should be a member of the L\u00b4evy-stable family\n\u2014 of which the Gaussian distribution is just a special case \u2014 with the normalization\nconstant depending on the tail index of the power-law [11].\nTherefore, it seems natural to investigate the tails of the distribution of log-returns\nin order to shed some light on its stability properties. However, this is a moot point:\nalthough some authors [20, 33] have reported power-law behavior with \u03b1 < 2, others\nhave reported distributions with power-law tails far away from the stability regime\n[21,23,29,30,40]. It could be argued that there is an endemic arbitrariness of the least-\nsquare regression used to study the power-law behaviour in empirical data, but that\nproblem could be overcome replacing it with maximum likelihood estimation together\nwith goodness-of-\ufb01t techniques [13]. This fact notwithstanding, it is also well known\nthat an estimated tail index above two is not an evidence against stability: it could well\nhave been produced by a stable distribution with \u03b1 as low as 1.65 with the situation\ngetting worse as we approach the \u03b1 = 2 limit [16,36]. And to round this o\ufb00, it is not\nonly di\ufb03cult to discriminate between di\ufb00erent power-laws or even between stability or\nthe lack of it; the sole task of distinguishing a power-law from a stretched exponential\nis still subject to debate [32], since certain empirical distributions of log-returns seem\nto decay asymptotically slower than any power-law [15,24,28].\nAmong the distributions with tails lighter than power-laws, a family that has been\nused with success to model log-returns are the Generalized Hiperbolic laws. The em-\nbryo of this family of distributions is the Hyperbolic distribution, \ufb01rst proposed by\nRalph Alger Bagnold [6] to model the size distribution of the wind-blown sand. Later,\nBarndor\ufb00-Nielsen \u2014 still with the problem of the distribution of particle size in mind \u2014\ngeneralized it to the family of Generalized Hyperbolic (GH ) distributions [7], of which\nthe hyperbolic distribution is a special case. Di\ufb00erent subclasses of this family have been\nsince then proposed as alternatives to both Gaussian and L\u00b4evy-stable laws as statisti-\ncal models of \ufb01nancial returns, namely, the Skewed Student\u2019s t distribution [10,41], the\nHyperbolic distribution by Eberlein and Keller [19,27], the Variance-Gamma of Madan\nand Seneta [31] and the Normal Inverse Gaussian (NIG) by Barndor\ufb00-Nielsen [8]. One\nof the most appealing properties of this family is its tail behavior, which is a power-law\nmodulated by an exponential.",
    "chunk_index": 1,
    "start_char": 2792,
    "end_char": 5969,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "special case. Di\ufb00erent subclasses of this family have been\nsince then proposed as alternatives to both Gaussian and L\u00b4evy-stable laws as statisti-\ncal models of \ufb01nancial returns, namely, the Skewed Student\u2019s t distribution [10,41], the\nHyperbolic distribution by Eberlein and Keller [19,27], the Variance-Gamma of Madan\nand Seneta [31] and the Normal Inverse Gaussian (NIG) by Barndor\ufb00-Nielsen [8]. One\nof the most appealing properties of this family is its tail behavior, which is a power-law\nmodulated by an exponential. These lighter tails seem well suited to \ufb01t the empirical\ndistributions of log-returns as the studies cited above show, since the data seem to have\ntails heavier than Gaussian but still lighter than the L\u00b4evy-stable laws.\nAs can be inferred, the question of the true nature of the distribution of log-returns\n(or even its tail behavior) and the origin of its apparent stability is far from being\nsettled, and therefore, the study of diverse \ufb01nancial time series drawn from di\ufb00erent\ninstruments, markets and time periods is pertinent in order to shed some light on this\nissue. In this paper we will carry out a thorough study of the distributional properties\nof the high-frequency log-returns of the index Ibex35 from the Madrid Stock Exchange.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n3\nAfter reviewing the basic properties of both the L\u00b4evy-stable and Generalized Hyperbolic\nlaws in Section 2, we will perform a series of \ufb01ts of these families to the observed log-\nreturns as well as di\ufb00erent statistical tests to quantify their goodness-of-\ufb01t (Section\n3). There, we will also study in close detail the tail behavior of the data in order to\nelucidate its possible stability and we will address the question of the scaling symmetry\nof the data. In Section 4 we will sum up the results of the previous section and we will\nconfront them to those obtained in other studies. The conclusions will be expounded\nin Section 5.\n2. L\u00b4evy-stable laws and GH distributions\n2.1. L\u00b4evy-stable laws. L\u00b4evy-stable laws do not have a closed analytical form for its\nprobability density function in general, but they can be readily de\ufb01ned in terms of their\ncharacteristic function \u03d5(t):\n(1)\n\u03d5(t) = exp[it\u00b5 \u2212|\u03b4t|\u03b1(1 \u2212i\u03b2 sgn(t)\u03a6)]\n(2)\n\u03a6 =\n\uf8f1\n\uf8f2\n\uf8f3\ntg \u03c0\u03b1\n2\nif \u03b1 \u0338= 1\n\u22122\n\u03c0 log |t|\nif \u03b1 = 1\nThe characteristic exponent \u03b1 \u2208(0, 2] determines the weight of the tails and the\nskewness parameter \u03b2 \u2208[\u22121, 1] its asymmetry. The parameters \u00b5 and \u03b4 are its location\nand scale parameters respectively. The Gaussian distribution is a special case of this\nfamily with \u03b1 = 2, \u03b2 = 0. A random variable X is the limit in distribution of normalized\nsums of i.i.d random variables if and only if X has a L\u00b4evy-stable law [11].\n2.2. Generalized Hiperbolic laws. The Generalized Hyperbolic distribution can be\nparametrized in several ways. Following Prause [42], its probability density function\ncan be written as:\n(3)\nf(x;",
    "chunk_index": 2,
    "start_char": 5447,
    "end_char": 8347,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "is a special case of this\nfamily with \u03b1 = 2, \u03b2 = 0. A random variable X is the limit in distribution of normalized\nsums of i.i.d random variables if and only if X has a L\u00b4evy-stable law [11].\n2.2. Generalized Hiperbolic laws. The Generalized Hyperbolic distribution can be\nparametrized in several ways. Following Prause [42], its probability density function\ncan be written as:\n(3)\nf(x; \u03bb, \u03b4, \u03b1, \u00b5, \u03b2) =\n(\u03b3/\u03b4)\u03bb\n\u221a\n2\u03c0K\u03bb(\u03b4\u03b3) e\u03b2(x\u2212\u00b5)K\u03bb\u22121/2\n\u0010\n\u03b1\np\n\u03b42 + (x \u2212\u00b5)2\n\u0011\n\u0010p\n\u03b42 + (x \u2212\u00b5)2/\u03b1\n\u00111/2\u2212\u03bb\nwhere \u03b3 =\np\n\u03b12 + \u03b22 and K\u03bb\u22121/2 is the modi\ufb01ed Bessel function of the third kind\nwith index \u03bb \u22121/2. The parameter \u03b1 > 0 determines the shape of the distribution\nand 0 \u2264|\u03b2| < \u03b1 its skewness. The usual location and scale parameters are \u00b5 and\n\u03b4.\nThe parameter \u03bb characterizes certain subclasses and in\ufb02uences the size of the\nmass contained in the tails.\nThese distributions can be thought as mean-variance\nmixtures of Gaussian distributions where the mixing distribution is the Generalized\nInverse Gaussian distribution [18].\nLetting \u03bb = \u22121\n2 we obtain the Normal Inverse Gaussian distribution (NIG), whose\nprobability density function is:\n\n4\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\n(4)\nf(x) =\n\u03b1\u03b4K1\n\u0010\n\u03b1\np\n\u03b42 + (x \u2212\u00b5)2\n\u0011\n\u03c0\np\n\u03b42 + (x \u2212\u00b5)2\ne\u03b4\u03b3+\u03b2(x\u2212\u00b5)\nAll its moments are well de\ufb01ned since it decays as x\u03b1e\u2212\u03b2x. The NIG distribution is\nthe only subclass of the GH family which is closed under convolution; this fact greatly\nsimpli\ufb01es the computations for option pricing [17].\nLetting \u03bb = \u2212\u03bd\n2 and \u03b1 \u2192|\u03b2| in the formula 3 above, we obtain the GH skew Student\u2019s\nt distribution. Its density is given by:\n(5)\nf(x) =\n2\n1\u2212\u03bd\n2 \u03b4\u03bd|\u03b2|\n1+\u03bd\n2 K \u03bd+1\n2\n\u0010p\n\u03b22(\u03b42 + (x \u2212\u00b5)2)\n\u0011\ne\u03b2(x\u2212\u00b5)\n\u0393(\u03bd\n2)\u221a\u03c0\n\u0010p\n\u03b42 + (x \u2212\u00b5)2\n\u0011 \u03bd+1\n2\nThis is the only GH subfamily with di\ufb00erent asymptotic behaviour of its density\nfunction: one tail is a power-law with characteristic exponent equal to \u2212\u03bd/2 \u22121 and\nthe other is a power-law with exponent \u2212\u03bd/2 \u22121 modulated by a factor e\u22122|\u03b2||x|. If the\nasymmetry parameter \u03b2 is zero, we recover the classical Student\u2019s t distribution with\nsymmetric and power-law tails, with a well de\ufb01ned second moment for \u03bd > 2 [1].\n3. Analysis of the data\n3.1. The data. Our data set contains the price ticks of the index Ibex35 of the Madrid\nStock Exchange1. The index Ibex35 is a weighted index formed by the 35 most liquid\nSpanish stocks traded at the Madrid Stock Exchange. The data set covers the period\nfrom January 2nd 2009 to December 31st 2010 and comprises 510 market days.\nThe values of the index are not updated evenly; the records oscillate between two\nand around twelve seconds.\nIn order to have a well de\ufb01ned time interval we have\nsampled these ticks in \ufb01fteen-seconds intervals obtaining a series with 1036321 records.\nFrom this time series we have obtained the log-returns.",
    "chunk_index": 3,
    "start_char": 7961,
    "end_char": 10717,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "35 most liquid\nSpanish stocks traded at the Madrid Stock Exchange. The data set covers the period\nfrom January 2nd 2009 to December 31st 2010 and comprises 510 market days.\nThe values of the index are not updated evenly; the records oscillate between two\nand around twelve seconds.\nIn order to have a well de\ufb01ned time interval we have\nsampled these ticks in \ufb01fteen-seconds intervals obtaining a series with 1036321 records.\nFrom this time series we have obtained the log-returns. However, some issues had to\nbe taken into account before doing this. First, we have to discard the discontinuity\ncreated overnight to avoid artifacts: we therefore focus exclusively on intraday returns.\nSecond, there is a 30 second uncertainty in the closing time of the session in order to\navoid arbitrages: we have accordingly taken a security margin \ufb01nishing our sessions at\n17:29. Some authors [32] have also pointed out that the volatility pattern present during\nthe day (the \u201clunch e\ufb00ect\u201d; see Figure 1) should be taken into account by normalizing\neach return with the average absolute return of that time of the session. However, as\nhappened in the study cited above, in ours we have not observed substantial di\ufb00erences\nbetween the raw and the normalized data; therefore, we have worked exclusively with\nraw returns. The \ufb01nal return series contains 1035810 records, with 2031 records for each\nmarket day (Figure 2). The sample statistics can be found in Table 1 once normalized\nin scale and location.\n1Data obtained from www.tickdata.com.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n5\nFigure 1. Lunch e\ufb00ect. At 15:30 CET Wall Street opens, and at 14:30\nCET and 16:00 CET macroeconomic indicators in the USA are an-\nnounced.\nMAX.\nmin.\n\u00b5\n\u03c32\n\u03b2\n\u03ba\n29.181\n-28.184\n0.000\n1.000\n-0.241\n13.659\nTable 1. Sample statistics.\n3.2. Estimation of the parameters. Estimation of the parameters of all the distribu-\ntions has been accomplished using the method of maximum likelihood. The asymptotic\nproperties and optimality of this method of estimation are widely acknowledged [43].\nHowever, for the family of L\u00b4evy-stable laws, parameter estimation via maximum likeli-\nhood is not straightforward due to the fact that an analytical expression of the proba-\nbility density function is not available, and therefore, the method is only applicable by\nnumerical approximation which is very time consuming due to the sample size. Other\nfaster possibilities include methods based on the sample quantiles [35] or regression via\nthe sample characteristic function; see [46] for a survey of the most usual estimation\nmethods for this family of distributions. The estimated parameters can be found in Ta-\nble 2, and Figure 3 shows the semilog plots of the estimated densities and the empirical\nhistogram.\n\n6\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nFigure 2. Ibex35 normalized logarithmic returns.\nParameters:\n\u00b5\n\u03b4\n\u03b2\n\u03b1\n\u03bd\n\u03bb\nL\u00b4evy-stable\n0.0071\n0.4825\n0.0102\n1.5358\n\u2014\n\u2014\nGH\n0.0101\n0.6495\n-0.0103\n0.6296\n\u2014\n-0.5352\nStudent\u2019s t\n0.0101\n0.9643\n-0.0089\n\u2014\n2.7029\n\u2014\nNIG\n0.0101\n0.6365\n-0.0103\n0.6490\n\u2014\n\u2014\nTable 2. Estimated distribution parameters.\n3.3.",
    "chunk_index": 4,
    "start_char": 10238,
    "end_char": 13339,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "distributions. The estimated parameters can be found in Ta-\nble 2, and Figure 3 shows the semilog plots of the estimated densities and the empirical\nhistogram.\n\n6\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nFigure 2. Ibex35 normalized logarithmic returns.\nParameters:\n\u00b5\n\u03b4\n\u03b2\n\u03b1\n\u03bd\n\u03bb\nL\u00b4evy-stable\n0.0071\n0.4825\n0.0102\n1.5358\n\u2014\n\u2014\nGH\n0.0101\n0.6495\n-0.0103\n0.6296\n\u2014\n-0.5352\nStudent\u2019s t\n0.0101\n0.9643\n-0.0089\n\u2014\n2.7029\n\u2014\nNIG\n0.0101\n0.6365\n-0.0103\n0.6490\n\u2014\n\u2014\nTable 2. Estimated distribution parameters.\n3.3. Goodness-of-\ufb01t tests. To quantify the goodness-of-\ufb01t of the estimated distribu-\ntions three di\ufb00erent statistical tests have been used: the \u03c72, the Kolmogorov\u2013Smirnov\nand the Anderson\u2013Darling [4] tests. These last two tests \u2014 based on the cumulative\ndistribution function rather than on the probability density function as the simpler\n\u03c72 test \u2014 make a better use of the information contained in the sample since it does\nnot need to be binned. Their drawback, however, is that they are much more compu-\ntationally intensive since the distribution function has to be evaluated at the sample\npoints and this implies millions of numerical integrations of trascendental functions.\nApart from this, only when the parameters are part of the hypothesis the large-sample\ndistribution of the test statistic is known; for estimated parameters (like in this case),\nthis distribution is not known except for a few special cases [45]. Montecarlo simula-\ntion \u2014 the usual approach to tackle this problem [46] \u2014 is not feasible here due to the\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n7\nFigure 3. Histogram and estimated pdfs.\nsample size and to the associated computational time needed to obtain the distribution\nfunction. Anyway \u2014as Anderson [3] points out\u2014 the percentage points for the tests\nwhen the parameters are estimated are much smaller than those obtained when the\nparameters are known: a rejected hypothesis using these latter percentage points will\nthen also be rejected with an even higher con\ufb01dence level when using the former. In\nany case \u2014and as a general rule\u2014 the lower the value of the test statistic, the better\nthe \ufb01t.\nTest statistic\n\u03c72\nK-S\nA-D\nL\u00b4evy-stable\n17133.87\n0.0175\n512.17\nGH\n4556.36\n0.0147\n53.88\nStudent\u2019s t\n6967.21\n0.0148\n193.73\nNIG\n4911.18\n0.0147\n52.30\nTable 3. Goodness-of-\ufb01t statistics.\nThe values of the test statistics obtained for our data and the upper bounds for the\ncritical points of the tests for the given con\ufb01dence levels are shown in Tables 3 and 4.\nAs it can be observed, the null hypothesis is rejected for any given reasonable con\ufb01dence\nlevel for all the distributions. However, the hyperbolic distributions clearly outperform\nthe L\u00b4evy-stable law.\n\n8\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nCon\ufb01dence\n\u03c72\nK-S\nA-D\n5%\n231.92\n0.00134\n0.4614\n1%\n245.48\n0.00160\n0.7435\nTable 4. Critical points for the goodness-of-\ufb01t tests.\nFor the members of the GH family, a likelihood ratio (\u039b) test has been also performed.\nThis will allow us to quantify which of the two subclasses (i.e. NIG or Student\u2019s t) is\nthe soundest and whether or not the GH model can be reduced to one of its subfamilies.",
    "chunk_index": 5,
    "start_char": 12840,
    "end_char": 15958,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "distributions. However, the hyperbolic distributions clearly outperform\nthe L\u00b4evy-stable law.\n\n8\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nCon\ufb01dence\n\u03c72\nK-S\nA-D\n5%\n231.92\n0.00134\n0.4614\n1%\n245.48\n0.00160\n0.7435\nTable 4. Critical points for the goodness-of-\ufb01t tests.\nFor the members of the GH family, a likelihood ratio (\u039b) test has been also performed.\nThis will allow us to quantify which of the two subclasses (i.e. NIG or Student\u2019s t) is\nthe soundest and whether or not the GH model can be reduced to one of its subfamilies.\nThe values obtained for the statistic \u22122 log \u039b are tabulated in Table 5 along with the p-\nvalues of a \u03c72\n1 variable, its large-sample distribution under the hypothesis of asymptotic\nnormality of the maximum likelihood estimators. According to this, if we accept the\nhypothesis that the data follows a GH distribution, we cannot reject with a con\ufb01dence\nlevel of 2% the hypothesis that it in fact follows a NIG distribution, while the hypothesis\nthat the data follows a Student\u2019s t distribution is rejected at any reasonable con\ufb01dence\nlevel2.\nDistribution\n\u22122 log \u039b\np-value\nNIG\n5.49\n0.02\nStudent\u2019s t\n4551.78\n< 10\u221216\nTable 5. Likelihood-ratio statistics for the GH subfamilies\n3.4. Asymptotic behavior. Since all of the usual distributions are rejected as plau-\nsible hypothesis for the data, we have also studied in detail the asymptotic behavior of\nthe tails. On a log-log plot (Figure 4) they seem to \ufb01t rather well a straight line. Using\nthe methodology proposed in [13] to analyze and estimate power-laws in empirical data,\nwe have obtained a value of 4.60 (resp. 4.28) for the characteristic exponent \u03b1 and a a\nvalue of 7.76 (resp. 6.70) for the scale parameter xmin for the right (resp. left) tail.\nEven though the tails seem to follow a power-law well outside the stability region,\na robust test to reject the hypothesis of stability or even of an exponential behavior\nis needed. It is well known that for moderate sample sizes an observed tail-index well\nabove two cannot be used as an evidence against stability since it is highly unreliable\nestimator; if the distribution was really stable, an estimation of the tail parameter using\nthe full sample by maximum likelihood would be more pertinent [36].\n3.5. Stability of the data. According to the results of the last paragraph and con-\nsidering the sample size, the most plausible hypothesis is the lack of stability of the\ndistribution of returns. However, sampling the returns at di\ufb00erent time scales t \u2014 from\n2We also performed all the tests for the Hyperbolic and Variance-Gamma distributions which yielded\neven worse p-values than those obtained for the Student\u2019s t. We thus decided not to include them\namong our results.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n9\nFigure 4. Tails of the complementary cumulative distribution function.\n\ufb01fteen seconds up to half a day \u2014 and rescaling it with t1/2 its distribution seems to\nremain stable3 (Figure 5, top panel).",
    "chunk_index": 6,
    "start_char": 15428,
    "end_char": 18381,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "all the tests for the Hyperbolic and Variance-Gamma distributions which yielded\neven worse p-values than those obtained for the Student\u2019s t. We thus decided not to include them\namong our results.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n9\nFigure 4. Tails of the complementary cumulative distribution function.\n\ufb01fteen seconds up to half a day \u2014 and rescaling it with t1/2 its distribution seems to\nremain stable3 (Figure 5, top panel). This suggests that this symmetry must therefore\nbe the result of the presence of long memory in the data or to the temporal dependence\nof the parameters.\nTo support this facts, a reshu\ufb04ing of the data has been performed (Figure 5, middle\npanel). It can be readily observed that Gaussianity is reached in a few minutes, as\ncould be expected from the central limit theorem. Finally, we have also performed a\ndaily reshu\ufb04ing of the returns to verify if this scaling symmetry could be an artifact of\nthe lunch e\ufb00ect. As can be observed in Figure 5 (bottom panel), the scaling symmetry\nof the data still holds, a fact that points towards long\u2013range correlations as the most\nplausible explanation for this symmetry. It goes without saying that the long memory\nexhibited by the data and its autocorrelations deserve an in-depth study that shall be\naddresed in future work.\n3The value of 1/2 for the scaling exponent was obtained by Detrended Fluctuation Analysis, and it is\nthe one expected for independent identically distributed random variables with \ufb01nite second moment.\n\n10\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\nFigure 5. Rescaled probability distributions of the Ibex35 index ob-\nserved at di\ufb00erent time intervals (dotted line: N(0, 1)). Top: raw data.\nMiddle: reshu\ufb04ed data. Bottom: daily reshu\ufb04ed data.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n11\n4. Discussion\nAs far as we know, this is the \ufb01rst study of the high-frequency returns of the Ibex35,\nso we will necessarily compare our results with those obtained for similar market in-\ndexes.\nPlaten and Sidorowicz, in their study of several world stock indexes [39], found that\nfor the daily returns of the Madrid stock exchange the best \ufb01t was provided by a\nStudent\u2019s t with 4.51 degrees of freedom, while the NIG distribution \ufb01t was not as\nsound. The same results were obtained for a broad group of world stock indexes as an\nextension of a previous study [25]. This result is in stark contrast with our \ufb01ndings,\nwhere the NIG distribution outperforms the other members of the GH family as well\nas the L\u00b4evy-stable laws. As a matter of fact, according to the likelihood-ratio test, the\n\ufb01ve-parameter GH family could be reduced to the four-parameter NIG model without\nmuch loss.\nThe L\u00b4evy-stable distributions seem to be also very well suited to model the log-returns\nof many stock market indexes: this is the case e.g. of the daily returns of the Hong\nKong Hang Seng index. Further, for this index the L\u00b4evy-stable law provides a much\nbetter \ufb01t than the NIG distribution [12].",
    "chunk_index": 7,
    "start_char": 17946,
    "end_char": 20917,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "matter of fact, according to the likelihood-ratio test, the\n\ufb01ve-parameter GH family could be reduced to the four-parameter NIG model without\nmuch loss.\nThe L\u00b4evy-stable distributions seem to be also very well suited to model the log-returns\nof many stock market indexes: this is the case e.g. of the daily returns of the Hong\nKong Hang Seng index. Further, for this index the L\u00b4evy-stable law provides a much\nbetter \ufb01t than the NIG distribution [12]. Similar results have been observed for the\nIPC mexican index: the hypothesis of stability could not be rejected at 5% con\ufb01dence\nlevel while the hypothesis of NIG distributed daily log-returns was clearly rejected [2].\nHowever, according to our \ufb01ndings, the L\u00b4evy-stable model is not the best option for\nmodeling the high-frequency returns of the Ibex35 since the NIG distribution is a much\nbetter candidate.\nConsidering the tail behaviour of the distribution of log-returns, it is documented\nthat the S&P500 index follows a power law with \u03b1 \u22483 [22,23], while the characteristic\nexponent of the German DAX lies in the range between 3 and 4 [30]. This is the most\ncommonly accepted range for the characteristic exponent of the tails of the distribution\nof log-returns. However, the consensus is not complete, and some authors [26,32] claim\nthat a characteristic exponent in the range \u03b1 \u2208[3, 5] could be expected, and even\nthat the decay could well be exponential rather than hyperbolic. We have obtained\na seeming power-law behavior for the right (resp. left) tail of the distribution with\nexponent \u03b1 \u22484.60 (resp. \u03b1 \u22484.28), inside the accepted \u03b1 \u2208[3, 5] range. The only\nprobability distribution analyzed in our study that could have this asymptotic behavior\nis the Student\u2019s t. The overall \ufb01t, however, rules it out as a plausible model.\nThe scaling invariance of the \ufb01nancial time series was \ufb01rst proposed and exploited by\nMandelbrot in his investigation of the variations of cotton prices [33]. In what regards\nto stock market indexes, scaling has been observed in the high-frequency returns of\nS&P500 index [34] and in the OBX index of the Oslo stock exchange [44] among\nothers. In both cases the aggregated log-returns were rescaled with n\u22121/\u03b1, \u03b1 being\nthe estimated tail parameter for the L\u00b4evy-stable \ufb01t (1.4 for the S&P500 and 1.64 for\nthe OBX, values that are similar to what we have estimated (1.53)). In our study,\nhowever, rescaling using the estimated tail exponent for the L\u00b4evy-stable \ufb01t destroys\nthe symmetry; a value of 1/\u03b1 = 0.5 obtained by DFA \u2014and the one expected for \ufb01nite\nsecond moment random variables\u2014 was used instead.\n\n12\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\n5. Summary\nWe have performed a statistical analysis of the high frequency log-returns of the\nIbex35 index of the Madrid stock exchange over a two year period (2009-2010). Partic-\nular attention has been paid to describing the best probability distribution for the data\nsince this question is still controversial in the recent literature, the only fact commonly\naccepted (although not yet fully incorporated into pricing models) is the departure from\nnormality.",
    "chunk_index": 8,
    "start_char": 20467,
    "end_char": 23570,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "for \ufb01nite\nsecond moment random variables\u2014 was used instead.\n\n12\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\n5. Summary\nWe have performed a statistical analysis of the high frequency log-returns of the\nIbex35 index of the Madrid stock exchange over a two year period (2009-2010). Partic-\nular attention has been paid to describing the best probability distribution for the data\nsince this question is still controversial in the recent literature, the only fact commonly\naccepted (although not yet fully incorporated into pricing models) is the departure from\nnormality. Our results show that among the members of the family of Generalized Hy-\nperbolic laws the Normal Inverse Gaussian distribution is the one that provides the\nbest \ufb01t for the data. Furthermore, the 5-parameter GH family could be well reduced\nto the 4-parameter NIG family without signi\ufb01cant loss. This distribution also clearly\noutperforms the L\u00b4evy-stable laws as a statistical model for this index.\nThe tails of the distribution of log-returns behave as power laws with exponents\n\u03b1 \u22484.28 (left tail) and \u03b1 \u22484.60 (right tail), a fact that according to the generalized\ncentral limit theorem would not be compatible with the stability of the distribution un-\nder aggregation. However, the empirical distribution of log-returns has been observed\nto be stable over several time scales, ranging from a few seconds up to a few hours.\nWe conjecture that time correlations among the data are probably responsible for this\nobserved stability, since reshu\ufb04ing the data destroys these time correlations and re-\nstores the expected convergence results predicted by the central limit theorem. A more\nthorough analysis of these time correlations shall be conducted in future work, together\nwith the development of derivative pricing models that take into account more realistic\ndistributions for the underlying assets.\nReferences\n[1] K. Aas and I.H. Ha\ufb00, The generalized hyperbolic skew Student\u2019s t-distribution, Journal of Financial\nEconometrics 4 (2006), no. 2, 275.\n[2] L. Alfonso, R. Mansilla, and C.A. Terrero-Escalante, On the scaling of the distribution of daily\nprice \ufb02uctuations in the mexican \ufb01nancial market index, Physica A: Statistical Mechanics and its\nApplications 391 (2012), 2990\u20132996.\n[3] T. W. Anderson, Anderson-Darling tests of goodness-of-\ufb01t, International Encyclopedia of Statis-\ntical Science, Springer, 2011.\n[4] T.W. Anderson and D.A. Darling, A test of goodness of \ufb01t, Journal of the American Statistical\nAssociation 49 (1954), no. 268, 765\u2013769.\n[5] L. Bachelier, Th\u00b4eorie de la sp\u00b4eculation, Gauthier-Villars, 1900.\n[6] R.A. Bagnold, The physics of blown sand and desert dunes, Methuen, 1941.\n[7] O. Barndor\ufb00-Nielsen, Exponentially decreasing distributions for the logarithm of particle size,\nProceedings of the Royal Society of London A: Mathematical and Physical Sciences 353 (1977),\nno. 1674, 401.\n[8] O.E. Barndor\ufb00-Nielsen, Normal inverse gaussian distributions and the modeling of stock returns,\nResearch report 300 (1995).\n[9] F. Black and M. Scholes, The pricing of options and corporate liabilities, The journal of political\neconomy 81 (1973), no. 3, 637\u2013654.\n[10] R.C. Blattberg and N.J. Gonedes, A comparison of the stable and student distributions as statis-\ntical models for stock prices, The Journal of Business 47 (1974), no.",
    "chunk_index": 9,
    "start_char": 23000,
    "end_char": 26323,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "the Royal Society of London A: Mathematical and Physical Sciences 353 (1977),\nno. 1674, 401.\n[8] O.E. Barndor\ufb00-Nielsen, Normal inverse gaussian distributions and the modeling of stock returns,\nResearch report 300 (1995).\n[9] F. Black and M. Scholes, The pricing of options and corporate liabilities, The journal of political\neconomy 81 (1973), no. 3, 637\u2013654.\n[10] R.C. Blattberg and N.J. Gonedes, A comparison of the stable and student distributions as statis-\ntical models for stock prices, The Journal of Business 47 (1974), no. 2, 244\u2013280.\n[11] L. Breiman, Probability, Addison-Wesley, 1968.\n[12] K. Burnecki, J. Gajda, and G. Sikora, Stability and lack of memory of the returns of the Hang\nSeng index, Physica A: Statistical Mechanics and its Applications 390 (2011), 3136\u20133146.\n\nSTATISTICAL ANALYSIS OF THE IBEX35 INDEX\n13\n[13] A. Clauset, C.R. Shalizi, and M.E.J. Newman, Power\u2013law distributions in empirical data, Arxiv\npreprint arXiv:0706.1062 (2007).\n[14] R. Cont, Empirical properties of asset returns: stylized facts and statistical issues, Quantitative\nFinance 1 (2001), no. 2, 223\u2013236.\n[15] R. Cont, M. Potters, and J.P. Bouchaud, Scaling in stock market data: stable laws and beyond,\nCNRS Workshop on Scale Invariance, Les Houches (Graner Dubrulle and Sornette, eds.), 1997.\n[16] W.H. DuMouchel, Estimating the stable index \u03b1 in order to measure tail thickness: a critique,\nThe Annals of Statistics 11 (1983), no. 4, 1019\u20131031.\n[17] E. Eberlein, Application of generalized hyperbolic l\u00b4evy motions to \ufb01nance, L\u00b4evy processes: theory\nand applications (O.E. Barndor\ufb00-Nielsen, T. Mikosch, and S.I. Resnick, eds.), Birkhauser, 2001.\n[18] E. Eberlein and E. Hammerstein, Generalized hyperbolic and inverse Gaussian distributions: lim-\niting cases and approximation of processes, Seminar on Stochastic Analysis, Random Fields and\nApplications IV, vol. 58, Centro Stefano Franscini, Ascona, 2002, pp. 221\u2013264.\n[19] E. Eberlein and U. Keller, Hyperbolic distributions in \ufb01nance, Bernoulli 1 (1995), no. 3, 281\u2013299.\n[20] E.F. Fama, The behavior of stock-market prices, The journal of Business 38 (1965), no. 1, 34\u2013105.\n[21] J. D. Farmer, Physicists attempt to scale the ivory towers of \ufb01nance, Computing in Science &\nEngineering 1 (1999), no. 6, 26\u201339.\n[22] P. Gopikrishnan, M. Meyer, L.A.N. Amaral, and H.E. Stanley, Inverse cubic law for the distribu-\ntion of stock price variations, The European Physical Journal B-Condensed Matter and Complex\nSystems 3 (1998), no. 2, 139\u2013140.\n[23] P. Gopikrishnan, V. Plerou, L.A.N. Amaral, M. Meyer, and H.E. Stanley, Scaling of the distribu-\ntion of \ufb02uctuations of \ufb01nancial market indices, Physical Review E 60 (1999), no. 5, 5305.\n[24] C. Gourieroux and J. Jasiak, Truncated maximum likelihood, goodness of \ufb01t tests and tail analysis,\nSonderforschungsbereich 373, 1998.\n[25] S.R. Hurst and E. Platen, The marginal distributions of returns and volatility, L1\u2013Statistical\nProcedures and Related Topics, IMS Lecture Notes\u2013Monograph Series, vol. 31, 1997, pp. 301\u2013\n314.\n[26] S. Kinsella and F. O\u2019Brien, Maximum likelihood estimation of stable paretian distribution applied\nto index and option data, Proceedings of the INFINITI Conference on International Finance, 2009,\npp. 8\u20139.\n[27] U. K\u00a8uchler, K. Neumann, M. S\u00f8rensen, and A. Streller, Stock returns and hyperbolic distributions,\nMathematical and Computer Modelling 29 (1999), no.",
    "chunk_index": 10,
    "start_char": 25792,
    "end_char": 29168,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "tail analysis,\nSonderforschungsbereich 373, 1998.\n[25] S.R. Hurst and E. Platen, The marginal distributions of returns and volatility, L1\u2013Statistical\nProcedures and Related Topics, IMS Lecture Notes\u2013Monograph Series, vol. 31, 1997, pp. 301\u2013\n314.\n[26] S. Kinsella and F. O\u2019Brien, Maximum likelihood estimation of stable paretian distribution applied\nto index and option data, Proceedings of the INFINITI Conference on International Finance, 2009,\npp. 8\u20139.\n[27] U. K\u00a8uchler, K. Neumann, M. S\u00f8rensen, and A. Streller, Stock returns and hyperbolic distributions,\nMathematical and Computer Modelling 29 (1999), no. 10-12, 1\u201315.\n[28] J. Laherrere and D. Sornette, Stretched exponential distributions in nature and economy:\u201cfat tails\u201d\nwith characteristic scales, The European Physical Journal B: Condensed Matter and Complex\nSystems 2 (1998), no. 4, 525\u2013539.\n[29] T. Lux, The stable paretian hypothesis and the frequency of large returns: an examination of\nmajor german stocks, Applied \ufb01nancial economics 6 (1996), no. 6, 463\u2013475.\n[30]\n, The limiting extremal behaviour of speculative returns: an analysis of intra-daily data\nfrom the Frankfurt Stock Exchange, Applied Financial Economics 11 (2001), no. 3, 299\u2013315.\n[31] D.B. Madan and E. Seneta, The variance gamma (VG) model for share market returns, Journal\nof Business 63 (1990), no. 4, 511\u2013524.\n[32] Y. Malevergne, V. Pisarenko, and D. Sornette, Empirical distributions of stock returns: between\nthe stretched exponential and the power law?, Quantitative Finance 5 (2005), no. 4, 379\u2013401.\n[33] B. B. Mandelbrot, The variation of certain speculative prices, The Journal of Business 36 (1963),\nno. 4, 394\u2013419.\n[34] R.N. Mantegna and H.E. Stanley, Scaling behaviour in the dynamics of an economic index, Nature\n376 (1995), no. 6535, 46\u201349.\n[35] J.H. McCulloch, Simple consistent estimators of stable distribution parameters, Communications\nin Statistics: Simulation and Computation 15 (1986), no. 4, 1109\u20131136.\n[36]\n, Measuring tail thickness to estimate the stable index \u03b1: a critique, Journal of Business\n& Economic Statistics (1997), 74\u201381.\n\n14\nPABLO SU\u00b4AREZ-GARC\u00b4IA AND DAVID G\u00b4OMEZ-ULLATE\n[37] R.C. Merton, Theory of rational option pricing, The Bell Journal of Economics and Management\nScience 4 (1973), no. 1, 141\u2013183.\n[38] A. Pagan, The econometrics of \ufb01nancial markets, Journal of empirical \ufb01nance 3 (1996), no. 1,\n15\u2013102.\n[39] E. Platen and R. Rendek, Empirical evidence on Student-t log-returns of diversi\ufb01ed world stock\nindices, Journal of statistical theory and practice 2 (2008), no. 2, 233\u2013251.\n[40] V. Plerou, P. Gopikrishnan, L.A.N. Amaral, M. Meyer, and H.E. Stanley, Scaling of the distribu-\ntion of price \ufb02uctuations of individual companies, Physical Review E 60 (1999), no. 6, 6519\u20136529.\n[41] Peter D. Praetz, The distribution of share price changes, The Journal of Business 45 (1972), no. 1,\n49\u201355.\n[42] K. Prause, The generalized hyperbolic model: Estimation, \ufb01nancial derivatives, and risk measures,\nPh.D. thesis, University of Freiburg, 1999.\n[43] S.D. Silvey, Statistical inference, vol. 7, Chapman & Hall/CRC, 1975.\n[44] J.A. Skjeltorp, Scaling in the Norwegian stock market, Physica A: Statistical Mechanics and its\nApplications 283 (2000), no. 3-4, 486\u2013528.\n[45] M.A. Stephens, Asymptotic results for goodness-of-\ufb01t statistics with unknown parameters, The\nAnnals of Statistics (1976), 357\u2013369.\n[46] R.",
    "chunk_index": 11,
    "start_char": 28559,
    "end_char": 31934,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "of Business 45 (1972), no. 1,\n49\u201355.\n[42] K. Prause, The generalized hyperbolic model: Estimation, \ufb01nancial derivatives, and risk measures,\nPh.D. thesis, University of Freiburg, 1999.\n[43] S.D. Silvey, Statistical inference, vol. 7, Chapman & Hall/CRC, 1975.\n[44] J.A. Skjeltorp, Scaling in the Norwegian stock market, Physica A: Statistical Mechanics and its\nApplications 283 (2000), no. 3-4, 486\u2013528.\n[45] M.A. Stephens, Asymptotic results for goodness-of-\ufb01t statistics with unknown parameters, The\nAnnals of Statistics (1976), 357\u2013369.\n[46] R. Weron, Computationally intensive Value at Risk calculations, Handbook of Computational\nStatistics, Springer, Berlin (2004), 911\u2013950.\nE-mail address: pasuarez@fis.ucm.es\nE-mail address: dgullate@fis.ucm.es\nDepartamento de F\u00b4\u0131sica Te\u00b4orica II, Universidad Complutense, 28040 Madrid, Spain.",
    "chunk_index": 12,
    "start_char": 31388,
    "end_char": 32222,
    "paper_title": "Scaling stability and distribution of the high-fre",
    "paper_category": "q-fin.ST",
    "paper_filename": "Scaling_stability_and_distribution_of_the_high-fre.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Scaling_stability_and_distribution_of_the_high-fre.pdf"
  },
  {
    "text": "Stochastic leverage e\ufb00ect in high-frequency data: a Fourier based\nanalysis.\nImma Valentina Curato\u2217and Simona Sanfelici\u2020\nMarch 9, 2021\nAbstract\nThe stochastic leverage e\ufb00ect, de\ufb01ned as the standardized covariation between the re-\nturns and their related volatility, is analyzed in a stochastic volatility model set-up. A novel\nestimator of the e\ufb00ect is de\ufb01ned using a pre-estimation of the Fourier coe\ufb03cients of the\nreturn and the volatility processes. The consistency of the estimator is proven. Moreover,\nits \ufb01nite sample properties are studied in the presence of microstructure noise e\ufb00ects. The\nFourier methodology is applied to S&P500 futures prices to investigate the magnitude of the\nstochastic leverage e\ufb00ect detectable at high-frequency.\nJEL Classi\ufb01cation: C13, C14, C51, C58\nKeywords: Fourier analysis, leverage e\ufb00ect, high-frequency data, microstructure noise\n1\nIntroduction\nThe leverage e\ufb00ect is one of the most striking empirical regularity observed in \ufb01nancial time\nseries. In its classical interpretation given by [Black, 1976] and [Christie, 1982], the e\ufb00ect refers\nto the negative and constant correlation typically observed between returns and their respective\nvolatilities. [Bekaert and Wu, 2000, Campbell, 1987, Campbell and Hentschel, 1992, French et al., 1987,\nFiglewski and Wang, 2001, Ghysel et al., 2005, Nelson, 1991] and [Wu, 2001] empirically inves-\ntigate the presence of constant and negative correlation between returns and volatilities across\ndi\ufb00erent \ufb01nancial asset types. They observe that the e\ufb00ect is in general larger for aggregate\nmarket index returns than for individual stocks, see discussion in [Tauchen et al., 1996], and\ndetectable at frequencies lower than or equal to 1 day.\nIn the empirical literature employing high-frequency data, i.e. intra-daily data, there is no\nconsensus in interpreting the leverage e\ufb00ect as above. [A\u00a8\u0131t-Sahalia et al., 2013, Bollerslev et al., 2006,\n\u2217Corresponding author: Ulm University, Institute of Mathematical Finance, Helmholtzstrae 18, 89069 Ulm,\nGermany. E-mail: imma.curato@uni-ulm.de\n\u2020University of Parma, Department of Economics, Via J. Kennedy, 6, 43125 Parma, Italy.\nE-mail:\nsi-\nmona.sanfelici@unipr.it\n1\narXiv:1910.06660v3 [q-fin.ST] 8 Mar 2021\n\nTauchen et al., 1996] demonstrate the presence of a constant and negative correlation between\nreturns and volatilities, the analysis in [Bandi and Ren\u00b4o, 2012, Figlewski and Wang, 2001, Yu, 2012]\nsupport the claim of a time-varying e\ufb00ect, and [A\u00a8\u0131t-Sahalia et al., 2017, A\u00a8\u0131t-Sahalia and Jacod, 2014,\nCarr and Wu, 2007, Kalnina and Xiu, 2017, Mancino and Toscano, 2020, Wang and Mykland, 2014,\nMykland et al., 2009] analyze the presence of stochastic correlation between returns and volatil-\nities. We follow this last strand of literature.\nIn a high-frequency framework, it is more appropriate to de\ufb01ne the leverage e\ufb00ect following\n[A\u00a8\u0131t-Sahalia et al., 2013], namely, as the instantaneous correlation\nR(t) =\n\u27e8dp, d\u03c32\u27e9\np\n\u27e8dp, dp\u27e9\u27e8d\u03c32, d\u03c32, \u27e9\n,\n(1)\nwhich corresponds to the standardized quadratic covariation between the increments of the\nlogarithmic asset price p, i.e. the return process, and the increments of the volatility process \u03c32.\n[A\u00a8\u0131t-Sahalia et al., 2013] observe that the magnitude of the leverage e\ufb00ect (1) detected in\nthe data (using a classical realized covariance estimator) is near zero if we use data in a daily\ntime window, and becomes negative if we use a weekly to a monthly time window.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3440,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "as the instantaneous correlation\nR(t) =\n\u27e8dp, d\u03c32\u27e9\np\n\u27e8dp, dp\u27e9\u27e8d\u03c32, d\u03c32, \u27e9\n,\n(1)\nwhich corresponds to the standardized quadratic covariation between the increments of the\nlogarithmic asset price p, i.e. the return process, and the increments of the volatility process \u03c32.\n[A\u00a8\u0131t-Sahalia et al., 2013] observe that the magnitude of the leverage e\ufb00ect (1) detected in\nthe data (using a classical realized covariance estimator) is near zero if we use data in a daily\ntime window, and becomes negative if we use a weekly to a monthly time window. However,\nthe leverage e\ufb00ect should not change its value on di\ufb00erent time horizons, as it is an intrinsic\nfeature of the model underlying the data. The authors observe that several sources of bias arise.\nOne is due to the latent (i.e. non-observable) volatility path, and a second one to the presence\nof microstructure noise. We observe the latter when using data at a frequency higher than 5\nminutes, e.g. tick-data. [A\u00a8\u0131t-Sahalia et al., 2013] employ di\ufb00erent proxies of the volatility path\nto overcome these problems, namely, local averages of integrated volatility estimators and bias\ncorrections. Unfortunately, the methodology described in [A\u00a8\u0131t-Sahalia et al., 2013] only works\nunder the assumption that the volatility is a stationary process and R(t) is equal to a constant,\nas in the [Heston, 1993] model set-up.\nTo avoid the problems related to the estimation (1), [A\u00a8\u0131t-Sahalia and Jacod, 2014, Formula\n8.42] examine an alternative measure of the leverage e\ufb00ect. They estimate the standardized\nquadratic covariation between p and \u03c32 in a time window [0, T],\nRT =\n\u27e8p, \u03c32\u27e9T\np\n\u27e8p, p\u27e9T \u27e8\u03c32, \u03c32\u27e9T\n,\n(2)\nwhere the integrated volatility, i.e. the quadratic variation of p, and the integrated volatility\nof volatility, i.e. the quadratic variation of \u03c32, appear at the denominator, and the integrated\nleverage appears at the numerator. We call RT the stochastic leverage e\ufb00ect. For instance, if\nwe assume that p and \u03c32 follow the Heston model, then RT = \u03c1; otherwise, RT is in general a\nrandom quantity.\nIn this paper, we analyze an estimation methodology for the stochastic leverage e\ufb00ect de-\nsigned to work in the presence of microstructure noise. To this end, it is crucial to make a\n2\n\nclear distinction between an estimator of the integrated leverage (appearing at the numera-\ntor of RT ) and an estimator of the stochastic leverage e\ufb00ect. The latter is a plug-in estimator\nthat combines estimates of the former, of the integrated volatility and the integrated volatility\nof volatility. [A\u00a8\u0131t-Sahalia et al., 2017] and [Wang and Mykland, 2014] discuss the estimation of\nthe integrated leverage in the presence of microstructure noise. In the former, the authors also\nanalyze models with jumps. [Kalnina and Xiu, 2017] and [A\u00a8\u0131t-Sahalia et al., 2017] employ the\nplug-in estimator for RT de\ufb01ned in [A\u00a8\u0131t-Sahalia and Jacod, 2014] in di\ufb00erent simulation analy-\nsis. [A\u00a8\u0131t-Sahalia et al., 2017] examine an estimation of RT in a Heston model set-up and in the\nabsence of microstructure noise e\ufb00ects.",
    "chunk_index": 1,
    "start_char": 2901,
    "end_char": 5941,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "2017] and [Wang and Mykland, 2014] discuss the estimation of\nthe integrated leverage in the presence of microstructure noise. In the former, the authors also\nanalyze models with jumps. [Kalnina and Xiu, 2017] and [A\u00a8\u0131t-Sahalia et al., 2017] employ the\nplug-in estimator for RT de\ufb01ned in [A\u00a8\u0131t-Sahalia and Jacod, 2014] in di\ufb00erent simulation analy-\nsis. [A\u00a8\u0131t-Sahalia et al., 2017] examine an estimation of RT in a Heston model set-up and in the\nabsence of microstructure noise e\ufb00ects. Their analysis depends on several bias corrections, espe-\ncially applied to the volatility of volatility estimation, and the tuning of parameters identifying,\nfor example, the length of the time windows of data used to estimate the latent volatility path.\nOn the other hand, [Kalnina and Xiu, 2017] use a volatility instrument as the VIX to perform\ntheir estimation.\nWe present a methodology to estimate the stochastic leverage e\ufb00ect, which is based on\na continuous-time model. We refer the reader to Remark 2.1 for more details on this mod-\nelling assumption. Therefore, when comparing our methodology with the state-of-the-art liter-\nature, we refer to [Wang and Mykland, 2014] for an estimation of the integrated leverage, and\nto [A\u00a8\u0131t-Sahalia and Jacod, 2014] and [A\u00a8\u0131t-Sahalia et al., 2017] for discussing theoretical and nu-\nmerical features of an estimator of RT . The paper of [Kalnina and Xiu, 2017] is based on a\ndi\ufb00erent modelling framework, and we do not consider it further.\nThe target of this paper is twofold. First of all, we want to develop an estimation strategy\nfor RT that constitutes an alternative to the one proposed in [A\u00a8\u0131t-Sahalia and Jacod, 2014], and\nhandles data contaminated by microstructure noise. Secondly, we want to determine a selection\nstrategy for the tuning parameters appearing in our proposed methodology and analyze the\nperformance of the estimator in set-up other than the Heston model. Notably, this latter point\nis critical for using tick-data because they are not always well described by a Heston model, see\nRemark 2.1. To the best of our knowledge, the numerical and empirical analysis conducted in the\npaper is the \ufb01rst examining the presence of the stochastic leverage e\ufb00ect (2) at high frequency\nin the presence of microstructure noise e\ufb00ects.\nOur estimator employs the Fourier methodology introduced in [Malliavin and Mancino, 2002],\nsee also [Mancino et al., 2017] for a complete overview. We call it the Fourier estimator of\nthe stochastic leverage e\ufb00ect (in short, FESL). We choose this methodology because it avoids\nestimating the latent volatility path. This step is mandatory in the estimators appearing in\n[A\u00a8\u0131t-Sahalia et al., 2017] and is one reason behind several bias corrections applied to their esti-\nmations of RT .\nTo de\ufb01ne a Fourier estimator of RT , we then combine three di\ufb00erent estimators: the Fourier\nestimator of the integrated leverage (FEL), of the integrated volatility (FEV) and the inte-\ngrated volatility of volatility (FEVV) which have been de\ufb01ned in [Curato and Sanfelici, 2015],\n[Malliavin and Mancino, 2002] and [Sanfelici et al., 2015], respectively. When estimating the nu-\n3",
    "chunk_index": 2,
    "start_char": 5457,
    "end_char": 8603,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "estimators appearing in\n[A\u00a8\u0131t-Sahalia et al., 2017] and is one reason behind several bias corrections applied to their esti-\nmations of RT .\nTo de\ufb01ne a Fourier estimator of RT , we then combine three di\ufb00erent estimators: the Fourier\nestimator of the integrated leverage (FEL), of the integrated volatility (FEV) and the inte-\ngrated volatility of volatility (FEVV) which have been de\ufb01ned in [Curato and Sanfelici, 2015],\n[Malliavin and Mancino, 2002] and [Sanfelici et al., 2015], respectively. When estimating the nu-\n3\n\nmerator and denominator of (2), we handle the latent volatility by computing N Fourier coef-\n\ufb01cients of the volatility process. This step requires the preliminary computation of M Fourier\ncoe\ufb03cients of the returns. We call the parameters M and N cutting frequency parameters in\nthe following. In the Fourier set-up described in this paper, M and N play a role similar to the\ntuning parameters in [A\u00a8\u0131t-Sahalia et al., 2017].\nThe consistency and the \ufb01nite sample properties of the FESL are strictly related to a\nthorough analysis of the consistency and \ufb01nite sample properties of the FEL, the FEV, and\nthe FEVV used in the estimation. [Malliavin and Mancino, 2009] analyze the consistency of the\nFEV, whereas [Mancino and Sanfelici, 2008] study its \ufb01nite sample properties and a selection\nstrategy for the cutting frequency parameters appearing in the estimation. Regarding the FEVV,\n[Sanfelici et al., 2015] analyze its consistency, \ufb01nite sample properties, and selection of param-\neters M and N. Unfortunately, the theoretical results available for the FEL are not su\ufb03cient\nto directly obtain the consistency of the FESL because the three consistency theorems related\nto the FEL, the FEV, and the FEVV, available in the literature, hold under di\ufb00erent assump-\ntions. Hence, we prove a new consistency result for the FEL as detailed in Section 2. Moreover,\nthe \ufb01nite sample properties of the FEL, in the presence of microstructure noise, have not yet\nbeen analyzed in the literature. We focus on them in Section 3 and conclude that the FEL is\nasymptotically unbiased, although it has a diverging mean squared error.\nWe propose a variance corrected estimator of the FEL in the presence of microstructure\nnoise. Moreover, in an extensive simulation study, we analyze selection strategies for the pa-\nrameters M and N appearing in the FEL and its variance corrected version. We use Monte-\nCarlo data sets drawn from [Heston, 1993], and the generalized Heston model presented in\n[Veerart and Veerart, 2012] and study how the selection of the parameters M and N impacts\nthe mean squared error and the sample variance of the estimation. Our \ufb01ndings suggest that\nthe parameters obtained by minimizing the mean squared error of the FEL are equivalent to\nthose obtained minimizing its sample variance. Moreover, we note that using the variance cor-\nrected estimator reduces the sample variance of the \ufb01nal estimation by a half. Having a selec-\ntion strategy for the parameters M and N, we show a comparison between the performance\nof the FEL and the realized covariance-based estimator of the integrated leverage presented by\n[Wang and Mykland, 2014].",
    "chunk_index": 3,
    "start_char": 8083,
    "end_char": 11247,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "squared error and the sample variance of the estimation. Our \ufb01ndings suggest that\nthe parameters obtained by minimizing the mean squared error of the FEL are equivalent to\nthose obtained minimizing its sample variance. Moreover, we note that using the variance cor-\nrected estimator reduces the sample variance of the \ufb01nal estimation by a half. Having a selec-\ntion strategy for the parameters M and N, we show a comparison between the performance\nof the FEL and the realized covariance-based estimator of the integrated leverage presented by\n[Wang and Mykland, 2014]. To conclude, we also perform a sensitivity analysis on the FEL on\ndata sets generated from the generalized Heston model de\ufb01ned in [Veerart and Veerart, 2012]:\nnote that RT is a random quantity in this set-up.\nThe paper has the following structure. In Section 2, we introduce the model set-up, the\nde\ufb01nition of the FEL, and the FESL together with their asymptotic properties in the absence\nof microstructure noise. In Section 3, we analyze the \ufb01nite sample properties of the FEL in the\npresence of microstructure noise and the selection strategy for the cutting frequency parameters.\nIn Section 4, we discuss how to implement the FESL in the presence of microstructure noise.\nSection 5 applies our results to S&P500 futures prices. Section 6 concludes. The Appendix\ncontains the proofs of all statements presented in the paper.\n4\n\n2\nEstimation of the stochastic leverage e\ufb00ect in the absence of\nmicrostructure\nWe assume throughout that the logarithmic asset price and the volatility process are a solution\nto the system of equations\n(\ndp(t)\n= a(t) dt + \u03c3(t) dW(t)\nd\u03c32(t)\n= b(t) dt + \u03b3(t) dZ(t),\n(3)\nwhere W(t), t \u22650 and Z(t), t \u22650 are two correlated standard Brownian motions. Their corre-\nlation process is \u03c1(t) with values in [\u22121, 1]. We consider p(t) the underlying e\ufb03cient logarithmic\nprice process.\nRemark 2.1. The choice of a continuous-time modelling set-up for p(t) is motivated by the\nempirical work of [Christensen et al., 2014] where the authors analyze the presence of jumps\nin tick data. They observe that our ability to distinguish true discrete jumps from continuous\ndi\ufb00usive variation diminishes as we increase the sampling frequency. For example, a short-lived\nburst of volatility is likely to be identi\ufb01ed as a jump when working with data sampled at a\nfrequency lower than 5 minutes but it is compatible with a continuous path when working with\ntick data. Thus, we do not consider jumps in our model. We add instead di\ufb00erent randomness\nsources in the dynamics of the logarithmic asset price and its respective volatility that aim to\ndescribe the variability observed in tick data. An exemplary model in this set-up is the generalized\nHeston model de\ufb01ned by [Veerart and Veerart, 2012].\nWe perform our analysis in a time window [0, T] for T > 0, and such that the processes\nappearing in model (3) satisfy the following assumption:\n\u2022 (H1) a(t), b(t), \u03c3(t), \u03b3(t) and \u03c1(t) are R-valued processes, almost surely continuous on",
    "chunk_index": 4,
    "start_char": 10679,
    "end_char": 13685,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "the dynamics of the logarithmic asset price and its respective volatility that aim to\ndescribe the variability observed in tick data. An exemplary model in this set-up is the generalized\nHeston model de\ufb01ned by [Veerart and Veerart, 2012].\nWe perform our analysis in a time window [0, T] for T > 0, and such that the processes\nappearing in model (3) satisfy the following assumption:\n\u2022 (H1) a(t), b(t), \u03c3(t), \u03b3(t) and \u03c1(t) are R-valued processes, almost surely continuous on\n[0, T] such that\nE\nh\nsup\nt\u2208[0,T]\n|a(t)|2i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|b(t)|2i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03c3(t)|4i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03b3(t)|4i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03c1(t)|2i\n< \u221e.\nWe start by developing an estimation strategy for (2) in the absence of microstructure\nnoise and studying its consistency. We aim to de\ufb01ne a plug-in estimator in Section 2.2 which\nemploys the FEL, the FEV, and the FEVV, respectively de\ufb01ned in [Curato and Sanfelici, 2015],\n[Malliavin and Mancino, 2002], and [Sanfelici et al., 2015]. As the \ufb01rst step, we analyze under\nwhich set of assumptions the estimators above are all consistent. Let Sn := {0 = t0 \u2264t1 \u2264\n5\n\n. . . \u2264tn = T}, be the set of observation times, and de\ufb01ne \u03c4(n) = maxi=0,...,n\u22121 |ti+1 \u2212ti|. The\nFEV and FEVV are both consistent under the assumptions that N4\nM \u21920, and M\u03c4(n) \u21920\nas n, M, N \u2192\u221eand \u03c4(n) \u21920, see [Sanfelici et al., 2015] and [Malliavin and Mancino, 2009].\nUnfortunately, the consistency of the FEL has not been proved in [Curato and Sanfelici, 2015]\nunder an assumption of type M\u03c4(n) \u21920. Hence, to de\ufb01ne a consistent Fourier estimator of (2),\nwe need to prove that the FEL is consistent under a new set of assumptions, see Remark 2.2.\nTo start with, we brie\ufb02y remind the de\ufb01nition of the FEL.\n2.1\nFourier estimator of the integrated leverage\nLet (p(t), \u03c32(t)) be a solution to (3). We de\ufb01ne the leverage process \u03b7(t) as\n\u27e8dp(t), d\u03c32(t)\u27e9= \u03c3(t)\u03b3(t)\u03c1(t)dt = \u03b7(t)dt.\n(4)\nWe are interested in estimating the integrated covariation between the logarithmic price and the\nvolatility process, which appears at the numerator of (2), by determining an estimator for\n\u03b7 =\nZ T\n0\n\u03b7(t)dt.\n(5)\nWe follow a methodology based on the use of the Fourier coe\ufb03cients of the process \u03b7(t).\nFollowing [Malliavin and Mancino, 2002], we de\ufb01ne the Fourier coe\ufb03cients of the returns\nand of the increments of the volatility process as\nc(l; dp) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT ltdp(t),\n(6)\nand\nc(l; d\u03c32) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT ltd\u03c32(t),\n(7)\nfor each l \u2208Z. Note that for all l \u0338= 0 and by using the integration by parts formula, we can\nrewrite (7) as\nc(l; d\u03c32) = il2\u03c0\nT c(l; \u03c32) + 1\nT (\u03c32(T) \u2212\u03c32(0)),\n(8)\nwhere\nc(l; \u03c32) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT lt\u03c32(t)dt.",
    "chunk_index": 5,
    "start_char": 13212,
    "end_char": 15832,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "0\ne\u2212i 2\u03c0\nT ltdp(t),\n(6)\nand\nc(l; d\u03c32) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT ltd\u03c32(t),\n(7)\nfor each l \u2208Z. Note that for all l \u0338= 0 and by using the integration by parts formula, we can\nrewrite (7) as\nc(l; d\u03c32) = il2\u03c0\nT c(l; \u03c32) + 1\nT (\u03c32(T) \u2212\u03c32(0)),\n(8)\nwhere\nc(l; \u03c32) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT lt\u03c32(t)dt.\nGiven two functions \u03a6 and \u03a8 on the integers Z, we say that their Bohr convolution product\nexists if the following limit exists for all integers h\n(\u03a6 \u2217\u03a8)(h) := lim\nN\u2192\u221e\n1\n2N + 1\nX\n|l|\u2264N\n\u03a6(l)\u03a8(h \u2212l).\n6\n\nUnder Assumption (H1) and for a \ufb01xed h, we de\ufb01ne \u03a6(l) := c(l; d\u03c32) and \u03a8(h \u2212l) :=\nc(h \u2212l, dp), then the limit in probability of the Bohr convolution product exists and converges\nto the h-th Fourier coe\ufb03cient of the leverage process. This result immediately follows from\n[Malliavin and Mancino, 2009, Theorem 2.1]. The h-th Fourier coe\ufb03cient of \u03b7(t) is then de\ufb01ned\nas\nc(h; \u03b7) = lim\nN\u2192\u221e\nT\n2N + 1\nX\n|l|\u2264N\nc(l; d\u03c32)c(h \u2212l; dp) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT ht\u03b7(t)dt.\n(9)\nThe de\ufb01nition of the Fourier coe\ufb03cients of \u03b7(t) has the obvious drawback to being feasible\nonly when continuous observations of the logarithmic price and the volatility paths are available.\nBy using the methodology described in [Curato and Sanfelici, 2015, Curato, 2019], it is possi-\nble to give an estimator of the Fourier coe\ufb03cients of \u03b7(t) when discrete and non-equidistant\nobservations of p(t) are available on the time grid Sn and the volatility is latent. Hereafter, we\nindicate the discrete observed returns by \u03b4i = p(ti+1) \u2212p(ti) for all i = 0, ..., n \u22121.\nAn estimator of the h-th Fourier coe\ufb03cient of the leverage process can be de\ufb01ned as\ncn,M,N(h; \u03b7) =\nT\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT cn,M(l; \u03c32)cn(h \u2212l; dp),\n(10)\nfor any integer h such that |h| \u2264N, where cn(s; dp) are the discrete Fourier coe\ufb03cients of the\nreturns\ncn(s; dp) = 1\nT\nn\u22121\nX\ni=0\ne\u2212is 2\u03c0\nT ti\u03b4i(p)\n(11)\nfor |s| \u2264N + M, and cn,M(h; \u03c32) are the Fourier coe\ufb03cients of the volatility process introduced\nin [Malliavin and Mancino, 2002] for |l| \u2264N\ncn,M(l; \u03c32) =\nT\n2M + 1\nX\n|s|\u2264M\ncn(s; dp)cn(l \u2212s; dp).\n(12)\nThe estimators are written as functions of n, M, and N which stand for the number of\nobservations available, the number of the discrete Fourier coe\ufb03cients of the returns, and of the\nFourier coe\ufb03cients of the volatility process, respectively.\nFinally, the FEL is obtained from De\ufb01nition (10) for h = 0\n\u02c6\u03b7n,M,N = Tcn,M,N(0; \u03b7) =\nT 2\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT cn,M(l; \u03c32)cn(\u2212l; dp).\nWe can also give a more explicit form of the FEL. By employing the normalized Dirichlet kernel\nDN(t) =\n1\n2N + 1\nX\n|l|\u2264N\nei 2\u03c0\nT lt,\n(13)\n7",
    "chunk_index": 6,
    "start_char": 15550,
    "end_char": 18074,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "discrete Fourier coe\ufb03cients of the returns, and of the\nFourier coe\ufb03cients of the volatility process, respectively.\nFinally, the FEL is obtained from De\ufb01nition (10) for h = 0\n\u02c6\u03b7n,M,N = Tcn,M,N(0; \u03b7) =\nT 2\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT cn,M(l; \u03c32)cn(\u2212l; dp).\nWe can also give a more explicit form of the FEL. By employing the normalized Dirichlet kernel\nDN(t) =\n1\n2N + 1\nX\n|l|\u2264N\nei 2\u03c0\nT lt,\n(13)\n7\n\nand its \ufb01rst derivative\nD\u2032\nN(t) =\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT ei 2\u03c0\nT lt,\n(14)\nwe obtain\n\u02c6\u03b7n,M,N =\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nn\u22121\nX\nk=0\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)\u03b4i\u03b4j\u03b4k.\n(15)\nWe now focus on showing the consistency of the estimator (15).\nRemark 2.2. In [Curato and Sanfelici, 2015], it is shown that the FEL (15), in the absence of\nmicrostructure noise, is consistent if N2/M \u21920 and M\u03c4(n) \u2192a with a > 0 as n, M, N \u2192\u221e\nand \u03c4(n) \u21920. We now prove consistency under the assumptions N2/M \u21920 and MN\u03c4(n) \u21920\nas n, M, N \u2192\u221eand \u03c4(n) \u21920. Note, that the assumption MN\u03c4(n) \u21920 implies that M\u03c4(n) \u2192\n0 which is the assumption under which the consistency of the FEV and the FEVV holds. Chang-\ning the asymptotic rate between the parameters M, N, and \u03c4(n) implies a complete di\ufb00erent\nconsistency\u2019s proof of the FEL with respect to the one given by [Curato and Sanfelici, 2015]. In\nparticular, we do not employ Malliavin calculus as in the proof of [Curato and Sanfelici, 2015,\nTheorem 3.1].\nTheorem 2.3. We assume that Assumption (H1) and\nN2\nM \u21920\nand\nMN\u03c4(n) \u21920\n(16)\nhold true as n, M, N \u2192\u221eand \u03c4(n) \u21920. Then\n\u02c6\u03b7n,M,N\nP\u2212\u2192\u03b7.\n(17)\n2.2\nFourier estimator of the stochastic leverage e\ufb00ect\nWe can now de\ufb01ne the Fourier estimator of the stochastic leverage e\ufb00ect (FESL) by combining\nthe FEL, the FEV and the FEVV. The FESL takes the form\n\u02c6RT =\n\u02c6\u03b7n,M,M\nq\n\u02c6\u03c32\nn,M \u02c6\u03b32\nn,M,N\n,\nwhere\n\u02c6\u03c32\nn,M =\nT 2\n2M + 1\nX\n|s|\u2264M\ncn(s; dp)cn(\u2212s; dp),\n(18)\nand\n\u02c6\u03b32\nn,M,N = lim\nN\u2192\u221e\nT 2\n2N + 1\nX\n|l|\u2264N\n\u0010\n1 \u2212|l|\nN\n\u0011\nl2 4\u03c02\nT 2 cn,M(l; \u03c32)cn,M(\u2212l; \u03c32).\n(19)\nFinally, we obtain the consistency of the estimator RT by using the continuous mapping\n8\n\ntheorem and the results proved in Theorem 2.3, Theorem 3.2 in [Sanfelici et al., 2015], and\nTheorem 3.4 in [Malliavin and Mancino, 2009].\nCorollary 2.4. We assume that Assumption (H1) and\nN4\nM \u21920\nand\nMN\u03c4(n) \u21920\nhold true as n, M, N \u2192\u221eand \u03c4(n) \u21920. Then\n\u02c6RT\nP\u2212\u2192RT .\nRemark 2.5. To obtain a central limit theorem for an estimator of RT is necessary to have\ncentral limit theorem results for each estimator appearing in its de\ufb01nition. These results have\nto hold under the same set of assumptions. A central limit theorem for an estimator of RT\nfollows from applying the delta method, see [A\u00a8\u0131t-Sahalia and Jacod, 2014].",
    "chunk_index": 7,
    "start_char": 17687,
    "end_char": 20267,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "N4\nM \u21920\nand\nMN\u03c4(n) \u21920\nhold true as n, M, N \u2192\u221eand \u03c4(n) \u21920. Then\n\u02c6RT\nP\u2212\u2192RT .\nRemark 2.5. To obtain a central limit theorem for an estimator of RT is necessary to have\ncentral limit theorem results for each estimator appearing in its de\ufb01nition. These results have\nto hold under the same set of assumptions. A central limit theorem for an estimator of RT\nfollows from applying the delta method, see [A\u00a8\u0131t-Sahalia and Jacod, 2014]. All this said, in the\nrealized covariance-based literature, explicit calculations leading to a central limit theorem of an\nestimator for RT are not present. Proving a central limit theorem for the estimator \u02c6RT is also\noutside the scope of our paper. However, we plan to analyze the latter in future research projects\nemploying the FESL.\n3\nFinite sample properties of the Fourier estimator of the inte-\ngrated leverage\nThe \ufb01nite sample properties of the FEV and the FEVV have been analyzed in the presence\nof microstructure noise in [Mancino and Sanfelici, 2008] and [Sanfelici et al., 2015], respectively.\nWe study in this section the \ufb01nite sample properties of the FEL.\nThe results contained in this section hold under the following assumption.\n\u2022 (H2) a(t), b(t), \u03c3(t), \u03b3(t) and \u03c1(t) are R-valued processes, almost surely continuous on\n[0, T] such that\nE\nh\nsup\nt\u2208[0,T]\n|a(t)|8i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|b(t)|8i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03c3(t)|8i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03b3(t)|8i\n< \u221e,\nE\nh\nsup\nt\u2208[0,T]\n|\u03c1(t)|8i\n< \u221e.\nMoreover, we add microstructure noise to the underlying e\ufb03cient logarithmic price p(t)\n9\n\nde\ufb01ned in (3) by assuming that the logarithm of the observed price is\nep(ti) = p(ti) + \u03b6(ti),\nfor i = 0, . . . , n,\n(20)\nwhere \u03b6(t) is the microstructure noise. We also assume the following\n\u2022 (H3) The random shocks (\u03b6(ti))0\u2264i\u2264n are independent and identically distributed with\nbounded sixth moment. Moreover, the random shocks are independent of p(t).\nWe de\ufb01ne \u03f5i = \u03b6(ti+1) \u2212\u03b6(ti) and e\u03b4i = ep(ti+1) \u2212ep(ti). Then, the FEL (15) in the presence\nof microstructure noise becomes\ne\u03b7n,M,N =\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nn\u22121\nX\nk=0\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)e\u03b4ie\u03b4je\u03b4k.\n(21)\nWe can disentangle (21) as\nX\ni\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)e\u03b4ie\u03b4je\u03b4k\n(22)\n+\nX\ni,j:i\u0338=j\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj)e\u03b42\ni e\u03b4j +\nX\ni,j\nD\u2032\nN(ti \u2212tj)e\u03b4ie\u03b42\nj\n=\nX\ni\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)\u03b4i\u03b4j\u03b4k\n(23)\n+\nX\ni,j:i\u0338=j\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj)\u03b42\ni \u03b4j +\nX\ni,j\nD\u2032\nN(ti \u2212tj)\u03b4i\u03b42\nj\n(24)\n+\u03b7\u03f5\nn,M,N,\nwhere the sum of the components (23) and (24) corresponds to the FEL in the absence of mi-\ncrostructure noise and all the noise components are contained in \u03b7\u03f5\nn,M,N. The explicit expression\nof the latter can be found in the Appendix, see (42).\nIn the modeling set-up (3), the integrated leverage can be positive or negative. Therefore, we\nanalyze the bias of the estimator in absolute value. The de\ufb01nition of the FEL does not require\nthe use of equidistant data.",
    "chunk_index": 8,
    "start_char": 19841,
    "end_char": 22659,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "n,M,N,\nwhere the sum of the components (23) and (24) corresponds to the FEL in the absence of mi-\ncrostructure noise and all the noise components are contained in \u03b7\u03f5\nn,M,N. The explicit expression\nof the latter can be found in the Appendix, see (42).\nIn the modeling set-up (3), the integrated leverage can be positive or negative. Therefore, we\nanalyze the bias of the estimator in absolute value. The de\ufb01nition of the FEL does not require\nthe use of equidistant data. However, for simplicity of computation, we assume equidistant\nobservations in the time window [0, T].\nTheorem 3.1. We assume that Assumptions (H2), (H3) and\nN2\nM \u21920 and MN\nn\n\u21920\n(25)\nhold true as N, M, n \u2192\u221e. Then the estimator e\u03b7n,M,N is asymptotically unbiased. More precisely,\n\f\f\fE[e\u03b7n,M,N \u2212\u03b7]\n\f\f\f \u2264\n\f\f\fE[\u03b7n,M,N \u2212\nZ T\n0\n\u03b7(t) dt]\n\f\f\f +\n\f\f\fE[\u03b7\u03f5\nn,M,N]\n\f\f\f\n10\n\n\u2264\u0393(n, M, N) + \u039b(n, N) + \u03a8(N) +\n\f\f\f2(n \u22121)\n\u0010\nDM\n\u0010T\nn\n\u0011\n\u22121\n\u0011\nD\u2032\nN\n\u0010T\nn\n\u0011\nE[\u03b63]\n\f\f\f,\nwhere\n\u0393(n, M, N) \u2264N(M + N)\nn\n8\u03c02T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 +\nN\n\u221a\n2M + 1 2\u03c0 T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 ,\n\u039b(n, N) \u2264N\n\u221an 4\u03c0T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 + N2\nn 4\u03c02(1 + T\n1\n2 ) E\nh\nsup\n[0,T]\n\u03c34(t)\ni 1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 1\n2 ,\nand,\n\u03a8(N) \u2264\n1\n\u221a\n2N + 1 T E\nh\nsup\n[0,T]\n\u03b72(t)\ni 1\n2 .\nThe mean squared error of the estimator (21) has the following bias-variance decomposition\nE[(e\u03b7n,M,N \u2212\u03b7)2] = V ar(e\u03b7n,M,N) + E[e\u03b7n,M,N \u2212\u03b7]2 + V ar(\u03b7) \u22122Cov(e\u03b7n,M,N, \u03b7).\n(26)\nThis decomposition di\ufb00ers from the classical bias-variance decomposition which can be found in\nthe parametric statistics literature because the quantity we aim to estimate, i.e. the integrated\nleverage \u03b7, is a random variable and not a constant parameter.\nTheorem 3.2. We assume that Assumptions (H2), (H3) and\nN2\nM \u21920 and MN\nn\n\u21920\n(27)\nhold true as N, M, n \u2192\u221e. Then\nE[(e\u03b7n,M,N \u2212\u03b7)2] \u2192\u221e.\nThe theorem above highlights a divergent element in (26) that we try to identify using\na numerical analysis in the next section. A diverging mean squared error in the presence of\nmicrostructure noise is a phenomenon already observed for integrated estimators in a high-\nfrequency setting. For example, the variance of the realized volatility estimator diverges in\nthe presence of microstructure noise e\ufb00ects as discussed by [Bandi and Russell, 2008]. In this\nframework, the presence of microstructure noise is usually handled by using pre-averaging, see\n[Jacod et al., 2009]. This methodology is used in [A\u00a8\u0131t-Sahalia et al., 2017] and [Wang and Mykland, 2014]\nto de\ufb01ne estimators of the integrated leverage robust to microstructure noise. However, the\nFourier approach automatically \ufb01lters the noise components on its own, see discussion in [Mancino et al., 2017,\nChapter 5]. Therefore, correcting our estimation by using a pre-averaging approach is not an\noption.",
    "chunk_index": 9,
    "start_char": 22190,
    "end_char": 24920,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "noise e\ufb00ects as discussed by [Bandi and Russell, 2008]. In this\nframework, the presence of microstructure noise is usually handled by using pre-averaging, see\n[Jacod et al., 2009]. This methodology is used in [A\u00a8\u0131t-Sahalia et al., 2017] and [Wang and Mykland, 2014]\nto de\ufb01ne estimators of the integrated leverage robust to microstructure noise. However, the\nFourier approach automatically \ufb01lters the noise components on its own, see discussion in [Mancino et al., 2017,\nChapter 5]. Therefore, correcting our estimation by using a pre-averaging approach is not an\noption.\nWe de\ufb01ne instead a variance corrected version of (21). We call the term (22) by \u03a5n,M,N.\nThis term contains all the cross products of the noisy returns \u02dc\u03b4i\u02dc\u03b4j\u02dc\u03b4k with i \u0338= j \u0338= k and is\n11\n\ncorrelated to e\u03b7n,M,N. Moreover, it has expected value equal to zero as shown in the Corollary\nbelow.\nCorollary 3.3. We assume that the assumptions of Theorem 3.1 hold. Then, the addend (22)\nhas expected value equal to zero.\nProof. The term (22) can be decomposed as\nX\ni\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)\u03b4i\u03b4j\u03b4k\n+\nX\ni,j,k:i\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)(\u03b4i\u03b4j\u03f5k + \u03b4j\u03b4k\u03f5i + \u03b4k\u03b4i\u03f5j + \u03b4i\u03f5j\u03f5k + \u03b4j\u03f5i\u03f5k\n+ \u03b4k\u03f5i\u03f5j + \u03f5i\u03f5j\u03f5k).\nThe thesis follows straightforwardly from the results contained in the proof of Theorem 3.1.\nWe then de\ufb01ne the estimator\n\u03b7\u2217\nn,M,N = e\u03b7n,M,N \u2212b \u03a5n,M,N\n(28)\nwhich is asymptotically unbiased because of Theorem 3.1 and Corollary 3.3 and has\nV ar(\u03b7\u2217\nn,M,N) = V ar(e\u03b7n,M,N) \u22122 b Cov(e\u03b7n,M,N, \u03a5n,M,N) + b2 V ar(\u03a5n,M,N).\n(29)\nHence, the estimator \u03b7\u2217\nn,M,N has smaller variance than the estimator e\u03b7n,M,N provided that\nb2 V ar(\u03a5n,M,N) < 2 b Cov(e\u03b7n,M,N, \u03a5n,M,N).\nThe optimal coe\ufb03cient b\u2217minimizing the variance of the estimator \u03b7\u2217\nn,M,N is given by\nb\u2217\nM,N = Cov(e\u03b7n,M,N, \u03a5n,M,N)\nV ar(\u03a5n,M,N)\n.\n(30)\nPlugging this value in (29) and simplifying, we \ufb01nd\nV ar(\u03b7\u2217\nn,M,N)\nV ar(e\u03b7n,M,N) = (1 \u2212Corr(e\u03b7n,M,N, \u03a5n,M,N)2),\nwhich gives us the variance reduction ratio obtained by using the estimator (28).\nRemark 3.4. A similar variance reduction appears, for instance, in the classical control variate\nmethod to reduce the variance of the sample mean estimator, see ([Glasserman, 2004, Section\n4.1]).\n12\n\nIn the next section, we determine selection strategies for the cutting frequency parameters\nM and N of the FEL and its variance corrected version.\n3.1\nSelection strategy for the cutting frequency parameters: a numerical\nstudy\nWe assume two di\ufb00erent models for the underlying e\ufb03cient price process, i.e. the classical model\nproposed by [Heston, 1993] and the generalized Heston model proposed by [Veerart and Veerart, 2012].\nThe microstructure noise satis\ufb01es Assumption (H3).\nWe simulate second-by-second return and variance paths over a daily trading period of\nT = 6 hours, for a total of 100 trading days and n = 21600 observations per day.\nThe \ufb01rst data generating process is\nH :\n(\ndp(t)\n= \u03c3(t)dW1(t)\nd\u03c32(t)\n= \u03b1(\u03b2 \u2212\u03c32(t))dt + \u03bd\u03c3(t)dW2(t),\n(31)\nwhere W1 and W2 are correlated Brownian motions. The parameter values used in the simulations\nare \u03b1 = 0.01, \u03b2 = 0.2, \u03bd = 0.05 and the correlation parameter is set to \u03c1 = \u22120.2.",
    "chunk_index": 10,
    "start_char": 24350,
    "end_char": 27425,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "The microstructure noise satis\ufb01es Assumption (H3).\nWe simulate second-by-second return and variance paths over a daily trading period of\nT = 6 hours, for a total of 100 trading days and n = 21600 observations per day.\nThe \ufb01rst data generating process is\nH :\n(\ndp(t)\n= \u03c3(t)dW1(t)\nd\u03c32(t)\n= \u03b1(\u03b2 \u2212\u03c32(t))dt + \u03bd\u03c3(t)dW2(t),\n(31)\nwhere W1 and W2 are correlated Brownian motions. The parameter values used in the simulations\nare \u03b1 = 0.01, \u03b2 = 0.2, \u03bd = 0.05 and the correlation parameter is set to \u03c1 = \u22120.2.\nThe second data generating process is\nGH :\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\ndp(t)\n= \u03c3(t)dX(t)\ndX(t)\n= \u03c1(t)dW1(t) +\np\n1 \u2212\u03c12(t)dW2(t)\nd\u03c32(t)\n= \u03b1(\u03b2 \u2212\u03c32(t))dt + \u03bd\u03c3(t)dW1(t),\n(32)\nand the in\ufb01nitesimal variation of \u03c1(t) is given by\nd\u03c1(t) = ((2\u03be \u2212\u03b7) \u2212\u03b7\u03c1(t))dt + \u03b8\np\n(1 + \u03c1(t))(1 \u2212\u03c1(t))dW0,\nwhere \u03b7, \u03be and \u03b8 are positive constants and W0 is a Brownian motion. The processes W0(t), W1(t)\nand W2(t) are assumed to be independent. The parameter values used in the simulation are\n\u03b1 = 0.01, \u03b2 = 0.2, \u03bd = 0.05 and \u03be = 0.02, \u03b7 = 0.5, \u03b8 = 0.5, where the last three parameters\nare chosen in the range prescribed by [Veerart and Veerart, 2012] such that \u03c1(t) \u2208[\u22121, 1]. We\nset the initial values as \u03c32(0) = \u03b2, p(0) = log(100) and \u03c1(0) = \u22120.04. The noise-to-signal ratio\nstd(\u03b6)/std(r) is equal to 0.8, where r is the 1-second returns.\nWhen processing simulated data, the natural approach in optimizing estimators depending\non tuning parameters is to choose those values that minimize the \ufb01nite sample mean squared\nerror (MSE). Therefore, one possible choice is to select the cutting frequency parameters M and\nN by following this methodology. We analyze two types of MSE-based optimal strategies. The\n\ufb01rst directly minimizes the MSE of the FEL e\u03b7n,M,N, whereas the second one is described below.\nOperatively, the variance corrected estimator (28) can be implemented by the following\nprocedure:\n13\n\nStep 1: Given a sample of n observed returns and for all M \u2208{range} and N \u2208{range}, let\ne\u03b71\nn,M,N, e\u03b72\nn,M,N, . . . , e\u03b7d\nn,M,N be d replications of the Fourier estimate of the integrated lever-\nage in a Monte Carlo experiment. Along with e\u03b7i\nn,M,N, on each replication we also calculate\n\u03a5i\nn,M,N;\nStep 2: let M\u2217, N\u2217:= argmin VAR(\u03a5n,M,N) and let \u03a5\u2217:= \u03a5n,M\u2217,N\u2217;\nStep 3: plug the selected correction \u03a5\u2217into equation (28)\n\u03b7\u2217\nn,M,N = e\u03b7n,M,N \u2212b\u2217\nM,N \u03a5\u2217,\nwhere\nb\u2217\nM,N = COV(e\u03b7n,M,N, \u03a5\u2217)\nVAR(\u03a5\u2217)\nand COV, VAR denote the sample covariance and the sample variance, respectively. For\neach M and N, compute d replications \u03b7i\u2217\nn,M,N (i = 1, . . . , d) of the estimator;\nStep 4: choose the cutting frequency parameters \u02c6\nM and \u02c6N which minimize the \ufb01nite sample MSE\nof the corrected estimates \u03b7i\u2217\nn,M,N for i = 1, . . . , d.\nThe magnitude of the variance correction given by the estimator (28) is tuned by formula\n(30), where V ar(\u03a5n,M,N) appears at the denominator.",
    "chunk_index": 11,
    "start_char": 26928,
    "end_char": 29735,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "VAR(\u03a5\u2217)\nand COV, VAR denote the sample covariance and the sample variance, respectively. For\neach M and N, compute d replications \u03b7i\u2217\nn,M,N (i = 1, . . . , d) of the estimator;\nStep 4: choose the cutting frequency parameters \u02c6\nM and \u02c6N which minimize the \ufb01nite sample MSE\nof the corrected estimates \u03b7i\u2217\nn,M,N for i = 1, . . . , d.\nThe magnitude of the variance correction given by the estimator (28) is tuned by formula\n(30), where V ar(\u03a5n,M,N) appears at the denominator. We \ufb01rst set the parameter (30) to mini-\nmize the denominator and enhance the e\ufb00ectiveness of the correction. Afterwards, in Step 4, we\nchoose the optimal MSE-based cutting frequency parameters \u02c6\nM and \u02c6N. This procedure provides\nbetter empirical results than optimizing the parameters M and N in (28) simultaneously.\nTable 1 shows the MSE reduction obtained by using the estimator (28) versus the FEL\n(21). The parameter values \u02c6\nM and \u02c6N are selected following the MSE-based optimal strategies\ndescribed above. Since both estimators are only asymptotically unbiased and in the case of the\nH \u2212model\nGH \u2212model\n\u03b7\n-1.013673e-04\n-4.603226e-05\nMSE\nBIAS\n\u02c6\nM\n\u02c6N\nMSE\nBIAS\n\u02c6\nM\n\u02c6N\ne\u03b7n,M,N\n2.40e-07\n2.79e-05\n887\n1\n1.70e-07\n4.67e-06\n2404\n2\n\u03b7\u2217\nn,M,N\n1.43e-07\n4.63e-05\n889\n1\n1.49e-07\n4.76e-06\n2638\n1\nTable 1: Finite sample performance of the FEL e\u03b7n,M,N and of the estimator \u03b7\u2217\nn,M,N. \u03b7 represents\nthe average real integrated leverage for each data set. The value of the MSE and BIAS in the\ntable are computed w.r.t. the optimal parameters \u02c6\nM and \u02c6N.\nHeston model the selected cutting frequency parameters \u02c6\nM and \u02c6N are rather small, the variance\ncorrected estimator entails a slight increase of the bias, while for the generalized Heston model\nthe bias remains almost the same. We notice that in both cases the optimal MSE-based \u02c6\nM turns\nout to be much smaller than the Nyquist frequency (i.e. \u02c6\nM << n/2), whereas \u02c6N is very small,\nas prescribed by the asymptotic growth conditions in Theorem 2.3.\n14\n\nLet us now analyse the MSE and the sample variance (VAR) of the FEL e\u03b7n,M,N in the\npresence of noise for the Heston and the generalized Heston model data sets as a function of M\nand N.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n0.5\n1\n1.5\nMSE\n\u00d710-6\nFourier estimator\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nM\n0\n2\n4\nMSE\n\u00d710-5\n0\n1000\n800\n10\n0.5\n\u00d710-4\n600\n8\nM\n6\nN\n400\n1\n4\n200\n2\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n2\n4\n6\nMSE\n\u00d710-7\nFourier estimator\n500\n1000\n1500\n2000\n2500\n3000\nM\n0\n1\n2\n3\nMSE\n\u00d710-5\n0\n3000\n2500\n10\n1\n\u00d710-4\n2000\n8\nM\n1500\n6\nN\n2\n1000\n4\n500\n2\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n0.5\n1",
    "chunk_index": 12,
    "start_char": 29263,
    "end_char": 31788,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "10\n0.5\n\u00d710-4\n600\n8\nM\n6\nN\n400\n1\n4\n200\n2\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n2\n4\n6\nMSE\n\u00d710-7\nFourier estimator\n500\n1000\n1500\n2000\n2500\n3000\nM\n0\n1\n2\n3\nMSE\n\u00d710-5\n0\n3000\n2500\n10\n1\n\u00d710-4\n2000\n8\nM\n1500\n6\nN\n2\n1000\n4\n500\n2\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n0.5\n1\n1.5\nVAR\n\u00d710-6\nFourier estimator\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nM\n0\n2\n4\n6\nVAR\n\u00d710-5\n0\n1000\n800\n10\n0.5\n\u00d710-4\n600\n8\nM\n6\nN\n400\n1\n4\n200\n2\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN\n0\n2\n4\n6\nVAR\n\u00d710-7\nFourier estimator\n500\n1000\n1500\n2000\n2500\n3000\nM\n0\n1\n2\n3\nVAR\n\u00d710-5\n0\n3000\n2500\n10\n1\n\u00d710-4\n2000\n8\nM\n1500\n6\nN\n2\n1000\n4\n500\n2\n0\n0\nFigure 1: MSE and sample variance of the FEL as a function of M and N under microstructure\ne\ufb00ects. Left panels: H model. Right panels: GH model.\nFrom Figure 1, it is evident that the sample variance of the estimator has the same order\nof magnitude as the MSE. Moreover, by analysing the relative di\ufb00erence (MSE-VAR)/MSE for\nthe FEL (21) and the variance corrected estimator (28) as a function of M and N in Figure 2,\nwe observe that this ratio is negligible for both estimators except for the lowest values of M.\nMoreover, it never exceeds 0.1 so that the di\ufb00erence MSE-VAR never exceeds 10% of the MSE.\nWe also \ufb01nd that the remaining terms in the MSE decomposition (26) are at least one\norder of magnitude smaller than the sample variance, which is then the largest term of the FEL\n(21) in the presence of noise. The same conclusions apply when analyzing the variance corrected\nestimator (28). We conclude that minimizing the MSE of the FEL (21) or of the variance\ncorrected estimator (28), as a way to determine the optimal cutting frequency parameters \u02c6\nM\nand \u02c6N, is equivalent to minimizing the sample variance of the estimators. Following this selection\n15\n\n-0.1\n1000\n0\n800\n10\n600\n0.1\n8\n(MSE-VAR)/MSE - Fourier Estimator\nM\n6\nN\n400\n0.2\n4\n200\n2\n0\n0\n-0.1\n1000\n0\n800\n10\n600\n0.1\n8\n(MSE-VAR)/MSE - Variance corrected estimator\nM\n6\nN\n400\n0.2\n4\n200\n2\n0\n0\nFigure 2: Relative di\ufb00erence between MSE and variance of the FEL e\u03b7n,M,N and the variance\ncorrected estimator \u03b7\u2217\nn,M,N as a function of M and N. Left panels: H model. Right panels: GH\nmodel.\nH \u2212model\nGH \u2212model\n\u03b7\n-1.013673e-04\n-4.603226e-05\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\ne\u03b7n,M,N\n2.43e-07\n887\n1\n1.75e-07\n2404\n2\n\u03b7\u2217\nn,M,N\n1.44e-07\n0.59\n889\n1\n1.51e-07\n0.86\n2638\n1\nTable 2: Finite sample performance of the FEL e\u03b7n,M,N and of the estimator \u03b7\u2217\nn,M,N.",
    "chunk_index": 13,
    "start_char": 31547,
    "end_char": 33897,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "0\nFigure 2: Relative di\ufb00erence between MSE and variance of the FEL e\u03b7n,M,N and the variance\ncorrected estimator \u03b7\u2217\nn,M,N as a function of M and N. Left panels: H model. Right panels: GH\nmodel.\nH \u2212model\nGH \u2212model\n\u03b7\n-1.013673e-04\n-4.603226e-05\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\ne\u03b7n,M,N\n2.43e-07\n887\n1\n1.75e-07\n2404\n2\n\u03b7\u2217\nn,M,N\n1.44e-07\n0.59\n889\n1\n1.51e-07\n0.86\n2638\n1\nTable 2: Finite sample performance of the FEL e\u03b7n,M,N and of the estimator \u03b7\u2217\nn,M,N. \u03b7 represents\nthe average real integrated leverage for each data set. The optimal parameters \u02c6\nM and \u02c6N are\nselected by minimization of the sample variance. The value of the VAR in the table is computed\nw.r.t. the optimal parameters \u02c6\nM and \u02c6N. The symbol \u03bb denotes the variance reduction ratio\nV ar(\u03b7\u2217\nn, \u02c6\nM, \u02c6\nN)/V ar(e\u03b7n, \u02c6\nM, \u02c6\nN).\nstrategy, the parameters \u02c6\nM and \u02c6N do not yield a minimum value of the estimator bias. The\noptimized estimator is then a\ufb00ected by a non-negligible bias, which is, however, very small. In\nour simulation, we always get a signi\ufb01cant digit after the comma. Note that implementing this\nkind of selection strategy for the variance corrected estimator (28) means changing Step 4 of\nits implementation by minimizing the sample variance. The results obtained by selecting the\noptimal cutting frequency parameters by minimizing the sample variance of the estimators (21)\nand (28) are displayed in Table 2. We highlight that the selected parameters M and N are the\nsame as those selected by MSE minimization.\n3.2\nBenchmark analysis\nWe want now to analyse the FEL performance compared to the estimator of the integrated\nleverage proposed by [Wang and Mykland, 2014] in the presence of noise. The latter is based on\npre-averaging and blocking that allows us to deal with the noise contained in the data. Here two\n16\n\nnested levels of blocks are required: the \ufb01rst one, of size M, de\ufb01nes the range of pre-averaging,\nand the second one, of size L, is used for computing the realized covariance between returns and\nvolatility increments. Our choice for the blocking parameters M and L is the following: we let\nM vary from 2 seconds to 300 seconds (i.e. 5-minute block size). Then, up to rounding, for each\nvalue of M we de\ufb01ne n\u2032 = n/M and let L = [\n\u221a\nn\u2032]. Coherently with our previous approach, we\nthen choose the optimal parameters by directly minimizing the MSE over the range of M\u2019s. We\ncall this estimator WM1.\nIn their paper, the authors provide a rule to choose the optimal values of M and L that\nminimize the asymptotic variance in the presence of microstructure e\ufb00ects. However, the imple-\nmentation of this rule requires a preliminary estimate of the integrated volatility, the integrated\nquarticity and the integrated sixth power of volatility, besides the estimation of the spot quar-\nticity and the di\ufb00usion coe\ufb03cient \u03b3(t) in (3).",
    "chunk_index": 14,
    "start_char": 33456,
    "end_char": 36261,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "call this estimator WM1.\nIn their paper, the authors provide a rule to choose the optimal values of M and L that\nminimize the asymptotic variance in the presence of microstructure e\ufb00ects. However, the imple-\nmentation of this rule requires a preliminary estimate of the integrated volatility, the integrated\nquarticity and the integrated sixth power of volatility, besides the estimation of the spot quar-\nticity and the di\ufb00usion coe\ufb03cient \u03b3(t) in (3). To reduce possible sources of estimation errors,\nwe compute these quantities from the model (31) by Riemann integration rule. We call this\nestimator WM2. Our results are resumed in Table 3. We notice that the \ufb01rst procedure, which\nis completely unfeasible, provides a worse estimate than the Fourier methodology presented in\nTable 1 and 2 both in terms of bias and variance. On the other hand, the second procedure pro-\nvides a very good estimate in terms of bias, while the variance and MSE are nevertheless slightly\nlarger than those obtainable by the Fourier approach. This does not come as a surprise, since\nthe estimator proposed by [Wang and Mykland, 2014] contains a bias correction factor while the\nFourier estimator achieves unbiasedness only asymptotically. As a further evidence, in Figure 3\n[Wang and Mykland, 2014]\nH \u2212model\nEstimator\nMSE\nVAR\nBIAS\nM\nL\nWM1\n3.90e-06\n3.76e-06\n-4.24e-04\n4\n73\nWM2\n3.09e-07\n3.12e-07\n-7.96e-06\n2\n2460\nTable 3: Finite sample properties of the estimator by [Wang and Mykland, 2014] under mi-\ncrostructure e\ufb00ects.\nwe can see that the estimate proposed by [Wang and Mykland, 2014] is largely dependent on\nthe choice of the block size M and its MSE is increasing with this parameter, while the bias\nremains rather stable around zero.\n3.3\nSensitivity analysis on the generalized Heston model\nWe examine the FEL behaviour in the presence of noise depending on the choice of some param-\neters of the GH model. The process \u03c1(t) in the GH model is a linear transformation of a Jacobi\nprocess which takes values in [\u22121, 1]. Moreover, \u03c1(t) is mean reverting to \u03b6 = (2\u03be \u2212\u03b7)/\u03b7 at speed\n\u03b7. These kinds of processes are ideal di\ufb00usions to model stochastic correlation. Its properties are\nsummarized by [Veerart and Veerart, 2012].\nAn interesting feature of this process is that it tends to a jump process with state-space\n17\n\n0\n50\n100\n150\n200\n250\n300\nBlock size M\n-1\n0\n1\nInt. Lev.\n\u00d710-3\nRealized Leverage\n0\n50\n100\n150\n200\n250\n300\nBlock size M\n0\n2\n4\n6\nMSE\n\u00d710-5\n0\n50\n100\n150\n200\n250\n300\nBlock size M\n-1\n0\n1\nBIAS\n\u00d710-3\nFigure 3: MSE-based integrated leverage estimate by [Wang and Mykland, 2014] together with\nits MSE and BIAS as a function of the block size M.\n{\u22121, 1} and constant intensities if \u03b8 tends to in\ufb01nity. Roughly speaking, when \u03b8 increases the\nprocess exhibits a jump-type behaviour while the smaller the parameter \u03b8, the smoother are the\nsample paths. Therefore, the \ufb02uctuations of the process \u03c1(t) can be ampli\ufb01ed by increasing the\nparameter \u03b8.",
    "chunk_index": 15,
    "start_char": 35809,
    "end_char": 38742,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "100\n150\n200\n250\n300\nBlock size M\n-1\n0\n1\nBIAS\n\u00d710-3\nFigure 3: MSE-based integrated leverage estimate by [Wang and Mykland, 2014] together with\nits MSE and BIAS as a function of the block size M.\n{\u22121, 1} and constant intensities if \u03b8 tends to in\ufb01nity. Roughly speaking, when \u03b8 increases the\nprocess exhibits a jump-type behaviour while the smaller the parameter \u03b8, the smoother are the\nsample paths. Therefore, the \ufb02uctuations of the process \u03c1(t) can be ampli\ufb01ed by increasing the\nparameter \u03b8. Fig. 4 shows the sensitivity of the FEL (21) and of the variance corrected estimator\n(28) in terms of MSE with respect to the choice of \u03b8. All the other model parameters are set\nas in Section 3.1 and the cutting frequency parameters are determined by minimization of the\nsample variance. As expected, the MSE of both estimators slightly deteriorates as the jump-type\nbehaviour of the correlation process is emphasized.\n0\n0.5\n1\n1.6\n1.7\n1.8\n1.9\nMSE\n10-7 Int. lev. estimator\n0\n0.5\n1\n1.4\n1.5\n1.6\n1.7\nMSE\n10-7Corr. lev. estimator\nFigure 4: Sensitivity of the FEL e\u03b7n,M,N and of the estimator \u03b7\u2217\nn,M,N with respect to the choice\nof \u03b8 in the presence of microstructure noise. Parameter values: \u03b1 = 0.01, \u03b2 = 0.2, \u03bd = 0.05 and\n\u03be = 0.02, \u03b7 = 0.5.\nWe also examine the sensitivity of the FEL to the mean reversion parameter \u03b7. The e\ufb00ect\nis examined in Fig. 5, where the MSE is plotted as a function of \u03b6 ranging in [\u22121, 1] and of\nthe corresponding \u03b7 = 2\u03be/(\u03b6 + 1). In this case, the variability of the MSE is small. Moreover,\nwe highlight that the Fourier estimator is not a\ufb00ected much by the speed of mean reversion\nand performs slightly better when the speed of mean reversion is lower. That makes the Fourier\nmethodology particularly suitable to apply in a general setting where we can assume that the\n18\n\n0\n0.1\n0.2\n0.3\n0.4\n1.6\n1.62\n1.64\n1.66\nMSE\n10-7 Int. lev. estimator\n0\n0.1\n0.2\n0.3\n0.4\n1.42\n1.44\n1.46\n1.48\nMSE\n10-7Corr. lev. estimator\n-1\n-0.5\n0\n0.5\n1\n1.6\n1.62\n1.64\n1.66\nMSE\n10-7 Int. lev. estimator\n-1\n-0.5\n0\n0.5\n1\n1.42\n1.44\n1.46\n1.48\nMSE\n10-7Corr. lev. estimator\nFigure 5: Sensitivity of the FEL e\u03b7n,M,N and of the estimator \u03b7\u2217\nn,M,N with respect to the choice\nof \u03b7 and \u03b6 = (2\u03be \u2212\u03b7)/\u03b7 in the presence of microstructure noise. Parameter values: \u03b1 = 0.01, \u03b2 =\n0.2, \u03bd = 0.05 and \u03be = 0.02, \u03b8 = 0.5.\nGH model is the data generating process.\n4\nEstimation of the stochastic leverage e\ufb00ect in the presence of\nmicrostructure noise\nIn the following, we denote by \u02dc\u03c32\nn,M and \u02dc\u03b32\nn,M,N the FEV and the FEVV in the presence of\nmicrostructure noise, respectively. We can then obtain a plug-in estimator of (2) by using the\nFEL de\ufb01ned in (21) at the numerator and \u02dc\u03c32\nn,M and \u02dc\u03b32\nn,M,N at the denominator\n\u02dcRT =\n\u02dc\u03b7n,M,N\nq\n\u02dc\u03c32\nn,M \u02dc\u03b32\nn,M,N\n.",
    "chunk_index": 16,
    "start_char": 38251,
    "end_char": 40964,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "= 0.05 and \u03be = 0.02, \u03b8 = 0.5.\nGH model is the data generating process.\n4\nEstimation of the stochastic leverage e\ufb00ect in the presence of\nmicrostructure noise\nIn the following, we denote by \u02dc\u03c32\nn,M and \u02dc\u03b32\nn,M,N the FEV and the FEVV in the presence of\nmicrostructure noise, respectively. We can then obtain a plug-in estimator of (2) by using the\nFEL de\ufb01ned in (21) at the numerator and \u02dc\u03c32\nn,M and \u02dc\u03b32\nn,M,N at the denominator\n\u02dcRT =\n\u02dc\u03b7n,M,N\nq\n\u02dc\u03c32\nn,M \u02dc\u03b32\nn,M,N\n.\n(33)\nSimilarly, we can also consider a second estimator of (2) by using the Fourier estimator\n\u03b7\u2217\nn,M,N de\ufb01ned in (28) at the numerator\nR\u2217\nT =\n\u03b7\u2217\nn,M,N\nq\n\u02dc\u03c32\nn,M \u02dc\u03b32\nn,M,N\n.\n(34)\nThe estimators (33) and (34) depend on the choice of the cutting frequency parameters M\nand N that strongly a\ufb00ect the quality of the estimates.\nFor the FEL at the numerator, we follow the selection strategy for the parameters M and\nN described in Section 3.1. If we have simulated data, we generate a certain number of daily\nsamples and minimize the sample variance of the FEL. On the other hand, if we are working\nin a real data framework, we need several days of observations (e.g. 100 days) to perform the\n19\n\nparameter selection.\nFor the FEV \u02dc\u03c32\nn,M the cutting frequency is determined by minimizing the MSE estimate\ndetermined in [Mancino and Sanfelici, 2008, Theorem 3] on each day. Finally, for the FEVV\n\u02dc\u03b32\nn,M,N the frequencies M and N can be chosen in the range de\ufb01ned in [Sanfelici et al., 2015,\nRemark 4.3].\nIn this section, we use simulated data while performing an estimation with real data in\nSection 5. We again simulate second-by-second return and variance paths over a daily trading\nperiod of T = 6 hours from the GH model (32) and choose the parameter values and the noise-\nto-signal ratio as in Section 3.1. In Table 4, we list the value of the cutting frequency parameters\nselected for the simulation, together with the MSE achieved by each estimator appearing in (33)-\n(34) and the average value over 100 days of the corresponding true (following the GH model)\nintegrated leverage, integrated volatility and integrated volatility of volatility.\nEstimates\nReference Value\nMSE\nM\nN\ne\u03b7n,M,N\n-4.60e-05\n1.70e-07\n2404\n2\n\u03b7\u2217\nn,M,N\n-4.60e-05\n1.49e-07\n2638\n1\n\u02dc\u03c32\nn,M\n1.00e-02\n2.38e-06\n819\n-\n\u02dc\u03b32\nn,M,N\n2.51e-05\n3.71e-07\n146\n3\nTable 4: GH data set. MSE and parameters\u2019 selection related to the estimators appearing in (33)\nand (34). The reference values corresponds to the average over 100 days of the corresponding\ntrue integrated values.\nFigure 6 shows RT estimated using (33) and (34). The true RT is plotted in red. In our\nsimulation, both plots displayed in Figure 6 seem to provide a good approximation of RT .",
    "chunk_index": 17,
    "start_char": 40503,
    "end_char": 43168,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "in (33)\nand (34). The reference values corresponds to the average over 100 days of the corresponding\ntrue integrated values.\nFigure 6 shows RT estimated using (33) and (34). The true RT is plotted in red. In our\nsimulation, both plots displayed in Figure 6 seem to provide a good approximation of RT .\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate without correction (true RT in red)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate with correction (true RT in red)\nFigure 6: Upper panel: Daily RT estimates performed with \u02dcRT . Lower panel: Daily RT estimates\nperformed with R\u2217\nT . The true values of RT are plotted in red.\n20\n\nFigure 7: Log-prices and returns plots for S&P500 futures in the years 2007 (left panels) and\n2008 (right panels).\n5\nEmpirical analysis\nWe analyse the leverage e\ufb00ect pattern in a tick data set by using the estimators of the stochastic\nleverage e\ufb00ect \u02dcRT and R\u2217\nT de\ufb01ned in (33) and (34), respectively.\nWe consider transaction data of the S&P500 futures recorded at the Chicago Mercantile\nExchange (CME) for the period from January 3, 2007, to December 31, 2008 (502 days). During\nthis period, the United States experienced the subprime mortgage crisis, a nationwide \ufb01nancial\ncrisis that contributed to the U.S. recession of December 2007 till June 2009. It was triggered by\na large decline in home prices after the collapse of a housing bubble during 2006. That induced\na large banking crisis in 2007 and the \ufb01nancial crisis in 2008. In nine days from October 1 to 9,\n2008 the S&P500 lost 21.6% of its value. Table 5 describes the main features of our data set.\nYear\nN. trades\nVariable\nMean\nStd. Dev.\nMin\nMax\n2007\n566409\nS&P 500 index\n1484.84\n44.30\n1375.00\n1586.50\nlog-return\n5.00e-6\n1.81e-2\n-1.64\n2.33\n2008\n557982\nS&P 500 index\n1226.55\n186.89\n739.00\n1480.20\nlog-return\n-9.03e-5\n4.75e-2\n-8.66\n6.12\nTable 5: Summary statistics for the sample of the traded CME S&P500 futures for the period\nfrom January 3rd 2007 to December 31st 2008 (502 days).\nFigure 7 shows the plot of the log-prices and returns for the raw transaction data. High-\nfrequency returns are contaminated by microstructure e\ufb00ects, such as transaction costs and\nbid-and-ask bounce e\ufb00ects, leading to biases in the variance measures. Figure 8 shows the au-\ntocorrelation function for the log-returns. Raw data exhibit a strongly signi\ufb01cant positive \ufb01rst-\norder autocorrelation and higher-order autocorrelations remain signi\ufb01cant up to lag 8 in 2007\nand up to lag 15 in 2008. We then perform an analysis of the stochastic leverage e\ufb00ect using the\nestimators eRT and R\u2217\nT . First, we plot the daily values of the factors appearing in the estimator\neRT and R\u2217\nT . The upper panels of Figure 9 show the daily FEL (21) and its variance corrected\nversion (28) in 2007 (left) and 2008 (right). The middle panels show the FEV together with the\n21",
    "chunk_index": 18,
    "start_char": 42867,
    "end_char": 45752,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "strongly signi\ufb01cant positive \ufb01rst-\norder autocorrelation and higher-order autocorrelations remain signi\ufb01cant up to lag 8 in 2007\nand up to lag 15 in 2008. We then perform an analysis of the stochastic leverage e\ufb00ect using the\nestimators eRT and R\u2217\nT . First, we plot the daily values of the factors appearing in the estimator\neRT and R\u2217\nT . The upper panels of Figure 9 show the daily FEL (21) and its variance corrected\nversion (28) in 2007 (left) and 2008 (right). The middle panels show the FEV together with the\n21\n\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nLag\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nSample Autocorrelation\nSample Autocorrelation Function\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nLag\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nSample Autocorrelation\nSample Autocorrelation Function\nFigure 8: Autocorrelation function for S&P500 futures in the years 2007 (left panel) and 2008\n(right panel).\n5-minute sparse sampled realized volatility estimator (which we consider as a benchmark of our\nestimates) and the lower panels show the FEVV in 2007 and 2008. The estimated quantities in\n2007 and 2008 are very di\ufb00erent in magnitude. The year 2008 displays the largest values (both\nnegative and positive) of all the metrics, coherently with the occurrence of the \ufb01nancial crisis.\nDuring 2007 the integrated leverage is rather small and mostly negative. All the estimations\nare almost \ufb02at during 2008 up to September 16 (day 177 in our sample), when the integrated\nleverage exhibits the \ufb01rst large negative spike. The second negative spike is on September 29\n(day 186), which corresponds to the beginning of the \ufb01nancial crisis. Our \ufb01nding highlights the\npresence of persistent positive and negative integrated leverage, especially in periods of \ufb01nancial\nturmoil. We notice that both the FEL (21) and the variance corrected estimator (28) catch the\nsame positive and negative spikes of the integrated leverage; nevertheless, the estimator (28)\nexhibits a smaller variability.\nWhen estimating the integrated leverage, a larger variability can be observed than estimat-\ning other quantities such as volatility or quarticity. According to the analysis of Section 4, for\nboth estimators e\u03b7n,M,N and \u03b7\u2217\nn,M,N the cutting frequency parameters M and N are chosen such\nto minimize the sample variance over the whole one year sample. Their optimal values are listed\nin Table 6, along with the sample variance achieved by the FEL (21) and its variance corrected\ncounterpart (28). Due to the presence of microstructure e\ufb00ects, the optimal cutting frequency\n\u02c6\nM turns out to be much smaller than the Nyquist frequency (i.e. M \u226an/2 = 2460).\nWe highlight that the Fourier estimator makes use of all the n observed prices and it \ufb01lters\nout microstructure e\ufb00ects by a suitable choice of M and N, instead of reducing the sampling\nfrequency.\nWe conclude this section by showing RT estimates obtained with the estimators (33) and\n(34). The graphs do not show evident di\ufb00erences between the years 2007 and 2008. That is how it\n22",
    "chunk_index": 19,
    "start_char": 45234,
    "end_char": 48194,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "out to be much smaller than the Nyquist frequency (i.e. M \u226an/2 = 2460).\nWe highlight that the Fourier estimator makes use of all the n observed prices and it \ufb01lters\nout microstructure e\ufb00ects by a suitable choice of M and N, instead of reducing the sampling\nfrequency.\nWe conclude this section by showing RT estimates obtained with the estimators (33) and\n(34). The graphs do not show evident di\ufb00erences between the years 2007 and 2008. That is how it\n22\n\n0\n50\n100\n150\n200\n250\ndays\n-10\n-5\n0\n5\nLeverage\n10-6\nInt. Lev.: Fourier (blue), Estimator (95) (red)\n0\n50\n100\n150\n200\n250\ndays\n0\n5\nInt. Vol\n10-4\nIntegrated Volatility: Fourier (blue), Realized Vol. (red)\n0\n50\n100\n150\n200\n250\ndays\n0\n2\nVoV\n10-5\nInt. Vol. of Vol..: Fourier\n0\n50\n100\n150\n200\n250\ndays\n-2\n0\n2\nLeverage\n10-4\nInt. Lev.: Fourier (blue), Estimator (95) (red)\n0\n50\n100\n150\n200\n250\ndays\n0\n0.005\n0.01\nInt. Vol\nIntegrated Volatility: Fourier (blue), Realized Vol. (red)\n0\n50\n100\n150\n200\n250\ndays\n0\n1\n2\nVoV\n10-3\nInt. Vol. of Vol..: Fourier\nFigure 9: Upper panels: FEL (21) (blue) and variance corrected estimator (28) (red) . Middle\npanels: FEV (blue) and the realized volatility estimator (red) in the years 2007 (left panel) and\n2008 (right panel). Lower panels: FEVV (blue) in the years 2007 (left panel) and 2008 (right\npanel).\nS&P500 futures\n2007\nEstimate\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\ne\u03b7n,M,N\n-2.44e-07\n1.20e-12\n363\n1\n\u03b7\u2217\nn,M,N\n-9.23e-08\n3.02e-13\n0.25\n143\n1\nS&P500 futures\n2008\nEstimate\nVAR\n\u03bb\n\u02c6\nM\n\u02c6N\ne\u03b7n,M,N\n-2.01e-06\n5.94e-10\n281\n3\n\u03b7\u2217\nn,M,N\n-1.76e-06\n1.51e-10\n0.25\n285\n3\nTable 6: The FEL, its variance corrected counterpart (28) and their sample variance computed\nw.r.t. the optimal parameters \u02c6\nM and \u02c6N. The symbol \u03bb denotes the variance reduction ratio\nV ar(\u03b7\u2217\nn, \u02c6\nM, \u02c6\nN)/V ar(e\u03b7n, \u02c6\nM, \u02c6\nN). The optimal cutting frequency parameters are obtained by mini-\nmization of the sample variance over each year. The estimates in the \ufb01rst column correspond to\naverages over all the year.\n23\n\n0\n50\n100\n150\n200\n250\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate without correction\n0\n50\n100\n150\n200\n250\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate with correction\n0\n50\n100\n150\n200\n250\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate without correction\n0\n50\n100\n150\n200\n250\ndays\n-1\n-0.5\n0\n0.5\n1\nRT\nDaily RT estimate with correction\nFigure 10: RT estimated by the FESL (33) (upper panels) and by its variance corrected version\n(34) (lower panels) in the years 2007 (left panels) and 2008 (right panels).\nshould be. Indeed, we expect to see a similar behaviour of the stochastic leverage e\ufb00ect estimates\nbecause we assume that the data generating process is the same for both the 2007 and 2008 data\nsets.",
    "chunk_index": 20,
    "start_char": 47741,
    "end_char": 50365,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "RT\nDaily RT estimate with correction\nFigure 10: RT estimated by the FESL (33) (upper panels) and by its variance corrected version\n(34) (lower panels) in the years 2007 (left panels) and 2008 (right panels).\nshould be. Indeed, we expect to see a similar behaviour of the stochastic leverage e\ufb00ect estimates\nbecause we assume that the data generating process is the same for both the 2007 and 2008 data\nsets. The obtained estimates seem to express a fundamental measure of the asymmetry between\nreturns and volatilities observed across the years.\n6\nConclusions\nWe de\ufb01ne an estimator of the stochastic leverage e\ufb00ect which combines the Fourier estimator of\nthe integrated leverage (FEL), of the integrated volatility (FEV) and the integrated volatility\nof volatility (FEVV) de\ufb01ned in [Curato and Sanfelici, 2015], [Malliavin and Mancino, 2002] and\n[Sanfelici et al., 2015], respectively. We call it the Fourier estimator of the stochastic leverage\ne\ufb00ect (FESL). An advantage of this estimator is that it avoids estimating the latent volatility\npath. This step is mandatory in concurrently realized covariance-based estimators appearing in\nthe literature and is one reason behind several bias corrections required by them.\nWe show the consistency of the FESL in the absence of microstructure noise. Then, we focus\non analyzing the behaviour of the FESL in the presence of microstructure noise. The latter is\nstrictly related to the \ufb01nite sample properties of the FEL, the FEV, and the FEVV used in\nthe estimation. The only estimator for which a thorough analysis of the latter is missing in the\nliterature is the FEL. We \ufb01ll this gap and determine that it is asymptotically unbiased but has\na diverging mean squared error.\nWe propose a variance corrected estimator of the FEL to hinder its variability in the \ufb01nite\nsample. We then examine selection strategies for the cutting frequency parameters appearing\nin the estimation methodology. Numerically, we observe that the parameter values obtained by\nminimizing the mean squared error of the FEL are equivalent to those obtained minimizing\n24\n\nits sample variance. Moreover, we note that using the variance corrected estimator reduces the\nsample variance of the \ufb01nal estimation by a half and that a selection strategy based on its sample\nvariance is also directly applicable when real data are at disposal. Finally, we investigate the\nperformance of the FESL in empirical analysis and detect the presence of the stochastic leverage\ne\ufb00ect in the S&P 500 future prices data set for the years 2007 and 2008.\nAppendix: Proofs\nIn the proofs below, we make often use of following Lemma, see [Katznelson, 2004].\nLemma 6.1. Let DN(t) be the normalized Dirichlet kernel de\ufb01ned in (13), then the following\nproperties are satis\ufb01ed.\n1.\nR T\n0 |DN(u)|2 du =\nT\n2N+1,\n2. \u2200p > 1, there exists a constant Cp such that\nR T\n0 |DN(u)|p du =\nCp\n2N+1.\nProof of Theorem 2.3: Throughout the proof we indicate with \u03c6n(s) := supk=0,...,n{tk : tk \u2264s}.\nMoreover, we use the following integral notation for the discrete Fourier coe\ufb03cients of the returns\n(11)\ncn(s;",
    "chunk_index": 21,
    "start_char": 49958,
    "end_char": 53038,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "of following Lemma, see [Katznelson, 2004].\nLemma 6.1. Let DN(t) be the normalized Dirichlet kernel de\ufb01ned in (13), then the following\nproperties are satis\ufb01ed.\n1.\nR T\n0 |DN(u)|2 du =\nT\n2N+1,\n2. \u2200p > 1, there exists a constant Cp such that\nR T\n0 |DN(u)|p du =\nCp\n2N+1.\nProof of Theorem 2.3: Throughout the proof we indicate with \u03c6n(s) := supk=0,...,n{tk : tk \u2264s}.\nMoreover, we use the following integral notation for the discrete Fourier coe\ufb03cients of the returns\n(11)\ncn(s; dp) = 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT s\u03c6n(u)dp(u).\nIn the proof, C will denote a positive constant, not necessarily the same at di\ufb00erent occurrences.\nApplying the product rule to the term cn(s; dp)cn(l \u2212s; dp) and using notations (13) and\n(14), we obtain the following error decomposition.\n\u03b7n,M,N \u2212\u03b7 = \u03b71\nn,N \u2212\u03b7 + \u03b72\nn,M,N\n(35)\n=\nT 2\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT l\u03c6n(t)\u03c32(t)dt 1\nT\nZ T\n0\nei 2\u03c0\nT l\u03c6n(u)dp(u)\n|\n{z\n}\n\u03b71\nn,N\n\u2212\nZ T\n0\n\u03b7(t) dt\n(36)\n+\nT 2\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n \n1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l\u03c6n(u)DM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n|\n{z\n}\n+ 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT l\u03c6n(t)\nZ t\n0\nDM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n!\n1\nT\nZ T\n0\nei 2\u03c0\nT l\u03c6n(u), dp(u)\n|\n{z\n}\n\u03b72\nn,M,N\n,\nwhere the variable t \u2208(0, T]. By using the Cauchy-Schwartz inequality we have that\n25\n\nE[|\u03b72\nn,M,N|] \u2264\nT 2\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh\f\f\f 1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l\u03c6n(u)DM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n+ 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT l\u03c6n(t)\nZ t\n0\nDM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2i 1\n2 E\nh\f\f\f 1\nT\nZ T\n0\nei 2\u03c0\nT l\u03c6n(u)dp(u)\n\f\f\f\n2i 1\n2 .\nFor each |l| \u2264N, the L2-norm of the Fourier coe\ufb03cients of the returns\nE\nh\f\f\f\nZ T\n0\nei 2\u03c0\nT l\u03c6n(u)dp(u)\n\f\f\f\n2\n] \u2264C,\nunder Assumption (H1). On the other hand,\nE\nh\f\f\f 1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l\u03c6n(u)DM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n+ 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT l\u03c6n(t)\nZ t\n0\nDM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2i\n\u2264CE\nh\f\f\f 1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l\u03c6n(u)DM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2i\n(37)\n+ CE\nh\f\f\f 1\nT\nZ T\n0\ne\u2212i 2\u03c0\nT l\u03c6n(t)\nZ t\n0\nDM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2i\n.\n(38)\nThe addends (37) and (38) have the same order of magnitude in L2-norm. We then show\nonly the estimation of the term (37). The latter is less than or equal to\nCE\nh \f\f\f 1\nT\nZ T\n0\nZ t\n0\n(e\u2212i 2\u03c0\nT l\u03c6n(u) \u2212e\u2212i 2\u03c0\nT lu)\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2\n|\n{z\n}\n(T1)",
    "chunk_index": 22,
    "start_char": 52565,
    "end_char": 54757,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "e\u2212i 2\u03c0\nT l\u03c6n(t)\nZ t\n0\nDM(\u03c6n(t) \u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2i\n.\n(38)\nThe addends (37) and (38) have the same order of magnitude in L2-norm. We then show\nonly the estimation of the term (37). The latter is less than or equal to\nCE\nh \f\f\f 1\nT\nZ T\n0\nZ t\n0\n(e\u2212i 2\u03c0\nT l\u03c6n(u) \u2212e\u2212i 2\u03c0\nT lu)\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u) dp(t)\n\f\f\f\n2\n|\n{z\n}\n(T1)\ni\n+CE\nh \f\f\f 1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l\u03c6n(u)\n1\n2M + 1\nX\n|s|\u2264M\n(e\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) \u2212e\u2212i 2\u03c0\nT s(t\u2212u)) dp(u) dp(t)\n\f\f\f\n2\n|\n{z\n}\n(T2)\ni\n+CE\nh \f\f\f 1\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT luDM(t \u2212u) dp(u) dp(t)\n\f\f\f\n2\n|\n{z\n}\n(T3)\ni\n.\n26\n\nThe term (T1), after applying the It\u02c6o isometry, is less than or equal to\nCE\nh Z T\n0\n\f\f\f\nZ t\n0\n(e\u2212i 2\u03c0\nT l\u03c6n(u) \u2212e\u2212i 2\u03c0\nT lu)\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u)\n\f\f\f\n2\n\u03c32(t) dt\ni\n(T11)\n+CE\nh Z\n[0,T]2\n\u0010 Z t\n0\n(e\u2212i 2\u03c0\nT l\u03c6n(u) \u2212e\u2212i 2\u03c0\nT lu)\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u)\n\u0011\n\u0010 Z z\n0\n(ei 2\u03c0\nT l\u03c6n(v) \u2212ei 2\u03c0\nT lv)\n1\n2M + 1\nX\n|s|\u2264M\nei 2\u03c0\nT s(\u03c6n(z)\u2212\u03c6n(v)) dp(v)\n\u0011\na(z) a(t) dz dt\ni\n(T12)\n\u2264CE\nh Z T\n0\nZ t\n0\n(|l|2\u03c0\nT |\u03c6n(t) \u2212t| + l2 4\u03c02\nT 2 o(|\u03c6n(t) \u2212t|2))2 du dt\ni\n+CE\nh Z T\n0\nZ\n[0,t]2(|l|2\u03c0\nT |\u03c6n(t) \u2212t| + l2 4\u03c02\nT 2 o(|\u03c6n(t) \u2212t|2))\n\u00d7(|l|2\u03c0\nT |\u03c6n(s) \u2212s| + l2 4\u03c02\nT 2 o(|\u03c6n(s) \u2212s|2)) dv ds dt\ni\n+C(N2\u03c4(n) + o(1))E\nh Z\n[0,T]2\n\u0010 Z t\n0\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u)\nZ z\n0\n1\n2M + 1\nX\n|s|\u2264M\nei 2\u03c0\nT s(\u03c6n(z)\u2212\u03c6n(v)) dp(v)\n\u0011\na(z) a(t) dz dt\ni\n.\nTo obtain the last inequality, we apply several times Taylor\u2019s formula, Assumption (H1) and the\nH\u00a8older and Cauchy-Schwarz inequalities. Note that the \ufb01rst two addends from the right hand\nside correspond to the estimation of the term (T11) whereas the third addend estimates from\nabove (T12). Thus,\nE[(T1)] \u2264CN2\u03c4 2(n) + o(1).\nAnalogously, we can show that\nE[(T2)] \u2264CM2\u03c4 2(n) + o(1).\nIt remains to analyze the last addend of (37).\nE[(T3)] \u2264CE\nh Z T\n0\n\f\f\f\nZ t\n0\ne\u2212i 2\u03c0\nT luDM(t \u2212u) dp(u)\n\f\f\f\n2\n\u03c32(t) dt\ni\n+\nh Z\n[0,T]2\n\u0010 Z t\n0\ne\u2212i 2\u03c0\nT luDM(t \u2212u) dp(u)\nZ z\n0\nei 2\u03c0\nT lzDM(z \u2212v) dp(v)\n\u0011\na(t) a(z) dt dz\ni\n.\nUsing iteratively the H\u00a8older and the Cauchy-Schwarz inequalities, we obtain the term above is\n27",
    "chunk_index": 23,
    "start_char": 54408,
    "end_char": 56451,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "t\n0\ne\u2212i 2\u03c0\nT luDM(t \u2212u) dp(u)\n\f\f\f\n2\n\u03c32(t) dt\ni\n+\nh Z\n[0,T]2\n\u0010 Z t\n0\ne\u2212i 2\u03c0\nT luDM(t \u2212u) dp(u)\nZ z\n0\nei 2\u03c0\nT lzDM(z \u2212v) dp(v)\n\u0011\na(t) a(z) dt dz\ni\n.\nUsing iteratively the H\u00a8older and the Cauchy-Schwarz inequalities, we obtain the term above is\n27\n\nless than or equal to\n\u2264CE\nh Z T\n0\nZ t\n0\nD2\nM(t \u2212u) \u03c32(u) du dt\ni\n+ CE\nh Z T\n0\n\u0010 Z t\n0\n|DM(t \u2212u)|p\u2032 a(u) du\n\u0011 2\np\u2032 dt\ni\n+CE\nh Z\n[0,T]2\n\u0010 Z t\n0\nD2\nM(t \u2212u) du\n\u0011\ndtdz\ni 1\n2 E\nh Z\n[0,T]2\n\u0010 Z z\n0\nD2\nM(z \u2212v) dv\n\u0011\ndtdz\ni 1\n2\n+CE\nh Z\n[0,T]2\n\u0010 Z t\n0\n|DM(t \u2212u)|p\u2032 du\n\u0011 2\np\u2032 dtdz\ni 1\n2 E\nh Z\n[0,T]2\n\u0010 Z z\n0\nD2\nM(z \u2212v) dv\n\u0011\ndtdz\ni 1\n2\n+CE\nh Z\n[0,T]2\n\u0010 Z t\n0\nD2\nM(t \u2212u) du\n\u0011\ndtdz\ni 1\n2 E\nh Z\n[0,T]2\n\u0010 Z z\n0\n|DM(z \u2212v)|p\u2032 dv\n\u0011 2\np\u2032 dtdz\ni 1\n2\n+CE\nh Z\n[0,T]2\n\u0010 Z t\n0\n|DM(t \u2212u)|p\u2032 du\n\u0011 2\np\u2032 dtdz\ni 1\n2 E\nh Z\n[0,T]2\n\u0010 Z z\n0\n|DM(z \u2212v)|p\u2032 dv\n\u0011 2\np\u2032 dtdz\ni 1\n2 ,\nfor p\u2032 \u2208(1, 2). By Lemma 6.1,\nE[(T3)] \u2264\nC\n2M + 1 +\nC\n(2M + 1)\n2\np\u2032 +\nC\n(2M + 1)\n2+p\u2032\n2p\u2032\n.\nThus\nE[|\u03b72\nn,M,N|] \u2264C\nN\n\u221a\nM + 1 + CN2\u03c4(n) + CNM\u03c4(n) + o(1),\nwhich converges to zero under Assumption (16). We now show that the term (36) converges to\nzero in L1-norm.\nE[|\u03b71\nn,N \u2212\u03b7|]\n= E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT l(ti\u2212tj)\nZ tj+1\ntj\n\u03c32(t) dt\nZ ti+1\nti\ndp(u) \u2212\nZ t\n0\n\u03b7(t) dt\n\f\f\f\ni\n=E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ T\n0\n(ei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u)) \u2212ei 2\u03c0\nT l(t\u2212u))\u03c32(u) du a(t)dt\n\f\f\f\ni\n(39)\n+ E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ T\n0\n(ei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u)) \u2212ei 2\u03c0\nT l(t\u2212u))\u03c32(u) du \u03c3(t) dW(t)\n\f\f\f\ni\n(40)\n+ E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\ne\u2212i 2\u03c0\nT lu\u03c32(u) du\nZ T\n0\nei 2\u03c0\nT ltdp(t) \u2212\nZ T\n0\n\u03b7(t) dt\n\f\f\f\ni\n.\n(41)\n28\n\nBy using Taylor\u2019s formula, the term (39) is less than or equal to\nC\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh Z T\n0\nZ T\n0\ne\u2212i 2\u03c0\nT l(t\u2212u)\u00102\u03c0\nT |l||\u03c6n(t) \u2212t \u2212\u03c6n(u) + u|\n+ l2 4\u03c02\nT 2 o(|\u03c6n(t) \u2212t \u2212\u03c6n(u) + u|2)dudt\ni\n\u2264CN2\u03c4(n) + o(1).\nAnalogously, term (40) is less than or equal to CN2\u03c4(n).",
    "chunk_index": 24,
    "start_char": 56207,
    "end_char": 58009,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "2\u03c0\nT ltdp(t) \u2212\nZ T\n0\n\u03b7(t) dt\n\f\f\f\ni\n.\n(41)\n28\n\nBy using Taylor\u2019s formula, the term (39) is less than or equal to\nC\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh Z T\n0\nZ T\n0\ne\u2212i 2\u03c0\nT l(t\u2212u)\u00102\u03c0\nT |l||\u03c6n(t) \u2212t \u2212\u03c6n(u) + u|\n+ l2 4\u03c02\nT 2 o(|\u03c6n(t) \u2212t \u2212\u03c6n(u) + u|2)dudt\ni\n\u2264CN2\u03c4(n) + o(1).\nAnalogously, term (40) is less than or equal to CN2\u03c4(n). Let us analyze the term (41). By using\nformula (8)\nE\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\ne\u2212i 2\u03c0\nT lu\u03c32(u) du\nZ T\n0\nei 2\u03c0\nT ltdp(t) \u2212\nZ T\n0\n\u03b7(t) dt\n\f\f\f\ni\n= E\nh\f\f\f\nT 2\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT c(l; \u03c32)c(\u2212l; dp) \u2212\nZ T\n0\n\u03b7(t) dt\n\f\f\f\ni\n= E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\n\u0010\nc(l; d\u03c32) \u22121\nT\nZ T\n0\nd\u03c32(u)\n\u0011\nc(\u2212l; dp) \u2212\nZ T\n0\n\u03b7(t) dt\n\f\f\f\ni\n.\nWe now use the product rule and obtain\nE\nh\f\f\f\nZ T\n0\nZ t\n0\nDN(t \u2212u)dp(u)d\u03c32(t)\nM1,N(T)\n+\nZ T\n0\nZ t\n0\nDN(t \u2212u) d\u03c32(u)dp(t)\nM2,N(T)\n\u2212\nZ T\n0\nZ t\n0\nDN(u)dp(u)d\u03c32(t)\nM3,N(T)\n\u2212\nZ T\n0\nZ t\n0\nDN(t)d\u03c32(u)dp(t)\nM4,N(T)\n\u2212\nZ T\n0\nDN(u)\u03b7(u)du\nM5,N(T)\n\f\f\f\ni\n.\nLet us analyze the \ufb01rst double integral M1,N(T)\nE\nh\f\f\f\nZ T\n0\nZ t\n0\nDN(t \u2212u)dp(u)d\u03c32(s)\n\f\f\f\ni\n= E\nh\f\f\f\nZ T\n0\nZ t\n0\nDN(t \u2212u)\u03c3(u)dW(u)\u03b3(s)dZ(s)\n+\nZ T\n0\nZ t\n0\nDN(t \u2212u)\u03c3(u)dW(u)b(s)ds +\nZ T\n0\nZ t\n0\nDN(t \u2212u)a(u)d(u)\u03b3(s)dZ(s)\n+\nZ T\n0\nZ t\n0\nDN(t \u2212u)a(u)dub(s)ds\n\f\f\f\ni\nThe \ufb01rst two summands of the decomposition above have a L1-norm of order O(N\u22121\n2 )\nand the third and the fourth ones are of order O(N\u22121\np ), where p \u2208(1, 2). These estimations\nare performed by means of the use of Lemma 6.1, the H\u00a8older and Cauchy-Schwarz inequalities.\n29\n\nAnalogous calculations follow for the terms M1,N(T), M2,N(T), M3,N(T), M4,N(T).\nBy Lemma 6.1, we have\nE[|M5,N(2\u03c0)|] \u2264CE\nh\nsup\nt\u2208[0,T]\n|\u03b7(t)|\ni\u0010 Z T\n0\n|DN(u)|pdu\n\u0011 1\np \u2264C\nN\n1\np\n.\nChoosing p \u2208(1, 2) we obtain that the term M5,N(2\u03c0) converges to zero in L1-norm as\nN \u2192\u221e. Thus,\nE\nh\f\f\f\n4\u03c02\n2N + 1\nX\n|l|\u2264N\nilc(l; \u03bd)c(\u2212l; dp) \u2212\nZ 2\u03c0\n0\n\u03b7(t)dt\n\f\f\f\ni\n\u2264\nC\n\u221a\nN\n+ C\nN\n1\np\nTherefore, under Assumption (16), it follows the consistency of the estimator \u03b7n,M,N.\nRemark 6.2. In the proof of Theorem 2.3, we show that the drift components of the logarithmic\nprice and the volatility process appear in terms that are negligible in probability. To shorten the\nproofs of the theorems below, from now on, we omit calculations involving the drift terms a(t)\nand b(t) as they follow the same rationale of the one presented in the proof above.",
    "chunk_index": 25,
    "start_char": 57690,
    "end_char": 59922,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "the consistency of the estimator \u03b7n,M,N.\nRemark 6.2. In the proof of Theorem 2.3, we show that the drift components of the logarithmic\nprice and the volatility process appear in terms that are negligible in probability. To shorten the\nproofs of the theorems below, from now on, we omit calculations involving the drift terms a(t)\nand b(t) as they follow the same rationale of the one presented in the proof above.\nIn the proof of Theorem 3.1 and 3.2, an explicit formulation of the term \u03b7\u03f5\nn,M,N is pivotal.\nHence,\n\u03b7\u03f5\nn,M,N =\nX\ni,j,k:i\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)(\u03b4i\u03b4j\u03f5k + \u03b4j\u03b4k\u03f5i + \u03b4k\u03b4i\u03f5j + \u03b4i\u03f5j\u03f5k\n+ \u03b4j\u03f5i\u03f5k + \u03b4k\u03f5i\u03f5j + \u03f5i\u03f5j\u03f5k)\n+\nX\ni,j:i\u0338=j\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj) (\u03b4i\u03f5j + \u03f52\ni \u03b4j + \u03f52\ni \u03f5j + 2\u03b4i\u03b4j\u03f5i + 2\u03b4i\u03f5j\u03f5i)\n(42)\n+\nX\ni,j\nD\u2032\nN(ti \u2212tj) (\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j).\nProof of Theorem 3.1. We \ufb01rst analyse the Bias due to the noise components.\nBecause of Assumption (H3), it holds\nE[\u03f5i\u03f5j\u03f5j]\n= 0 if i \u0338= j \u0338= k\nE[\u03f52\ni \u03f5j]\n=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n0\nif |i \u2212j| \u0338= 1,\n\u2212E[\u03b63]\nif j = i + 1,\nE[\u03b63]\nif j = i \u22121,\n30\n\nand\nE[\u03b7\u03f5\nn,M,N] =\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nD\u2032\nN(ti \u2212tj)E[\u03f52\nj\u03f5i] +\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj)E[\u03f52\ni \u03f5j]\n=\nn\u22122\nX\ni=0\nD\u2032\nN(ti+1 \u2212ti)E[\u03f52\ni \u03f5i+1] +\nn\u22121\nX\ni=1\nD\u2032\nN(ti\u22121 \u2212ti)E[\u03f52\ni \u03f5i\u22121]\n+\nn\u22122\nX\ni=0\nDM(ti+1 \u2212ti)D\u2032\nN(ti+1 \u2212ti)E[\u03f52\ni+1\u03f5i] +\nn\u22121\nX\ni=1\nDM(ti\u22121 \u2212ti)D\u2032\nN(ti\u22121 \u2212ti)E[\u03f52\ni\u22121\u03f5i]\n= (n \u22121)D\u2032\nN(T\nn )E[\u03f52\ni \u03f5i+1] + (n \u22121)D\u2032\nN(\u2212T\nn )E[\u03f52\ni \u03f5i\u22121]\n+(n \u22121)DM(T\nn )D\u2032\nN(T\nn )E[\u03f52\ni+1\u03f5i] + DM(T\nn )D\u2032\nN(\u2212T\nn )E[\u03f52\ni\u22121\u03f5i]\n= 2(n \u22121)D\u2032\nN(T\nn )(DM(T\nn ) \u22121)E[\u03b63].\nBy using Taylor\u2019s formula, it follows that D\u2032\nN(T\nn ) \u223cO\n\u0000 N2\nn ) and DM(T\nn ) \u223c1 \u2212O\n\u0000 M2\nn2 ).\nThus, under the Assumption (25), |E[\u03b7\u03f5\nn,M,N]| converges to zero as N, M, n \u2192\u221e.\nThe expected value of the term (23) is equal to\nX\ni,j,k: i\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj) E[\u03b4i\u03b4j\u03b4k] = 0,\nbecause E[\u03b4i\u03b4j\u03b4k] = 0.\nThe expected value of the term involving the components (24) equals\nE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj) \u03b42\ni \u03b4j\n+\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT l(ti\u2212tj) \u03b4i\u03b42\nj \u2212\nZ T\n0\n\u03b7(t)dt\ni\n= E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj)\nZ ti+1\nti\n\u03c32(u) du\nZ tj+1\ntj\ndp(t)\ni\n(A1)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=0",
    "chunk_index": 26,
    "start_char": 59509,
    "end_char": 61719,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT l(ti\u2212tj) \u03b4i\u03b42\nj \u2212\nZ T\n0\n\u03b7(t)dt\ni\n= E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj)\nZ ti+1\nti\n\u03c32(u) du\nZ tj+1\ntj\ndp(t)\ni\n(A1)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj)\nZ ti+1\nti\nZ t\nti\ndp(u)dp(t)\nZ tj+1\ntj\ndp(t)\ni\n(A2)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT l(ti\u2212tj)\nZ tj+1\ntj\nZ t\ntj\ndp(u)dp(t)\nZ ti+1\nti\ndp(t)\ni\n(A3)\n31\n\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nn\u22121\nX\nj=0\nei 2\u03c0\nT l(ti\u2212tj)\nZ tj+1\ntj\n\u03c32(u) du\nZ ti+1\nti\ndp(t) \u2212\nZ T\n0\n\u03b7(t)dt\ni\n(A4)\n.\nThe term (A1) can be further decomposed in\nE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\ni=1\ni\u22121\nX\nj=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj)\nZ ti+1\nti\n\u03c32(u) du\nZ tj+1\ntj\ndp(t)\ni\n(A\u2217\n1)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nn\u22121\nX\nj=1\nj\u22121\nX\ni=0\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj)\nZ ti+1\nti\n\u03c32(u) du\nZ tj+1\ntj\ndp(t)\ni\n.\nBy applying the tower property with respect to the sigma-algebra Fi+1, the second summand is\nzero because of the martingale property of the It\u02c6o integrals. Thus, the term (A1) is just equal\nto the term (A\u2217\n1). We call |(A\u2217\n1)| = \u0393(n, M, N).\n\u0393(n, M, N) =\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\nei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u))\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) dp(u) \u03c32(t) dt\ni\f\f\f\n=\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\nei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u))\n1\n2M + 1\nX\n|s|\u2264M\n(e\u2212i 2\u03c0\nT s(\u03c6n(t)\u2212\u03c6n(u)) \u2212e\u2212i 2\u03c0\nT s(t\u2212u)) dp(u) \u03c32(t) dt\ni\f\f\f\n+\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\n(e\u2212i 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u)) \u2212e\u2212i 2\u03c0\nT l(t\u2212u))\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(t\u2212u) dp(u) \u03c32(t) dt\ni\f\f\f\n+\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l(t\u2212u)DM(t \u2212u) dp(u) \u03c32(t) dt\ni\f\f\f.\nThe third summand is less than or equal to\nE\nh\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n\f\f\f\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT (l)(t\u2212u)DM(t \u2212u) dp(u) \u03c32(t) dt\n\f\f\f\ni\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 T E\nh Z T\n0\nD2\nM(u)du\ni 1\n2\n32",
    "chunk_index": 27,
    "start_char": 61456,
    "end_char": 63325,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "+ 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT l(t\u2212u)DM(t \u2212u) dp(u) \u03c32(t) dt\ni\f\f\f.\nThe third summand is less than or equal to\nE\nh\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n\f\f\f\nZ T\n0\nZ t\n0\ne\u2212i 2\u03c0\nT (l)(t\u2212u)DM(t \u2212u) dp(u) \u03c32(t) dt\n\f\f\f\ni\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 T E\nh Z T\n0\nD2\nM(u)du\ni 1\n2\n32\n\n\u22642\u03c0NE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 \u0010\nT\n2M + 1\n\u0011 1\n2\nby using the Cauchy Schwartz and H\u00a8older inequality, the It\u02c6o isometry and the properties\nof the rescaled Dirichlet kernel.\nBy means of the Taylor\u2019s formula, we obtain estimations for the \ufb01rst and second summand\nof \u0393(n, M, N) as follows\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\nei 2\u03c0\nT (l)(\u03c6n(t)\u2212\u03c6n(u))\n1\n2M + 1\nX\n|s|\u2264M\n(e\u2212i 2\u03c0\nT (s)(\u03c6n(t)\u2212\u03c6n(u)) \u2212e\u2212i 2\u03c0\nT (s)(t\u2212u)) dp(u) \u03c32(t) dt\ni\f\f\f\n\u2264E\nh\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\nei 2\u03c0\nT (l)(\u03c6n(t)\u2212\u03c6n(u))\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT (s)(t\u2212u)(s2\u03c0\nT (t \u2212u \u2212\u03c6n(t) + \u03c6n(u)) + o(1)) dp(u) \u03c32(t) dt\n\f\f\f\ni\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni\nZ T\n0\nE\nh\f\f\f\nZ t\n0\nei 2\u03c0\nT (l)(\u03c6n(t)\u2212\u03c6n(u))e\u2212i 2\u03c0\nT (s)(t\u2212u)(s2\u03c0\nT (t \u2212u \u2212\u03c6n(t) + \u03c6n(u)) + o(1)) dp(u)\n\f\f\f\ni\ndt\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni\nZ T\n0\nE\nh Z t\n0\n(s2 4\u03c02\nT 2 (t \u2212u \u2212\u03c6n(t) + \u03c6n(u))2 + o(1)) \u03c32(u)du\ni 1\n2 dt\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 Z T\n0\n\u0010 Z t\n0\ns244\u03c02\nn2 + o(1) du\n\u0011 1\n2 dt\n\u2264MN\nn\n8\u03c02T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 + o(1),\nand\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ t\n0\n(e\u2212i 2\u03c0\nT (l)(\u03c6n(t)\u2212\u03c6n(u)) \u2212e\u2212i 2\u03c0\nT (l)(t\u2212u))\n1\n2M + 1\nX\n|s|\u2264M\ne\u2212i 2\u03c0\nT s(t\u2212u) dp(u) \u03c32(t) dt\ni\f\f\f\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2\n33\n\n\u00d7\nZ T\n0\n\u0010 Z t\n0\n(l2 4\u03c02\nT 2 (\u03c6n(t) \u2212\u03c6n(u) \u2212t + u)2 + o(1)) du\n\u0011 1\n2 dt\n\u2264N2\nn 8\u03c02T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 + o(1).",
    "chunk_index": 28,
    "start_char": 63031,
    "end_char": 64746,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "|s|\u2264M\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2\n33\n\n\u00d7\nZ T\n0\n\u0010 Z t\n0\n(l2 4\u03c02\nT 2 (\u03c6n(t) \u2212\u03c6n(u) \u2212t + u)2 + o(1)) du\n\u0011 1\n2 dt\n\u2264N2\nn 8\u03c02T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 + o(1).\nLet us now further decompose the term (A4) in\n(A4) = E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\ne\u2212il 2\u03c0\nT \u03c6n(u)\u03c32(u) du\n\u0010 Z T\n0\neil 2\u03c0\nT \u03c6n(t) \u2212eil 2\u03c0\nT tdp(t)\n\u0011i\n(A4.1)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\neil 2\u03c0\nT \u03c6n(t) dp(t)\n\u0010 Z T\n0\ne\u2212il 2\u03c0\nT \u03c6n(u) \u2212e\u2212il 2\u03c0\nT u\u03c32(u) du\n\u0011i\n(A4.2)\n+E\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\ne\u2212il 2\u03c0\nT u \u03c32(u) du\nZ T\n0\neil 2\u03c0\nT t dp(t) \u2212\nZ T\n0\n\u03b7(t)dt\ni\n(A4.3)\n.\nWe call \u039b(n, N) = |(A2)| + |(A3)| + |(A4.1)| + |(A4.2)|.\nLet us \ufb01rst discuss the terms (A2) and (A3). For i \u0338= j, the terms\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\n1\n2M + 1\nX\n|s|\u2264M\nX\ni\u0338=j\nei 2\u03c0\nT (l\u2212s)(ti\u2212tj) E\nh Z ti+1\nti\nZ t\nti\ndp(u)dp(t)\nZ tj+1\ntj\ndp(t)\ni\nand\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nX\ni\u0338=j\nei 2\u03c0\nT l(ti\u2212tj) E\nh Z tj+1\ntj\nZ t\ntj\ndp(u)dp(t)\nZ ti+1\nti\ndp(t)\ni\nare zero because the It\u02c6o integrals appearing in the expectations are de\ufb01ned on non overlapping\nintervals. For i = j, let us evaluate the terms |(A2)| and |(A3)|. In this instance, (A2) and (A3)\nare both equal and\n\f\f\fE\nh\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nZ ti+1\nti\nZ t\nti\ndp(u)dp(t)\nZ ti+1\nti\ndp(t)\ni\f\f\f\n\u2264\n\f\f\f\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nn\u22121\nX\ni=0\nE\nh Z ti+1\nti\nZ t\nti\ndp(u)\u03c32(t)dt\ni\f\f\f\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT\nn\u22121\nX\ni=0\nE\nh\f\f\f\nZ ti+1\nti\nZ t\nti\ndp(u)\u03c32(t)dt\n\f\f\f\ni\n\u2264N 2\u03c0\nT\nn\u22121\nX\ni=0\nE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 (ti+1 \u2212ti)\n3\n2\n34\n\n\u2264N 2\u03c0\nT\n\u0010T\nn\n\u0011 3\n2 nE\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 = N\n\u221an 2\u03c0T\n1\n2 E\nh\nsup\n[0,T]\n\u03c32(t)\ni 3\n2 ,\nby using the It\u02c6o isometry and the H\u00a8older inequality. Moreover, because of the Cauchy-Schwartz\ninequality, |(A4.1)| is less than or equal to\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh\f\f\f\nZ T\n0\ne\u2212il 2\u03c0\nT \u03c6n(u)\u03c32(u) du\n\f\f\f\n2i 1\n2 E\nh\f\f\f\nZ T\n0\neil 2\u03c0\nT \u03c6n(t) \u2212eil 2\u03c0\nT \u03c6n(t)dp(t)\n\f\f\f\n2i 1\n2\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh Z T\n0\n\u03c34(u) du\ni 1\n2 T\n1\n2 E\nh Z T\n0\n(l2\u03c0\nT\nT\nn + o(1))2\u03c32(t) dt",
    "chunk_index": 29,
    "start_char": 64591,
    "end_char": 66454,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "Cauchy-Schwartz\ninequality, |(A4.1)| is less than or equal to\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh\f\f\f\nZ T\n0\ne\u2212il 2\u03c0\nT \u03c6n(u)\u03c32(u) du\n\f\f\f\n2i 1\n2 E\nh\f\f\f\nZ T\n0\neil 2\u03c0\nT \u03c6n(t) \u2212eil 2\u03c0\nT \u03c6n(t)dp(t)\n\f\f\f\n2i 1\n2\n\u2264\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E\nh Z T\n0\n\u03c34(u) du\ni 1\n2 T\n1\n2 E\nh Z T\n0\n(l2\u03c0\nT\nT\nn + o(1))2\u03c32(t) dt\ni 1\n2\n1\n2N + 1\nX\n|l|\u2264N\n|l|2\u03c0\nT E[ sup\nt\u2208[0,T]\n\u03c34(t)]\n1\n2 E[ sup\nt\u2208[0,T]\n\u03c32(t)]\n1\n2 T N\nn 2\u03c0 + o(1)\n\u2264N2\nn 4\u03c02 E[ sup\nt\u2208[0,T]\n\u03c34(t)]\n1\n2 E[ sup\nt\u2208[0,T]\n\u03c32(t)]\n1\n2 + o(1),\nby applying Taylor\u2019s Formula and the Cauchy-Schwartz inequality. Analogously, it can be shown\nthat |(A4.2)| is less than or equal to\nN2\nn 4\u03c02T\n1\n2 E[ sup\nt\u2208[0,T]\n\u03c34(t)]\n1\n2 E[ sup\nt\u2208[0,T]\n\u03c32(t)]\n1\n2 + o(1).\nIt remains to evaluate the term |(A4.3)| that we call \u03a8(N). By formula (8), |(A4.3)| can be\nexpressed as\n\f\f\fE\nh Z T\n0\nZ t\n0\nDN(t \u2212u)dp(u)d\u03c32(t) +\nZ T\n0\nZ t\n0\nDN(t \u2212u) d\u03c32(u)dp(t)\n\u2212\nZ T\n0\nZ t\n0\nDN(u)dp(u)d\u03c32(t) \u2212\nZ T\n0\nZ t\n0\nDN(t)d\u03c32(u)dp(t) \u2212\nZ T\n0\nDN(u)\u03b7(u)du\ni\f\f\f.\nThe It\u02c6o integrals have zero mean and the terms above are simply equal to\n\f\f\fE\nh\n\u2212\nZ T\n0\nDN(u)\u03b7(u)du\ni\f\f\f \u2264E\nh Z T\n0\nD2\nN(u)du\nZ T\n0\n\u03b7(u)2du\ni 1\n2\nafter applying the Cauchy-Schwartz inequality. Thus,\n\u03a8(N) \u2264\nT\n\u221a\n2N + 1 E[ sup\nt\u2208[0,T]\n\u03b7(t)2]\n1\n2 .\nThe terms \u0393(n, M, N), \u039b(n, N) and \u2126(N) converge to zero under Assumption (25) which\nconcludes the proof.\n35\n\nProof of Theorem 3.2. We have that\nE[(e\u03b7n,M,N \u2212\u03b7)2] = E\nh\u0010\nX\ni,j,k:i\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)e\u03b4ie\u03b4je\u03b4k\n(43)\n+\nX\ni,j:i\u0338=j\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj)e\u03b42\ni e\u03b4j +\nX\ni,j\nD\u2032\nN(ti \u2212tj)e\u03b4ie\u03b42\nj \u2212\u03b7\n\u00112i\nwhich is in turn equal to\nE\nh\u0010\n(\u03b7n,M,N \u2212\u03b7)\n+\n\u0010\nX\ni,j,k:i\u0338=j\u0338=k\nDM(ti \u2212tj)D\u2032\nN(tk \u2212tj)(\u03b4i\u03b4j\u03f5k + \u03b4j\u03b4k\u03f5i + \u03b4k\u03b4i\u03f5j\n+ \u03b4i\u03f5j\u03f5k + \u03b4j\u03f5i\u03f5k + \u03b4k\u03f5i\u03f5j + \u03f5i\u03f5j\u03f5k)\n\u0011\n+\n\u0010 X\ni,j:i\u0338=j\nDM(ti \u2212tj)D\u2032\nN(ti \u2212tj) (\u03b4i\u03f5j + \u03f52\ni \u03b4j + \u03f52\ni \u03f5j + 2\u03b4i\u03b4j\u03f5i + 2\u03b4i\u03f5j\u03f5i)\n\u0011\n+\n\u0010 X\ni,j\nD\u2032\nN(ti \u2212tj) (\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)\n\u0011\u00112i\n.\nThe term E[(\u03b7n,M,N \u2212\u03b7)2] corresponds to the mean squared error of the estimator (15) in\nthe absence of microstructure noise and converges to zero as n, M, N tend to in\ufb01nity. The error\ndecomposition (35) and the proof of Theorem 2.3 highlight that the term \u03b71\nn,N is in L2-norm\nbigger than (\u03b72\nn,M,N \u2212\u03b7). Then, to prove our claim, it is enough to analyse the convergence to\nzero of\nE[(\u03b71\nn,N \u2212\u03b7)2].",
    "chunk_index": 30,
    "start_char": 66164,
    "end_char": 68355,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "the mean squared error of the estimator (15) in\nthe absence of microstructure noise and converges to zero as n, M, N tend to in\ufb01nity. The error\ndecomposition (35) and the proof of Theorem 2.3 highlight that the term \u03b71\nn,N is in L2-norm\nbigger than (\u03b72\nn,M,N \u2212\u03b7). Then, to prove our claim, it is enough to analyse the convergence to\nzero of\nE[(\u03b71\nn,N \u2212\u03b7)2].\n(44)\nFollowing Remark 6.2, we have that (44) is equal to\nE\nh\u0010\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ T\n0\n(ei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u)) \u2212ei 2\u03c0\nT l(t\u2212u))\u03c32(u) du \u03c3(t) dW(t)\n+\nZ T\n0\nZ t\n0\nDN(t \u2212u) dp(u) d\u03c32(t) +\nZ T\n0\nZ t\n0\nDN(t \u2212u) d\u03c32(t) dp(u)\n\u2212\nZ T\n0\nZ t\n0\nDN(u)dp(u)d\u03c32(t) \u2212\nZ T\n0\nZ t\n0\nDN(u)d\u03c32(t)dp(u) \u2212\nZ T\n0\nDN(u) \u03b7(u)du\n\u00112i\n36\n\n\u22642E\nh\u0010\n1\n2N + 1\nX\n|l|\u2264N\nil2\u03c0\nT\nZ T\n0\nZ T\n0\n(ei 2\u03c0\nT l(\u03c6n(t)\u2212\u03c6n(u)) \u2212ei 2\u03c0\nT l(t\u2212u))\u03c32(u) du \u03c3(t) dW(t)\n\u00112i\n(45)\n+ 8E\nh\u0010 Z T\n0\nZ t\n0\nDN(t \u2212u) dp(u) d\u03c32(t)\n\u00112i\n+ 8E\nh\u0010 Z T\n0\nZ t\n0\nDN(t \u2212u) dp(u) d\u03c32(t)\n\u00112i\n(46)\n+ 8E\nh\u0010 Z T\n0\nDN(u) \u03b7(u)du\n\u00112i\n+ 8E\nh\u0010 Z T\n0\nDN(u) \u03b7(u)du\n\u00112i\n(47)\n\u2264128\u03c02\nT 2\nN4\nn2 E[ sup\nt\u2208[0,T]\n\u03c32(t)]E[ sup\nt\u2208[0,T]\n\u03c34(t)]\n(48)\n+ 16\nT 2\n2N + 1E[ sup\nt\u2208[0,T]\n\u03c32(t)]E[ sup\nt\u2208[0,T]\n\u03b32(t)]\n(49)\n+ 16\nT\n2N + 1E[ sup\nt\u2208[0,T]\n\u03b7(t)2],\n(50)\nwhere (48), (49), (50) correspond to the estimation of the summands (45), (46), (47), respectively.\nThus, (44) converges to zero as n, N \u2192\u221eand so does the mean squared error of the estimator\n(15). However, whenever a noise component appears in the decomposition (43), the related terms\ndiverge to in\ufb01nity as n, M, N goes to in\ufb01nity. As exemplary calculation, we will show that\nE[\n\u0010 X\ni,j\nD\u2032\nN(ti \u2212tj) (\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)\n\u0011\u00112\n]\n(51)\ndiverges as n, N \u2192\u221eand is greater than O(n2N). In order to handle the other terms in (43),\nthe strategies of computation addressed below have to be used. Ultimately, this leads to show\nthat the remaining terms in (43) are greater than O(NM2 + n2N\nM ).\nWe have that\nE\nh\u0010 X\ni,j\nD\u2032\nN(ti \u2212tj) \u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j\n\u00112i\n=\nX\ni,j\n(D\u2032\nN(ti \u2212tj))2E[(\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)2]\n(52)\n+\nX\ni,j,i\u2032,j\u2032:i\u0338=i\u2032,j\u0338=j\u2032\nD\u2032\nN(ti \u2212tj)D\u2032\nN(ti\u2032 \u2212tj\u2032)E[(\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)\n(\u03b4j\u2032\u03f5i\u2032 + \u03f52\nj\u2032\u03b4i\u2032 + \u03f52\nj\u2032\u03f5i\u2032 + 2\u03b4j\u2032\u03b4i\u2032\u03f5j\u2032 + 2\u03b4j\u2032\u03f5i\u2032\u03f5j\u2032)].\n(53)\nUnder Assumption (H2), we have that (51) is equal to\n37",
    "chunk_index": 31,
    "start_char": 67998,
    "end_char": 70210,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "X\ni,j\n(D\u2032\nN(ti \u2212tj))2E[(\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)2]\n(52)\n+\nX\ni,j,i\u2032,j\u2032:i\u0338=i\u2032,j\u0338=j\u2032\nD\u2032\nN(ti \u2212tj)D\u2032\nN(ti\u2032 \u2212tj\u2032)E[(\u03b4j\u03f5i + \u03f52\nj\u03b4i + \u03f52\nj\u03f5i + 2\u03b4j\u03b4i\u03f5j + 2\u03b4j\u03f5i\u03f5j)\n(\u03b4j\u2032\u03f5i\u2032 + \u03f52\nj\u2032\u03b4i\u2032 + \u03f52\nj\u2032\u03f5i\u2032 + 2\u03b4j\u2032\u03b4i\u2032\u03f5j\u2032 + 2\u03b4j\u2032\u03f5i\u2032\u03f5j\u2032)].\n(53)\nUnder Assumption (H2), we have that (51) is equal to\n37\n\nX\ni,j\n(D\u2032\nN(ti \u2212tj))2(E[\u03b42\nj ]E[\u03f52\ni ] + E[\u03f54\nj]E[\u03b42\ni ] + E[\u03f54\nj\u03f52\ni ] + 4E[\u03b42\nj \u03b42\ni ]E[\u03f52\nj] + 4E[\u03b42\nj ]E[\u03f52\ni \u03f52\nj])\n(54)\n+\nX\ni,j,i\u2032,j\u2032:i\u0338=i\u2032,j\u0338=j\u2032\nD\u2032\nN(ti \u2212tj)D\u2032\nN(ti\u2032 \u2212tj\u2032)E[\u03f52\nj\u03f5i\u03f52\nj\u2032\u03f5i\u2032].\n(55)\nIt holds that\nE[\u03f52\ni ] = 2E[\u03b62]\nE[\u03f54\ni ] = 2E[\u03b64] + 6E[\u03b62]2\nE[\u03f54\nj\u03f52\ni ]\n=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n4E[\u03b64]E[\u03b62] + 12E[\u03b62]3\nif |i \u2212j| \u0338= 1,\n9E[\u03b64]E[\u03b62] + E[\u03b66] + 6E[\u03b62]3 \u22124E[\u03b63]2\nif i = j \u22121,\n13E[\u03b64]E[\u03b62] + E[\u03b66] + 2E[\u03b62]3 \u22124E[\u03b63]2\nif i = j + 1.\nE[\u03f52\nj\u03f52\ni ]\n=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n4E[\u03b62]2\nif |i \u2212j| > 1,\n3E[\u03b62]2 + E[\u03b64]\nif i = j \u22121,\n3E[\u03b62]2 + E[\u03b64]\nif i = j + 1.\nE[\u03f53\ni ] = 0\nE[\u03f52\nj\u03f5i\u03f52\nj\u2032\u03f5i\u2032]\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif i \u0338= i\u2032, j \u0338= j\u2032, i \u0338= j\u2032, j \u0338= i\u2032,\n0\nif i \u0338= i\u2032, j \u0338= j\u2032, i = j\u2032, j = i\u2032 and |i \u2212j| \u0338= 1,\na\nif i \u0338= i\u2032, j \u0338= j\u2032, i = j\u2032, j = i\u2032 and i = j + 1,\nb\nif i \u0338= i\u2032, j \u0338= j\u2032, i = j\u2032, j = i\u2032 and i = j \u22121,\nwhere a = E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3, and b = E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3.\n38\n\nTherefore (51) is equal to\nX\ni,j\n(D\u2032\nN(ti \u2212tj))2(2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2 + 8E[\u03b42\ni \u03b42\nj ]E[\u03b62])\n(56)\n+\nX\ni,j:|i\u2212j|\u0338=1\n(D\u2032\nN(ti \u2212tj))216E[\u03b42\nj ]E[\u03b62]2 +\nX\ni,j:|i\u2212j|=1\n(D\u2032\nN(ti \u2212tj))2E[\u03b42\nj ]\n\u00d7 (12E[\u03b62]2 + 4E[\u03b64])\n(57)\n+\nX\ni,j:|i\u2212j|\u0338=1\n(D\u2032\nN(ti \u2212tj))2(4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)\n(58)\n+\nX\ni,j:i=j\u22121\n(D\u2032\nN(tj\u22121 \u2212tj))2(9E[\u03b64]E[\u03b62] + E[\u03b66] + 6E[\u03b62]3 \u22124E[\u03b63]2)\n(59)\n+\nX\ni,j:i=j+1\n(D\u2032\nN(tj+1 \u2212tj))2(13E[\u03b64]E[\u03b62] + E[\u03b66] + 2E[\u03b62]3 \u22124E[\u03b63]2)\n(60)\n+\nX\ni,j:i=j\u22121\nD\u2032\nN(tj\u22121 \u2212tj)D\u2032\nN(tj \u2212tj\u22121)(E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3)\n(61)\n+\nX\ni,j:i=j+1\nD\u2032\nN(tj+1 \u2212tj)D\u2032\nN(tj \u2212tj+1)(E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3)\n(62)\nComputing the summands from (59) to (62), we obtain\n(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nn (l\u2212l\u2032)\u0011\n\u00d7 (9E[\u03b64]E[\u03b62] + E[\u03b66] + 6E[\u03b62]3 \u22124E[\u03b63]2)\n(63)\n+(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 ei 2\u03c0\nn (l\u2212l\u2032)\u0011\n\u00d7 (13E[\u03b64]E[\u03b62] + E[\u03b66] + 2E[\u03b62]3 \u22124E[\u03b63]2)\n(64)\n+(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 e\u2212i 4\u03c0\nn",
    "chunk_index": 32,
    "start_char": 69917,
    "end_char": 72113,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nn (l\u2212l\u2032)\u0011\n\u00d7 (9E[\u03b64]E[\u03b62] + E[\u03b66] + 6E[\u03b62]3 \u22124E[\u03b63]2)\n(63)\n+(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 ei 2\u03c0\nn (l\u2212l\u2032)\u0011\n\u00d7 (13E[\u03b64]E[\u03b62] + E[\u03b66] + 2E[\u03b62]3 \u22124E[\u03b63]2)\n(64)\n+(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 e\u2212i 4\u03c0\nn l +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nn (l\u2032+l)\u0011\n\u00d7 (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3)\n(65)\n+(n \u22121)\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 e+i 4\u03c0\nn l +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 ei 2\u03c0\nn (l\u2032+l)\u0011\n\u00d7 (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3)\n(66)\nThe terms where the indices l and l\u2032 appear in (63) and (65) has to be summed up following\nthe rule highlighted in Table 7. The same applies for the terms where the indices l and l\u2032 appear\nin (64) and (66).\n39\n\n.\nIndices belonging to (63)\nIndices belonging to (65)\nl > 0, l\u2032 > 0\nl > 0, l\u2032 < 0\nl < 0, l\u2032 < 0\nl < 0, l\u2032 > 0\nl > 0, l\u2032 < 0\nl > 0, l\u2032 > 0\nl < 0, l\u2032 > 0\nl < 0, l\u2032 < 0\nTable 7: Summing rule: the terms has to be \ufb01rst summed according to the indices present in\neach row, then the resulting addends has to be summed with respect to the indices grouped by\ncolor.\nThe summands from (59) to (62) are then equal to\n(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (22E[\u03b64]E[\u03b62] + 2E[\u03b66] + 8E[\u03b62]3 \u22128E[\u03b63]2)\n(67)\n+ (n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 2 cos(4\u03c0\nn l)(E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3)\n(68)\n+ (n \u22121)\n1\n(2N + 1)2\nX\nl,l\u2032>0\nll\u2032 4\u03c02\nT 2 sin(2\u03c0\nn l) sin(2\u03c0\nn l\u2032)(34E[\u03b64]E[\u03b62] + 4E[\u03b66]\n+ 26E[\u03b62]3 \u22125E[\u03b63]2).\n(69)\nIf N = n\n1\n\u03b2 with \u03b2 >\nlog(n)\nlog(n)\u2212log(8) then 0 \u2264cos( 4\u03c0\nn l) \u22641 for all |l| \u2264N. If \u03b2 >\nlog(n)\nlog(n)\u2212log(2) then\n0 \u2264sin(2\u03c0\nn l) \u22641. We have that the choice of \u03b2 >\nlog(n)\nlog(n)\u2212log(8) is implied by the ratio in (27). In\nconclusion, the term (69) is greater than or equal to zero and the sum between (67) and (68) is\ngreater than or equal to\n(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (22E[\u03b64]E[\u03b62] + 2E[\u03b66] + 8E[\u03b62]3 \u22128E[\u03b63]2),\n(70)\nif (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3) > 0 and greater than or equal to\n(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (10E[\u03b64]E[\u03b62] \u221210E[\u03b62]3 \u22126E[\u03b63]2),\n(71)\nif (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3) < 0.\nLet us now analyse the summands from (56) to (58). They are equal to\n40",
    "chunk_index": 33,
    "start_char": 71808,
    "end_char": 73974,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "greater than or equal to\n(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (22E[\u03b64]E[\u03b62] + 2E[\u03b66] + 8E[\u03b62]3 \u22128E[\u03b63]2),\n(70)\nif (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3) > 0 and greater than or equal to\n(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (10E[\u03b64]E[\u03b62] \u221210E[\u03b62]3 \u22126E[\u03b63]2),\n(71)\nif (E[\u03b63]2 \u2212E[\u03b66] \u22126E[\u03b64]E[\u03b62] \u22129E[\u03b62]3) < 0.\nLet us now analyse the summands from (56) to (58). They are equal to\n40\n\n.\nl, l\u2032\ns\nll\u2032(cos(2\u03c0/n(ls \u2212l\u2032s)) + i sin(2\u03c0/n(ls \u2212l\u2032s))\n\u2212s\nll\u2032(cos(2\u03c0/n(\u2212ls + l\u2032s)) + i sin(2\u03c0/n(\u2212ls + l\u2032s))\nl, \u2212l\u2032\ns\n\u2212ll\u2032(cos(2\u03c0/n(ls + l\u2032s)) + i sin(2\u03c0/n(ls + l\u2032s))\n\u2212s\n\u2212ll\u2032(cos(2\u03c0/n(\u2212ls \u2212l\u2032s)) + i sin(2\u03c0/n(\u2212ls \u2212l\u2032s))\n\u2212l, \u2212l\u2032\ns\nll\u2032(cos(2\u03c0/n(\u2212ls + l\u2032s)) + i sin(2\u03c0/n(\u2212ls + l\u2032s))\n\u2212s\nll\u2032(cos(2\u03c0/n(ls \u2212l\u2032s)) + i sin(2\u03c0/n(ls \u2212l\u2032s))\n\u2212l, l\u2032\ns\n\u2212ll\u2032(cos(2\u03c0/n(\u2212ls \u2212l\u2032s)) + i sin(2\u03c0/n(\u2212ls \u2212l\u2032s))\n\u2212s\n\u2212ll\u2032(cos(2\u03c0/n(ls + l\u2032s)) + i sin(2\u03c0/n(ls + l\u2032s))\nTable 8: Coe\ufb03cients appearing in the summands from (72) to (75) with respect to the indices\nl, l\u2032, s. Note that ti \u2212tj = 2\u03c0\nn s and all the indices are considered positive constants.\n.\nl, l\u2032\ns\nll\u2032(2 cos(2\u03c0/n(ls \u2212l\u2032s)))\n\u2212s\nll\u2032(2 cos(2\u03c0/n(\u2212ls + l\u2032s)))\nl, \u2212l\u2032\ns\n\u2212ll\u2032(2 cos(2\u03c0/n(ls + l\u2032s)))\n\u2212s\n\u2212ll\u2032(2 cos(2\u03c0/n(\u2212ls \u2212l\u2032s)))\nTable 9: Coe\ufb03cients appearing in the summands from (72) to (75) with respect to the indices\nl, l\u2032, s. Note that ti \u2212tj = 2\u03c0\nn s and all the indices are considered positive constants.\nX\ni,j\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nT (l\u2212l\u2032)(ti\u2212tj)\u0011\n\u00d7 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2 + 8E[\u03b42\ni \u03b42\nj ]E[\u03b62])\n(72)\n+\nX\ni,j:|i\u2212j|\u0338=1\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nT (l\u2212l\u2032)(ti\u2212tj)\u0011\n\u00d7 16E[\u03b42\nj ]E[\u03b62]2\n(73)\n+\nX\ni,j:|i\u2212j|=1\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nT (l\u2212l\u2032)(ti\u2212tj)\u0011\n\u00d7 E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(74)\n+\nX\ni,j:|i\u2212j|\u0338=1\n\u0010\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 +\n1\n(2N + 1)2\nX\nl\u0338=l\u2032\nll\u2032 4\u03c02\nT 2 e\u2212i 2\u03c0\nT (l\u2212l\u2032)(ti\u2212tj)\u0011\n\u00d7 (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3).\n(75)\nWe \ufb01rst take care of the sum with respect to the indices i, j, l and l\u2032 appearing in the\nterms (72) to (75). To simply explain the calculations below, let us consider from now on that\nthe indices l and l\u2032 are positive and that there exists an s = 1, . . . , n \u22121 such that if ti > tj,\nti \u2212tj = s2\u03c0\nn . We do not consider s = 0 in the calculations below because D\u2032\nN(ti \u2212ti) = 0. In\n41",
    "chunk_index": 34,
    "start_char": 73593,
    "end_char": 75892,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "calculations below, let us consider from now on that\nthe indices l and l\u2032 are positive and that there exists an s = 1, . . . , n \u22121 such that if ti > tj,\nti \u2212tj = s2\u03c0\nn . We do not consider s = 0 in the calculations below because D\u2032\nN(ti \u2212ti) = 0. In\n41\n\nTable 8, we \ufb01nd, for \ufb01xed values of s, l, l\u2032, all the possible combination of the indices and the\nexpression of the terms ll\u2032e\u2212i 2\u03c0\nT (l\u2212l\u2032)(ti\u2212tj) appearing in the summands. The blue and the red\nelements appear in (72), (73), (74) and(75), respectively, the same number of times. We \ufb01rst\nsum each row of Table 8. We then obtain, Table 9. Summing up the blue and red terms obtained\nfor s and \u2212s, respectively, we have\nX\ni,j\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2\n+ 8E[\u03b42\ni \u03b42\nj ]E[\u03b62])\n(76)\nn\u22121\nX\ni=1\ni\u22121\nX\nj=0\n1\n(2N + 1)2\nX\nl,l\u2032>0\nll\u2032 4\u03c02\nT 2 4 sin(2\u03c0\nn ls) sin(2\u03c0\nn l\u2032s)\n\u00d7 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2 + 8E[\u03b42\ni \u03b42\nj ]E[\u03b62])\n(77)\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 16E[\u03b42\nj ]E[\u03b62]2\n(78)\n+\nn\u22121\nX\ni=2\ni\u22122\nX\nj=0\n1\n(2N + 1)2\nX\nl,l\u2032>0\nll\u2032 4\u03c02\nT 2 4 sin(2\u03c0\nn ls) sin(2\u03c0\nn l\u2032s)16E[\u03b42\nj ]E[\u03b62]2\n(79)\n+\nX\ni,j:|i\u2212j|=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(80)\n+ (n \u22121)\n1\n(2N + 1)2\nX\nl,l\u2032>0\nll\u2032 4\u03c02\nT 2 4 sin(2\u03c0\nn l) sin(2\u03c0\nn l\u2032) E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(81)\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)\n(82)\n+\nn\u22121\nX\ni=2\ni\u22122\nX\nj=0\n1\n(2N + 1)2\nX\nl,l\u2032>0\nll\u2032 4\u03c02\nT 2 4 sin(2\u03c0\nn ls) sin(2\u03c0\nn l\u2032s) (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)\n(83)\nIf N = n\n1\n\u03b2 such that \u03b2 >\nlog(n)\nlog(n)\u2212log(2(n\u22121)) then 0 \u2264sin(2\u03c0\nn ls) \u22641 for s = 1, . . . , n \u22121 and\nl > 0. The latter is straightforwardly implied by (27), being\nlog(n)\nlog(n)\u2212log(2(n\u22121)) negative. Therefore,\nthe summands (77), (79), (81) and (83) are greater than or equal to zero.\n42\n\nIn conclusion, (51) is possibly greater than or equal to two sums. The \ufb01rst one is\nX\ni,j\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2\n+ 8E[\u03b42\ni \u03b42\nj ]E[\u03b62]) +\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 16E[\u03b42\nj ]E[\u03b62]2\n+\nX\ni,j:|i\u2212j|=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(84)\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)",
    "chunk_index": 35,
    "start_char": 75639,
    "end_char": 77843,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "X\ni,j\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2\n+ 8E[\u03b42\ni \u03b42\nj ]E[\u03b62]) +\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 16E[\u03b42\nj ]E[\u03b62]2\n+\nX\ni,j:|i\u2212j|=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(84)\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)\n+(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (22E[\u03b64]E[\u03b62] + 2E[\u03b66] + 8E[\u03b62]3 \u22128E[\u03b63]2),\nand the second one is\nX\ni,j\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (2E[\u03b42\nj ]E[\u03b62] + 2E[\u03b42\ni ]E[\u03b64] + 6E[\u03b42\ni ]E[\u03b62]2 + 8E[\u03b42\ni \u03b42\nj ]E[\u03b62])\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 16E[\u03b42\nj ]E[\u03b62]2\n+\nX\ni,j:|i\u2212j|=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 E[\u03b42\nj ](12E[\u03b62]2 + 4E[\u03b64])\n(85)\n+\nX\ni,j:|i\u2212j|\u0338=1\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (4E[\u03b64]E[\u03b62] + 12E[\u03b62]3)\n+(n \u22121)\n1\n(2N + 1)2\nX\n|l|<N\nl2 4\u03c02\nT 2 (10E[\u03b64]E[\u03b62] \u221210E[\u03b62]3 \u22126E[\u03b63]2).\nNote that because of Assumption (H2), the terms E[\u03b42\ni ] and E[\u03b42\ni \u03b42\nj ] are positive and \ufb01nite\nconstants. The behaviour of the sums (84) and (85) is ruled by their \ufb01rst summands. In fact,\nX\ni,j\n1\n(2N + 1)2\nX\n|l|\u2264N\nl2 4\u03c02\nT 2 =\nn2\n(2N + 1)2\n4\u03c02\nT 2\n\u0010N3\n3 + N2\n2 + N\n6\n\u0011\n,\nwhich diverges as n, N \u2192\u221e.\nReferences\n[A\u00a8\u0131t-Sahalia et al., 2017] A\u00a8\u0131t-Sahalia, Y., Fan, J., Laeven, R., Wang, D., Yang, X., 2017. Esti-\n43\n\nmation of continuous and discontinuous leverage e\ufb00ects, Journal of the American Statistical\nAssociation 112, 1744\u20131758.\n[A\u00a8\u0131t-Sahalia et al., 2013] A\u00a8\u0131t-Sahalia, Y., Fan, J., Li, Y., 2013. The leverage e\ufb00ect puzzle: Dis-\nentangling sources of bias at high-frequency, Journal of Financial Economics 109, 224\u2013249.\n[A\u00a8\u0131t-Sahalia and Jacod, 2014] A\u00a8\u0131t-Sahalia, Y., Jacod J., 2014. High-Frequency Financial Econo-\nmetrics, Princeton University Press.\n[Bandi and Ren\u00b4o, 2012] Bandi, F.M., Ren\u00b4o, R., 2012. Time-varying leverage e\ufb00ects, Journal of\nEconometrics 169 (1), 94\u2013113.\n[Bandi and Russell, 2008] Bandi, F.M, Russell, J.R., 2008. Microstructure noise, realized vari-\nance, and optimal sampling, The Review of Economic Studies 75 (2), 339\u2013369.\n[Barucci and Mancino, 2010] Barucci, E., Mancino, M.E., 2010. Computation of volatility in\nstochastic volatility models with high-frequency data, International Journal of Theoretical\nand Applied Finance 13 (5), 767\u2013787.\n[Bekaert and Wu, 2000] Bekaert, G., Wu, G., 2000. Asymmetric volatility and risk in equity\nmarkets, The Review of Financial Studies 13, 1\u201342.\n[Black, 1976] Black, F., 1976. Studies of stock market volatility changes, Proceedings of the\nBusiness and Economic Statistic Section, American Statistical Association, 177\u2013181.\n[Bollerslev et al., 2006] Bollerslev, T., Litvinova, J., Tauchen, G., 2006. Leverage and volatility\nfeedback e\ufb00ects in high-frequency data, Journal of Financial Econometrics 4 (3), 353\u2013384.\n[Campbell, 1987] Campbell, J.Y., 1987.",
    "chunk_index": 36,
    "start_char": 77515,
    "end_char": 80278,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "767\u2013787.\n[Bekaert and Wu, 2000] Bekaert, G., Wu, G., 2000. Asymmetric volatility and risk in equity\nmarkets, The Review of Financial Studies 13, 1\u201342.\n[Black, 1976] Black, F., 1976. Studies of stock market volatility changes, Proceedings of the\nBusiness and Economic Statistic Section, American Statistical Association, 177\u2013181.\n[Bollerslev et al., 2006] Bollerslev, T., Litvinova, J., Tauchen, G., 2006. Leverage and volatility\nfeedback e\ufb00ects in high-frequency data, Journal of Financial Econometrics 4 (3), 353\u2013384.\n[Campbell, 1987] Campbell, J.Y., 1987. Stock returns and the term structure, Journal of Finan-\ncial Economics 18 (2), 373\u2013399.\n[Campbell and Hentschel, 1992] Campbell, J.Y., Hentschel, L., 1992. No news is good news: an\nasymmetric model of changing volatility in stock returns, Journal of Financial Economics 31,\n281\u2013331.\n[Carr and Wu, 2007] Carr, P., Wu, L., 2007. Stochastic skew in currency options, Journal of\nFinancial Econometrics 86 (1), 213\u2013247.\n[Christensen et al., 2014] Christensen, K., Oomen, R.C.A., Podolskij, M., 2014. Fact or fric-\ntion:jumps at ultra high-frequency, Journal of Financial Economics 114, 576\u2013599.\n[Christie, 1982] Christie, A.A., 1982. The stochastic behavior of common stock variances, Jour-\nnal of Financial Econometrics 10, 407\u2013432.\n[Curato, 2019] Curato, I.V., 2019. Estimation of the stochastic leverage e\ufb00ect using the Fourier\ntransform method, Stochastic Processes and their Applications 129, 3207\u20133238.\n44\n\n[Curato and Sanfelici, 2015] Curato, I.V., Sanfelici, S., 2015. Measuring the leverage e\ufb00ect in\na high-frequency trading framework, Handbook of high-frequency Trading, G.N. Gregoriou\nEd., Elsevier, Plattsburgh, NY, USA, 425\u2013446.\n[French et al., 1987] French, K.R., Schwert, G.W., Stambaugh, R.F., 1987. Expected stock re-\nturns and volatility, Journal of Financial Economics 19, 3\u201330.\n[Figlewski and Wang, 2001] Figlewski, S., Wang, X., 2001. Is the leverage e\ufb00ect a leverage ef-\nfect?, Working Paper, Department of Finance, New York University.\n[Ghysel et al., 2005] Ghysel, E., Santa-Clara, P., Valkanov, R., 2005. There is a risk-return trade-\no\ufb00after all, Journal of Financial Econometrics 2, 177\u2013210.\n[Glasserman, 2004] Glasserman, P., 2004. Monte Carlo Methods in Financial Engineering,\nSpringer Science & Business Media.\n[Heston, 1993] Heston, S., 1993. A closed-form solution for options with stochastic volatility\nwith applications to bond and currency options, Review of Financial Studies 6, 327\u2013344.\n[Kalnina and Xiu, 2017] Kalnina, I., Xiu, D., 2017. Nonparametric estimation of the leverage\ne\ufb00ect: a trade-o\ufb00between robustness and e\ufb03ciency. Journal of the American Statistical As-\nsociation 112, 384\u2013396.\n[Katznelson, 2004] Katznelson, Y., 2004. An introduction to harmonic analysis, Cambridge Uni-\nversity Press.\n[Jacod et al., 2009] Jacod, J., Li, Y., Mykland, P.A., Podolskij, M., Vetter, M., 2009. Microstruc-\nture noise in the continuous case: the pre-averaging approach, Stochastic Processes and Their\nApplications 119, 2249\u20132276.\n[Malliavin and Mancino, 2002] Malliavin, P., Mancino, M.E., 2002. Fourier series method for\nmeasurement of multivariate volatilities, Finance and Stochastics 4, 49\u201361.\n[Malliavin and Mancino, 2009] Malliavin, P., Mancino, M.E., 2009. A Fourier transform method\nfor nonparametric estimation of volatility, The Annals of Statistics 37 (4), 1983\u20132010.\n[Mancino et al., 2017] Mancino, M.E., Recchioni, M.C., Sanfelici, S., 2017. Fourier-Malliavin\nvolatility estimation: theory and practice, Springer Briefs in Quantitative Finance.",
    "chunk_index": 37,
    "start_char": 79721,
    "end_char": 83264,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "approach, Stochastic Processes and Their\nApplications 119, 2249\u20132276.\n[Malliavin and Mancino, 2002] Malliavin, P., Mancino, M.E., 2002. Fourier series method for\nmeasurement of multivariate volatilities, Finance and Stochastics 4, 49\u201361.\n[Malliavin and Mancino, 2009] Malliavin, P., Mancino, M.E., 2009. A Fourier transform method\nfor nonparametric estimation of volatility, The Annals of Statistics 37 (4), 1983\u20132010.\n[Mancino et al., 2017] Mancino, M.E., Recchioni, M.C., Sanfelici, S., 2017. Fourier-Malliavin\nvolatility estimation: theory and practice, Springer Briefs in Quantitative Finance.\n[Mancino and Sanfelici, 2008] Mancino, M.E., Sanfelici, S., 2008. Robustness of Fourier estima-\ntor of integrated volatility in the presence of microstructure noise, Computational statistics\nand data analysis 52 (6), 2966\u20132989.\n[Mancino and Toscano, 2020] Mancino, M.E., Toscano, G., 2020. Rate e\ufb03cient asymptotic nor-\nmality for the Fourier estimator of the leverage process, SSRN: 3692631.\n45\n\n[Mykland and Wang, 2012] Mykland, P.A., Wang, D., 2012. The estimation of leverage e\ufb00ect\nwith high-frequency data, Journal of the American Statistical Association 109 (505), 197\u2013215.\n[Mykland et al., 2009] Mykland, P.A., Zhang, L., 2009. Inference for continuous semimartingales\nobserved at high frequency, Econometrica 77, 1403\u20131455.\n[Nelson, 1991] Nelson, D.B., 1991. Conditional heteroskedasticity in asset returns: a new ap-\nproach, Econometrica 59 (2), 347\u2013370.\n[Sanfelici et al., 2015] Sanfelici, S., Curato, I.V., Mancino, M.E., 2015. High-frequency volatility\nof volatility estimation free from spot volatility estimates, Quantitative Finance 15 (8), 1331\u2013\n1345.\n[Tauchen et al., 1996] Tauchen, G., Zhang, H., Liu, M., 1996. Volume, volatility and leverage:\na dynamic analysis, Journal of Econometrics 74 (1), 177\u2013208.\n[Vetter, 2015] Vetter, M., 2015. Estimation of integrated volatility of volatility with applications\nto goodness-of-\ufb01t testing, Bernoulli 21 (4), 2393\u20132418.\n[Veerart and Veerart, 2012] Veraart, A.E.D., Veraart, L.A.M., 2012. Stochastic volatility and\nstochastic leverage, Annals of Finance 8, 205\u2013233.\n[Wang and Mykland, 2014] Wang, D., Mykland, P.A., 2014. The estimation of leverage e\ufb00ect\nwith high-frequency data, Journal of the American Statistical Association 109 (505), 197\u2013215.\n[Wu, 2001] Wu, G., 2001. The determinants of asymmetric volatility, Review of Financial Studies\n14, 837\u2013859.\n[Yu, 2012] Yu, J., 2012. A semiparametric stochastic volatility model, Journal of Econometrics,\n167(2), 473\u2013482.\n46",
    "chunk_index": 38,
    "start_char": 82667,
    "end_char": 85197,
    "paper_title": "Stochastic leverage effect in high-frequency data ",
    "paper_category": "q-fin.ST",
    "paper_filename": "Stochastic_leverage_effect_in_high-frequency_data_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Stochastic_leverage_effect_in_high-frequency_data_.pdf"
  },
  {
    "text": "Trends, Reversion, and Critical Phenomena\nin Financial Markets\nChristof Schmidhuber\nZurich University of Applied Sciences\nSchool of Engineering, Technikumstrasse 9\nCH-8401 Winterthur, Switzerland\nchristof@schmidhuber.ch\nDecember 14, 2020\narXiv:2006.07847v4 [q-fin.ST] 11 Dec 2020\n\nAbstract\nFinancial markets across all asset classes are known to exhibit trends, which have\nbeen exploited by traders for decades. However, a closer look at the data reveals that\nthose trends tend to revert when they become too strong. Here, we empirically measure\nthe interplay between trends and reversion in detail, based on 30 years of daily futures\nprices for equity indices, interest rates, currencies and commodities.\nWe \ufb01nd that trends tend to revert before they become statistically signi\ufb01cant. Our\nkey observation is that tomorrow\u2019s expected return follows a cubic polynomial of to-\nday\u2019s trend strength.\nThe positive linear term of this polynomial represents trend\npersistence, while its negative cubic term represents trend reversal. Their precise co-\ne\ufb03cients determine the critical trend strength, beyond which trends tend to revert.\nThese coe\ufb03cients are small but statistically highly signi\ufb01cant, if decades of data\nfor many di\ufb00erent markets are combined. We con\ufb01rm this by bootstrapping and out-\nof-sample testing. Moreover, we \ufb01nd that these coe\ufb03cients are universal across asset\nclasses and have a universal scaling behavior, as the trend\u2019s time horizon runs from a\nfew days to several years. We also measure the rate, at which trends have become less\npersistent, as markets have become more e\ufb03cient over the decades.\nOur empirical results point towards a potential deep analogy between \ufb01nancial\nmarkets and critical phenomena. In this analogy, the trend strength plays the role of\nan order parameter, whose dynamics is described by a Langevin equation. The cubic\npolynomial is the derivative of a quartic potential, which plays the role of the energy.\nThis supports the conjecture that \ufb01nancial markets can be modeled as statistical me-\nchanical systems near criticality, whose microscopic constituents are Buy/Sell orders.\nKeywords: trend following, mean reversion, futures markets, market e\ufb03ciency, critical\nphenomena, social networks\n\n1\nIntroduction\nIt is well-known that \ufb01nancial markets across all asset classes exhibit trends. These trends\nhave been exploited very successfully by the tactical trading industry over the past decades,\nincluding the former \u201dturtle traders\u201d [1] and today\u2019s CTA industry.\nA close look at the available data reveals that those trends tend to revert as soon as they\nbecome too strong. In this paper, we demonstrate this based on 30 years of daily futures\nreturns for equity indices, interest rates, currencies and commodities. We analyze trends\nwith 10 di\ufb00erent time horizons, ranging from 2 days to 4 years, and empirically measure\nthe critical strength, beyond which trends tend to revert. Here, the \u201dstrength\u201d of a trend is\nde\ufb01ned in terms of its statistical signi\ufb01cance, namely as the t-statistics of the trend.\nIn a \ufb01rst step, we measure the daily average return of a market as a function of the\nvalues of the 10 trend strengths on the previous day. In order to increase the statistical\nsigni\ufb01cance of the results, we aggregate across di\ufb00erent markets and time scales.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3307,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "from 2 days to 4 years, and empirically measure\nthe critical strength, beyond which trends tend to revert. Here, the \u201dstrength\u201d of a trend is\nde\ufb01ned in terms of its statistical signi\ufb01cance, namely as the t-statistics of the trend.\nIn a \ufb01rst step, we measure the daily average return of a market as a function of the\nvalues of the 10 trend strengths on the previous day. In order to increase the statistical\nsigni\ufb01cance of the results, we aggregate across di\ufb00erent markets and time scales. Our key\nobservation is that tomorrow\u2019s average return can be quite accurately modeled by a polyno-\nmial of today\u2019s trend strength. It consists of a positive linear term that is responsible for the\npersistence of trends, and a negative cubic term that is responsible for the reversion of trends.\nTrends tend to revert beyond a critical trend strength, where the two terms balance each\nother. The corresponding regression coe\ufb03cients are small, but statistically highly signi\ufb01cant.\nIn a second step, we re\ufb01ne this quantitative analysis. Using multiple nonlinear regression,\nwe empirically measure how the observed cubic function varies\n\u2022 with the time scale of the trends: we \ufb01nd that trends of medium strength persist at\nscales of several days to several years, while reversion dominates at shorter or longer\ntime scales. We model this scale-dependence by polynomial regression as well.\n\u2022 with the asset class: we \ufb01nd that the available data do not allow us to \ufb01t di\ufb00erent\nmodel parameters to di\ufb00erent asset classes. Within the limits of statistical signi\ufb01cance,\nthe model parameters are thus universal, i.e., independent of the asset.\n3\n\n\u2022 over time: we \ufb01nd that the patterns have gradually changed over the decades. In\nparticular, trends have become less persistent, and there is little evidence that classical\ntrend-following can perform as well in the future as it did in the past.\nSince \ufb01nancial market returns are only in a rough approximation independent, normally dis-\ntributed random variables, we cannot trust the standard signi\ufb01cance analyses for regression\nresults. Instead, we use bootstrapping and cross validation to con\ufb01rm that our results are\nstatistically highly signi\ufb01cant out-of-sample, and robust. Throughout this paper, we try\nhard not to introduce a single parameter more than is absolutely necessary to capture the\nessence of the empirically observed patterns. We \ufb01nd that we may \ufb01t at most 6 parameters\nto our 30-year data set, and identify what we believe are the 4-6 most relevant parameters.\nWhile trends have been exploited by the systematic trading industry for decades, they\narrived relatively late in academia. Early observations on market trends appear, e.g., in\n[2, 3]. Early literature on the interplay of trends and reversion has focused on their cross-\nsectional counterparts (momentum and value) for single stocks [4].\nWith the advent of\nalternative beta strategies [5, 6], trend-following has become an active academic research\narea [7, 8, 9, 10, 11, 12]. By now, there is an extensive literature on trend-following, includ-\ning backtests of its performance more than a century into the past [13, 14], and e\ufb00orts to\noptimize trend-following strategies by machine learning methods [15].",
    "chunk_index": 1,
    "start_char": 2819,
    "end_char": 6030,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "late in academia. Early observations on market trends appear, e.g., in\n[2, 3]. Early literature on the interplay of trends and reversion has focused on their cross-\nsectional counterparts (momentum and value) for single stocks [4].\nWith the advent of\nalternative beta strategies [5, 6], trend-following has become an active academic research\narea [7, 8, 9, 10, 11, 12]. By now, there is an extensive literature on trend-following, includ-\ning backtests of its performance more than a century into the past [13, 14], and e\ufb00orts to\noptimize trend-following strategies by machine learning methods [15]. For a recent review of\ntrend- and reversion strategies, see [16] and references therein.\nMuch of the \ufb01nancial literature in this \ufb01eld tries to improve trading strategies, be it by\nnew trend signals, by new algorithms for mapping signals to position sizes, by identifying\nmarket environments in which a given strategy works best or worst, or by reducing trading\ncosts or risks. However, while the results reported in our article also have implications for\ninvestors (e.g., they signal when to exit trends), our key motivation for publishing them\ngoes much further: as discussed in section 5, the cubic polynomial, the scaling relations, and\nthe universality that we observe all point towards a potential deep analogy between \ufb01nancial\nmarkets and statistical mechanical systems near second-order phase transitions. This in turn\nsupports the idea that markets can be modeled in terms of \u201dsocial networks\u201d of traders. Our\n4\n\nresults lay the empirical basis for systematically analyzing the nature of these networks.\nAs a corollary, our observations also support a modi\ufb01ed version of the e\ufb03cient market\nhypothesis: they suggest that market ine\ufb03ciencies do exist, but disappear before they be-\ncome strongly statistically signi\ufb01cant. In addition, our measurements quantify how markets\nhave become more e\ufb03cient with respect to trends over the decades.\n2\nData and De\ufb01nitions\n2.1\nData\nOur analysis is based on historical daily log-returns for the set of 24 futures contracts shown\nin table 1. This set is diversi\ufb01ed across four asset classes (equity indices, interest rates,\ncurrencies, commodities), three regions (Americas, Europe, Asia) and three commodity sec-\ntors (energy, metals, agriculture). We use futures returns, instead of the underlying market\nreturns, because futures returns are guaranteed to be marked-to-market daily. Moreover,\nthey are readily available for all asset classes and net of the risk-free rate, which also makes\nreturns in di\ufb00erent currencies and interest rate regimes comparable with each other.\nTable 1: Markets\nAmerica\nEurope\nAsia\nEquities\nS&P 500\nDAX 30\nNikkei 225\nTSE 60\nFTSE 100\nHang Seng\nInterest rates\nUS 10-year\nGermany 10-year\nJapan 10-year\nCanada 10-year\nUK 10-year\nAustralia 3-year\nCurrencies\nCAD/USD\nEUR/USD\nJPY/USD\nGBP/USD\nAUD/USD\nNZD/USD\nCommodities\nCrude Oil\nGold\nSoybeans\nNatural Gas\nCopper\nLive Cattle\nCom.-Sectors:\nEnergy\nMetals\nAgriculture\n5\n\nFor all contracts, we consider 30 years of daily price data, covering the period from Jan\n1, 1990, to Dec 31, 2019. The \ufb01rst two years 1990 and 1991 are merely used to compute the\ntrend strengths at the beginning of 1992 (see below), so the actual regression analysis covers\nonly 28 years.",
    "chunk_index": 2,
    "start_char": 5431,
    "end_char": 8705,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "Hang Seng\nInterest rates\nUS 10-year\nGermany 10-year\nJapan 10-year\nCanada 10-year\nUK 10-year\nAustralia 3-year\nCurrencies\nCAD/USD\nEUR/USD\nJPY/USD\nGBP/USD\nAUD/USD\nNZD/USD\nCommodities\nCrude Oil\nGold\nSoybeans\nNatural Gas\nCopper\nLive Cattle\nCom.-Sectors:\nEnergy\nMetals\nAgriculture\n5\n\nFor all contracts, we consider 30 years of daily price data, covering the period from Jan\n1, 1990, to Dec 31, 2019. The \ufb01rst two years 1990 and 1991 are merely used to compute the\ntrend strengths at the beginning of 1992 (see below), so the actual regression analysis covers\nonly 28 years. Daily prices Pi(t) were taken from Bloomberg, where i labels the asset and\nfutures are rolled 5 days prior to \ufb01rst notice. We de\ufb01ne normalized daily log-returns Ri(t):\nRi(t) = ri(t)\n\u03c3i\n,\nri(t) = ln\nPi(t)\nPi(t \u22121) ,\n\u03c32\ni = var(ri) ,\n\u00b5i = mean(ri) ,\n(1)\nwhere the long-term daily risk premium \u00b5i and the long-term daily standard deviation \u03c3i\nof a market i are measured over the whole 30-year period. For some futures markets, the\nlog-returns ri(t) had to be backtracked or proxied as follows:\n1. TSE 60 futures: their history begins on Sep 9, 1999. Before, the TSE 60 futures returns\nare proxied by the S&P 500 futures returns, which, in our analysis, thus have double\nweight during that period.\n2. Hang Seng index futures: their history begins on Apr 2, 1992. Before, their returns\nare proxied by Nikkei 225 futures returns. As the regression analysis begins only on\nJan 1, 1992, this is a minor data correction.\n3. DAX futures: their history begins on Nov 26, 1990. Before, FTSE 100 futures returns\nare used as a proxy. This data correction is also minor: it merely slightly a\ufb00ects the\ninitial trend strengths at the beginning of 1992, when the analysis begins.\n4. EUR futures: their history begins on May 21, 1998. Before, the Deutsche Mark is used\nas a substitute for the Euro. We have reconstructed Deutsche Mark futures returns\nfrom the spot exchange rate and German/U.S. Libor di\ufb00erentials.\n5. NZD futures: their history begins on May 9, 1997. Before, we have reconstructed the\nfutures returns from the spot exchange rate and the NZD/USD Libor di\ufb00erentials.\n6. German 10-year \u201cBund\u201d futures: their history begins on Nov 27, 1990. Before, we\nhave reconstructed futures returns from daily German 10-year and short-term interest\nrates, assuming a duration of 8. This data correction is also minor: it merely slightly\na\ufb00ects the initial trend strengths at the beginning of 2002, when the analysis begins.\n6\n\n7. Natural gas futures: their history begins on Apr 5, 1990. Before, 1.5-fold levered crude\noil futures returns are used as proxies for the natural gas futures returns, using the U.S.\nLibor rate as the cost of leverage. The 1.5-fold leverage re\ufb02ects the higher volatility of\nnatural gas compared with crude oil. Again, this data correction is minor, as it merely\na\ufb00ects the initial trend strengths at the beginning of 1992, when the analysis starts.",
    "chunk_index": 3,
    "start_char": 8138,
    "end_char": 11064,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "gas futures: their history begins on Apr 5, 1990. Before, 1.5-fold levered crude\noil futures returns are used as proxies for the natural gas futures returns, using the U.S.\nLibor rate as the cost of leverage. The 1.5-fold leverage re\ufb02ects the higher volatility of\nnatural gas compared with crude oil. Again, this data correction is minor, as it merely\na\ufb00ects the initial trend strengths at the beginning of 1992, when the analysis starts.\n2.2\nTime Scales\nWe will examine the interplay between trends and reversion at 10 di\ufb00erent time scales:\nTk = 2k business days with k \u2208{1, 2, 3, ..., 10}\nThis represents periods of approximately 2 days, 4 days, 8 days, 3 weeks, 6 weeks, 3 months,\n6 months, 1 year, 2 years, and 4 years. Thus, there are 10 di\ufb00erent trend strengths at each\npoint in time. A given asset may well be, e.g., in a long-term up-trend at the 1-year time\nscale, and at the same time in a short-term down-trend at the 3-week time scale.\n2.3\nTrend Strengths\nAs reviewed in [17, 18], there are many di\ufb00erent de\ufb01nitions of the strength of a trend, most\nof which are highly correlated. For the purpose of this study, we need a de\ufb01nition that\n\u2022 has only a single free parameter, the horizon T (to avoid over\ufb01tting historical data)\n\u2022 can be computed recursively (which will later help to relate it to critical phenomena)\nLet us develop the most convenient such de\ufb01nition step by step. For a given time horizon\nT, we de\ufb01ne the trend strength \u03c6i,T(t) of a market i at the end of day t \u2208Z as a weighted\naverage of past daily returns of that market (i.e., on or before day t) - more precisely, of the\nnormalized past daily log-returns (1) in excess of the long-term risk premium:\n\u03c6i,T(t) =\n\u221e\nX\nn=0\nwT(n) \u00b7 \u02c6Ri(t \u2212n)\nwith\n\u02c6Ri(t \u2212n) = Ri(t \u2212n) \u2212\u00b5i\n\u03c3i\n,\n(2)\nwhere wT(n) is a weight function for the time scale T. Removing the long-term risk premia\n\u00b5i in (2) is necessary to ensure that the long-term expectation value of the trend strengths\n7\n\n\u03c6i,T is zero. If we did not remove the risk premia, very long-term trends in equity- and bond\nmarkets, where such risk premia are generally assumed, would almost always be positive and\nnever revert. This mix-up of trends with risk premia would distort our results, as discussed\nin appendix A2. Note that the long-term risk premia are estimated over the whole time pe-\nriod in (2). However, to avoid any biases, in the out-of-sample cross-validation of section 4\nwe estimate the risk premia only from the training samples, excluding the validation samples.\nWe also normalize the weight function wT(n) such that the trend strength \u03c6i,T has stan-\ndard deviation 1. Assuming that market returns on di\ufb00erent days are independent from each\nother (which is true to high accuracy), this implies:\n\u221e\nX\nn=0\nw2\nT(n) = 1.",
    "chunk_index": 4,
    "start_char": 10626,
    "end_char": 13376,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "in (2). However, to avoid any biases, in the out-of-sample cross-validation of section 4\nwe estimate the risk premia only from the training samples, excluding the validation samples.\nWe also normalize the weight function wT(n) such that the trend strength \u03c6i,T has stan-\ndard deviation 1. Assuming that market returns on di\ufb00erent days are independent from each\nother (which is true to high accuracy), this implies:\n\u221e\nX\nn=0\nw2\nT(n) = 1.\n(3)\nWith this normalization, \u03c6i,T can be regarded as the statistical signi\ufb01cance of the trend.\nE.g., \u03c6i,T = 2 represents a highly signi\ufb01cant up-trend, while \u03c6i,T = \u22120.5 represents a weakly\nsigni\ufb01cant down-trend. This normalization makes all trend strengths comparable with each\nother, and will thus allow us to aggregate across di\ufb00erent markets and time scales below.\nThe simplest weight function is a step function (\ufb01g. 1, dotted line). In this case, the\ntrend strength \u03c6i,T is just proportional to the average log-return over the past T days. Un-\nfortunately, this weight function leads to arti\ufb01cial jumps of the trend strength \u03c6i,T on days\nwhen nothing happens, except that an outlier return leaves the rolling time window.\nThis can be avoided by an exponentially decaying weight function \u02dcwT(n) (\ufb01g.1, dashed\nline). Moreovoer, the corresponding trend strength \u03c8 can now be computed recursively:\n\u02dcwT(n)\n=\nMT e\u22122n/T\nwith normalization factor MT =\np\n1 \u2212e\u22124/T,\n(4)\n\u03c8i,T(t)\n=\n\u221e\nX\nn=0\n\u02dcwT(n) \u00b7 \u02c6Ri(t \u2212n) = e\u22122/T\u03c8i,T(t \u22121) + MT \u00b7 \u02c6Ri(t).\n(5)\nIt can be veri\ufb01ed that \u02dcwT satis\ufb01es (3). However, \u03c8 is quite volatile and jumps when an\noutlier return enters the rolling time window.\n8\n\nOne way to solve this problem is to use the common de\ufb01nition of \u03c6i,T in terms of a moving\naverage crossover: one subtracts the average log-price of asset i over a longer time period L\nfrom the average log price of the same asset over a shorter time period S. As pointed out\nin [17], this corresponds to a wedge-like weight function (\ufb01g. 1, solid line). It makes the\ntrend strength less volatile, as outlier returns a\ufb00ect it only gradually over the time period S.\nIt also \ufb01lters out short-term trends on time scales smaller than S, which helps to seperate\ntrends at di\ufb00erent time scales from each other.\nFigure 1: Our trend strength is de\ufb01ned as a weighted sum of past log-returns. The grey area\nshows the weight function used in this paper, compared with three standard alternatives.\nAll four weight functions shown here have the same average lookback period.\nUnfortunately, the moving price average has two parameters L, S (instead of just one\nparameter T) that must be \ufb01tted to the data in any analysis, which tends to reduce the\nstatistical signi\ufb01cance of the results. In this article, we will therefore use another similar\nweight function that involves only the single parameter T (\ufb01g. 1, grey area; for comparability\nwith the other weight functions, the \ufb01gure shows wT/2 instead of wT):\nwT(n) = NT \u00b7 (n + 1) \u00b7 exp(\u22122n\nT )\nwith\nNT = (1 \u2212e\u22124/T)2\n\u221a\n1 \u2212e\u22128/T .",
    "chunk_index": 5,
    "start_char": 12941,
    "end_char": 15915,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "period.\nUnfortunately, the moving price average has two parameters L, S (instead of just one\nparameter T) that must be \ufb01tted to the data in any analysis, which tends to reduce the\nstatistical signi\ufb01cance of the results. In this article, we will therefore use another similar\nweight function that involves only the single parameter T (\ufb01g. 1, grey area; for comparability\nwith the other weight functions, the \ufb01gure shows wT/2 instead of wT):\nwT(n) = NT \u00b7 (n + 1) \u00b7 exp(\u22122n\nT )\nwith\nNT = (1 \u2212e\u22124/T)2\n\u221a\n1 \u2212e\u22128/T .\n(6)\nWith this normalization factor NT, one can verify that (3) is indeed satis\ufb01ed. Moreover,\nthis de\ufb01nition, together with (5), allows for a recursive combined computation of the two\n9\n\nvariants \u03c8, \u03c6 of the trend strength (which will be important in section 5):\n\u03c6i,T(t)\n=\n\u221e\nX\nn=0\nwT(n) \u00b7 \u02c6Ri(t \u2212n) = e\u22122/T\u03c6i,T(t \u22121) + NT\nMT\n\u00b7 \u03c8i,T(t)\n(7)\nThe \u201daverage lookback period\u201d of this trend strength, i.e., the expectation value E[n + 1] of\nthe number of days we look back (where \u201dtoday\u201d, i.e. n = 0, counts as a 1-day lookback), is\nE[n + 1] =\n\u221e\nX\nn=0\n(n + 1) \u00b7 wT(n) \u00b7\n\u0002\n\u221e\nX\nn=0\nwT(n)\n\u0003\u22121 = T.\n(8)\nWe have veri\ufb01ed that, for a given horizon T, all de\ufb01nitions of the trend strength in fact\nyield quite similar results in our regression analysis of section 4, as long as the weight func-\ntion rises gradually, decays gradually, and the average lookback period is the same. However,\nwe use (6,7) here, as it is the simplest mathematical function that satis\ufb01es these criteria, and\nhas only the single free parameter T, and can be computed recursively. (6) was originally\nintroduced by the author in 2008 at Syndex Capital Management, and has been used to\nreplicate Managed Futures indices as part of a UCITS fund from 2010-2014.\nTo limit the impact of outlier values of \u03c6i,T on our results, we will cut it o\ufb00at \u00b12.5 in\nthe actual regression analysis of section 4, i.e., we will use the capped and \ufb02oored version\n\u03c6cap\ni,T = min (2.5, max (\u22122.5, \u03c6i,T).\nAccording to [11], the standard practise of the managed futures industry (which focuses on\ntrends, and not reversion) for this threshold is 2.0. We use the slightly higher value of 2.5,\nbecause this will allow us to study more precisely the regime where trends revert, yet it will\nnot give excessive weights to outliers. This is supported by Appendix A1, which compares\nthe results of our regression analysis of section 4 for thresholds from 2.0 to 3.0. For thresholds\n> 2.5, we get a higher adjusted R-squared. However, the results are less robust, and the\nstatistical signi\ufb01cance of the regression betas decreases. For thresholds < 2.5, the reversion\nregime would be largely removed from the analysis, leading to a lower adjusted R-squared\nwithout improving the overall statistical signi\ufb01cance of the results.\n10",
    "chunk_index": 6,
    "start_char": 15406,
    "end_char": 18168,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "A1, which compares\nthe results of our regression analysis of section 4 for thresholds from 2.0 to 3.0. For thresholds\n> 2.5, we get a higher adjusted R-squared. However, the results are less robust, and the\nstatistical signi\ufb01cance of the regression betas decreases. For thresholds < 2.5, the reversion\nregime would be largely removed from the analysis, leading to a lower adjusted R-squared\nwithout improving the overall statistical signi\ufb01cance of the results.\n10\n\n2.4\nDatabase\nTable 2 displays a small extract of the resulting database for our analysis. Only two of the\n7305 business days and only three of the 24 markets are shown. The third column shows\nthe normalized daily log-returns (1), which have standard deviation 1. The 7305 business\ndays cover only the 28-year period from Jan 1992 - Dec 2019, because the \ufb01rst two years\n1990-1992 were only used to compute the initial trend strengths at the beginning of 1992.\nThe full table with 7305 \u00d7 24 = 175\u2032320 lines is published along with this paper.\nTable 2: Database\nTrend strengths on previous day for 10 time scales\nDay Market\nRi(t)\n2d\n4d\n8d\n3w\n6w\n3m\n6m\n1y\n2y\n4y\n1\nS&P 500\n-0.2\n0.3\n0.7\n1.0\n0.6\n0.2\n0.3\n0.6\n0.6\n1.0\n1.6\n1\nEUR/$\n-0.1\n0.2\n0.2\n0.0\n-0.4\n-0.6\n-0.6\n-0.8\n-0.9\n-0.7\n-0.8\n1\nGold\n-0.5\n-0.3\n-0.7\n-0.7\n0.1\n1.1\n1.5\n1.1\n0.4\n0.1\n-0.4\n2\nS&P 500\n-0.3\n-0.1\n0.4\n0.9\n0.7\n0.2\n0.3\n0.6\n0.6\n1.0\n1.6\n2\nEUR/$\n-1.0\n-0.7\n-0.2\n-0.2\n-0.4\n-0.6\n-0.6\n-0.8\n-0.9\n-0.7\n-0.8\n2\nGold\n0.8\n0.3\n-0.2\n-0.6\n0.1\n1.1\n1.5\n1.1\n0.4\n0.1\n-0.4\n3\nQualitative Observations\nThis section begins with an exploratory analysis of our data. The analysis in this section\nis only qualitative, but it serves to motivate the speci\ufb01c quantitative, statistically rigorous\nregression analysis of the following section. We stress again that our aim is not to improve\nfutures trading strategies, which would have to include risk limits, trading cost minimization,\nand other features. Rather, we simply want to empirically measure and model the small\nautocorrelations of market returns as accurately as possible as a basis for future work.\n3.1\nNext-day Return vs. Trend Strength\nWe use the data of table 2 to measure the expected daily return of a futures market as a\nfunction of the trend strengths in that market on the previous day. To this end, we \ufb01rst con-\n11\n\nstruct 7305\u00b724\u00b710 = 1\u2032753\u2032200 pairs of data. Each pair consists of the normalized log-return\nRi(t) in that market on day t, and one of the 10 trend strengths \u03c6i,T(t \u22121) on the previous\nday. So each return appears in 10 data pairs, each time paired with a di\ufb00erent trend strength.\nWe then group those pairs into 15 bins of increasing trend strength from \u2212\u221eto -13/6,\n-13/6 to -11/6, ... ,-1/6 to+1/6, ... ,11/6 to 13/6, 13/6 to \u221e. The mean trend strength\nwithin each bin is shown on the x-axis of \ufb01g. 2 (left).",
    "chunk_index": 7,
    "start_char": 17705,
    "end_char": 20483,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "Ri(t) in that market on day t, and one of the 10 trend strengths \u03c6i,T(t \u22121) on the previous\nday. So each return appears in 10 data pairs, each time paired with a di\ufb00erent trend strength.\nWe then group those pairs into 15 bins of increasing trend strength from \u2212\u221eto -13/6,\n-13/6 to -11/6, ... ,-1/6 to+1/6, ... ,11/6 to 13/6, 13/6 to \u221e. The mean trend strength\nwithin each bin is shown on the x-axis of \ufb01g. 2 (left). Within each bin, we average over\nthe normalized return on the day after the trend has been measured. To obtain statistically\nsigni\ufb01cant results, we aggregate over the 28 years of daily returns for each market, across all\n24 markets, and across di\ufb00erent time scales. Fig. 2 (left) shows the results for the 4 monthly\ntrend strengths (i.e., aggregated over T = 6 weeks, 3 months, 6 months, and 1 year).\nFigure 2:\nLeft: The expectation value E(r) of the next day\u2019s return of a futures market is\na nonlinear function of the current trend strength \u03c6. As veri\ufb01ed by the extensive statistical\nanalysis of section 4, it can be modeled by a cubic polynomial of \u03c6, whose linear term b\u03c6\n(with b > 0) represents trend-persistence, and whose cubic term c\u03c63 (with c < 0) represents\ntrend-reversion. Right: As con\ufb01rmed by bootstrapping in section 4, the regression coe\ufb03cients\nb and c corresponding to the linear and cubic terms are statistically highly signi\ufb01cant.\nWe observe that the average next-day return is close to zero at zero trend strength,\nand grows linearly with the trend strength for small strengths. As the trend strength in-\ncreases further, the average next-day return peaks, then decreases again until it becomes zero\n12\n\nsomewhere below trend strength 2. For even stronger trends, the average return decreases\ndramatically. This behavior is mirrored on the left-hand side of the graph for down-trends.\nWe have veri\ufb01ed that this pattern remains almost the same if another day of delay is added,\ni.e., if the next-day return in our data pairs is replaced by the return 2 days later.\nThus, trends tend to revert when they become too strong. This makes sense intuitively:\nafter strong trends, markets tend to be overbought or oversold, so one expects a reversion to\n\u201dvalue\u201d. Our analysis quanti\ufb01es where exactly this happens: below a critical trend strength\nof 2, before trends become strongly statistically signi\ufb01cant. Note that this is not in line with\nclassical trend-following, which would follow the trend no matter how strong it becomes.\nThe dashed line in \ufb01g. 2 (left) indicates the trading position that a classical trend-follower\nwould take as a function of the trend strength.\n3.2\nDependence on the Time Scale\nIn a next step, we analyze how the pattern observed in the previous sub-section depends on\nthe time scale. To this end, we re\ufb01ne the bins used above: we split each bin into 10 smaller\nbins, one for each of the 10 time scales.",
    "chunk_index": 8,
    "start_char": 20068,
    "end_char": 22928,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "The dashed line in \ufb01g. 2 (left) indicates the trading position that a classical trend-follower\nwould take as a function of the trend strength.\n3.2\nDependence on the Time Scale\nIn a next step, we analyze how the pattern observed in the previous sub-section depends on\nthe time scale. To this end, we re\ufb01ne the bins used above: we split each bin into 10 smaller\nbins, one for each of the 10 time scales. The resulting 15\u00d710 re\ufb01ned bins are now too small\nand the results too noisy. To reduce the noise, we average the next-day returns over blocks\nof 3x3 neigboring bins (resp. 3x2 or 2x3 bins at the borders, 2x2 bins at the corners of the\nmatrix of 15\u00d710 bins), weighted by the number of returns in each bin. This yields the heat\nmap of \ufb01g. 3 (left). Fig. 2 (left) can be thought of as a horizontal cross-section through this\nheat map along the dashed line.\nWe observe that the trend- and reversion pattern of \ufb01g. 2 (left) is strongest on time scales\nfrom 1 month to 1 year. This is in line with the fact that typical trend-followers operate on\nthose time scales. As the time scale increases or decreases, the pattern becomes weaker. The\nregion where markets trend seems to disappear both for time scales of the order of economic\ncycles (several years) and for intra-week time scales. The phenomenon of reversion, on the\nother hand, appears to remain strong at all time scales.\n13\n\nFigure 3:\nLeft: A heat map shows how the expectation value of tomorrow\u2019s return depends\nboth on today\u2019s trend strength \u03c6 and its time horizon. Fig. 2 (left) can be thought of as a\ncross-section of Fig. 3 (left) along the dashed line. Right: The polynomial regression analysis\nof section 4 models the pattern of Fig. 3 (left) by an elliptic regime within which trends are\npersistent, and outside of which they revert. The values of the center and of the semi-axes\nof the ellipse are statistically highly signi\ufb01cant, as con\ufb01rmed by bootstrapping.\n3.3\nCounting Degrees of Freedom\nAs emphasized in [19], one must be very conservative in introducing new factors and pa-\nrameters in \ufb01nancial market models. Before modeling the observed patterns in detail, let us\ntherefore do a back-of-the-envelope calculation of how many parameters we can hope to \ufb01t\nin our model without over-\ufb01tting our daily return data, and what fraction of the variance of\nthese returns we can hope to explain by trend factors.\nOur 7\u2032305 \u00b7 24 = 175\u2032320 daily log-returns are not independent, because the 24 markets\nare correlated with each other. How many independent markets are there? The daily re-\nturns are normalized to have variance 1. For a portfolio that invests 1/24 in each market, we\n\ufb01nd a variance of \u03c32 \u223c1/8, just as if it contained nm = 8 independent assets. A principal\ncomponent analysis con\ufb01rms that the \ufb01rst 8 (resp. 12) principal components explain 65%\n(resp. 80%) of the variance of the returns of our 24 markets. In this sense, these returns\n14",
    "chunk_index": 9,
    "start_char": 22527,
    "end_char": 25438,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "24 = 175\u2032320 daily log-returns are not independent, because the 24 markets\nare correlated with each other. How many independent markets are there? The daily re-\nturns are normalized to have variance 1. For a portfolio that invests 1/24 in each market, we\n\ufb01nd a variance of \u03c32 \u223c1/8, just as if it contained nm = 8 independent assets. A principal\ncomponent analysis con\ufb01rms that the \ufb01rst 8 (resp. 12) principal components explain 65%\n(resp. 80%) of the variance of the returns of our 24 markets. In this sense, these returns\n14\n\ne\ufb00ectively live in a space of dimension nm \u223c8. Adding more markets to our 24 time series\ndoes not signi\ufb01cantly increase nm.\nWhat is the highest annualized Sharpe ratio S that one can hope to achieve by systemat-\nically trading a broadly diversi\ufb01ed set of highly liquid futures markets based on trends and\nreversion? Experience with the Managed Futures (\u201dCTA\u201d) industry suggests that S can be\nat best 1. The small number of CTA\u2019s that have achieved a higher Sharpe ratio for several\nyears in a row presumably also pursue other strategies that are not purely based on trends,\nor they are not market-neutral (in the sense of zero net exposure to each market over time).\nAn annualized Sharpe ratio of S = 1 implies a daily Sharpe ratio \u03c1 for each market of\n\u03c1 =\nS\n\u221a260 \u00b7 nm\n\u223c0.02.\nSo the predicted next-day return of a market has a correlation of \u03c1 = 0.02 with the actual\nnext-day return. E.g., if we only try to predict the sign of the next return, we can at best\nhope to be right on 51 and wrong on 49 out of 100 days. The adjusted R-squared (achieved\nout-of-sample in real trading) is then R2\nadj \u223c\u03c12 = 4 basis points (1bp = 4 \u00b7 10\u22124). Clearly,\nthe variance of \ufb01nancial market returns is overwhelmingly due to random noise.\nIf we \ufb01t k parameters to our data, and if our returns were independent and identically\ndistributed (\u201diid\u201d), then, for small R2, the adjusted R-squared would be approximately\nR2\nadj \u223cR2 \u2212k\nN \u223c4 bp\nwith\nN = 260 \u00b7 nm \u00b7 Y data points\n(9)\nfor Y years of daily data. If we require that the correction for the in-sample bias does not\nerode more than 20% of our R2, then we conclude that we cannot \ufb01t more than k \u223cN \u00b71 bp \u223c\n6 parameters to our 28 years of data. This 20%-requirement is not too conservative, as our\nreturns are only approximately \u201diid\u201d, and therefore the actual correction per \ufb01tted parameter\nwill be higher than 20% (below, cross-validation will show that it is indeed more than twice\nas big). We conclude that we must use parameters wisely, and not \u201dwaste\u201d them on features\nthat may be artifacts of our limited data set.\n15\n\n4\nRegression Analysis\nIn this section, we con\ufb01rm and quantify the observations of the previous section by nonlinear\nregression based on ordinary least squares.",
    "chunk_index": 10,
    "start_char": 24913,
    "end_char": 27656,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "returns are only approximately \u201diid\u201d, and therefore the actual correction per \ufb01tted parameter\nwill be higher than 20% (below, cross-validation will show that it is indeed more than twice\nas big). We conclude that we must use parameters wisely, and not \u201dwaste\u201d them on features\nthat may be artifacts of our limited data set.\n15\n\n4\nRegression Analysis\nIn this section, we con\ufb01rm and quantify the observations of the previous section by nonlinear\nregression based on ordinary least squares. To this end, we model the next-day return of a\nmarket as a polynomial function of both the current trend strength in that market and its\ntime scale. This regression is performed directly on the underlying 1\u2019753\u2019200 pairs of data,\nnot on the bins we have de\ufb01ned in the previous section. Thus, our results are independent\nof any choice of how to split the data into bins.\n4.1\nDependence on the Trend Strength\nThe graph in \ufb01g. 2 (left) suggests to model the next-day normalized log-return R(t + 1) (1)\nas a polynomial of the current trend strength \u03c6(t) (2) across all markets and time scales:\nR(t + 1) = a + b \u00b7 \u03c6(t) + d \u00b7 \u03c62(t) + c \u00b7 \u03c63(t) + ... + \u03f5(t + 1),\n(10)\nwhere \u03f5 represents random noise, and a measures the average risk premium \u00b5i/\u03c3i across all\nassets. Similar models with a polynomial random force have been postulated previously,\nnotably by econophysicists with a background in critical phenomena [20, 21, 22, 23]. Our\nobservations of the previous section give clear empirical support for a polynomial ansatz.\nWe will discuss the relationship with critical phenomena in more detail in section 5.\nTable 3\nRegression with linear and cubic terms\nCoe\ufb03cient\nValue\nError\nt-statistics\na\n1.33%\n\u00b10.41%\n3.3\nb\n1.29%\n\u00b10.43%\n3.0\nc\n\u22120.62%\n\u00b10.23%\n2.7\nR-squared\nSingle time scales\nAggregated across time scales\nR2\n1.31 bp\n4.91 bp\nR2\nadj\n1.03 bp\n3.98 bp\n16\n\nWe have performed a corresponding regression analysis on the 1\u2019753\u2019200 data pairs\n{rt+1, \u03c6t}. Using only the linear and cubic terms of (10) yields the results of table 3.\nSince market returns cannot be assumed to be independent, identically distributed normal\nvariables, we cannot trust the usual estimates of the t-statistics, adjusted R-squared, and\nF-statistic. Instead, the test statistics shown in table 3 are measured empirically as follows:\n\u2022 The standard errors of the coe\ufb03cients and their t-statistics are computed by boot-\nstrapping: from the 7305 days, we randomly create 5000 new samples of 7305 days\neach, with replacement. I.e., some days occur several times in a new sample, while\nother days do not occur at all. Regression on each new sample of days yields the\ndistribution of 5000 regression coe\ufb03cients b and c shown in \ufb01g. 2 (right). The errors\nof the coe\ufb03cients in table 3 represent half the di\ufb00erence between the 84th and 16th\npercentile, which equals the standard deviation in the case of a normal distribution.",
    "chunk_index": 11,
    "start_char": 27169,
    "end_char": 30038,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "I.e., some days occur several times in a new sample, while\nother days do not occur at all. Regression on each new sample of days yields the\ndistribution of 5000 regression coe\ufb03cients b and c shown in \ufb01g. 2 (right). The errors\nof the coe\ufb03cients in table 3 represent half the di\ufb00erence between the 84th and 16th\npercentile, which equals the standard deviation in the case of a normal distribution.\n\u2022 The adjusted R-squared is computed by 15-fold cross validation: we split our 7305-day\ntime window into 15 sub-windows of 487 consecutive days each. For each sub-window,\nwe predict the next-day returns based on the betas obtained by regression on the other\n14 sub-windows. The square of the correlation between the predicted and the actual\nreturns is the out-of-sample R-squared R2\nadj reported in table 3.\n\u2022 Table 3 also reports R2 and R2\nadj \u201daggregated across time scales\u201d. Those are based on\nusing the equally-weighted mean of the 10 trend strengths on each day to predict the\nnext-day return for each market. I.e., we combine the 10 di\ufb00erent trend factors into\na single one, which naturally has a higher predictive power than each single factor by\nitself. This regression is thus performed on only 7305 \u00b7 24 = 175\u2032320 pairs of data.\n\u2022 The F-statistics can be computed numerically to be F = 4.6 with a p-Value of 0.7%\nby modelling the distribution of regression coe\ufb03cients in \ufb01g. 2 (right) by an elliptical\ndistribution. However, the distributions in subsequent sections are not even approxi-\nmately elliptical. We will therefore use R2\nadj and not F to compare the out-of-sample\nexplanatory power of our models with each other.\n17\n\nThe regression results of table 3 con\ufb01rm and quantify our conclusions from the previous\nsection. We see that the values of b and c - although very small - are statistically highly\nsigni\ufb01cant, despite the fact that market returns are neither normally distributed, nor inde-\npendent, nor identically distributed. So is the average long-term risk premium a. The overall\nresult is signi\ufb01cant at the 99% level. The aggregated out-of-sample R2\nadj that combines the\npredictions from all 10 time scales matches our initial expectation of 4 bp (9). Note that the\ncorrection for the aggregated in-sample bias, R2 \u2212R2\nadj = 0.93 bp, is much bigger than what\nwould have been expected if returns were \u201diid\u201d, namely 2/(260 \u00b7 8 \u00b7 28) = 0.34 bp.\nWe have also tested the quadratic, quartic and quintic terms in \u03c6T in (10). None of them\nturned out to be statistically signi\ufb01cant at the 95% level. We therefore drop them from our\nanalysis to avoid over-\ufb01tting the historical data (i.p., the t-statistics for d is below 1).\n4.2\nDependence on the Time Scale\nNext, we try to re\ufb01ne our model by measuring the dependence of the coe\ufb03cients b and c\non the time scale, the asset class, and the time period.",
    "chunk_index": 12,
    "start_char": 29643,
    "end_char": 32457,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "them\nturned out to be statistically signi\ufb01cant at the 95% level. We therefore drop them from our\nanalysis to avoid over-\ufb01tting the historical data (i.p., the t-statistics for d is below 1).\n4.2\nDependence on the Time Scale\nNext, we try to re\ufb01ne our model by measuring the dependence of the coe\ufb03cients b and c\non the time scale, the asset class, and the time period. We begin with the time scale T: we\nmodel the expected return as a function of both the trend strength and T, trying to replicate\n\ufb01g. 3 (left). We \ufb01rst repeat the linear and cubic regression (10) of the previous sub-section\nfor each of the 10 time scales seperately. The resulting coe\ufb03cients b(T) and c(T) are plotted\nin \ufb01g. 4 (we neglect the overall risk premium a, which is not the focus of this paper).\nFrom the coe\ufb03cient b of the linear term, which models trends, we observe that trend-\nfollowing works best at time scales from 3 months to 1 year, where b peaks. This appears to\nbe in line with the time scales on which typical CTAs follow trends. Even at those scales,\nthe critical trend strength\n\u03c6c =\np\n\u2212b/c \u22641.91 ,\nbeyond which trends tend to revert, is below 2. So trends never become strongly signi\ufb01cant.\nFor scales below a few days and above several years, b seems to go to zero, which means that\ntrends are not persistent there. This is consistent with the heat map in \ufb01g. 2 (right).\n18\n\nFigure 4: Left: The coe\ufb03cient b(T) of the linear term (corresponding to the trending of\nmarkets) peaks at time scales T of 3 months to 1 year. Its scale dependence is modeled by\na parabola. Right: The coe\ufb03cient c of the cubic term (corresponding to the reversion of\nmarkets) does not show a clear dependence on the time scale.\nOn the other hand, the coe\ufb03cient c of the cubic term, which ensures that trends revert,\nis quite stable, except that its magnitude appears to be somewhat lower for the 2- and 4-year\nscales. The 2- and 4-year results must be taken with a grain of salt, though, as there are only\n14 independent 2-year trends and 7 independent 4-year trends in our 28-year time window.\nIndeed, a preliminary check based on 60 years of monthly returns resulted in c \u223c\u22120.6% at\nthe 8-year scale. The available data thus indicate that, unlike trend-following, mean rever-\nsion works at all time scales. This is also consistent with our earlier observation from the\nheat map in \ufb01g. 3 (left).\nTo quantify these observations, we re\ufb01ne our regression ansatz (10). We continue to\nmodel the cubic coe\ufb03cient c by a constant, but we model the dependence of the linear\ncoe\ufb03cient b(k) on the logarithm k of the time scale T = 2k by a parabola:\nb(k)\n=\nb \u2212e \u00b7 (k \u2212k0)2\n\u21d2R(t + 1)\n=\nb \u00b7\nn\n1 \u2212(k \u2212k0)2\n(\u2206k)2\no\n\u00b7 \u03c6(t) + c \u00b7 \u03c63(t) + \u03f5(t + 1)\n(11)\nwith (\u2206k)2 = b/e. The critical trend strength \u03c6c(k) = (\u2212b(k)/c)1/2, at which the expected\n19",
    "chunk_index": 13,
    "start_char": 32092,
    "end_char": 34882,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "from the\nheat map in \ufb01g. 3 (left).\nTo quantify these observations, we re\ufb01ne our regression ansatz (10). We continue to\nmodel the cubic coe\ufb03cient c by a constant, but we model the dependence of the linear\ncoe\ufb03cient b(k) on the logarithm k of the time scale T = 2k by a parabola:\nb(k)\n=\nb \u2212e \u00b7 (k \u2212k0)2\n\u21d2R(t + 1)\n=\nb \u00b7\nn\n1 \u2212(k \u2212k0)2\n(\u2206k)2\no\n\u00b7 \u03c6(t) + c \u00b7 \u03c63(t) + \u03f5(t + 1)\n(11)\nwith (\u2206k)2 = b/e. The critical trend strength \u03c6c(k) = (\u2212b(k)/c)1/2, at which the expected\n19\n\nreturn E(Rt+1) is zero (without the noise \u03f5), and beyond which trends revert, is then an\nellipse with semi-axes \u2206k and \u03c6c(k0). Altogether, we now \ufb01t 4 parameters to our data:\n\u2022 The \u201dpersistence of trends\u201d b, i.e. the value of b(k) at its peak\n\u2022 The \u201dstrength of reversion\u201d c\n\u2022 The range k0 \u00b1 \u2206k of the log of the time scales T = 2k at which markets may trend.\nTable 4\nRe\ufb01ned regression with 4 parameters\nCoe\ufb03cient\nValue\nError\nt-Stat.\nb\n2.00%\n\u00b10.48%\n4.2\nc\n\u22120.63%\n\u00b10.24%\n2.6\nk0\n5.78\n\u00b10.67\n8.6\n\u2206k\n4.87\n\u00b11.09\n4.5\nR-squared\nSingle time scales\nAggregated across time scales\nR2\n1.64 bp\n7.51 bp\nR2\nadj\n1.22 bp\n6.04 bp\nA nonlinear regression on the full underlying data set yields the results of table 4. Fig. 3\n(right) plots the elliptic region, which seperates the \u201ctrend regime\u201d (inside) from the \u201crever-\nsion regime\u201d (outside). For its second semi-axis, we \ufb01nd \u03c6c(k0) = 1.78\u00b10.32. This quanti\ufb01es\nthe empirical heat map in \ufb01g. 3 (left) and con\ufb01rms that highly signi\ufb01cant trends of strength\n(i.e., t-statistics) \u03c6c \u22652 always tend to revert.\nThe errors of the regression parameters in table 4 are again computed by bootstrapping.\nThe distribution of b and c looks the same as in the univariate case (Fig. 2, right). Fig 5\n(left) plots the distribution of the values of the center k0 and the semi-axis \u2206k of the ellipse\nthat separates the trending regime from the reversion regime. The \u201daggregate\u201d R2 and R2\nadj\nin table 4 now refer to a single factor that is a linear combination of the 10 trend strengths\nfor the 10 time scales, weighted by a parabolic weight function proportional to b(k). Note\nthat the aggregated adjusted R-squared now exceeds our original expectation (9) of 4 bp.\n20\n\nFigure 5: Left: Distribution of the regression coe\ufb03cients for the center k0 and width \u2206k of\nthe elliptic region within which trends are persistent, as obtained by bootstrapping. Right:\nDistribution of the linear and cubic regression coe\ufb03cients b, c for a rejected alternative model.\nWe have tried to further re\ufb01ne ansatz (11). First, b(k) in \ufb01g. 4 (left) seems to be tilted to\nthe right, which could be accounted for by models such as b(k) \u223cb\u2212e\u00b7(k\u2212k0)2+f \u00b7(k\u2212k0)3,\nor b(k) \u223cexp(f \u00b7 k) cos((k \u2212k0)/\u2206k). We \ufb01nd that such models increase the adjusted R-\nsquared at best marginally.",
    "chunk_index": 14,
    "start_char": 34416,
    "end_char": 37144,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "within which trends are persistent, as obtained by bootstrapping. Right:\nDistribution of the linear and cubic regression coe\ufb03cients b, c for a rejected alternative model.\nWe have tried to further re\ufb01ne ansatz (11). First, b(k) in \ufb01g. 4 (left) seems to be tilted to\nthe right, which could be accounted for by models such as b(k) \u223cb\u2212e\u00b7(k\u2212k0)2+f \u00b7(k\u2212k0)3,\nor b(k) \u223cexp(f \u00b7 k) cos((k \u2212k0)/\u2206k). We \ufb01nd that such models increase the adjusted R-\nsquared at best marginally. Therefore, we use the simplest model (11) in this paper, to avoid\nover-\ufb01tting the historical data.\nSecond, we also tested for a polynomial dependence of c on k. The most signi\ufb01cant\nansatz was that c(k) is also a parabola proportional to \u2212b(k). In this case, the critical trend\nstrength is constant across all time scales, and the region within which markets trend is\nrectangular instead of elliptic. The distribution of the parameters b(k0), c(k0) then turns out\nto have the shape of the stretched annulus shown in \ufb01g. 5 (right). However, this scenario\nseems less likely, as it yields a much lower adjusted R-squared (0.77 bp).\n4.3\nDependence on the Asset Class\nCan we re\ufb01ne our 4-parameter-model further by distingushing between asset classes, i.e., by\n\ufb01tting seperate regression parameters for equities, bonds, currencies and commodities?\n21\n\nTo test this, we have repeated the regression analysis of the previous section for these 4\nsub-sets of our data. Fig. 6 (left) shows the 16th, 50th and 84th percentile of the values of the\n4 regression parameters for each asset class, divided by the values of the regression parame-\nters for the overall sample. E.g., for equities, the quantiles for b are (1.80%, 2.82%, 4.01%),\nwhich are multiples of (0.90, 1.41, 2.01) of the overall regression coe\ufb03cient 2.00% (see table\n3). Those multiples are what is shown in the \ufb01rst bar of \ufb01g. 6 (left).\nFigure 6: Left:\nRatios of the values of our 4 regression parameters for equities, interest\nrates, FX rates and commodities, divided by their overall values across all asset classes. The\nratios do not di\ufb00er signi\ufb01cantly from 1. Right: The analoguous ratios for the early, middle\nand late third of the time period. At least b has decreased signi\ufb01cantly over time.\nFor each asset class, we observe that the values of all four parameters are within one\nstandard error of the overall parameter values. Thus, based on our data set, we cannot\njustify \ufb01tting di\ufb00erent parameters of our trend-reversion model to individual asset classes,\nlet alone to individual assets. This is consistent with our back-of-the-envelope caculation of\nsubsection 3.4, which suggests that we cannot \ufb01t as many as 4 \u00d7 4 = 16 parameters for the\nfour di\ufb00erent asset classes to our data. We thus use a single, universal model for all assets.\n22\n\n4.4\nDependence on the Time Period\nLastly, we investigate how our patterns have been evolving in time. First, we split up the\n28-year (7305-day) time window into three non-overlapping sub-periods of 2435 days each:\nAn early period from Jan 1992 to Apr 2001, a middle period from May 2001 to Aug 2010,\nand a late period from Sep 2010 to Dec 2019.",
    "chunk_index": 15,
    "start_char": 36678,
    "end_char": 39792,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "consistent with our back-of-the-envelope caculation of\nsubsection 3.4, which suggests that we cannot \ufb01t as many as 4 \u00d7 4 = 16 parameters for the\nfour di\ufb00erent asset classes to our data. We thus use a single, universal model for all assets.\n22\n\n4.4\nDependence on the Time Period\nLastly, we investigate how our patterns have been evolving in time. First, we split up the\n28-year (7305-day) time window into three non-overlapping sub-periods of 2435 days each:\nAn early period from Jan 1992 to Apr 2001, a middle period from May 2001 to Aug 2010,\nand a late period from Sep 2010 to Dec 2019.\nFig. 6 (right) shows the 16th, 50th and 84th percentile of the values of the 4 regression\nparameters for each of these time-windows, again divided by the values of the regression pa-\nrameters for the overall sample. The persistence of trends b has consistently and signi\ufb01cantly\ndecreased over time. With less consistency, this can also be observed for strength of reversion\nc, while no clear trend is visible for the range k0 \u00b1 \u2206k, within which trends persist. The\ndecrease of b is in line with the industry observation that trend-following no longer works\nas well as it used to: markets seem to have become more e\ufb03cient in this respect. Given the\ndecrease in trading costs, an increase in algorithmic trading, and an increase in assets under\nmanagement invested in trend-following, this is not surprising.\nTo verify and quantify these observations, let us introduce time t, measured in years, with\nits origin t = 0 on Dec 31, 2005, the center of our 28-year time window. We now further\nre\ufb01ne our model (11) by including linear trends in b and c while leaving k0 and \u2206k constant:\nb(t) = \u00afb \u00b7 (1 \u2212Qb \u00b7 t)\n,\nc(t) = \u00afc \u00b7 (1 \u2212Qc \u00b7 t)\nThe results of a regression analysis, including bootstrapping and cross-validation, are shown\nin table 5.\nThe decrease of c, which measures the strength of reversion, is only weakly\nsigni\ufb01cant. However, the decrease of b, which measures the persistence of trends, is signi\ufb01cant\nat the 97.5% con\ufb01dence level. In principle, we could compute the year Y0, in which b(t) = 0:\nY0 = 2005 + 1\nQb\n\u2208{2012, 2028}\nwith expectation value\nY0 \u223c2017.\nThus, if one were to take this linear down-trend of b literally, one would conclude that the\nphenomenon of persistent market trends may have already disappeared. However, there are\nother scenarios for the time decay of the persistence of trends that are consistent with our\n23\n\ndata. E.g., for an exponential decay scenario, in which trends never disappear, we \ufb01nd an\nonly slightly lower adjusted R-squared of 1.39 bp instead of 1.52 bp, with\nb \u223c\u00afb \u00b7 e\u2212Qt\nwith decay rate\nQ \u223c(24 years)\u22121.\nTable 5\nRe\ufb01ned regression with 6 parameters\nCoe\ufb03cient\nValue\nError\nt-Stat.",
    "chunk_index": 16,
    "start_char": 39204,
    "end_char": 41920,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "of trends that are consistent with our\n23\n\ndata. E.g., for an exponential decay scenario, in which trends never disappear, we \ufb01nd an\nonly slightly lower adjusted R-squared of 1.39 bp instead of 1.52 bp, with\nb \u223c\u00afb \u00b7 e\u2212Qt\nwith decay rate\nQ \u223c(24 years)\u22121.\nTable 5\nRe\ufb01ned regression with 6 parameters\nCoe\ufb03cient\nValue\nError\nt-Stat.\n\u00afb\n1.91%\n\u00b10.49%\n3.9\n\u00afc\n\u22120.62%\n\u00b10.25%\n2.5\nk0\n5.83\n\u00b10.50\n11.7\n\u2206k\n4.97\n\u00b10.69\n7.2\nQb\n0.088\n\u00b1 0.045\n2.0\nQc\n0.047\n\u00b1 0.052\n0.9\nR-squared\nSingle time scales\nAggregated across time scales\nR2\n1.97 bp\n8.90 bp\nR2\nadj\n1.49 bp\n6.98 bp\nWe have also tested scenarios where all 4 parameters or other subsets of them change\nat di\ufb00erent rates, but found that all of these scenarios signi\ufb01cantly reduce the adjusted R-\nsquared. It is left for future work to investigate the time evolution of the pattern of trends\nand reversion in more detail.\n5\nAnalogies with Critical Phenomena\nIn this section, we point out some striking analogies between the empirical observations of\nsections 3 and 4 and critical phenomena in statistical mechanics. Analogies between \ufb01nancial\nmarkets and critical phenomena, such as scaling relations, have long been observed [24]. Our\nresults go further: they seem to directly and speci\ufb01cally identify the trend strength with the\norder parameter of a Landau-type mean \ufb01eld theory with a quartic potential.\n24\n\nAnalogies with critical phenomena are plausible, if \ufb01nancial markets are regarded as\nstatistical mechanical systems, whose microscopic constituents are the Buy/Sell orders of\nindividual traders. It is conceivable that these orders can be modeled by degrees of freedom\nthat sit on the vertices of a hypothetical \u201dsocial network of traders\u201d. These degrees of free-\ndom may interact with each other in analogy with spins on a lattice, thereby creating the\nmacroscopic phenomena of trends (herding behavior) and reversion (contrarian behavior).\nTo imitate these phenomena and their interplay, various spin- and agent models have been\nproposed in the literature (see, e.g., [25, 26], and [27] for a recent review).\nCandidates for the \u201dsocial network of traders\u201d include small-world networks [28], scale-\nfree networks [29], or the Feynman diagrams of large-N \ufb01eld theory [30]. For a recent review\nof candidates for social networks, see [31]. To our knowledge, no convincing speci\ufb01c model\nhas emerged as a consensus so far. Our results provide an empirical basis for accepting or\nrejecting such candidates: any statistical-mechanical model of \ufb01nancial markets, if accurate,\nmust replicate the interplay of trends and reversion observed in this paper.\nTo make this precise, let us \ufb01rst reap the bene\ufb01ts of our recursive de\ufb01nitions (5,7) of the\ntrend strength, which lead to simple di\ufb00erential equations in the \u201dcontinuum limit\u201d T \u226b1:\n( d\ndt + 2\nT ) \u03c8(t) =\n2\n\u221a\nT\n\u00b7 \u02c6Ri(t),\n( d\ndt + 2\nT ) \u03c6(t) = 2\n\u221a\n2\nT\n\u03c8(t).\n(12)\nTo be speci\ufb01c, let us focus on the 6-month time horizon, i.e., T = 27 = 128 trading days\n(the results for other horizons are similar).",
    "chunk_index": 17,
    "start_char": 41593,
    "end_char": 44574,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "precise, let us \ufb01rst reap the bene\ufb01ts of our recursive de\ufb01nitions (5,7) of the\ntrend strength, which lead to simple di\ufb00erential equations in the \u201dcontinuum limit\u201d T \u226b1:\n( d\ndt + 2\nT ) \u03c8(t) =\n2\n\u221a\nT\n\u00b7 \u02c6Ri(t),\n( d\ndt + 2\nT ) \u03c6(t) = 2\n\u221a\n2\nT\n\u03c8(t).\n(12)\nTo be speci\ufb01c, let us focus on the 6-month time horizon, i.e., T = 27 = 128 trading days\n(the results for other horizons are similar). Combining (12) with the ansatz (10) implies the\nfollowing second-order stochastic di\ufb00erential equation for the trend strength \u03c6:\n( d\ndt + 1\n64)2\u03c6(t) = \u2212\u2202\n\u2202\u03c6V (\u03c6) +\n1\n256\u03f5(t)\nwith\nV (\u03c6) = \u2212b\n512 \u00b7 \u03c62 +\n|c|\n1024 \u00b7 \u03c64,\n(13)\nwith rescaled random noise \u03f5. Its simpler cousin \u03c8 in (5) obeys a \ufb01rst-order equation:\n( d\ndt + 1\n64)\u03c8(t) = \u2212\u2202\n\u2202\u03c6\n\u02dcV (\u03c8) +\n1\n4\n\u221a\n2\u03f5(t)\nwith\n\u02dcV (\u03c8) = \u2212\n\u02dcb\n4 \u00b7 \u03c82 + |\u02dcc|\n8 \u00b7 \u03c84,\n(14)\n25\n\nwith the following empirical parameter values, as measured by a regression analysis that is\nanalogous to that reported in section 4 for \u03c8:\nb = 2.94%, c = \u22120.95%, \u02dcb = 1.79%, \u02dcc = \u22120.66%.\n(14) is the purely dissipative Langevin equation, which is reminiscent of the earlier de-\nscription [20] of the dynamics of \ufb01nancial markets at intraday scales by another Langevin\nequation. In the theory of critical phenomena, the Langevin equation is well-known to de-\nscribe the dynamics of the order parameter of certain statistical mechanical systems near\nsecond-order phase transitions [32, 33]. This is consistent with the conjecture that the trend\nstrength (de\ufb01ned as either \u03c6 or \u03c8) plays the role of an order parameter, in analogy with the\nmagnetization in spin models.\nTo take the analogy further, statistical mechanical systems near second-order phase tran-\nsitions are characterized by universal critical exponents. E.g., a scalar \ufb01eld theory with a \u03c64\npotential similar to the potentials V in (13,14) describes water and steam and other physi-\ncal systems in the same universality class (such CO2 or the Ising model) near their critical\npoints [33]. For all systems within this universality class, the parameters b and c show the\nsame scaling behavior as a function of the length scale L (e.g., b \u223cL\u03ba for some exponent \u03ba).\nIn critical dynamics, scaling with L also translates into a scaling with the time horizon T [32].\nIn section 4, we have seen that - within the limits of statistical signi\ufb01cance - the values\nof the coe\ufb03cients b and c are the same for very di\ufb00erent markets, such as equity indices,\nbonds, FX-rates, and commodities. The parameters k0 and \u2206k in (11), which characterize\nhow b behaves under a rescaling of the time horizon T, are also the same. This could be an\nexpression of universality and scaling in \ufb01nancial markets. To con\ufb01rm this, it will be key to\nexamine how the scaling behavior in (11) extends to intra-day and multi-year time horizons\nT = 2k with k > 10 or k < 1.",
    "chunk_index": 18,
    "start_char": 44192,
    "end_char": 46974,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "limits of statistical signi\ufb01cance - the values\nof the coe\ufb03cients b and c are the same for very di\ufb00erent markets, such as equity indices,\nbonds, FX-rates, and commodities. The parameters k0 and \u2206k in (11), which characterize\nhow b behaves under a rescaling of the time horizon T, are also the same. This could be an\nexpression of universality and scaling in \ufb01nancial markets. To con\ufb01rm this, it will be key to\nexamine how the scaling behavior in (11) extends to intra-day and multi-year time horizons\nT = 2k with k > 10 or k < 1. For example, it might re\ufb02ect a complex critical exponent [34].\nTogether with the stochastic di\ufb00erential equations (13,14), the empirically observed scaling\nbehavior may uniquely specify a particular social network that models \ufb01nancial markets.\n26\n\nTo conclude this section, let us compare with some previous work. In [21], a related\nmodel for the dynamics of asset prices was postulated. The role of the trend was played by\nthe deviation of the current asset price from its unknown \u201dvalue\u201d. Terms of any order were\nconsidered in the polynomial potential, and the corresponding classical solutions were dis-\ncussed. Compared with [21], our trends are measurable, and we focus on a quartic potential,\nempirically observe the values of its coe\ufb03cients and their scale dependence, and provide a\nsimple and intuitive map between the quadratic (quartic) terms and trends (reversion).\nIn [22], another model with a polynomial random force similar to (10) was postulated.\nThe trend strength was de\ufb01ned by a moving average crossover (which does not lead to exact\ndi\ufb00erential operators such as (12) in the continuum limit). This model was applied in [23]\nto intraday returns for the USD/JPY and USD/EUR exchange rates during stress periods.\nInstead of our quartic potential with stable coe\ufb03cients, only a cubic potential was measured.\nMorevoer, its coe\ufb03cients, including their signs, were found to rapidly vary in time.\nHowever, these studies were based on very di\ufb00erent data sets, namely tick data (instead of\ndaily data) for single assets over time periods of several weeks (instead of decades). Thus, it\nis no surprise that the stable quartic potential (corresponding to the cubic trem in (10)) was\nnot found in [23]: as we have seen, in order to detect it with strong statistical signi\ufb01cance,\none needs not only decades of data, but also aggregate them over a broadly diversi\ufb01ed set\nof assets. Also, since the coe\ufb03cient of the cubic potential reported in [23] varies rapidly in\ntime, it can be expected to average out over long time scales. This is consistent with the\nfact that we do not observe a cubic potential in our empirical long-term analysis.\n6\nSummary and Discussion\nIn this paper, we have empirically observed the interplay of trends and reversion in \ufb01nan-\ncial markets, based on 30 years of daily futures returns across equity indices, interest rates,\ncurrencies and commodities. We have considered trends over ten di\ufb00erent time horizons of\nT = 2k days with k \u2208{1, 2, ..., 10}, ranging from 2 days to approximately 4 years. For a\n27",
    "chunk_index": 19,
    "start_char": 46446,
    "end_char": 49512,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "out over long time scales. This is consistent with the\nfact that we do not observe a cubic potential in our empirical long-term analysis.\n6\nSummary and Discussion\nIn this paper, we have empirically observed the interplay of trends and reversion in \ufb01nan-\ncial markets, based on 30 years of daily futures returns across equity indices, interest rates,\ncurrencies and commodities. We have considered trends over ten di\ufb00erent time horizons of\nT = 2k days with k \u2208{1, 2, ..., 10}, ranging from 2 days to approximately 4 years. For a\n27\n\ngiven market i on a given day t, we have de\ufb01ned the trend strength \u03c6i,k(t) as the statistical\nsigni\ufb01cance (t-statistics) of a smoothed version of its mean return over the past 2k days, in\nexcess of the market\u2019s long-term risk premium.\nOur key results, as illustrated in \ufb01gs 2 and 3, are the following: for a given market i and\neach time horizon labeled by k, tomorrow\u2019s normalized log-return Ri(t + 1) can accurately\nbe modeled by a cubic polynomial of today\u2019s trend strength in that market:\nRi(t + 1) = \u03b1i + b \u00b7 fk \u00b7 \u03c6i,k(t) + c \u00b7 \u03c63\ni,k(t) + \u03f5i(t + 1).\n(15)\nHere, \u03f5i represents random noise. \u03b1i is the normalized long-term risk premium of market i,\nwhich has not been not the focus of this paper. Instead, we have concentrated on determining\nthe coe\ufb03cients b, c, and the function fk, which measure how the expected return of an asset\nvaries in time. As discussed, we interpret b as the persistence of trends, and c as the strength\nof trend reversion. Within the limits of statistical signi\ufb01cance, we \ufb01nd that they are universal,\ni.e., the same for all assets. Over the past 30 years, we \ufb01nd from table 4:\nb \u223c+2.0% ,\nc \u223c\u22120.6%\n(16)\nWhile the strength of reversion is approximately constant, we \ufb01nd that the persistence of\ntrends depends on the time horizon of the trend. Within the range of time scales considered\nhere, it can be approximated by a parabolic function of the log of the time scale:\nfk \u223c1 \u2212(k \u2212k0)2\n\u2206k2\nwith\nk0 \u223c6 ,\n\u2206k \u223c5.\n(17)\nThis implies that trends may only be stable if the log of the time horizon is within the range\nk0 \u00b1 \u2206k, corresponding to time scales from a few days to several years. The parameters k0\nand \u2206k are also universal. By bootstrapping and cross-validation, we have found that all\nfour parameters in (16) and (17) are statistically highly signi\ufb01cant out-of-sample.\nLet us now discuss these results. First, they imply that trends tend to revert above a\ncritical trend strength, where the linear and cubic term in (15) balance each other. This\n28\n\ncritical trend strength lies below 2 in all cases. In other words, by the time a trend has be-\ncome statistically signi\ufb01cant, such that it is obvious in a price chart, it is already over.",
    "chunk_index": 20,
    "start_char": 48982,
    "end_char": 51682,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "signi\ufb01cant out-of-sample.\nLet us now discuss these results. First, they imply that trends tend to revert above a\ncritical trend strength, where the linear and cubic term in (15) balance each other. This\n28\n\ncritical trend strength lies below 2 in all cases. In other words, by the time a trend has be-\ncome statistically signi\ufb01cant, such that it is obvious in a price chart, it is already over. This\nsupports a variant of the e\ufb03cient market hypothesis [35, 36, 37]: ine\ufb03ciencies in \ufb01nancial\nmarkets are eliminated before they become strongly statistically signi\ufb01cant.\nDespite being insigni\ufb01cant, small trends can add value for investors through tactical asset\nallocation strategies, if accompanied by appropriate risk management and broad diversi\ufb01-\ncation across assets. While this paper does not recommend investment strategies, we note\nthat the inclusion of the cubic term in (15) appears to be a major improvement over classical\ntrend-following, as it takes investors out of trends before they are likely to revert (see also\nthe comments on systematic asset management in appendix A3). We believe that publishing\nsuch strategies and subjecting them to an academic discussion and independent review will\nensure a high level of professionality in asset management.\nTrend-following has been very successful in the 80\u2019s and 90\u2019s, when it was the proprietary\nstrategy of a limited number of traders. By now, large amounts of capital have \ufb02own into\nthis strategy, so it can no longer be expected to provide a \u201cfree lunch.\u201d Indeed, while we\nhave not observed a consistent weakening of the strength of reversion c, we have seen that\nthe persistence of market trends b has clearly decreased over the decades. This measures the\nrate, at which markets are becoming more e\ufb03cient with respect to trends.\nWhat will happen, when all investors try to exploit trends and reversion? Then both\nphenomena should weaken, until they earn a moderate equilibrium return that just compen-\nsates for the systematic risk of these strategies and their implementation costs. In this sense,\ntrend-following and mean reversion may just become \u201dalternative market factors\u201d as part\nof the general market portfolio. In fact, the weakening of b that we have observed here in-\ndicates that this development is already well underway at least for traditional trend-following.\nOn a conceptual level, our precise measurement of trends and reversion reveals intriguing\nanalogies with critical phenomena in physics. They support the conjecture that \ufb01nancial\n29\n\nmarkets can be modeled by statistical mechanical systems near second-order phase transi-\ntions. In such a model, Buy/Sell orders would represent microscopic degrees of freedom that\nlive on a \u201dsocial network\u201d of traders. The trend strength would play the role of an order\nparameter, whose dynamics is described by the stochastic di\ufb00erential equations (13,14). To-\ngether with an extension of the scaling behavior (17) to shorter and longer horizons, these\nequations provide an empirical starting point for developing such a model.\nIf such a statistical mechanical theory of \ufb01nancial markets can be established, it will\nintroduce powerful concepts from \ufb01eld theory into \ufb01nance, such as the renormalization group,\ncritical exponents, and Feynman diagrams.",
    "chunk_index": 21,
    "start_char": 51288,
    "end_char": 54563,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "microscopic degrees of freedom that\nlive on a \u201dsocial network\u201d of traders. The trend strength would play the role of an order\nparameter, whose dynamics is described by the stochastic di\ufb00erential equations (13,14). To-\ngether with an extension of the scaling behavior (17) to shorter and longer horizons, these\nequations provide an empirical starting point for developing such a model.\nIf such a statistical mechanical theory of \ufb01nancial markets can be established, it will\nintroduce powerful concepts from \ufb01eld theory into \ufb01nance, such as the renormalization group,\ncritical exponents, and Feynman diagrams. This will lead to a new and deeper understanding\nof \ufb01nancial markets, and phenomena such as trends, reversion, and shocks will become more\naccessible to scienti\ufb01c analysis. Further research in this direction is underway.\nAcknowledgements\nI would like to thank W. Breymann for discussions and encouragement to publish these\nobservations, and A. Ruckstuhl for advice on statistical matters. I would also like to thank J.\nBehrens for many interesting conversations and cooperation at Syndex Capital Management\nfrom 2008-2014, where some of these observations were initially made.\nThis research is\nsupported by grant no. CRSK-2 190659 from the Swiss National Science Foundation.\nReferences\n[1] For a popular review, see, e.g., Covel, M., 2007. The Complete TurtleTrader: The Legend,\nthe Lessons, the Results. Collins.\n[2] Cutler, D.M., Poterba, J.M. and Summers, L.H., 1991. Speculative dynamics. The Review\nof Economic Studies, 58(3), pp.529-546.\n[3] Silber, W.L., 1994. Technical trading: when it works and when it doesn\u2019t. The Journal\nof Derivatives, 1(3), pp.39-44.\n30\n\n[4] Asness, C.S., 1997. The interaction of value and momentum strategies. Financial Analysts\nJournal, 53(2), pp.29-36\n[5] Fung, W. and Hsieh, D.A., 1997. The risk in hedge fund strategies: Theory and evidence\nfrom trend followers. The review of \ufb01nancial studies 14, no. 2 (2001): 313-341.\n[6] Jaeger, L. and Wagner, C., 2005. Factor modeling and benchmarking of hedge funds:\ncan passive investments in hedge fund strategies deliver? The Journal of Alternative\nInvestments, 8(3), pp.9-36. Jaeger, L. and Pease, J., 2008. Alternative beta strategies\nand hedge fund replication. New York, NY: Wiley.\n[7] Mi\ufb00re, J. and Rallis, G., 2007. Momentum strategies in commodity futures markets.\nJournal of Banking & Finance, 31(6), pp.1863-1886.\n[8] Shen, Q., Szakmary, A.C. and Sharma, S.C., 2007. An examination of momentum strate-\ngies in commodity futures markets. Journal of Futures Markets: Futures, Options, and\nOther Derivative Products, 27(3), pp.227-256; Trend-following trading strategies in com-\nmodity futures: A re-examination. Journal of Banking & Finance, 34(2), pp.409.\n[9] Moskowitz, T.J., Ooi, Y.H. and Pedersen, L.H., 2012. Time series momentum. Journal\nof \ufb01nancial economics, 104(2), pp.228-250.\n[10] Menkho\ufb00, L., Sarno, L., Schmeling, M. and Schrimpf, A., 2012. Currency momentum\nstrategies. Journal of Financial Economics, 106(3), pp.660-684.\n[11] The Best of Strategies for the Worst of Times: Can Portfolios Be Crisis Proofed? Harvey,\nC.R., Hoyle, E., Rattray, S., Sargaison, M., Taylor, D. and Van Hemert, O., 2019. The\nJournal of Portfolio Management, 45(5), pp.7-28.\n[12] Baltas, N. and Kosowski, R., 2020. Demystifying time-series momentum strategies:\nVolatility estimators, trading rules and pairwise correlations.",
    "chunk_index": 22,
    "start_char": 53956,
    "end_char": 57366,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "series momentum. Journal\nof \ufb01nancial economics, 104(2), pp.228-250.\n[10] Menkho\ufb00, L., Sarno, L., Schmeling, M. and Schrimpf, A., 2012. Currency momentum\nstrategies. Journal of Financial Economics, 106(3), pp.660-684.\n[11] The Best of Strategies for the Worst of Times: Can Portfolios Be Crisis Proofed? Harvey,\nC.R., Hoyle, E., Rattray, S., Sargaison, M., Taylor, D. and Van Hemert, O., 2019. The\nJournal of Portfolio Management, 45(5), pp.7-28.\n[12] Baltas, N. and Kosowski, R., 2020. Demystifying time-series momentum strategies:\nVolatility estimators, trading rules and pairwise correlations. Market Momentum: Theory\nand Practice\u201d, Wiley.\n[13] Lemp\u00b4eri`ere, Y., Deremble, C., Seager, P., Potters, M., and Bouchaud, J. -P. 2014. \u201cTwo\nCenturies of Trend Following.\u201d Journal of Investment Strategies 3 (3): 41\u201361.\n31\n\n[14] Hurst, B., Ooi, Y.H. and Pedersen, L.H., 2017. A century of evidence on trend-following\ninvesting. The Journal of Portfolio Management, 44(1), pp.15-29;\n[15] Lim, B., Zohren, S. and Roberts, S., 2019. Enhancing time-series momentum strategies\nusing deep neural networks. The Journal of Financial Data Science, 1(4), pp.19-38.\n[16] Baz, J., Granger, N., Harvey, C.R., Le Roux, N. and Rattray, S., 2015. Dissecting\ninvestment strategies in the cross section and time series. Available at SSRN 2695101.\n[17] Levine, A. and Pedersen, L.H., 2016. Which trend is your friend? Financial Analysts\nJournal, 72(3), pp.51-66.\n[18] Bruder, B., Dao, T.L., Richard, J.C. and Roncalli, T., 2011. Trend \ufb01ltering methods\nfor momentum strategies. Available at SSRN 2289097.\n[19] Harvey, C.R., Liu, Y. and Zhu, H., 2016. . . . and the cross-section of expected returns.\nThe Review of Financial Studies, 29(1), pp.5-68.\n[20] Bouchaud, J.P. and Cont, R., 1998. A Langevin approach to stock market \ufb02uctuations\nand crashes. The European Physical Journal B-Condensed Matter and Complex Systems,\n6(4), pp.543-550.\n[21] Ide, K. and Sornette, D., 2002. Oscillatory \ufb01nite-time singularities in \ufb01nance, population\nand rupture. Physica A: Statistical Mechanics and its Applications, 307(1-2), pp.63-106.\n[22] Takayasu, M., Mizuno, T. and Takayasu, H., 2006. Potential force observed in market\ndynamics. Physica A: Statistical Mechanics and its Applications, 370(1), pp.91-97.\n[23] Watanabe, K., Takayasu, H. and Takayasu, M., 2009. Random walker in temporally\ndeforming higher-order potential forces observed in a \ufb01nancial crisis. Physical Review E,\n80(5), p.056110.\n[24] Mantegna, R.N. and Stanley, H.E., 1999. Introduction to econophysics: correlations and\ncomplexity in \ufb01nance. Cambridge university press.\n32\n\n[25] T. Lux, M. Marchesi, Scaling and criticality in a stochastic multi-agent model of a\n\ufb01nancial market, Nature 297 (1999) 498\u2013500.\n[26] Farmer, J.D. and Joshi, S., 2002. The price dynamics of common trading strategies.\nJournal of Economic Behavior and Organization, 49(2), pp.149-171.\n[27] Sornette, D., 2014. Physics and \ufb01nancial economics (1776\u20132014): puzzles, Ising and\nagent-based models. Reports on progress in physics, 77(6), p.062001.\n[28] D.J. Watts and S.H. Strogatz, \u201dCollective dynamics of \u2019small-world\u2019 networks\u201d, Nature\n393 (1998)\n[29] Barab\u00b4asi, A.L. and Albert, R., 1999. Emergence of scaling in random networks. Science,\n286(5439), pp.509-512.\n[30] Br\u00b4ezin, E., Itzykson, C., Parisi, G. and Zuber, J.B., 1993. Planar diagrams. In The\nLarge N Expansion In Quantum Field Theory And Statistical Physics (pp. 567-583).",
    "chunk_index": 23,
    "start_char": 56771,
    "end_char": 60209,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "Sornette, D., 2014. Physics and \ufb01nancial economics (1776\u20132014): puzzles, Ising and\nagent-based models. Reports on progress in physics, 77(6), p.062001.\n[28] D.J. Watts and S.H. Strogatz, \u201dCollective dynamics of \u2019small-world\u2019 networks\u201d, Nature\n393 (1998)\n[29] Barab\u00b4asi, A.L. and Albert, R., 1999. Emergence of scaling in random networks. Science,\n286(5439), pp.509-512.\n[30] Br\u00b4ezin, E., Itzykson, C., Parisi, G. and Zuber, J.B., 1993. Planar diagrams. In The\nLarge N Expansion In Quantum Field Theory And Statistical Physics (pp. 567-583).\n[31] Cimini, G., Squartini, T., Saracco, F., Garlaschelli, D., Gabrielli, A. and Caldarelli, G.,\n2019. The statistical physics of real-world networks. Nature Reviews Physics, 1(1).\n[32] Hohenberg, P.C. and Halperin, B.I., 1977. Theory of dynamic critical phenomena. Re-\nviews of Modern Physics, 49(3), p.435.\n[33] J. Zinn-Justin, Quantum Field Theory and Critical Phenomena, Oxford U. Press, 1989\n[34] Sornette, D., 1998. Discrete-scale invariance and complex dimensions. Physics reports,\n297(5), pp.239-270.\n[35] Mandelbrot, Benoit (January 1963). \u201dThe Variation of Certain Speculative Prices\u201d. The\nJournal of Business. 36 (4): 394. doi:10.1086/294632. ISSN 0021-9398.\n[36] Samuelson, Paul (1965). \u201dProof That Properly Anticipated Prices Fluctuate Ran-\ndomly\u201d. Industrial Management Review. 6: 41\u201349.\n[37] Fama, Eugene (1970). \u201dE\ufb03cient Capital Markets: A Review of Theory and Empirical\nWork\u201d. Journal of Finance. 25 (2): 383. doi:10.2307/2325486. JSTOR 2325486.\n33\n\nAppendix\nThe following table compares the regression results of subsection 4.2 with the results that\nwould be obtained for alternative choices of some of the parameters:\nTable 6\nDi\ufb00erent Caps/Floors\nInclusion of Risk Premia\nCap/Floor\n2.0\n2.25\n2.5\n2.75\n3.0\n0%\n50%\n100%\nb\n2.05%\n2.03%\n2.00%\n1.95%\n1.88%\n2.00%\n2.18%\n2.32%\nt-stat.\n4.0\n4.0\n4.2\n3.7\n3.6\n4.2\n4.4\n3.3\nc\n-0.75%\n-0.69%\n-0.63%\n-0.58%\n-0.53%\n-0.63%\n-0.62%\n-0.53%\nt-stat.\n2.7\n2.8\n2.6\n2.5\n2.4\n2.6\n2.6\n2.9\nk0\n5.90\n5.85\n5.78\n5.72\n5.69\n5.78\n6.75\n8.21\n\u2206k\n4.87\n4.90\n4.87\n4.77\n4.67\n4.87\n5.96\n7.20\nR2\nadj\n1.12\n1.39\n1.64\n1.82\n1.96\n1.64\n1.64\n1.59\nR2\nadj aggr.\n0.74\n0.98\n1.22\n1.40\n1.54\n1.22\n1.19\n1.22\nA1. Caps and Floors for the Trend Strength\nIn section 2, we have capped the magnitude of the trend strength at 2.5 to limit the e\ufb00ect\nof outliers on the results. Table 6 (col. 2-6) compares the results of section 4.2 for the\nalternative caps/\ufb02oors of 2.0, 2.25, 2.5, 2.75, and 3.0. We observe the following:\n\u2022 Increasing the cap beyond 2.5 increases the adjusted R-squared, but decreases the\nsigni\ufb01cance (t-statistics) of the regression betas. This makes sense intuitively, because\nthe results are now dominated by the regime of strong reversion at \u201doutlier\u201d trend\nstrength |\u03c6| > 2.5. As such outliers are rare, the statistical signi\ufb01cance decreases.\n\u2022 Decreasing the cap below 2.5 decreases the adjusted R-squared without improving the\noverall signi\ufb01cnce of the regression betas. This is also understandable, as it removes\nmuch of the reversion regime from the analysis. Thus, our cap/\ufb02oor of \u00b12.5 is a good\ncompromise, where neither trends nor reversion outliers dominate the results.\n34",
    "chunk_index": 24,
    "start_char": 59669,
    "end_char": 62808,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "trend\nstrength |\u03c6| > 2.5. As such outliers are rare, the statistical signi\ufb01cance decreases.\n\u2022 Decreasing the cap below 2.5 decreases the adjusted R-squared without improving the\noverall signi\ufb01cnce of the regression betas. This is also understandable, as it removes\nmuch of the reversion regime from the analysis. Thus, our cap/\ufb02oor of \u00b12.5 is a good\ncompromise, where neither trends nor reversion outliers dominate the results.\n34\n\nA2. Long-term Risk Premia\nIn equation (2), we have removed the long-term risk premia \u00b5i from the trend strengths \u03c6i,T.\nHere, we explain what happens if we do not remove the risk premia from the trend strengths:\n\u2022 Trends in markets, for which such risk premia are generally assumed, would then have\nan upward bias, i.e., positive expectation value. Especially very-long-term trends in\nequity and bond markets would almost always be positive and never revert.\n\u2022 Table 6 (col. 7-9) shows how this mix-up of trends and risk premia would modify the\nresults of subsection 4.2 (shown under \u201d0%\u201d). If 50% or 100% of the risk premia were\nincluded in the de\ufb01nition of the trend strength, the parameter k0 (which measures the\ntime horizon at which the persistence b of trends peaks) would strongly increase.\n\u2022 In fact, if we think of risk premia as trends with in\ufb01nite time horizon, we expect that,\nwithout removing risk premia, the trending regime of sub-section 4.2 would extend all\nthe way to in\ufb01nite horizon. We would then model it by a parabola instead of an ellipse.\nA3. Comments on Systematic Asset Management\nThe key motivation for this article is to lay the empirical basis for a statistical-mechanical\nmodel of \ufb01nancial markets, which can hopefully explain the analogies with critical phenomena\nin physics. Nevertheless, let us brie\ufb02y comment on implications for systematic trading:\n\u2022 According to the back-of-the-envelope estimate of sub-section 3.3, an annualized Sharpe\nratio of order 1 for a systematic futures trading strategy corresponds to an adjusted\nR-squared of about 4 basis points in predicting daily returns of individual markets.\n\u2022 By the same argument, the aggregated adjusted R-squared of 6 basis points of sub-\nsection 4.2. corresponds to an annual Sharpe ratio of\n\u221a\n1.5 for a market-neutral strategy.\n\u2022 Trading costs reduce the Sharpe ratio, especially at intra-month time horizons. Diversi-\nfying into new types of markets or including risk premia can increase the Sharpe ratio.\n\u2022 Risk control mechanisms, such as sizing positions based on current market volatility,\nor stop-losses in the reversion regime, can either decrease or increase the Sharpe ratio.\n35",
    "chunk_index": 25,
    "start_char": 62378,
    "end_char": 64987,
    "paper_title": "Trends Reversion and Critical Phenomena in Financi",
    "paper_category": "q-fin.ST",
    "paper_filename": "Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.ST/Trends_Reversion_and_Critical_Phenomena_in_Financi.pdf"
  },
  {
    "text": "arXiv:1005.0182v2 [q-fin.TR] 28 Oct 2010\nEPJ manuscript No.\n(will be inserted by the editor)\nA Multi Agent Model for the Limit Order Book Dynamics\nM. Bartolozzi1,2\n1 Research Group, Boronia Capital, Sydney NSW 2065\n2 Australia Special Research Centre for the Subatomic Structure of Matter (CSSM), University of Adelaide, Adelaide SA 5005\nthe date of receipt and acceptance should be inserted later\nAbstract. In the present work we introduce a novel multi-agent model with the aim to reproduce the\ndynamics of a double auction market at microscopic time scale through a faithful simulation of the matching\nmechanics in the limit order book. The agents follow a noise decision making process where their actions\nare related to a stochastic variable, the market sentiment, which we de\ufb01ne as a mixture of public and\nprivate information. The model, despite making just few basic assumptions over the trading strategies of\nthe agents, is able to reproduce several empirical features of the high-frequency dynamics of the market\nmicrostructure not only related to the price movements but also to the deposition of the orders in the\nbook.\nPACS.\nMarket Microstructure \u2013 Econophysics \u2013 Detrended Fluctuation Analysis \u2013 Multi-Agent Models\n1 Introduction\nIn the past few years, following the increasing power of\ntechnological infrastructures, high-frequency trading, which\nbroadly speaking includes every strategies which holding\nperiod is shorter than a day, has bloomed among the ma-\njor \ufb01nancial institutions around the globe and, nowadays,\nit accounts for about 70% of all the volume traded in US\nequities. The same trend is evident also in Europe where\nthe number of transactions for the most liquid contracts\nhas recently experienced an exponential growth: in the\nEurex Stoxx futures index, for example, the number of\ndaily trades has gone from 12500 in January 2005 up to a\nmaximum of 150000 in November 2008 (Bartolozzi et al.,\n2007a). As a consequence for the growing interest in the\nshort time scales, the exchanges has started to provide live\nfeeds of every single order submitted (buy or sell), there-\nfore, making the market microstructure a primary \ufb01eld of\ninterest for the \ufb01nancial practitioners.\nThe progressive shift towards the very high-frequencies\nhas not been unnoticed in the Econophysics community\nwhich, in the meanwhile, has become an established \ufb01eld\nof study among physicists (Bouchaud and Potters, 1999;\nMantegna and Stanley, 1999; Paul and Baschnagel, 1999;\nVoit, 2005). In the multi-agent framework, a common fea-\nture in the physicist\u2019s approach to the market microstruc-\nture of order driven markets is the lack of a proper utility\nfunction, which is often invoked in Economics/Econometric\nliterature in order to study the rational behaviour of the\nagents (Bailey, 2005), and, consequently, a simple statisti-\ncal approach to the problem is preferred (Bak et al., 1997;\nSend o\ufb00print requests to:\nmbartolo@physics.adelaide.edu.au\nMaslov, 2000; Matassini and Franci, 2001; Raberto et al.,\n2001; Smith et al., 2003; Daniels et al., 2003; Iori et al.,\n2003; Farmer et al., 2005; Mike and Farmer, 2008; Zaccaria et al.,\n2010).",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3141,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "which is often invoked in Economics/Econometric\nliterature in order to study the rational behaviour of the\nagents (Bailey, 2005), and, consequently, a simple statisti-\ncal approach to the problem is preferred (Bak et al., 1997;\nSend o\ufb00print requests to:\nmbartolo@physics.adelaide.edu.au\nMaslov, 2000; Matassini and Franci, 2001; Raberto et al.,\n2001; Smith et al., 2003; Daniels et al., 2003; Iori et al.,\n2003; Farmer et al., 2005; Mike and Farmer, 2008; Zaccaria et al.,\n2010). This line of thought has been referred to as zero in-\ntelligence (Farmer et al., 2005): a recent review on some\nof the models proposed so far can be found in (Slanina,\n2008). For an Econometric approach to the market mi-\ncrostructure, instead, the reader can refer to (O\u2019Hara,\n1997).\nOrder driven markets are based on the principle of\ncontinuous double auction where the price of an asset is\ncontinuously adjusted in order to account for the in\ufb02ow\nof demand and supply. In particular, the orders placed by\nthe traders, that is the number of contracts they want to\nbuy or sell along with the corresponding price, are orga-\nnized and matched in the limit order book (LOB) which\nultimately de\ufb01nes the microstructure of the market. At\nevery instant in time, the LOB, a sketch of which is given\nfor clarity in Fig. 1, is described by two sets of limit or-\nders in opposite \u201cdirections\u201d, one for the long orders (buy\nside) and one for the short (sell side). Each order is char-\nacterized by a limit price, that is the price the trader\nis willing to buy/sell, and the number of contracts re-\nquested/o\ufb00ered, or volume. The minimum price di\ufb00erence\ngreater than zero between two orders is called tick size\nand, in real markets, depends on the speci\ufb01c contract.\nWhen more than one limit order is send to the same price\nthen the exchange rank them in a stack by arrival time:\nthe \ufb01rst to be executed will be the oldest1. The mid-point\n1 Note that while this ranking describes a typical situation,\nthe execution of limit orders can slightly change depending on\nthe rules of the speci\ufb01c exchange\n\n2\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\nFig. 1. Cartoon representation of the LOB. Note that gaps\nbetween price levels can be present, especially in non-liquid\ncontracts.\nprice, Pm, which is usually referred as the \u201cprice\u201d of an\nasset, is the mean value between the best ask and the best\nbid, being, respectively, the lowest price on the ask side\nand the higher price on the buy of the LOB. The execu-\ntion of a limit order, that is the actual trade, is triggered\nif, and only if, an order in the opposite direction matches\nits price quote. The incoming order can either be another\nlimit order, in which case the order executed is at best ask\nor bid, or a market order, that is a request to execute a\ncertain volume, starting from the best price, immediately\nand until it has been completely \ufb01lled.",
    "chunk_index": 1,
    "start_char": 2662,
    "end_char": 5555,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "side\nand the higher price on the buy of the LOB. The execu-\ntion of a limit order, that is the actual trade, is triggered\nif, and only if, an order in the opposite direction matches\nits price quote. The incoming order can either be another\nlimit order, in which case the order executed is at best ask\nor bid, or a market order, that is a request to execute a\ncertain volume, starting from the best price, immediately\nand until it has been completely \ufb01lled. This latter type of\norder is considered very \u201caggressive\u201d given that the price\nof the spread is paid upfront and it is usually associated\nwith \u201cimpatience\u201d traders. Cancellations also play an im-\nportant role in the dynamics of the LOB. In fact, limit\norders that have not been \ufb01lled in a reasonable amount\nof time, according to the trader\u2019s necessity, are usually\nremoved from the LOB.\nIn the present work we introduce a novel multi-agent\nmodel2 with the aim to reproduce the dynamics of the\nmarket microstructure. Our agents relate their actions to\na stochastic variable, the \u201cmarket sentiment\u201d, which in-\ncludes feedbacks from both public and private informa-\ntion. Besides, the model relies on a realistic LOB me-\nchanics which takes into account every possible change\non \u201cevent\u201d base such as the arrival of limit orders, market\norders or cancelations as well as a faithful order matching\nmechanism.\nThe paper is organize as following: in the next sec-\ntion we introduce our model while the numerical results\n2 For \u201cagents\u201d here we indicate all possible \ufb01nancial institu-\ntions such as investment banks, hedge funds etc... rather that\nthe occasional trader.\nof the simulations are presented in Sec. 3. Discussions and\nconclusion are left in the last section.\n2 The Model\n2.1 Brief outline\nOur market model evolves in discrete time steps3 during\nwhich each agent may undertake a certain action or just\nwait for a more pro\ufb01table opportunity. These actions can\nbe broadly divided into cancellation and active trading,\nthe latter including both limit and market orders. In par-\nticular, all decision steps are based on dynamical proba-\nbilities which are function of both the private and public\ninformation, the former being related to the \u201cstate\u201d of the\nmarket as we will see in the following sections. Moreover,\nwe assume that our agents can have just one open posi-\ntion at the time and, therefore, before issuing a new order\nthey need to close their current position. By using this\nlimitation we neglect explicit market making activity.\nAnother central part of our model is the order gener-\nation step where the speci\ufb01cations for each order such as\ntype (limit or market), price (for limit orders) and volume\nare decided. In the next sections we explain in details all\nthe fundamental building blocks.\n2.2 Public and private information update\nAt the beginning of each time step, we assume that the\nagents have access to the current state of the LOB, that\nis they can",
    "chunk_index": 2,
    "start_char": 5099,
    "end_char": 8027,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "explicit market making activity.\nAnother central part of our model is the order gener-\nation step where the speci\ufb01cations for each order such as\ntype (limit or market), price (for limit orders) and volume\nare decided. In the next sections we explain in details all\nthe fundamental building blocks.\n2.2 Public and private information update\nAt the beginning of each time step, we assume that the\nagents have access to the current state of the LOB, that\nis they can \u201csee\u201d every order placed in the market: all\nthe indicators derived by this knowledge, such as the mid-\npoint price for example, are classi\ufb01ed as public informa-\ntion. However, these are not used in their \u201craw\u201d form but\nthey get \u201csmoothed\u201d: this \ufb01ltered information, that we re-\nfer as \u201cperceived\u201d, is the one which is actually used in the\nprocess of decision making. The reason behind this extra\nstep is to take our simulations one step closer to reality\nwhere indicators are usually pre-processed in order to get\nrid of some noise4. The \ufb01lter used in our simulations is an\nexponential moving average (EMA) (Fusai and Ronconi,\n2008) which is de\ufb01ned for a generic time series x(t) as\n\u02c6x(t) = 1\nLx(t) +\n\u0012\n1 \u22121\nL\n\u0013\n\u02c6x(t \u22121),\n(1)\nwhere L is proportional to the memory of the process.\nBy using the previous equation Eq.(1), we de\ufb01ne the per-\nceived volatility as\n\u02c6\u03bd(t) =\nq\n\u02c6r2(t),\n(2)\n3 It is important to stress that in real markets the arrival\nof new information at microscopic time scale is heterogeneous\nin time and often people consider the trade time as a more\nappropriate scale. While the output of the model is updated in\ndiscrete time steps for simplicity, the LOB is changed on event\nbase.\n4\nAs a consequence, short term memory is induced in the\nsignal itself.\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n3\nbeing r the one step return of the mid point price\nr(t) = Pm(t)\u2212Pm(t\u22121). Note that, for time being, we have\nnot included in the public information any news realize,\nthe importance of which has been questioned over time,\nsee\n(Cutler et al., 1989) for example or\n(Joulin et al.,\n2008) for a more recent criticism.\nRegarding the private information, instead, we assume\nthat it can be represented by a simple Gaussian process,\nindependent for each trader, with zero mean and standard\ndeviation proportional to the perceived volatility of the\nmarket, \u02c6\u03bd(t): this information can be thought to represent\nthe convolution of the trading strategies used by the agent.\n2.3 Cancellation of orders form the LOB\nAt each time step, agents having an outstanding order\nin the LOB will evaluate the possibility of removing it\naccording to two di\ufb00erent criteria. The \ufb01rst corresponds\nto a simple time-out, that is the order is automatically\nremoved if it has not been executed in a Tmax number\nof time steps relatively long if compared to the average\ntime for a transaction and \ufb01xed to 100 time steps in our\nsimulations.",
    "chunk_index": 3,
    "start_char": 7564,
    "end_char": 10468,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "the convolution of the trading strategies used by the agent.\n2.3 Cancellation of orders form the LOB\nAt each time step, agents having an outstanding order\nin the LOB will evaluate the possibility of removing it\naccording to two di\ufb00erent criteria. The \ufb01rst corresponds\nto a simple time-out, that is the order is automatically\nremoved if it has not been executed in a Tmax number\nof time steps relatively long if compared to the average\ntime for a transaction and \ufb01xed to 100 time steps in our\nsimulations. The second criterion, instead, is related to a\nstrategic decision based on the current market condition.\nIn particular, we de\ufb01ne a cancellation probablity, \u03c8\u02c6\u03bd(t) \u2208\n[0, 1], common to all the agents as\n\u03c8\u02c6\u03bd(t) = 1 \u2212e\u2212\u03b3 \u02c6\u03bd(t),\n(3)\nbeing \u03b3 = 0.02 a sensitivity parameter. The former ex-\npression, which relies just on the perceived volatility \u02c6\u03bd(t)\nin terms of information, is justi\ufb01ed by the empirical ob-\nservation that the cancellation frequency increases when\nthe market is highly volatile: this is clearly a case of self-\nreinforcing e\ufb00ect triggered by the herding behaviour of the\nmarket participants (Cont and Bouchaud, 2000; Bartolozzi and Thomas,\n2004).\n2.4 Active trading and market sentiment\nWhile the cancellation step takes place, agents with no\norders in the LOB evaluate the possibility to enter the\nmarket. Their decision is based on a stochastic variable\nwhich represents the \u201clevel of con\ufb01dence\u201d in their price\nforecast: the market sentiment, \u03c6. This quantity, which\nis a core part of our model, relates the public and the\nprivate information through a multiplicative process and,\nspeci\ufb01cally, for the ith agent at time t we de\ufb01ne\n\u03c6i(t) = \u03c60 \u00b7 \u03bai \u00b7 \u03c8\u22c6\n\u02c6\u03bd(t) \u00b7 \u03b7(t) \u00b7 \u01ebi(t),\n(4)\nwhere each terms represents a di\ufb00erent aspect which may\nimpact on the decision making process as following:\n\u2013 \u03c60 is a strength parameter common to all the agents\nwhich, as we will see in the next section, \ufb01xes the trad-\ning frequency of the model and, therefore, the time\nscale.\n\u2013 The parameter \u03bai represents the agent\u2019s degree of risk\naversion and it used in order to diversify the appetite\nfor risk. Its value is randomly selected, at the beginning\nof the simulation, from a uniform distribution bounded\nin [0.25, 0.75].\n\u2013 \u03c8\u22c6\n\u02c6\u03bd = 1\u2212\u03c8\u02c6\u03bd(t) is the volatility risk and mimics the fact\nthat traders are more cautious to send orders during\nhigh volatility periods given that the risk associated to\nthese is higher as well.\n\u2013 \u03b7(t), instead, is a proxy for the liquidity risk: a sparse\nLOB has a potential large negative impact on the exe-\ncution of a trade and, therefore, it decreases the prob-\nability that an agent is willing to accept the risk. In\nthe present work we assume \u03b7(t) = \u00af\nN(t)/N where N is\nthe maximum number of orders in the LOB, equivalent\nalso to the total number of agents in the simulation,\nand \u00afN(t) the number of orders in the LOB at time\nstep t.",
    "chunk_index": 4,
    "start_char": 9964,
    "end_char": 12827,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "for the liquidity risk: a sparse\nLOB has a potential large negative impact on the exe-\ncution of a trade and, therefore, it decreases the prob-\nability that an agent is willing to accept the risk. In\nthe present work we assume \u03b7(t) = \u00af\nN(t)/N where N is\nthe maximum number of orders in the LOB, equivalent\nalso to the total number of agents in the simulation,\nand \u00afN(t) the number of orders in the LOB at time\nstep t. From the de\ufb01nition we have that \u03b7(t) \u2208[0, 1].\n\u2013 \u01ebi(t) represents the private information which is drawn\nfrom a Gaussian distribution with zero mean and stan-\ndard deviation equal to \u02c6\u03bd(t), \u01ebi(t) = \u02c6\u03bd(t) \u00b7 G(0, 1).\nThe market sentiment, Eq.(4), can be thought as the\nconvolution between the agent\u2019s trading strategies, the\nprivate information, and the risk factors evaluated via the\npublic information (volatility and liquidity in this case):\nthe stronger is the signal the more likely will be for the\ntrader to take a decision.\nThe next step involves the mapping of \u03c6i(t) into a trad-\ning probability, pi(t) \u2208[0, 1], via a the following transfer\nfunction\npi(t) = 2\n\u03c0 |arctg [\u03c6i(t)] |,\n(5)\nwhich represents the probability for the ith agent to sub-\nmit an order at time step t. Conversely, an agent will not\ntake any action with probability 1 \u2212pi(t).\nThe direction of a trade, assuming that short selling\nis always allowed, di(t) = +1 for long orders (buy) and\ndi(t) = \u22121 for short orders (sell), is also derived from the\nmarket sentiment according to\ndi(t) = \u03c6i(t)\n|\u03c6i(t)|,\n(6)\nas if \u03c6i(t) was a momentum indicator for the market\ndirection. Besides it is worth underlying that, according\nto the previous de\ufb01nition, the direction of a trade depends\nonly on the sign of the private information, Eq.(4), and,\ntherefore, there is 50% chances for a trade to be a buy or\na sell5.\n2.5 Order generation\nEach time an agent submits an order to the market its\nspeci\ufb01cations are de\ufb01ned in order generation step which\naddresses the following two points:\n5 The market sentiment, \u03c6i(t), and consequently the trade\ndirection, can be statistically skewed in one direction depend-\ning on di\ufb00erent market factors and, therefore, leading to herd-\ning e\ufb00ects. The study of this phenomenon will be the topic of\nfuture investigations.\n\n4\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n\u2013 Limit or market order? The decision to submit a limit\nor a market order is related to the impatience of the\ntrader: if an order needs to be \ufb01lled as soon as possible\nhe/she may accept to pay the cost of the spread up-\nfront and send a market one. The alternative would be\na less aggressive limit order that will execute just at\na pre-\ufb01xed price: the further away from the opposite\nbest the more time the order will require to get \ufb01lled.",
    "chunk_index": 5,
    "start_char": 12410,
    "end_char": 15150,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "related to the impatience of the\ntrader: if an order needs to be \ufb01lled as soon as possible\nhe/she may accept to pay the cost of the spread up-\nfront and send a market one. The alternative would be\na less aggressive limit order that will execute just at\na pre-\ufb01xed price: the further away from the opposite\nbest the more time the order will require to get \ufb01lled.\n\u2013 Size of the order? The number of lots that the trader\nis willing to buy or sell can be related to several fac-\ntors such as to the speci\ufb01c execution strategy, to the\navailable liquidity in the market, the volatility at that\npoint in time etc... Moreover, in a real trading envi-\nronment single large orders are usually splitted into\nsmaller ones in order to minimize the cost related to\ntheir impact. While the latter is an important issue\nin practice, in order to keep things simple, we do not\ntackle this problem in the present work: orders are sent\nin just one single chunk.\nIn our model the \ufb01rst point is addressed by setting\nthe submission price of an order, which can be interpreted\nas the degree of \u201caggressiveness\u201d, probabilistically via a\nvalue drawn from log-normal distribution6, \u03be, as following\nPl = Pb \u2212(\u03be \u2212QP ),\n(7)\nfor a long order (buy) Pl while for a short (sell), Ps\nPs = Pa + (\u03be \u2212QP ),\n(8)\nbeing Pb and Pa the best bid and the best ask, respec-\ntively, and QP the q-quantile, q = 0.5 in our case, of the\ndistribution. Besides, the value of \u03be is rounded up to an\ninteger value and, therefore, \ufb01xing, without losing of gen-\nerality, the tick size of our market to 1. It is also important\nto stress that following the former procedure we explicitly\nassume that limit orders can be submitted asymptotically\nfar from the best price as shown in the sketched in Fig. 2.\nAfter the submission price has been \ufb01xed, the type of\nthe order is decided based on its relative position to the\nbest prices: if the submission price results to be greater\nthan the ask price and the order is long (or lower than\nthe bid price and the trade is short) then we interpret this\nas a market order and all the volume will be completely\n\ufb01lled starting from the best price. All the other orders are\nconsidered limit orders and, for a \ufb01xed price, they are or-\nganized in stacks from the oldest to the newest being the\nformers the \ufb01rst to be executed7. This modelling approach\nis motivated by two empirical observations: the distribu-\ntion of the volumes in the LOB, when averaged over long\nperiods, displays \u201cfat tails\u201d and that the frequency of mar-\nket order represents just a fraction of that of limit orders\nsubmitted in the market (Bouchaud et al., 2002).\n6 The parameters used in the simulations for the log-normal\ndistribution are 7 for the mean and 10 for the standard devia-\ntion.",
    "chunk_index": 6,
    "start_char": 14789,
    "end_char": 17530,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "to the newest being the\nformers the \ufb01rst to be executed7. This modelling approach\nis motivated by two empirical observations: the distribu-\ntion of the volumes in the LOB, when averaged over long\nperiods, displays \u201cfat tails\u201d and that the frequency of mar-\nket order represents just a fraction of that of limit orders\nsubmitted in the market (Bouchaud et al., 2002).\n6 The parameters used in the simulations for the log-normal\ndistribution are 7 for the mean and 10 for the standard devia-\ntion.\n7\nIf the submission price coincides with the opposite best\nwe give 50% probability for the trade to be a limit order or a\nmarket order.\nFig. 2. The price at which an order is submitted, and therefore\nits \u201caggressiveness\u201d, is drown from a log-normal distribution\nwhich q-quantile is centered at the best bid (long order) or best\nask (short order). The \u201cdirection\u201d of the distribution is also\nchosen in accordance with the order direction. In the cartoon\nabove we sketch the probability of price submission for a long\n(buy) order.\nThe second issue concerns the order size, that is the\nnumber of contracts, or volume, that an agent is willing\nto buy or sell. Coherently with the stochastic nature of\nthis process, this is drawn from a log-normal distribution8,\nrounded to integer, with the following constrains: the or-\nder has to be greater or equal than one and smaller than\none quarter of the total volume contained in the appro-\npriate side of the LOB9.\nBefore moving to the next section, we wish to under-\nline that the order generation step used in our model is\njust a \ufb01rst order approximation of what really happens\nin real execution strategies. In particular, we do not ad-\ndress the problem of splitting large volumes into smaller\nones, typically used in order to minimize the transaction\ncosts: this issue, despite being very important for market\nimpact, goes beyond the scope of this work at present.\n8 The parameters of the distribution used in the simulations\nare the same as for the price submission that is 7 for the mean\nand 10 for the standard deviation.\n9 In the simulation this limit is hardly reached: usually, single\norders tend to be relatively small compared to the total volume\navailable in the LOB. Moreover, before submitting a market\norder we also perform a liquidity check: if the number of lots\nin the requested side of the LOB is less or equal Nmin, with\nNmin = min(50, N/10), then the order is not issued. Liquidity\nchecks on the volumes in the book are often done also in real\ntrading in order to have an estimate of the risk associated with\nthe trade itself.\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n5\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n2\n4\n6\n8\n10\n12\n14\n16\n18\n\u03ba\n \u03c60\nFig. 3. Kurtosis of the one step returns estimated over an\nensemble of three simulations each one consisting in 105 time\nsteps.",
    "chunk_index": 7,
    "start_char": 17035,
    "end_char": 19892,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "order is not issued. Liquidity\nchecks on the volumes in the book are often done also in real\ntrading in order to have an estimate of the risk associated with\nthe trade itself.\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n5\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n2\n4\n6\n8\n10\n12\n14\n16\n18\n\u03ba\n \u03c60\nFig. 3. Kurtosis of the one step returns estimated over an\nensemble of three simulations each one consisting in 105 time\nsteps. It is noticeable how this quantity converges to 3, the\nvalue for a Gaussian process, as \u03c60 increases. By decreasing\n\u03c60, instead, the activity gets very low and the dynamics of\nthe model starts to be ruled by few large market movements.\nThe parameters for these simulation have been declared in the\nprevious section except for the EMA history L, Eq.(1), which\nin this instance has been \ufb01xed to 5.\n3 Numerical simulations at short time scales\nIn this section we report the results of the numerical sim-\nulations of our microscopic market model for N = 10000\nagents10. The trading frequency, which can be interpreted\nas a time scale for the simulation, is \ufb01xed by the param-\neter \u03c60: as its value gets smaller the trading activity be-\ncomes relatively lower and gaps within the LOB, leading\nto large price changes, are more likely to appear. On the\nother hand, if the activity is very high, the book is almost\nalways full and large \ufb02uctuations will occur more sporad-\nically. The scaling of the \ufb02uctuations with the activity is\nemphasized in Fig. 3 where we report the kurtosis, \u03ba, of\nthe one step returns against \u03c60.\nFor \u03c60 = 0.165, the value that we use in the rest of\nthis section, the activity resembles that of the market at\nvery short time scales, from seconds to minutes depending\non the speci\ufb01c contract, where non-Gaussian \ufb02uctuations\nplay a fundamental role. In this regime, which is the one\nof interest in the current work, there is a probability of\napproximately 30% for the mid-point price to remain un-\nchanged after one time step.\nAnother important parameter is the EMA history, L\nin Eq.(1). In fact, we have found that activity clustering,\nsuch as high volatile periods, are particular evident when\n10\nNumerical tests have shown that the outcomes are indi\ufb00er-\nent to the number of agents as long as their number is relatively\nlarge.\n0\n500\n1000\n1500\n2000\n2500\n3000\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nr\n t\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n\u03c1\n \u03c4 \nFig. 4. (Top) A window of the one step returns time series\ngenerated by the model with N = 10000, \u03c60 = 0.165 and\nL = 5. The time series, standardized according to r(t) \u2192\n(r(t) \u2212\u00afr)/\u03c3(r) being \u00afr the average return and \u03c3(r) the stan-\ndard deviation, displays an intermittent character.",
    "chunk_index": 8,
    "start_char": 19444,
    "end_char": 22134,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "\u22122\n0\n2\n4\n6\nr\n t\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n\u03c1\n \u03c4 \nFig. 4. (Top) A window of the one step returns time series\ngenerated by the model with N = 10000, \u03c60 = 0.165 and\nL = 5. The time series, standardized according to r(t) \u2192\n(r(t) \u2212\u00afr)/\u03c3(r) being \u00afr the average return and \u03c3(r) the stan-\ndard deviation, displays an intermittent character. (Bottom)\nAutocorrelation function, \u03c1(\u03c4), related to the time series on\ntop: a signi\ufb01cant anticorrelation is evident for the \ufb01rst time\nsteps.\n3 \u2272L \u227210. In the simulations we have \ufb01xed L = 5 for all\nthe agents.\n3.1 Price and volatility dynamics\nA sample of the time series of one step returns, r(t),\ngenerated by the model is reported in Fig. 4 along with\nits autocorrelation function, \u03c1(\u03c4). From the plots we can\nnotice an intermittent dynamics, characteristic of \ufb01nan-\ncial time series at short time scales, as well as a signi\ufb01-\ncant negative correlation up to few time steps. This lat-\nter e\ufb00ect, known empirically as \u201cbid-ask bounce\u201d, in the\npresent model is purely a result of the order book me-\nchanics. However, for real high-frequency data this e\ufb00ect\ncan last up to few minutes depending on the speci\ufb01c mar-\nket (Bouchaud and Potters, 1999): this is an important\nindication that, on top of the LOB mechanics, there must\nbe other mechanisms, such as memory feedbacks or order\nsplitting for example, responsible for the enhancement of\nthe negative correlation in price changes.\nThe anticorrelated hehaviour of the returns is also con-\n\ufb01rmed by an estimate of the Hurst exponent, H, done via\nthe detrended \ufb02uctuation analysis algorithm (Peng et al.,\n1994; Bartolozzi et al., 2007b). According to this method,\noriginally developed during the Nile\u2019s river dam project\n(Hurst, 1951; Feder, 1988), a time series is persistent if\nH > 0.5, anti-persistent if H < 0.5 or uncorrelated if H =\n0.5. The value found for the one step returns in our model\n\n6\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n10\n\u22126\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n\u03a8\nr\nFig. 5. Probability distribution function, \u03a8, for the one step\nreturns standardized as in Fig. 4 (solid line). This distribution\nis leptokurtic, that is the tails are \u201cfatter\u201d when compared to\nthose of a Gaussian (dotted line).\nis H = 0.495(2) over an ensemble of 3 runs of 105 sam-\nples each, being the error on the last digit, reported in the\nbrackets, estimated via the bootstarp method (Efron and Tibshirani,\n1994). Noticeably, by removing the zero from the time se-\nries the value of the former exponent is statistically equiv-\nalent, in fact we \ufb01nd in this case H0 = 0.491(3)\nThe probability distribution function or pdf, denoted\nas \u03a8, of the one step returns is reported in Fig.",
    "chunk_index": 9,
    "start_char": 21776,
    "end_char": 24504,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "of 3 runs of 105 sam-\nples each, being the error on the last digit, reported in the\nbrackets, estimated via the bootstarp method (Efron and Tibshirani,\n1994). Noticeably, by removing the zero from the time se-\nries the value of the former exponent is statistically equiv-\nalent, in fact we \ufb01nd in this case H0 = 0.491(3)\nThe probability distribution function or pdf, denoted\nas \u03a8, of the one step returns is reported in Fig. 5 where\nthe large \ufb02uctuations observed in Fig. 4 (top) give rise\nto a leptokurtic shape of the distribution, that is the tails\nare \u201cfatter\u201d than those of a Gaussian. This feature, per-\nsistent up to time scales of weeks, is well documented\nin di\ufb00erent empirical works (Bouchaud and Potters, 1999;\nMantegna and Stanley, 1999; Voit, 2005). However, while\nin most of the previous examples the asymptotic decay of\nthe tails can be described by a power law, in our simula-\ntions this is more consistent with an exponential one. From\nthe same plot it is also possible to notice a relatively large\npresence of zero returns, which is also a common feature\nof \ufb01nancial price time series at very high-frequencies.\nIn Fig. 6 (Top), instead, we report the instantaneous\nvolatility de\ufb01ned as \u03bd(t) = |r(t)| where it is noticeable\nthe presence of clustering, that is periods of high activ-\nity tend to be patched together. This memory e\ufb00ect is\nhighlighted by the slowly decaying autocorrelation func-\ntion, Fig. 6 (Bottom), and by the Hurst exponent estimate\nwhich gives the value of H = 0.61(2), indicating a signi\ufb01-\ncant persistency. Besides, we have veri\ufb01ed that the corre-\nlation, despite being weaker, is still present after the zero\nreturns removal, H0 = 0.56(1). A similar behaviour is also\nobserved in real markets, see, for example, in (Liu et al.,\n1997; Gopikrishnan et al., 1999; Liu et al., 1999).\n0\n500\n1000\n1500\n2000\n2500\n3000\n0\n2\n4\n6\n8\n10\n\u03bd\n t\n10\n0\n10\n1\n10\n2\n10\n\u22124\n10\n\u22122\n10\n0\n\u03c1\n \u03c4 \nFig. 6. (Top) Absolute value of returns, or instantaneous\nvolatility, for N = 10000, \u03c60 = 0.165 and L = 5. Patches of\nhigh-volatility periods are visible. (Bottom) Autocorrelation\nfunction relative to the time series on top. The instantaneous\nvolatility results to be correlated positively up to few hundreds\nsteps.\nAs pointed out in the previous section, we would like\nto stress the role played by the memory induced via the\nperceived volatility, Eq.(2), and, therefore, the parameter\nL, in the volatility clustering. In fact, the clustering phe-\nnomenon seems to be particulary relevant only when the\nformer feedback is present: this is a strong indication that\nthe use of short moving averages in high-frequency trading\nstrategies, implicitly inducing memory e\ufb00ects, can be the\nsource of the bursts of patched volatility observed at short\ntime scales. Also note that, following the previous observa-\ntion, in our model both the market sentiment, Eq.(4), and\nthe cancellation process, Eq.(3), contribute to the volatil-\nity clustering.",
    "chunk_index": 10,
    "start_char": 24080,
    "end_char": 27024,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "L, in the volatility clustering. In fact, the clustering phe-\nnomenon seems to be particulary relevant only when the\nformer feedback is present: this is a strong indication that\nthe use of short moving averages in high-frequency trading\nstrategies, implicitly inducing memory e\ufb00ects, can be the\nsource of the bursts of patched volatility observed at short\ntime scales. Also note that, following the previous observa-\ntion, in our model both the market sentiment, Eq.(4), and\nthe cancellation process, Eq.(3), contribute to the volatil-\nity clustering. In fact, it can be shown via numerical ex-\nperiments that by \u201cswitching o\ufb00\u201d the dependence from\n\u02c6\u03bd(t) in one of these two terms, for example by substitut-\ning it with a constant, the clustering e\ufb00ect gets noticeably\nreduced.\n3.2 Traded volume and impact\nSo far we have examined the bahaviour of the price returns\nand volatility. However, both these quantities are related\nto other fundamental factors, one of the most important\nbeing the traded volume, V . Some authors, in fact, have\nrecently argued that large transactions could be responsi-\nble for the non-Gaussian \ufb02uctuations observed in the fat\ntails of the returns distribution (Gabaix et al., 2003) even\nthough more recent empirical work seems to suggest that\nliquidity crises, that is the temporary presence of gaps in\nthe LOB, should be at the origin of those (Bouchaud et al.,\n2002; Daniels et al., 2003; Farmer et al., 2004; Lillo and Farmer,\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n7\n0\n500\n1000\n1500\n2000\n2500\n3000\n0\n20\n40\n60\n80\nV\n t\n10\n0\n10\n1\n10\n2\n10\n0\n\u03c1\n \u03c4 \nFig. 7. (Top) Window of 3000 samples for the traded volume\nin the model along with the relative autocorrelation function\n(Bottom). The parameters used are N = 10000, \u03c60 = 0.165\nand L = 5.\n2005; Weber and Rosenow, 2005; Farmer, 2006; Weber and Rosenow,\n2006). Moreover, the dynamics of the traded volume time\nseries, like the volatility, has been observed to be charac-\nterized by clusters of intense activity followed by relatively\nquite periods (Lillo and Farmer, 2004).\nFor our model, the cumulative traded volume during\neach time step is reported in Fig. 7, where a behaviour very\nsimilar to that of volatility is evident. The intuitive rela-\ntion between the two is further con\ufb01rmed by their cross-\ncorrelation coe\ufb03cient which we found greater than 30%.\nMoreover, the estimate Hurst exponent for the traded vol-\nume results H = 0.70(2) (H0 = 0.66(1)), matchings rea-\nsonably well the value found empirically in (Lillo and Farmer,\n2004), namely H = 0.73(7). It also worth pointing out that\na possible introduction of order splitting in the execution\npart of the model, which is missing at the moment, should\nenhance this correlation.\nWe also estimate the average instantaneous change in\nprice subsequent to a trade of a certain volume V , usually\nknown as impact function.",
    "chunk_index": 11,
    "start_char": 26473,
    "end_char": 29353,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "rea-\nsonably well the value found empirically in (Lillo and Farmer,\n2004), namely H = 0.73(7). It also worth pointing out that\na possible introduction of order splitting in the execution\npart of the model, which is missing at the moment, should\nenhance this correlation.\nWe also estimate the average instantaneous change in\nprice subsequent to a trade of a certain volume V , usually\nknown as impact function. This quantity is very important\nin practical applications being a proxy for the liquidity of\nthe market: its value indicates how, approximately, an or-\nder can penetrate \u201cdeep\u201d into the LOB. The impact func-\ntion for our model, Fig. 8, displays a linear behaviour as\nlong as V is relatively small. However, due to the presence\nof \u201cvolume barriers\u201d, as we will see in the next section,\nthis proportionality is lost as the volume increases. While\nthe same shape for the impact function is observed in real\nmarkets, see (Weber and Rosenow, 2005, 2006) for exam-\nple, zero-intelligence models usually reproduce only the\nlinear part, with the exception of (Farmer et al., 2004).\nThe values reported in Fig. 8 have also another impor-\ntant implication. In fact, the average price response to the\n\u22121\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n\u2329 r \u232aV\nV\nFig. 8. Instantaneous impact function where the negative part\ncorresponds to sell initialized orders. Note that while the im-\npact is linear for V \u223c0, for large volumes this feature is lost.\nAlso, the values of the traded volume in the plot have been\nrescaled by the maximum volume traded in order to \ufb01t the in-\nterval [-1 1]. In this case the rescaling has been made simply to\nfacilitate the visual comparison of di\ufb00erent impact functions\nin case of changing the parameters of the volume distribution,\nsee Sec. 2.5. The simulation has been run with N = 10000,\n\u03c60 = 0.165 and L = 5.\nlarge volumes is not enough to justify the extreme \ufb02uctu-\nations observed in the pdf of returns that can be greater\nthan 10 ticks, Fig. 5, which, therefore, have to be related\nor to rallies of orders in one direction or to a temporary\nlack of liquidity between the levels of the LOB. Of course\nnothing prevents both scenarios to be realized at the same\ntime.\n3.3 LOB average shape, spread and imbalance\ndynamics\nIn this \ufb01nal section we turn our attention to the way the\norders are \u201con average\u201d deployed in the LOB as well as\nthe dynamics of some quantities related to the relative\nposition of the orders such as the spread and the imbalance\nbetween buy and sell.\nFirstly, we show the \u201cmean\u201d shape of the LOB, that is\nthe average volume present at a \ufb01xed distance, |P \u2212Pm|,\nfrom the mid-point price. From the plot, Fig. 9, it is clear\nthat the liquidity increases steeply up to a maximum, lo-\ncated few ticks away from Pm, and then it starts to de-\ncay as we move away from it.",
    "chunk_index": 12,
    "start_char": 28944,
    "end_char": 31794,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "orders are \u201con average\u201d deployed in the LOB as well as\nthe dynamics of some quantities related to the relative\nposition of the orders such as the spread and the imbalance\nbetween buy and sell.\nFirstly, we show the \u201cmean\u201d shape of the LOB, that is\nthe average volume present at a \ufb01xed distance, |P \u2212Pm|,\nfrom the mid-point price. From the plot, Fig. 9, it is clear\nthat the liquidity increases steeply up to a maximum, lo-\ncated few ticks away from Pm, and then it starts to de-\ncay as we move away from it. The peak in volume can\nbe thought as a sort of \u201cvolume barrier\u201d which prevents\nlarge orders to get too deep into the LOB and, therefore,\ncon\ufb01rming the speculations made in the previous section.\nVery similar shapes have been found in empirical studies\nwhere it has been suggested that the asymptotic shape of\n\n8\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n\u2329 V \u232a\n|P\u2212Pm|\nFig. 9. Average shape of the limit order book reported against\nthe distance from the mid-point price. Note that, given the\nsymmetry of the model, we have aggregated the data on the\nask and the bid side of the book so to have better statistics.\nThe parameters used are the same as in Fig. 7.\nthe LOB, related to the \u201cpatient\u201d traders, may follow a\npower law decay (Bouchaud et al., 2002). It is also impor-\ntant to stress that the smooth \u201cmean\u201d shape of the LOB\nis not signi\ufb01cant for the \u201cinstantaneous\u201d shape which, in\nfact, can be relatively sparse.\nAnother important quantity related to the placement\nof the orders in the LOB is the spread, S. This is the\ndi\ufb00erence in price between the best ask and the best bid\nand it represents the cost that a trader has to pay up-\nfront in order to execute a market order. Moreover, the\nspread is a further proxy for the liquidity: large spreads\nwould indicate shallow markets while very liquid ones will\ntend to keep a very small spread at all times, possibly\nclose to one tick. Dynamically, the spread displays persis-\ntency in time with 0.7 \u2272H \u22720.8\n(Plerou et al., 2005;\nCajueiro and Tabak, 2007; Gu et al., 2007) depending on\nthe market and the time scale of observation, while the\nasymptotic shape of its pdf can be characterized by a\npower law function (Plerou et al., 2005). The same two\nfeatures appear also in our simulations, Fig. 10, even though\nthe Hurst exponent result to be relatively smaller if com-\npared to the empirical \ufb01ndings, namely H = 0.56(2) (H0 =\n0.56(2)): this discrepancy may arise from the fact that at\nthe moment we are considering only trivial execution al-\ngorithms.",
    "chunk_index": 13,
    "start_char": 31288,
    "end_char": 33877,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "law function (Plerou et al., 2005). The same two\nfeatures appear also in our simulations, Fig. 10, even though\nthe Hurst exponent result to be relatively smaller if com-\npared to the empirical \ufb01ndings, namely H = 0.56(2) (H0 =\n0.56(2)): this discrepancy may arise from the fact that at\nthe moment we are considering only trivial execution al-\ngorithms.\nLastly, we focus our attention to the dynamic imbal-\nance between buy and sell orders or simply the volume\nimbalance, \u2206V (t), de\ufb01ned as\n\u2206V (t) =\nNb(t)\nX\ni=1\nV b\ni (t) \u2212\nNa(t)\nX\ni=1\nV a\ni (t),\n(9)\n0\n500\n1000\n1500\n2000\n2500\n3000\n2\n4\n6\n8\n10\n12\nS\n t\n10\n0\n10\n1\n10\n2\n10\n0\n\u03c1\n \u03c4 \nFig. 10. (Top) A sample for the spread time series (in ticks):\nit is possible to notice how, occasionally, the spread can be-\ncome relatively large. (Bottom) Autocorrelation function for\nthe spread time series. The parameters used are the same as\nin Fig. 7.\nwhere, for a time t, V b,a\ni\nrepresents the volume of ith limit\norder and Nb,a their total number, respectively, for the bid\nand the ask side of the LOB. The time series, Fig. 11, as\nexpected for a market near-equilibrium, displays a mean\nreverting behaviour, that is the imbalance tends to \ufb02uctu-\nate all time, more or less symmetrically, around a \u201cpseudo-\nequilibrium\u201d price, possibly close to criticality Bartolozzi et al.\n(2005, 2006). The same dynamics has been observed for\n\u2206V in di\ufb00erent futures contracts (Bartolozzi et al., 2007a).\n4 Discussion and conclusions\nIn the present work we have developed a multi-agent frame-\nwork characterized by a realistic order book keeping as\na tool for the study of the activity of a double auction\nmarket at microscopic time scales. Inside this framework\nthe model relies just on few basic assumptions related to\nthe agent\u2019s strategic behaviour. Among the most impor-\ntant ones, the order submission process makes use of a\nstochastic variable, the market sentiment, which is related\nto both the public and the private information. Besides,\nthe agents lack of the concept of \u201cutility maximization\u201d\noften invoked in the economics literature and embodying,\nin mathematical terms, the concept of \u201crationality\u201d.\nDespite its simplicity, the model manages to reproduce\nseveral empirical features of the high-frequency dynamics\nof the stock market such as the negative correlation in\nmarket returns, clustering in the trading activity (such as\nvolatility, traded volume and bid-ask spread) as well as\na non-linear response of the price change to the volume\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n9\n0\n500\n1000\n1500\n2000\n2500\n3000\n\u2212200\n\u2212150\n\u2212100\n\u221250\n0\n50\n100\n150\n200\n250\n\u2206 V\n t\nFig. 11. (Top) Time series of imbalance between demand and\nsupply displaying a clear mean reverting nature. The parame-\nters used for the simulation are the same as in Fig. 7.\ntraded.",
    "chunk_index": 14,
    "start_char": 33525,
    "end_char": 36334,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "response of the price change to the volume\n\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\n9\n0\n500\n1000\n1500\n2000\n2500\n3000\n\u2212200\n\u2212150\n\u2212100\n\u221250\n0\n50\n100\n150\n200\n250\n\u2206 V\n t\nFig. 11. (Top) Time series of imbalance between demand and\nsupply displaying a clear mean reverting nature. The parame-\nters used for the simulation are the same as in Fig. 7.\ntraded. Moreover, the similarities with the real markets\nextend also in the way the orders are deployed on the\nLOB as observed through the average shape of the book\nand the volume imbalance time series.\nIn conclusion, our model indicates that large part of\nthe dynamics of the stock market at very short time scales\ncan be explained without the requiring any particular ra-\ntional approach from an agent prospective if not some\nmemory feedback which, in our model, are represented\nby short term moving averages of the public information.\nMoreover, our results con\ufb01rm that large price movements\nare more likely to be related to a temporary lack of liquid-\nity in the LOB rather than to large volume transactions.\nOur future work will involve adding more realistic fea-\ntures and feedbacks in the model. In particular, one in-\nteresting e\ufb00ect to take into consideration would be the\nherding phenomenon which, it has been argued, to be at\nthe origin of dramatic liquidity crises which can ultimately\nlead to \ufb01nancial crashes (Sornette, 2004). Other factors\nthat can claim to play a fundamental role at these time\nscales are news realize and market making strategies: both\nof them should to be taken into account.\nAcknowledgement\nThe author would like to thank Prof. Tony Thomas and\nProf. Derek Leinweber for a careful reading of the manuscript\nas well as Matt Fender for the IT support.\nReferences\nBailey, R. E., 2005. The economics of \ufb01nancial markets.\nCambridge University Press, Cambridge, UK.\nBak, P., Paczuski, M., Shubik, M., 1997. Price variations\nin a stock market with many agents. Physica A 246,\n430.\nBartolozzi, M., Leinweber, D. B., Thomas, A. W., 2005.\nSelf-organized criticality and stock market dynamics: an\nempirical study. Physica A 350, 451\u2013465.\nBartolozzi, M., Leinweber, D. B., Thomas, A. W., 2006.\nScale-free avalanche dynamics in the stock market.\nPhysica A 370, 132\u2013139.\nBartolozzi, M., Mellen, C., Chan, F., 2007a. Internal re-\nport. Boronia Capital.\nBartolozzi, M., Mellen, C., Di Matteo, T., Aste, T., 2007b.\nMulti-scale correlations in di\ufb00erent futures markets.\nThe European Physical Journal B 58(2), 207\u2013220.\nBartolozzi, M., Thomas, A. W., 2004. Stochastic cellular\nautomata model for stock market dynamics. Physical\nReview E 69, 046112.\nBouchaud, J.-P., Mzard, M., Potters, M., 2002. Statistical\nproperties of stock order books: empirical results and\nmodels. Quantitative Finance 2(4), 251 \u2013 256.\nBouchaud, J.-P., Potters, M., 1999. Theory of \ufb01nancial\nrisk. Cambridge University Press, Cambridge.\nCajueiro, D., Tabak, B., 2007. Characterizing bid-ask\nprices in the brazilian equity market. Physica A 373,\n627.\nCont, R., Bouchaud, J.-P., 2000. Herd behaviour and\naggregate \ufb02uctuations in \ufb01nancial markets.",
    "chunk_index": 15,
    "start_char": 35958,
    "end_char": 39057,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "A. W., 2004. Stochastic cellular\nautomata model for stock market dynamics. Physical\nReview E 69, 046112.\nBouchaud, J.-P., Mzard, M., Potters, M., 2002. Statistical\nproperties of stock order books: empirical results and\nmodels. Quantitative Finance 2(4), 251 \u2013 256.\nBouchaud, J.-P., Potters, M., 1999. Theory of \ufb01nancial\nrisk. Cambridge University Press, Cambridge.\nCajueiro, D., Tabak, B., 2007. Characterizing bid-ask\nprices in the brazilian equity market. Physica A 373,\n627.\nCont, R., Bouchaud, J.-P., 2000. Herd behaviour and\naggregate \ufb02uctuations in \ufb01nancial markets. Macroeco-\nnomics Dynamics 4, 170.\nCutler, D., Poterba, J., Summers, L., 1989. What moves\nstock prices? Journal of Portfolio Managment 15, 4\u201312.\nDaniels, M. G., Farmer, J. D., Gillemot, L., Iori, G.,\nSmith, E., 2003. Quantitative model of price di\ufb00usion\nand market friction based on trading as mechanistic ran-\ndom process. Physical Review Letters 90, 1008102.\nEfron, B., Tibshirani, R., 1994. An Introduction to the\nBootstrap. Chapman & Hall, London.\nFarmer, J. D., 2006. Comment on \u2019large stock price\nchanges: volume or liquidity?\u2019. Quantitative Finance\n6(1), 1\u20133.\nFarmer, J. D., Gillemot, L., Lillo, F., Mike, S., Sen, A.,\n2004. What really causes large price changes? Quanti-\ntative Finance 4(4), 383\u2013397.\nFarmer, J. D., Patelli, P., Zovko, I. I., 2005. The predictive\npower of zero intelligence in \ufb01nancial markets. Proceed-\nings of the National Academy of Science pf the USA\n102, 2254\u20132259.\nFeder, J., 1988. Fractals. Plenum Press, New York & Lon-\ndon.\nFusai, G., Ronconi, A., 2008. Implementing models in\nquantitative \ufb01nance: methods and cases. Springer-\nVerlag, Berlin.\nGabaix, X., Gopikrishnan, P., Pleru, V., Stanley, H. E.,\n2003. A theory of power-law distributions in \ufb01nancial\nmarkets. Nature 423, 267.\nGopikrishnan, P., Plerou, V., Amaral, L., Meyer, M., Stan-\nley, H.-E., 1999. Scaling of the distribution of \ufb02uctua-\ntions of \ufb01nancial market indices. Physical Review E 60,\n5305.\n\n10\nM. Bartolozzi: A Multi Agent Model for the Limit Order Book Dynamics\nGu, G.-F., Chen, W., Zhou, W.-X., 2007. Quantiying the\nbid-ask spread in the chinese stock market using limit-\norder book data. European Physical Journal B 57, 81 \u2013\n87.\nHurst, H., 1951. Long-term storage in reservoirs. Trans.\nAmer. Soc. Civil Eng. 116, 770\u2013799.\nIori, G., Daniels, M. G., Farmer, J. D., Gillemo, L., Kr-\nishnamurthy, S., Smith, E., 2003. An analysis of price\nimpact function in order-driven markets. Physica A 324,\n146.\nJoulin, A., Lefevre, A., Grunberg, D., Bouchaud, J.-P.,\n2008. Stock price jumps: news and volume play a minor\nrole. Wilmott Magazine Sep/Oct, 1\u20137.\nLillo, F., Farmer, M. J. D., 2004. Long memory of the\ne\ufb03cient market. Nonlinear Dynamics and Econometrics\n8(3), 1.\nLillo, F., Farmer, M. J. D., 2005. The key role of liquid-\nity \ufb02uctuations in determining the large price changes.\nFluctuations and noise letters 5(2), L209.\nLiu, J., Cizeau, P., Meyer, M., Peng, C.-K., Stanley, H.,\n1997. Correlations in economic time series. Physica A\n245, 437\u2013440.\nLiu, Y., Gopikrishnan, P., Cizeau, P., Meyer, M., Peng,\nC.-K., Stanley, H., 1999. Statistical properties of the\nvolatility of price \ufb02uctuations. Physical Review E 60,\n1390.\nMantegna, R.",
    "chunk_index": 16,
    "start_char": 38485,
    "end_char": 41690,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "Nonlinear Dynamics and Econometrics\n8(3), 1.\nLillo, F., Farmer, M. J. D., 2005. The key role of liquid-\nity \ufb02uctuations in determining the large price changes.\nFluctuations and noise letters 5(2), L209.\nLiu, J., Cizeau, P., Meyer, M., Peng, C.-K., Stanley, H.,\n1997. Correlations in economic time series. Physica A\n245, 437\u2013440.\nLiu, Y., Gopikrishnan, P., Cizeau, P., Meyer, M., Peng,\nC.-K., Stanley, H., 1999. Statistical properties of the\nvolatility of price \ufb02uctuations. Physical Review E 60,\n1390.\nMantegna, R. N., Stanley, H. E., 1999. An introduction\nto econophysics: correlation and complexity in \ufb01nance.\nCambridge University Press, Cambridge.\nMaslov, S., 2000. Simple model of a limit order-driven\nmarket. Physica A 278(3-4), 571\u2013578.\nMatassini, L., Franci, F., 2001. How traders enter the mar-\nket through the book. preprint: cond-mat/0103106.\nMike, S., Farmer, J. D., 2008. An empirical behavioural\nmodel of liquidity and volatility. Jourmnal of Economic\nDynamic and Control 32, 2000.\nO\u2019Hara, M., 1997. Market Microstructure Theory. Black-\nwell Publishing, Malden, USA.\nPaul, W., Baschnagel, J., 1999. Stochastic Process from\nPhysics to Finance. Springer, Berlin.\nPeng, C.-K., Buldyrev, S. V., Havlin, S., Simons, M., Stan-\nley, H. E., Goldberger, A. L., Feb 1994. Mosaic organi-\nzation of DNA nucleotides. Physical Review E 49 (2),\n1685\u20131689.\nPlerou, V., Gopikrishnan, P., Stanley, H.-E., 2005. Quan-\ntifying \ufb02uctuations in market liquidity: Analysis of the\nbid ask spread. Physical Review E 71, 046131.\nRaberto, M., Cinotti, S., Focardi, S. M., Marchesi, M.,\n2001. Agent-based simulation of a \ufb01nancial market.\nPhysica A 299, 319.\nSlanina, F., 2008. Critical comparison of several order-\nbook models for stock market \ufb02uctuations. preprint:\nphysics/0801.0631.\nSmith, E., Farmer, J. D., Gillemot, L., Krishnamurthy, S.,\n2003. Statistical theory of the continuous double auc-\ntion. Quantitative Finance 3(6), 481 \u2013 514.\nSornette, D., 2004. Why stock market crash. Princeton\nUniversity Press, Princeton and Oxford.\nVoit, J., 2005. The Statistical Mechanics of Financial Mar-\nkets. Spriger-Verlag, Berlin.\nWeber, P., Rosenow, B., 2005. Order book approach to\nprice impact. Quantitative Finance 5(4), 357.\nWeber, P., Rosenow, B., 2006. Large stock price changes:\nvolume or liquidity? Quantitative Finance 6(1), 7.\nZaccaria, A., Cristelli, M., Al\ufb01, V., Ciulla, F., Pietronero,\nL., 2010. The asymmetric statistics of the order book:\nthe role of discretness and non-uniform limit order de-\nposition. Phys. Rev. E 81, 066101.",
    "chunk_index": 17,
    "start_char": 41176,
    "end_char": 43702,
    "paper_title": "A Multi Agent Model for the Limit Order Book Dynam",
    "paper_category": "q-fin.TR",
    "paper_filename": "A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/A_Multi_Agent_Model_for_the_Limit_Order_Book_Dynam.pdf"
  },
  {
    "text": "arXiv:0906.1387v3 [q-fin.TR] 18 May 2010\nAsymmetric statistics of order books: The role of\ndiscreteness and evidence for strategic order\nplacement\nA. Zaccaria, M. Cristelli, V. Al\ufb01, F. Ciulla, L. Pietronero\nOctober 29, 2018\nAbstract\nWe show that the statistics of spreads in real order books is character-\nized by an intrinsic asymmetry due to discreteness e\ufb00ects for even or odd\nvalues of the spread. An analysis of data from the NYSE order book points\nout that traders\u2019 strategies contribute to this asymmetry. We also inves-\ntigate this phenomenon in the framework of a microscopic model and, by\nintroducing a non-uniform deposition mechanism for limit orders, we are\nable to quantitatively reproduce the asymmetry found in the experimental\ndata. Simulations of our model also show a realistic dynamics with a sort\nof intermittent behavior characterized by long periods in which the order\nbook is compact and liquid interrupted by volatile con\ufb01gurations. The\norder placement strategies produce a non-trivial behavior of the spread\nrelaxation dynamics which is similar to the one observed in real markets.\n1\nIntroduction\nThe order book is the double auction mechanism [1, 2, 3] which permits to\nprocess and store the orders placed by investors in a modern \ufb01nancial market.\nThis system is the elementary mechanism of price formation as a consequence\nof the arrival of proposals (orders) of buying or selling. There are two classes\nof orders: market orders and limit orders. The market ones correspond to or-\nders to buy/sell at the best available price (called best bid/ask), hence they\nare immediately executed. The limit orders instead are orders to buy or sell at\na given price which can be not necessarily the best one. By consequence limit\norders may not immediately ful\ufb01lled and then they are stored in the order book.\nThe di\ufb00erence between the best ask and the best bid is de\ufb01ned as the spread s.\nThe order prices are not continuous but discrete and expressed in units of ticks.\nAlso the volume of an order is an integer multiple of a certain amount of shares.\nThe mid-price between best ask and best bid can be considered a conventional\nde\ufb01nition of the price of a stock. The majority of the spread variations are due\nto a limit order which falls inside the spread or to a market order which matches\nall the orders placed at the best price.\nIn recent years the complex dynamics of the order book has attracted the sci-\nenti\ufb01c community attention. One of the reasons of this increasing interest is\nthe availability of a large amount of experimental data which has permitted an\n1\n\nextensive statistical analysis of the order book properties and has revealed a\nnumber of interesting features and regularities. The main empirical evidences\nof the order book are: the fat-tailed distribution of the price of new limit or-\nders [4, 5, 6]; the non-trivial power law correlation of the transaction price\nsigns [7, 8, 9, 10, 11]; the peculiar shape of the limit order volume distribution\nwith fat tails and peaked away from the best quote [4, 5];",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3046,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "the availability of a large amount of experimental data which has permitted an\n1\n\nextensive statistical analysis of the order book properties and has revealed a\nnumber of interesting features and regularities. The main empirical evidences\nof the order book are: the fat-tailed distribution of the price of new limit or-\nders [4, 5, 6]; the non-trivial power law correlation of the transaction price\nsigns [7, 8, 9, 10, 11]; the peculiar shape of the limit order volume distribution\nwith fat tails and peaked away from the best quote [4, 5]; the non linear response\nof the order book to order arrivals [12, 13].\nMoreover the microscopic market level of the order book shows the stylized facts\npresent in \ufb01nancial markets at the aggregate level [3, 14, 15, 16]. To interpret\nthese empirical evidences a series of theoretical models has been introduced\n[17, 18, 19, 20, 6, 21]. In particular, the models by Farmer et al. [6, 21] show as\neven a \u201czero intelligence\u201d mechanism can reproduce many of the experimental\nfeatures of real order books.\nIn a previous work we have introduced a model [22] where the zero intelligence\nparadigm is adopted and in this paper we are going to interpret some experi-\nmental results in the framework of this model.\nOne of the most challenging issue in this \ufb01eld is the identi\ufb01cation of new empir-\nical features in order to discriminate and validate the various models proposed.\nIn this paper we investigate the e\ufb00ects induced by the discrete nature of order\nbooks. In fact, as we have already noticed, the price of the orders is not a con-\ntinuous variable but it can only be a multiple of a quantity called tick which is a\nfraction of the currency used in the the market considered. A \ufb01rst consequence\nof this aspect is the spontaneous emergence of asymmetries in the system. For\ninstance, two con\ufb01gurations of the order book with an even spread or an odd\none (in units of ticks) are not a priori equivalent for the mechanism of deposi-\ntion of limit orders inside the spread. We investigate the fraction of odd spreads\nfor a data set from the NYSE market \ufb01nding indeed a strong evidence for this\nasymmetry. However, as we are going to see in detail in the paper, the problem\nis more subtle than expected and also the agent strategies play an important\nrole to explain quantitatively this phenomenon. The strategic order placement\nis also the origin of the non-trivial relaxation pattern observed when a spread\n\ufb02uctuation takes place.\nThe paper is organized in the following way.\nIn Sec. 2 we show the experimental evidence of the asymmetry of spreads.\nIn Sec. 3 we propose an interpretation of this empirical feature in terms of a\nmicroscopic model we introduced in [22].\nIn Sec. 4 we perform a more detailed data analysis in order to investigate further\naspect which contributes to the asymmetry.\nIn Sec. 5 our model is properly modi\ufb01ed to take into account the new results\nfound in the data analysis.",
    "chunk_index": 1,
    "start_char": 2506,
    "end_char": 5438,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "a spread\n\ufb02uctuation takes place.\nThe paper is organized in the following way.\nIn Sec. 2 we show the experimental evidence of the asymmetry of spreads.\nIn Sec. 3 we propose an interpretation of this empirical feature in terms of a\nmicroscopic model we introduced in [22].\nIn Sec. 4 we perform a more detailed data analysis in order to investigate further\naspect which contributes to the asymmetry.\nIn Sec. 5 our model is properly modi\ufb01ed to take into account the new results\nfound in the data analysis.\nIn Sec. 6 a detailed investigation of the role of the agent strategies for the place-\nment of order inside the spread is performed in the framework of our model.\nIn Sec. 7 we study the e\ufb00ects of strategic order placements on the spread relax-\nation dynamics with respect to a pure zero-intelligence mechanism.\nFinally conclusions and perspectives are discussed in Sec. 8.\n2\n\n2\nEmpirical evidences\nIn this section we show the empirical evidences that have led us to investigate\nthe asymmetric nature of the order book.\nIn our analysis of the order book we have considered a data set which spans\na period of nearly 80 trading days between October 2004 and February 2005.\nThis data set includes a series of high-frequency (tick-by-tick) information for\n20 stocks from the NYSE market. These stocks have been chosen to be het-\nerogeneous in their level of capitalization 1. The information we have for each\nstock is the whole list of transactions and quotes, which are the prices of e\ufb00ec-\ntive deals and of orders respectively. Our data set lacks the information about\nthe whole order book apart of the best bid and the best ask. Therefore a new\nquote appears only if the best bid or the best ask have been updated. From\nthis data set we have reconstructed the sequence of market and limit orders by\nlooking to the spread variations. If the spread has increased with respect to its\nprevious value we refer to the event as a market order. On the contrary, the\nevent is de\ufb01ned as a limit order if the spread has decreased. We assume that\nthe probability that a cancellation of a limit order may change the spread is so\nsmall to be neglected. We have also expressed all the prices in units of ticks,\nso the spread results to be an integer number. At the end of this re\ufb01nement,\nour data set is composed only by the series of the events in which the spread\nhas changed, each labelled to be a market or a limit order. In this paper we\nrestrict our analysis to limit order events. The complete analysis, including also\nmarket orders, will be matter of future researches. To investigate the intrinsic\nasymmetry that a discrete and \ufb01nite spread generates, we have \ufb01rstly analyzed\nhow the fraction of odd spreads depends on the average value of the spread\nfor each stock. In Fig.",
    "chunk_index": 2,
    "start_char": 4937,
    "end_char": 7704,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "has changed, each labelled to be a market or a limit order. In this paper we\nrestrict our analysis to limit order events. The complete analysis, including also\nmarket orders, will be matter of future researches. To investigate the intrinsic\nasymmetry that a discrete and \ufb01nite spread generates, we have \ufb01rstly analyzed\nhow the fraction of odd spreads depends on the average value of the spread\nfor each stock. In Fig. 1 we have plotted, for each stock, the daily fraction of\nodd-valued spreads as a function of the average value of the spread. There are\n80 di\ufb00erent points for each stock and also a further average over all the days\nis plotted in the inset. We can observe that for almost all points the fraction\nof odd-valued spreads is larger than 0.5 which would be the expected value if\nthe spread was very large (s \u2192\u221e). This asymmetry is more marked for stocks\nwith a smaller average spread and it goes diminishing while the average spread\nincreases. A small average spread usually corresponds to stocks with a large\ncapitalization. In the next sections we are going to investigate these results in\nthe framework of the model we have introduced in a previous work [22].\n3\nA model for limit order deposition: uniform\ncase\nIn this section we propose a simple explanation of the evidences shown in Sec. 2\non the basis of the order book model introduced in [22].\nFirst of all we brie\ufb02y recall the main properties of the model. At each time\nstep an order is placed. This order can be a sell or a buy order with the same\nprobability 1/2, and a market or a limit order with probability \u03c0 = 1/3 and\n1 \u2212\u03c0 = 2/3 respectively. A limit order is placed with an uniform distribution\n1Market capitalization is de\ufb01ned as the number of shares of a company multiplied by their\nprice. It is the simplest measure of a company\u2019s size.\n3\n\n2\n4\n6\n8\n10\n12\n14\n16\n<spread> (ticks)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nfraction of odd spreads\nah\navo\nba\nbro\ncai\ndri\nge\nglk\ngm\njwn\nkss\nmcd\nmhs\nmik\nmls\npg\ntxi\nudi\nvno\nwgr\n2\n4\n6\n8\n10\n12\n14\n<spread> (ticks)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nfraction of odd spreads\naverage\nFigure 1: (Color online) Fraction of odd-valued spreads (in units of ticks) vs\ndaily average spread for di\ufb00erent stocks. We observe a systematic deviation\nfrom the symmetric case in which the fraction is 0.5. In the inset we plot a\nfurther average over a period of 80 days.\nin the interval ]b(t), b(t) + ks(t)] if it is a sell order and [a(t) \u2212ks(t), a(t)[ if it is\na buy order, where b(t) and a(t) are the best bid and the best ask respectively,\ns(t) is the spread and k > 1 is a constant. In \ufb01rst approximation k\u22121 is the\nprobability for a limit order to be put inside the spread (therefore causing a\nprice change).",
    "chunk_index": 3,
    "start_char": 7287,
    "end_char": 9980,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "the fraction is 0.5. In the inset we plot a\nfurther average over a period of 80 days.\nin the interval ]b(t), b(t) + ks(t)] if it is a sell order and [a(t) \u2212ks(t), a(t)[ if it is\na buy order, where b(t) and a(t) are the best bid and the best ask respectively,\ns(t) is the spread and k > 1 is a constant. In \ufb01rst approximation k\u22121 is the\nprobability for a limit order to be put inside the spread (therefore causing a\nprice change).\nThis dependence on the previous spread value creates a sort\nof autoregressive mechanism for the order deposition. The tick size and the\nvolume of the orders are both constant and \ufb01xed (\u2206q = 1 and \u03c9 = 1). Finally,\na cancellation process avoids the divergence of the volume stored in the order\nbook.\nIn this framework we can evaluate a number of quantities, for example the\nprobability that, given a spread s at time t, the new spread s\u2032 \u0338= s at time\nt + \u2206t is even or odd, where \u2206t is the time to wait to have a variation of the\nspread. The dependence on the parameter k is removed because we consider\nonly the conditioned probability that an event occurs. In this paper we indicate\nwith s the value of the spread before an incoming event and with s\u2032 the new value\nof the spread consequent to the variation. Here we restrict our analysis only to\nevents due to limit order arrival. The probability to have an odd spread in the\n\ufb01nal state turns to be dependent on the parity of the spread s. Straightforward\ncalculations give the probabilities\nP(e|o, s) = 1\n2\nP(o|o, s) = 1\n2\nP(o|e, s) = 1\n2\ns\ns\u22121\nP(e|e, s) = 1\n2\ns\u22122\n(s\u22121).\n(1)\nwhere o and e denote if the the spread is odd or even respectively. One can\nsee that P(o|e) > P(e|e): given an even spread s in the initial state, the next\nspread s\u2032 has a larger probability to be odd rather than even. As expected, both\nP(o|e) and P(e|e) tend to 1/2 for s \u2192\u221e. This simple argument gives a \ufb01rst\nexplanation of the empirical evidence of an excess of odd spreads shown in the\n4\n\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nspread (ticks)\n0\n0.2\n0.4\n0.6\n0.8\n1\nP ( odd | spread )\nFigure 2: (Color online) Conditional probability to have an odd-valued spread\ns\u2032 given an initial spread s for real data. The plot shows clearly that with high\nprobability an odd spread is followed by an even one (0.8) and vice-versa.\nprevious section.\n4\nData analysis\nNow we intend to give an interpretation of Fig. 1 in view of the results obtained\nin the previous section. The dispersion of the points plotted in Fig. 1 can be\ntraced back to the spurious e\ufb00ect introduced by the fact that many di\ufb00erent\nspreads s give their contribution to the \ufb01nal average.",
    "chunk_index": 4,
    "start_char": 9551,
    "end_char": 12142,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "initial spread s for real data. The plot shows clearly that with high\nprobability an odd spread is followed by an even one (0.8) and vice-versa.\nprevious section.\n4\nData analysis\nNow we intend to give an interpretation of Fig. 1 in view of the results obtained\nin the previous section. The dispersion of the points plotted in Fig. 1 can be\ntraced back to the spurious e\ufb00ect introduced by the fact that many di\ufb00erent\nspreads s give their contribution to the \ufb01nal average. Therefore the next step\nis to investigate the frequency of odd spreads conditioned to a given s, in order\nto compare Eq. 1 to real data.\nWe can identify the variations of the spread caused by limit orders imposing\nthe condition s > s\u2032 because only limit orders inside the spread can decrease\nits value (the same argument has been also followed, for example, by [23]). In\nFig. 2 we show the conditional probability to have an odd spread s\u2032 starting\nfrom a spread s, as a function of s. The pattern strongly oscillates around the\nvalue 1/2: an even spread is most likely followed by an odd one as predicted by\nEq. 1, but surprisingly also the viceversa is true.\nThis apparently strange (with respect to the result of Eq. 1) behavior can be\nattributed to a non-uniform depositions of limit orders inside the spread. We\nstudied the distribution of the spread variations conditioned to a given value\nof the spread s for real data. We found that a consistent fraction \u03b1 \u223c0.7 of\nthe limit orders inside the spread is placed at the quote adjacent to the best,\nas we show in Fig. 3. A reasonable way to model this non-uniform distribution\nof the limit orders inside the spread is through a piecewise constant function.\nIn this way the probability to put a limit order at the \ufb01rst adjacent quote is\n\u03b1, and the probability to put the order in one of the other remaining quotes is\nequally distributed. This tendency can be also interpreted in terms of agents\u2019\nstrategies. In fact, the placement an order far from the (previous) best bid or\n5\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12\n\u2206 s (ticks)\n0\n0.2\n0.4\n0.6\n0.8\nP ( \u2206 s | s ) \ns = 4\ns = 6\ns = 8\ns = 10\ns = 12\n1\n10\n\u2206 s (ticks)\n0.01\n0.1\n1\nP ( \u2206 s | s ) \ns=4\ns=6\ns=8\ns=10\ns=12\nFigure 3: (Color online) Probability to have a variation \u2206s of the spread given an\ninitial s for di\ufb00erent spread values (empirical data). The most probable variation\nis always \u2206s = 1. The probabilities of the other variations (\u2206s = 2, 3, ...) are\nweakly decreasing functions and, in \ufb01rst approximation, can be considered as\nconstant. The dashed line is the power law with exponent 1.8 found in [24] for\nLondon Stock Exhange.",
    "chunk_index": 5,
    "start_char": 11672,
    "end_char": 14269,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "s=8\ns=10\ns=12\nFigure 3: (Color online) Probability to have a variation \u2206s of the spread given an\ninitial s for di\ufb00erent spread values (empirical data). The most probable variation\nis always \u2206s = 1. The probabilities of the other variations (\u2206s = 2, 3, ...) are\nweakly decreasing functions and, in \ufb01rst approximation, can be considered as\nconstant. The dashed line is the power law with exponent 1.8 found in [24] for\nLondon Stock Exhange. In both cases \u2206s = 1 is highly preferred.\nask can be seen as a risky operation in which the agent, by disagreeing with\nother agents\u2019 evaluations, tries to trade quickly paying a kind of virtual cost\nequal to the distance between the best and her order quote. In other words, an\norder near the best is the most conservative position able to change the spread.\nThe next step is to \ufb01nd how much the probability \u03b1 to place a limit order near\nthe corresponding best depends on the spread s and on the stock considered. In\nFig. 4 we plot \u03b1 as a function of s for three stocks which cover a wide range of\ncapitalization, \ufb01nding very di\ufb00erent behaviors depending on the stocks and on\nthe values of the spreads. One possible explaination for this variety of behaviors\nis the lack of statistics: in fact, liquid stocks (that is, stocks characterized by\nfast order execution and small transaction impact on the price) usually have\nsmall spreads, and hence the statistics for large spreads is poor, and viceversa\nfor illiquid stocks. We can appropriately weight the contribution of di\ufb00erent\nstocks by averaging on all the data. The resulting curve is approximately a\nconstant as a function of the spread.\n5\nA model for limit order deposition:\nnon-uniform case\nIn the previous section we have observed a systematic deviation of the experi-\nmental probabilities P(o|e, s) and P(e|o, s) from the ones of Eq. 1 derived from\nthe hypothesis of uniform order deposition. In this section we are going to show\nthat this discrepancy is mainly due to the non-uniform probability of order\nplacement inside the spread, and therefore to agents\u2019 strategies.\nIn order to include the e\ufb00ect of a non-uniform order deposition, we can gener-\n6\n\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\ns (ticks)\n0\n0.2\n0.4\n0.6\n0.8\n1\n \u03b1 (s)\nbro\nge\ntxi\naverage\nFigure 4: (Color online) Probability \u03b1(s) to place an order at the quote adjacent\nto the best one as a function of the spread. We have plotted this probabilities for\nthree representative stocks and also an average over all the 20 stocks. The very\ndi\ufb00erent values observed can be explained by considering the di\ufb00erent statistics\nof the stocks. To properly address this e\ufb00ect we have performed an average\nover the 20 stocks of our data set. The result is a value of \u03b1(s) approximately\nconstant.\nalize Eq.",
    "chunk_index": 6,
    "start_char": 13831,
    "end_char": 16569,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "the spread. We have plotted this probabilities for\nthree representative stocks and also an average over all the 20 stocks. The very\ndi\ufb00erent values observed can be explained by considering the di\ufb00erent statistics\nof the stocks. To properly address this e\ufb00ect we have performed an average\nover the 20 stocks of our data set. The result is a value of \u03b1(s) approximately\nconstant.\nalize Eq. 1 in the following way\nP(e|o, s) = P s\u22121\n2\nj=1 g(2j|s)\nP(o|o, s) = P s\u22123\n2\nj=0 g(2j + 1|s)\nP(o|e, s) = P s\u22122\n2\nj=0 g(2j + 1|s)\nP(e|e, s) = P s\u22122\n2\nj=1 g(2j|s).\n(2)\nwhere g(i|s) is the probability mass function of the deposition mechanism for\nlimit orders inside the spread and i = 1, . . . , s \u22121 is the distance from the best\nquote of the placement price.\nIn Fig. 4 we have seen that the probability \u03b1 that a limit order produces a\nspread variation equal to one is weakly dependent on the value of the spread.\nThis suggests a simple approximation for g(i|s) with a piecewise function. Now\nwe discuss how to introduce this property in our model.\nIf a limit order falls inside the book or at the best quotes the mechanism for\norder deposition is left unchanged.\nInstead, if a limit order is placed inside\nthe spread, the probabilities associated to the s \u22121 available quotes are no\nmore uniform but highly peaked around the quote adjacent to the best. The\ndeposition probabilities for a buy or a sell order, for a given s, become\ng(i|s) =\n\uf8f1\n\uf8f2\n\uf8f3\ng(1|s) = \u03b1\ng(i|s) = 1\u2212\u03b1\ns\u22122\ni = 2, ..., s \u22121.\n(3)\nFor buy orders the index i is the distance from the best bid while for sell orders\nthe index i is the distance from the best ask. In the previous section we have\nseen that in real markets \u03b1 can be approximately considered as constant and\n7\n\nits value is about 0.7. Clearly Eq. 3 is meaningful only for s \u22653 since for s = 1\nlimit orders cannot fall inside the spread and for s = 2 a limit order inside the\nspread is always placed at the quote adjacent to the best one.\nIt follows directly from Eq. 3 that the transition probabilities now read\nP(e|o, s) = \u03b1 + 1\u2212\u03b1\n2\ns\u22123\ns\u22122\nP(o|o, s) = 1\u2212\u03b1\n2\ns\u22121\ns\u22122\nP(o|e, s) = \u03b1 + 1\u2212\u03b1\n2\nP(e|e, s) = 1\u2212\u03b1\n2 .\n(4)\nIn Fig.\n5 we plot a comparison between the expression of Eq.\n4 and the\ncorresponding probabilities evaluated from our data set. The experimental re-\nsults are obtained by averaging over all the 80 trading days and over all the 20\nstocks. The oscillating behavior can be explained by considering that a vari-\nation of spread of one tick (\u2206s = 1) is highly preferred with respect to other\nvariations. Hence an odd spread goes more likely to an even one and vice-versa.",
    "chunk_index": 7,
    "start_char": 16182,
    "end_char": 18774,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": ".\n(4)\nIn Fig.\n5 we plot a comparison between the expression of Eq.\n4 and the\ncorresponding probabilities evaluated from our data set. The experimental re-\nsults are obtained by averaging over all the 80 trading days and over all the 20\nstocks. The oscillating behavior can be explained by considering that a vari-\nation of spread of one tick (\u2206s = 1) is highly preferred with respect to other\nvariations. Hence an odd spread goes more likely to an even one and vice-versa.\nWe are now able to separate the two e\ufb00ects that contribute to enhance the frac-\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nspread (ticks)\n0\n0.2\n0.4\n0.6\n0.8\n1\nP ( odd | spread )\nmodel\nexperimental\nFigure 5: (Color online) Comparison between the experimental data and the\nphenomenological model described in the text. The phenomenological proba-\nbilities of the model (dots) show a small systematic overestimation of the os-\ncillations with respect to the experimental ones (triangles). This e\ufb00ect can be\neasily understood in terms of the approximation considered in Eq. 3.\ntion of odd spreads and produce the pattern of Fig. 1 through a simple Monte\nCarlo simulation. These two contributions are the intrinsic asymmetry due to\ndiscreteness and non-uniformity of order deposition. As initial conditions we\ngenerate some sequences of spreads with di\ufb00erent means, in order to represent\ndi\ufb00erent virtual stocks. Starting from each sequence we simulate the transition\nto odd or even spreads s\u2032 according to the probabilities of Eq. 1 and Eq. 4. In\nsuch a way we can evaluate the average fraction of odd spreads for each virtual\nstock. In Fig. 6 we compare the empirical average asymmetry with the results\nof the Monte Carlo simulations in the two cases just mentioned. The intrinsic\nasymmetry alone is not able to \ufb01t properly the experimental data which instead\n8\n\nare well reproduced by considering the two combined e\ufb00ects. Nevertheless we\ncan observe some deviations for large spreads (> 6). This is due to the fact that\nwe have assumed a constant probability to place order at quotes di\ufb00erent from\nthe one adjacent to the best. When the spread grows the error introduced by\nthis assumption becomes larger. It is worth noticing that here we are neglecting\n2\n4\n6\n8\n10\n<spread>\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nfraction of odd spreads\naverage (NYSE data)\nuniform - Eq. (1)\n\u03b1=0.7 - Eq. (4)\nFigure 6: (Color online) Fraction of odd-valued spread vs average spread for\nexperimental data and for two di\ufb00erent Monte Carlo simulations. The \ufb01rst sim-\nulation (squares) is performed using the uniform order deposition mechanism.\nThe second one (crosses) is instead performed with the non uniform mechanism.\nThese two simulations permit to investigate the two contributions to the asym-\nmetry between odd and even spreads. As expected we observe that the intrinsic\nasymmetry does not reproduce the experimental pattern (dots) while the two\ncombined e\ufb00ects \ufb01t the experimental data very well.",
    "chunk_index": 8,
    "start_char": 18302,
    "end_char": 21223,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "data and for two di\ufb00erent Monte Carlo simulations. The \ufb01rst sim-\nulation (squares) is performed using the uniform order deposition mechanism.\nThe second one (crosses) is instead performed with the non uniform mechanism.\nThese two simulations permit to investigate the two contributions to the asym-\nmetry between odd and even spreads. As expected we observe that the intrinsic\nasymmetry does not reproduce the experimental pattern (dots) while the two\ncombined e\ufb00ects \ufb01t the experimental data very well. The small discrepancy\nfound for large spreads is originated by the approximation made in Eq. 3.\nthe correlations between the spread values. Anyway we can recover the experi-\nmental behavior even with this uncorrelated sequence since we are averaging on\ntimes far longer than the time scales of the spread correlation.\n6\nThe e\ufb00ects of the strategic deposition of or-\nders\nAn interesting question concerns the role of the parameter \u03b1 and how the non-\nuniform deposition inside the spread a\ufb00ects the order book statistical properties.\nOur model allows this kind of investigation and permits to study the e\ufb00ect of\ndi\ufb00erent strategies of order placement inside the spread.\nA numerical simulation reveals that if the same set of parameters of Sec. 3 and\nof [22] is used, the spread dynamics diverges for \u03b1 > 0.85. To explain this e\ufb00ect\nwe have to consider the interplay between market and limit orders. Market\norders tend to move away the two best quotes eroding the book. Instead limit\norders tend to reduce the spread by coupling the processes followed by best ask\nand best bid. In such a way the process for the spread is somehow stationary.\n9\n\nIn this framework the deposition rules play an important role in softening or\nstrengthening the coupling action of limit orders. In fact the coupling between\na(t) and b(t) is ruled by two elements. The \ufb01rst one is the fraction of orders\nwhich fall inside the spread. A larger fraction of these orders produces a stronger\ncoupling between the best quotes. The second is the mechanism of deposition of\norders inside the spread, which governs the average spread variation produced\nby a limit order. In order to analyze the e\ufb00ect of the deposition mechanism we\ncan reason as it follows. By considering the symmetry of the uniform case, we\n\ufb01nd\n< \u2206s >\ns\n= 1\n2\n(5)\nwhile in the non-uniform case, from Eq. 4, we obtain\n< \u2206s >\ns\n= \u03b1\ns + (1 \u2212\u03b1)[s(s \u22121) \u22122]\n2s(s \u22122)\n.\n(6)\nWe recall that s \u22653 since for s = 1 limit orders cannot be placed inside the\nspread and for s = 2 we always have \u2206s/s = 1/2. The inequality < \u2206s > /s \u2265\n1/2 for Eq. 6 is satis\ufb01ed when\n2 \u2264s \u2264\u03b1 + 1\n\u03b1\n(7)\nand we observe that (\u03b1 + 1)/\u03b1 > 3 only for \u03b1 < 0.5 (see Fig. 7).",
    "chunk_index": 9,
    "start_char": 20720,
    "end_char": 23393,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "since for s = 1 limit orders cannot be placed inside the\nspread and for s = 2 we always have \u2206s/s = 1/2. The inequality < \u2206s > /s \u2265\n1/2 for Eq. 6 is satis\ufb01ed when\n2 \u2264s \u2264\u03b1 + 1\n\u03b1\n(7)\nand we observe that (\u03b1 + 1)/\u03b1 > 3 only for \u03b1 < 0.5 (see Fig. 7). Consequently\nthe average spread variation and the coupling action of limit orders in the non-\nuniform case are never larger than the one produced by the uniform mechanism\nin the region of parameters investigated (s \u22653 and \u03b1 > 0.5).\nFixed \u03b1 \u22640.8 we can analyze the properties of our model and we will come back\non the problem of the stability with respect to \u03b1 at the end of this section. The\nset of \ufb01gures in Fig. 8 clari\ufb01es the role of a non-uniform deposition inside the\nspread. In Fig. 8a and Fig. 8b we have plotted the probability density functions\nfor the price variations (returns) and for the spreads respectively. We observe\nthat in the non-uniform case the system produces larger \ufb02uctuations and larger\naverage spreads, as we expected from the previous discussion. It is interesting\nto notice that when the system tends to a regime in which the order book is\nalways compact, i.e. in which most of the quotes inside the book are occupied,\nthe statistical properties becomes nearly independent on the deposition details\n(\u03c0 \u22640.25).\nFig. 8c reveals that the non-uniform deposition also ampli\ufb01es the \ufb02uctuations\nof the granularity g, de\ufb01ned as the linear density of the volume stored in a\nside of the order book [22]. In particular the non-uniform deposition shows a\nnon-trivial temporal structure for granularity that resembles an intermittency\nphenomenon. Since most of the arriving limit orders are placed adjacent to\nthe best quote the order book stays for long times in a quiet and compact\nstate characterized by an average spread whose value is nearly one. This is\nthe dominant regime of our simulated order book, but sometimes bursts of\nvolatility are observed. In fact, when a large \ufb02uctuation of spread occurs, the\nautoregressive property and the non-uniformity of the limit order deposition\nmake the relaxation towards the compact state very slow. This intermittency\nis directly related to the volatility correlation that is far longer in the non-\nuniform case than in the uniform one (see Fig. 8d). We want to stress that a\n10\n\n0\n0.1 0.2\n0.3 0.4\n0.5 0.6 0.7 0.8 0.9\n1\n\u03b1\n1\n2\n3\n4\n5\n6\n7\n8\nspread\ns=2\ns=(\u03b1+1)/\u03b1\n<\u2206s>/s < 0.5\n<\u2206s>/s > 0.5\nFigure 7: Phase diagram of the relative average spread variation for the non-\nuniform mechanism. The relative average spread variation < \u2206s > /s is 0.5\nfor the uniform case. < \u2206s > /s is larger than 0.5 only for spreads included\nin the region between s = 2 and s = (\u03b1 + 1)/\u03b1. The highlighted region (s \u22653\nand \u03b1 > 0.5) corresponds to a realistic scenario.",
    "chunk_index": 10,
    "start_char": 23148,
    "end_char": 25897,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "2\n3\n4\n5\n6\n7\n8\nspread\ns=2\ns=(\u03b1+1)/\u03b1\n<\u2206s>/s < 0.5\n<\u2206s>/s > 0.5\nFigure 7: Phase diagram of the relative average spread variation for the non-\nuniform mechanism. The relative average spread variation < \u2206s > /s is 0.5\nfor the uniform case. < \u2206s > /s is larger than 0.5 only for spreads included\nin the region between s = 2 and s = (\u03b1 + 1)/\u03b1. The highlighted region (s \u22653\nand \u03b1 > 0.5) corresponds to a realistic scenario. We see that in this region\nthe relative average variation of the spread produced by a limit order is always\nsmaller than 0.5. Therefore the non-uniform deposition reduces the coupling\naction of limit orders with respect to the uniform case.\nsmall volatility clustering is already present in the uniform case. Its origin can\nbe traced back to the dependence on the past spread values of the deposition\nmechanism.\nThis simple e\ufb00ect introduces an exponential correlation and so\n\ufb01xes a characteristic time-scale. The non-uniform deposition instead ampli\ufb01es\nthis e\ufb00ect because the mechanism sets a further and longer time-scale that\ndepends on \u03b1 (the correlation length increases for increasing values of \u03b1). The\ncorrelation functions in Fig. 8d obey to an exponential decay except for short\ntime lags where some spurious e\ufb00ects take place. The bursts of volatility of\nthe non-uniform case are even more evident if we represent the complete order\nbook. In Fig. 9, that corresponds to the uniform case, the order book is always\ncompact. Instead in Fig. 10 we plot the non-uniform case and we \ufb01nd that the\nsystem stays for most of the time in a regime which is very similar to the one\nof Fig. 9 (regions I,III,V) but sometimes regions characterized by large spreads\nand large price movements appear (regions II,IV,VI). It is worth noticing that\nin this model large price \ufb02uctuations emerge spontaneously, being triggered by\na random spread variation (and vice-versa).\nThis mechanism resembles the\nphenomenon of self-organized criticality [25, 26].\nIt can be argued that also for larger values of \u03c0 the uniform case could produce\nthis intermittency because this correspond to an increase of the time-scale on\nwhich the autoregressive mechanism is able to produce local volatility clustering\nas we can observe in Fig. 8d. Nevertheless the correlation is still far shorter\nwhen \u03c0 = 0.33 with respect to the one generated by the non-uniform case with\n\u03c0 = 0.3. Furthermore the magnitude of the correlation is smaller than the one\nof the non-uniform case with lower values of \u03c0. Finally a visual inspection of the\n11\n\n1\n10\n100\nreturns\n10\n-5\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\npdf(returns)\n\u03b1=0.7 \u03c0=0.33\n\u03b1=0.7 \u03c0=0.30\n\u03b1=0.7 \u03c0=0.25\nunif \u03c0=0.33\nunif \u03c0=0.30\nunif \u03c0=0.25\na)\n0\n5\n10\n15\nspread\n0\n0.1\n0.2\n0.3\n0.4\n0.5\npdf(spread)\n<s>=2.77 \u03b1=0.7 \u03c0=0.33\n<s>=2.17 \u03b1=0.7 \u03c0=0.30\n<s>=1.71 \u03b1=0.7 \u03c0=0.25\n<s>=15.8 unif \u03c0=0.33\n<s>=2.07 unif \u03c0=0.30\n<s>=1.80 unif \u03c0=0.25\nb)\n0\n5\n10\n15\n20\n25\ng\n0",
    "chunk_index": 11,
    "start_char": 25482,
    "end_char": 28348,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "lower values of \u03c0. Finally a visual inspection of the\n11\n\n1\n10\n100\nreturns\n10\n-5\n10\n-4\n10\n-3\n10\n-2\n10\n-1\n10\n0\npdf(returns)\n\u03b1=0.7 \u03c0=0.33\n\u03b1=0.7 \u03c0=0.30\n\u03b1=0.7 \u03c0=0.25\nunif \u03c0=0.33\nunif \u03c0=0.30\nunif \u03c0=0.25\na)\n0\n5\n10\n15\nspread\n0\n0.1\n0.2\n0.3\n0.4\n0.5\npdf(spread)\n<s>=2.77 \u03b1=0.7 \u03c0=0.33\n<s>=2.17 \u03b1=0.7 \u03c0=0.30\n<s>=1.71 \u03b1=0.7 \u03c0=0.25\n<s>=15.8 unif \u03c0=0.33\n<s>=2.07 unif \u03c0=0.30\n<s>=1.80 unif \u03c0=0.25\nb)\n0\n5\n10\n15\n20\n25\ng\n0\n0.1\n0.2\n0.3\npdf(g)\n\u03b1=0.7 \u03c0=0.33\n\u03b1=0.7 \u03c0=0.30\n\u03b1=0.7 \u03c0=0.25\nunif \u03c0=0.33\nunif \u03c0=0.30\nunif \u03c0=0.25\nc)\n0\n200\n400\n600\n800\n1000\ntime (ticks)\n0\n0.05\n0.1\n0.15\n0.2\nAutocorrelation |r|\n\u03b1=0.7 \u03c0=0.33\n\u03b1=0.7 \u03c0=0.30\nunif \u03c0=0.33\nunif \u03c0=0.30\nd)\nFigure 8: (Color online) Statistical properties of the simulated order book for\nthe uniform and non-uniform case. In panels a) and b) we plot the probability\ndensity functions for the returns (\u2206p) and spreads respectively for di\ufb00erent\nmarket order rates (\u03c0). These plots show that the average \ufb02uctuations of spreads\nand returns are larger in the non-uniform case. When the order book turns to\na compact regime (\u03c0 \u22640.25), the statistical properties of the model become\nnearly independent on the deposition details. Panel c) reveals that the non-\nuniform deposition produces non-trivial \ufb02uctuations of liquidity/granularity. In\npanel d) we plot the autocorrelation of the absolute values of returns and we\nobserve the presence of persistent volatility. The decay of the autocorrelation\nfunction of the absolute values of returns is exponential except for short time\nlags where spurious e\ufb00ects take place. This persistent behavior suggests the\npresence of an intermittent dynamics for the order book characterized by bursts\nof volatility.\n12\n\norder book reveals that an intermittency of a kind appears but it is very small\nand we never obtain an order book with sudden transitions from a compact\nregime to a volatile regime as it happens in region II of Fig. 10.\nFigure 9: (Color online) Snapshot of the simulated order book in the uniform\ncase (\u03c0 = 0.3). The order book is always in a compact regime in which the aver-\nage spread is nearly 1 and a small and local volatility clustering is observed due\nto the autoregressive deposition rules. For higher values of \u03c0 small deviations\nfrom the compact regime appear but these phenomena cannot be compared to\nthe intermittency produced when the order deposition is non-uniform.\nNow we discuss the stability of the model for \u03b1 \u22650.8. We have seen in Figs.\n8 a,b that, in order to make the system stable with respect to \u03b1, a possible\nsolution is the reduction of the probability of market orders \u03c0. In such a way it\nis possible to increase the average length of limit order sequences and then to\ncompensate the lower coupling. However increasing values of \u03b1 (> 0.8) would\nimply a choice for \u03c0 \u22480.25 (or even smaller) and in this region of parameters\nthe order book is always compact and all volatility bursts disappear.",
    "chunk_index": 12,
    "start_char": 27945,
    "end_char": 30835,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "in order to make the system stable with respect to \u03b1, a possible\nsolution is the reduction of the probability of market orders \u03c0. In such a way it\nis possible to increase the average length of limit order sequences and then to\ncompensate the lower coupling. However increasing values of \u03b1 (> 0.8) would\nimply a choice for \u03c0 \u22480.25 (or even smaller) and in this region of parameters\nthe order book is always compact and all volatility bursts disappear. However\nit is worth noticing that these ranges of parameters are usually not observed in\nempirical data.\n7\nSpread relaxation: role of the strategic order\nplacement.\nPonzi et al. in [24] studied the relaxation dynamics after an opening or a closing\nof the spread in LSE 2 order book. They \ufb01nd a slow relaxation of the spread\ntowards the mean value. This decay is compatible with a power law and the\nauthors argue that the absence of a characteristic time scale is due to the pres-\nence of a strategic placement of the orders.\n2London Stock Exchange\n13\n\nFigure 10: (Color online) Snapshot of the simulated order book in the non-\nuniform case (\u03c0 = 0.3 and \u03b1 = 0.7). These deposition rules produce an order\nbook which is typically quiet and compact as in the uniform case (region I, III,\nV) but exhibits bursts of activity due to large \ufb02uctuations of the spread (region\nII, IV, VI). The system gives rise to a sort of intermittency since volatility is\nvery persistent and clustered.\nWe want to verify this empirical \ufb01ndings in the framework of the model intro-\nduced in the previous sections. In this respect we de\ufb01ne, as in [24], the quantity\nG(\u03c4|\u2206) = E[s(t + \u03c4)|s(t) \u2212s(t \u22121) = \u2206] \u2212E[s(t)]\n(8)\nwhere E[\u00b7] is the average on the whole time serie, \u2206is the spread variation\noccured at time t (i.e. \u03c4 = 0) and \u03c4 \u22650. It is worth noticing that, in our\nmodel, \u03c4 is expressed in time event units di\ufb00erently from the analysis in [24]\nwhich is performed in physical time. The mapping between these two time units\nis not necessarily linear and therefore a quantitative agreement should not be\nalways expected.\nWe perform the spread relaxation analysis in both the uniform case and non-\nuniform case and we report the results in Fig. 11. The non-uniform mechanism\nproduces a plateau of a kind and then for \u03c4 > 100 a faster decay to normal\nspread values. Instead in the uniform case the spread relaxation is much faster\nthan the previous case. Now we analyze in detail the non-uniform case. In\nFig. 12 we report the function G(\u03c4|\u2206) for positive and negative values of \u2206\n(corresponding respectively to openings and closings of the spread at time \u03c4 =\n0). As in [24], we observe two slightly di\ufb00erent patterns for negative and positive\n\u2206. Instead, we do not observe such a di\ufb00erence in the uniform case, this means\nthe nature of the relaxation dynamics is completely di\ufb00erent for these two cases.",
    "chunk_index": 13,
    "start_char": 30385,
    "end_char": 33213,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "relaxation is much faster\nthan the previous case. Now we analyze in detail the non-uniform case. In\nFig. 12 we report the function G(\u03c4|\u2206) for positive and negative values of \u2206\n(corresponding respectively to openings and closings of the spread at time \u03c4 =\n0). As in [24], we observe two slightly di\ufb00erent patterns for negative and positive\n\u2206. Instead, we do not observe such a di\ufb00erence in the uniform case, this means\nthe nature of the relaxation dynamics is completely di\ufb00erent for these two cases.\nWe can conclude that, in order to obtain a realistic spread relaxation function,\na simple change to a pure zero-intelligence model consists in the non-uniform\nmechanism of limit orders deposition inside the spread described in section 5.\n14\n\n10\n0\n10\n1\n10\n2\n10\n3\ntime (event time)\n10\n-2\n10\n-1\n10\n0\nG(\u03c4|\u2206) (ticks)\n\u2206=4 unif\n\u2206=2 unif\n\u2206=4 \u03b1=0.7\n\u2206=2 \u03b1=0.7\nFigure 11: (Color online) Spread decay given \u2206in the uniform case (solid lines)\nand in the non-uniform case (dashed lines). We observe a much slower decay in\nthe latter case.\n8\nConclusions and perspectives\nThe order book is a system which is intrinsically discrete, for instance the quotes\nof placement of an order must be a multiple of the tick size. We have investi-\ngated which are the e\ufb00ects of this discreteness \ufb01nding non-trivial aspects and\ndeviations with respect to a continuous regime.\nThe starting point of this work has been the observation that odd and even\nspreads are not equivalent for limit order deposition when the available quotes\ninside the spread are discrete. In fact, if a uniform deposition of orders inside\nthe spread is assumed, the system spontaneously prefers odd spreads.\nOne of results of this paper con\ufb01rms that this asymmetry is present in real order\nbooks and that the fraction of odd spreads is signi\ufb01cantly above 0.5. However\nthe asymmetry observed cannot be explained quantitatively only by consider-\ning the discrete nature of the order book. Indeed we have found that a second\ne\ufb00ect also contributes to modulate the asymmetry, the fact that agents prefer\nthe quote adjacent to the best one when they place orders inside the spread.\nBoth these contributions have been investigated in the framework of a micro-\nscopic model introduced in a previous work [22]. The model permits to compare\nthe e\ufb00ects of uniform and non-uniform deposition mechanisms for limit orders\ninside the spread. We have found that the asymmetry can be quantitatively\nreproduced in the framework of our model by introducing a non-uniform depo-\nsition mechanism.\nAnother result is the emergence of a sort of intermittent dynamics in which a\nregime characterized by a compact and liquid order book dominates but bursts\nof volatility also appear. This intermittent behavior is also observed in real\norder books. In this respect, in the framework of our model, we compare the\nuniform and the non-uniform mechanism for order deposition with respect to\nthe dynamics of the spread relaxation when a \ufb02uctuation occurs. We \ufb01nd that\nthe introduction of a simple rule of order placement is su\ufb03cient to reproduce\n15",
    "chunk_index": 14,
    "start_char": 32714,
    "end_char": 35776,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "a non-uniform depo-\nsition mechanism.\nAnother result is the emergence of a sort of intermittent dynamics in which a\nregime characterized by a compact and liquid order book dominates but bursts\nof volatility also appear. This intermittent behavior is also observed in real\norder books. In this respect, in the framework of our model, we compare the\nuniform and the non-uniform mechanism for order deposition with respect to\nthe dynamics of the spread relaxation when a \ufb02uctuation occurs. We \ufb01nd that\nthe introduction of a simple rule of order placement is su\ufb03cient to reproduce\n15\n\n10\n0\n10\n1\n10\n2\n10\n3\ntime (event time)\n10\n-1\n10\n0\n10\n1\nG(\u03c4|\u2206) (ticks)\n\u2206=10\n\u2206=9\n\u2206=8\n\u2206=7\n\u2206=6\n\u2206=5\n\u2206=4\n\u2206=3\n\u2206=2\n10\n0\n10\n1\n10\n2\n10\n3\ntime (event time)\n10\n-2\n10\n-1\n10\n0\n10\n1\nG(\u03c4|\u2206) (ticks)\n\u2206=\u221210\n\u2206=\u22129\n\u2206=\u22128\n\u2206=\u22127\n\u2206=\u22126\n\u2206=\u22125\n\u2206=\u22124\n\u2206=\u22123\nFigure 12: (Color online) Spread decay in the non-uniform case for positive\n(top panel) and negative (bottom panel) values of \u2206. The value of G(\u03c4|\u2206) is\nhigher for higher absolute values of \u2206. The patterns found in our model are\nvery similar to the empirical results of Ponzi et al. in [24].\n16\n\nthe peculiar pattern observed in real data (see [24]). The observed relaxation\ncannot be explained by a pure zero-intelligence model (see also [27]).\nAn interesting point is how the agents\u2019 strategies take into account the discrete\nproperties of the order book. These question will be matter of future works.\nReferences\n[1] J. Hasbrouck. Empirical Market Microstructure. Oxford University Press,\n2007.\n[2] L. Harris. Trading and Exchanges. Oxford University Press, 2003.\n[3] J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative\nPricing: From Statistical Physics to Risk Management. Cambridge Uni-\nversity Press, 2003.\n[4] J.-P. Bouchaud, M. M\u00b4ezard, and M. Potters. Statistical properties of stock\norder books: empirical results and models. Quantitative Finance, pages\n251\u2013256, 2002.\n[5] M. Potters and J.-P. Bouchaud. More statistical properties of order books\nand price impact. Physica A, 324:133\u2013140, 2003.\n[6] S. Mike and J.D. Farmer. An empirical behavioral model of liquidity and\nvolatility. Journal of Economic Dynamics and Control, 32:200\u2013234, 2008.\n[7] J.-P. Bouchaud, J. D. Farmer, and F. Lillo. How markets slowly digest\nchanges in supply and demand, 2008.\n[8] J.-P. Bouchaud, Y. Gefen, M. Potters, and M. Wyart. Fluctuations and\nresponse in \ufb01nancial markets: the subtle nature of \u2018random\u2019 price changes.\nQuantitative Finance, 4:176\u2013190, 2004.\n[9] J.-P. Bouchaud, J. Kockelkoren, and M. Potters. Random walks, liquidity\nmolasses and critical response in \ufb01nancial markets. Quantitative Finance,\n6:115\u2013123, 2006.\n[10] F. Lillo and J.D. Farmer. The long memory of the e\ufb03cient market. Studies\nin Nonlinear Dynamics & Econometrics, 8:1226\u20131226, 2004.\n[11] J.D. Farmer, A. Gerig, F. Lillo, and S. Mike. Market e\ufb03ciency and the long-\nmemory of supply and demand: is price impact variable and permanent or\n\ufb01xed and temporary? Quantitative Finance, 6:107\u2013112, 2006.\n[12] F. Lillo, J.D. Farmer, and R.N. Mantegna. Econophysics: Master curve for\nprice-impact function.",
    "chunk_index": 15,
    "start_char": 35197,
    "end_char": 38281,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "molasses and critical response in \ufb01nancial markets. Quantitative Finance,\n6:115\u2013123, 2006.\n[10] F. Lillo and J.D. Farmer. The long memory of the e\ufb03cient market. Studies\nin Nonlinear Dynamics & Econometrics, 8:1226\u20131226, 2004.\n[11] J.D. Farmer, A. Gerig, F. Lillo, and S. Mike. Market e\ufb03ciency and the long-\nmemory of supply and demand: is price impact variable and permanent or\n\ufb01xed and temporary? Quantitative Finance, 6:107\u2013112, 2006.\n[12] F. Lillo, J.D. Farmer, and R.N. Mantegna. Econophysics: Master curve for\nprice-impact function. Nature, 421:129\u2013130, 2003.\n[13] V. Plerou, P. Gopikrishnan, X. Gabaix, and H.E. Stanley.\nQuantifying\nstock-price response to demand \ufb02uctuations. Phys. Rev. E, 66(2):027104,\nAug 2002.\n[14] R.N. Mantegna and H.E. Stanley. An Introduction to Econophysics: Cor-\nrelation and Complexity in Finance. Cambridge University Press, 2002.\n17\n\n[15] B. Mandelbrot. Forecasts of future prices, unbiased markets and \u201dmartin-\ngale\u201d models. Journal of Business, 1966.\n[16] R. Cont. Empirical properties of asset returns: stylized facts and statistical\nissues. Quantitative Finance, 1:223\u2013236, 2001.\n[17] F. Slanina. Critical comparison of several order-book models for stock-\nmarket \ufb02uctuations. The European Physical Journal B, 61:225\u2013240, 2008.\n[18] M. Raberto, S. Cincotti, S.M. Focardi, and M. Marchesi.\nAgent-based\nsimulation of a \ufb01nancial market. Physica A, 299:319\u2013327, 2001.\n[19] S. Maslov.\nSimple model of a limit order-driven market.\nPhysica A,\n278:571\u2013578, 2000.\n[20] D. Challet and R. Stinchcombe. Non-constant rates and overdi\ufb00usive prices\nin simple models of limit order markets. Quant. Fin., 3:155\u2013162, 2003.\n[21] M.G. Daniels, J.D. Farmer, L. Gillemot, G. Iori, and E. Smith. Quanti-\ntative model of price di\ufb00usion and market friction based on trading as a\nmechanistic random process. Phys. Rev. Lett., 90(10):108102, Mar 2003.\n[22] M. Cristelli, V. Al\ufb01, L. Pietronero, and A. Zaccaria. Liquidity crisis, gran-\nularity of the order book and price \ufb02uctuations. Eur. Phys. J. B, DOI:\n10.1140/epjb/e2009-00353-6, 2009\n[23] Z. Eisler, J. K\u00b4ertesz, and F. Lillo. The limit order book on di\ufb00erent time\nscales. PROC.SPIE, 6601:66010G, 2007.\n[24] A. Ponzi, F. Lillo, and R.N. Mantegna. Market reaction to a bid-ask spread\nchange: A power-law relaxation dynamics. Phys. Rev. E, 80:016112, 2009.\n[25] P. Bak.\nHow Nature Works: The Science of Self-Organized Criticality.\nCopernicus Press, New York, 1996.\n[26] H.J. Jensen. Self-organized criticality. Cambridge University Press, Cam-\nbridge, 1998.\n[27] B. Toth, J. K\u00b4ertesz, and J.D. Farmer.\nStudies of the limit order book\naround large price changes. Eur. Phys. J. B, 71:499-510, 2009.\n18",
    "chunk_index": 16,
    "start_char": 37744,
    "end_char": 40409,
    "paper_title": "Asymmetric statistics of order books The role of d",
    "paper_category": "q-fin.TR",
    "paper_filename": "Asymmetric_statistics_of_order_books_The_role_of_d.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Asymmetric_statistics_of_order_books_The_role_of_d.pdf"
  },
  {
    "text": "Copula-Based Trading of Cointegrated\nCryptocurrency Pairs\nMASOOD TADI *\nFaculty of Mathematics and Physics, Charles University\nPrague, Czech Republic\ntadim@karlin.mff.cuni.cz\nJI\u02c7R\u00b4I WITZANY\nFaculty of Finance and Accounting, Prague University of Economics and Business\nPrague, Czech Republic\njiri.witzany@vse.cz\nAbstract\nThis research introduces a novel pairs trading strategy based on copulas for cointegrated\npairs of cryptocurrencies. To identify the most suitable pairs, the study employs linear\nand non-linear cointegration tests along with a correlation coefficient measure and fits\ndifferent copula families to generate trading signals formulated from a reference asset\nfor analyzing the mispricing index. The strategy\u2019s performance is then evaluated by\nconducting back-testing for various triggers of opening positions, assessing its returns\nand risks. The findings indicate that the proposed method outperforms buy-and-hold\ntrading strategies in terms of both profitability and risk-adjusted returns.\nKeywords: Statistical arbitrage, Pairs trading, Cointegration, Copulas, Cryptocurrency\nmarket.\nAcknowledgements: This paper has been prepared under financial support of a grant\nGA \u02c7CR 22-19617S \u201cModeling the structure and dynamics of energy, commodity and\nalternative asset prices\u201d, which the authors gratefully acknowledge.\n*Corresponding author\n1\narXiv:2305.06961v2 [q-fin.TR] 11 Jun 2023\n\n1\nIntroduction\nPairs trading is a well-known algorithmic trading strategy that capitalizes on temporary\nabnormal relationships among two or multiple assets whose historical prices tend to\nmove together. When this relationship begins to exhibit abnormal behavior, it triggers\nthe opening of trading positions. These positions are closed as soon as the pairs return\nto their normal behavior (Vidyamurthy 2004).\nIn the decentralized cryptocurrency\nmarket, pairs trading can be profitable, offering two potential arbitrage opportunities:\nexchange-to-exchange arbitrage and statistical arbitrage.\nHowever, implementing a\nstatistical arbitrage strategy based on exchange-to-exchange arbitrage can be risky and\npose numerous challenges.\nIn contrast, statistical arbitrage opportunities present\nsimilar profit potentials with lower risk (Pritchard 2018).\nAs stated by Krauss (2017), pairs trading is characterized by a formation and a trading\nperiod. During the formation period, the objective is to identify pairs of assets that\nexhibit similar price movements. This is commonly achieved through co-movement\ncriteria, which can be measured using various methods such as distance metrics\n(distance approach), for instance, the minimum sum of squared distances of\nnormalized\nasset\nprices,\nor\nstatistical\nrelationships\nlike\ncointegration\nrules\n(cointegration approach).\nFurthermore, the parameters of the trading period are\nestimated during the formation period. During the trading timeframe, irregularities in\npairs\u2019 price movement are aimed to benefit from statistical arbitrage opportunities and\ncreate signals to open long/short positions. Advanced strategies may utilize a range of\nmathematical tools, such as stochastic processes, stochastic control techniques,\ncopulas, and machine learning methods, to enhance the effectiveness of their outcomes.\nThis paper aims to implement a copula-based pairs trading strategy on cointegrated\ncryptocurrency pairs. First, we start with a literature review on existing pairs trading\nstrategies.\nWe also provide an overview of the cryptocurrency exchange and data\nsources used in this study. The theoretical framework of the strategy includes linear\nand non-linear cointegration tests to identify cointegrated pairs and the copula concept\nto model their dependence structure.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3710,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "learning methods, to enhance the effectiveness of their outcomes.\nThis paper aims to implement a copula-based pairs trading strategy on cointegrated\ncryptocurrency pairs. First, we start with a literature review on existing pairs trading\nstrategies.\nWe also provide an overview of the cryptocurrency exchange and data\nsources used in this study. The theoretical framework of the strategy includes linear\nand non-linear cointegration tests to identify cointegrated pairs and the copula concept\nto model their dependence structure. We discuss different families of copulas and the\nmethods for copula estimation. We then outline the methodology for implementing the\ncopula-based trading strategy, including ranking, selection of the assets, and the\ngeneration of the trading signals. Next, we present the empirical results of back-testing\nthe strategy to a dataset of historical cryptocurrency prices.\nWe analyze the\nperformance of the strategy using various metrics, including profitability, risk-adjusted\nreturn, and maximum drawdown. In addition, we perform a comparative analysis of\nour results with the buy-and-hold strategy.\n2\n\n2\nLiterature Review\nThere is extensive literature on pair trading strategies that use various concepts, such as\nthe distance approach, cointegration analysis, or the concept of copulas. The distance\napproach involves calculating the historical price spread or price difference between\ntwo related assets and monitoring this spread over time.\nThe spread is usually\ncalculated as the difference between the prices of the two assets, either as a raw spread\nor as a normalized spread, such as the z-score or the percent difference.\nAs an\nexample, Gatev, Goetzmann, and Rouwenhorst (2006) defined the normalized spread,\nSij\nt , between assets i and j at time t by\nSij\nt = P i\nt\n\u2217\u2212P j\nt\n\u2217\n(1)\nP i\nt\n\u2217= P i\nt\nP i\n0\n=\ntY\n\u03c4=1\nP i\n\u03c4\nP i\n\u03c4\u22121\n=\ntY\n\u03c4=1\n\u00001 + ri\n\u03c4\n\u0001\n= cri\nt,\n(2)\nwhere P i\nt is the price of the asset i at time t, ri\nt is the t-period\u2019s return on asset i at time\nt, and cri\nt is the cumulative total return of asset i up to time t (cri\n0 = 1). The calculation\nof the spread sum of squared distance is performed using the following equation:\nSSDi,j =\nT\nX\nt=1\n\u0000Sij\nt\n\u00012 =\nT\nX\nt=1\n(cri,t \u2212crj,t)2.\n(3)\nPairs are chosen for the trading period based on their ascending order of SSDi,j during\nthe formation stage.\nThe initial pairs at the top of the list are selected, and basic\nnon-parametric threshold rules are employed to generate trading rules. An alternative\nmethod was used by Chen et al. (2019), who identified the most suitable pairs for the\ntrading period using Pearson correlation of assets\u2019 returns instead of finding the\nminimum sum of the squared distance. Krauss (2017) found that to maximize the profit\nof the distance approach strategy, the spread of each selected pair needs to have high\nvolatility, indicating the potential divergence of the two assets.\nThe pair\u2019s spread\nshould also have a mean-reverting property.",
    "chunk_index": 1,
    "start_char": 3181,
    "end_char": 6142,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "to generate trading rules. An alternative\nmethod was used by Chen et al. (2019), who identified the most suitable pairs for the\ntrading period using Pearson correlation of assets\u2019 returns instead of finding the\nminimum sum of the squared distance. Krauss (2017) found that to maximize the profit\nof the distance approach strategy, the spread of each selected pair needs to have high\nvolatility, indicating the potential divergence of the two assets.\nThe pair\u2019s spread\nshould also have a mean-reverting property.\nThis approach\u2019s key advantage is its\nsimplicity and transparency, making it suitable for large-scale empirical applications.\nHuck (2015) found that the cointegration approach outperformed the distance approach\nin selecting effective pairs. The cointegration approach aims to identify a long-term\nequilibrium between non-stationary time series (e.g., asset prices) that move together.\nThis equilibrium can be linear or nonlinear. Engle and Granger (1987) introduced the\nfirst cointegration test, which is based on linear regression and the unit-root test of\nresiduals in the equilibrium. Typically, the augmented Dickey-Fuller test is used for the\nunit-root test.\nOther improvements of the Engle-Granger cointegration test were\nintroduced by Phillips and Ouliaris (1990) and Johansen (1991).\nHighly volatile\n3\n\nmarkets, such as cryptocurrency, usually exhibit nonlinear features. Therefore, we can\nextend the Engle-Granger cointegration test by adjusting the error correction model\nand applying nonlinear unit root tests to increase the reliability of the study. These\nextensions have been studied by Enders and Siklos (2001), Hansen and Seo (2002),\nand Kapetanios, Shin, and Snell (2006). Several research papers have been published\nthat specifically explore the application of pairs trading in the cryptocurrency market.\nThese include studies by Broek and Sharif (2018), Pritchard (2018), Kakushadze and\nYu (2019), Leung and Nguyen (2019), and Tadi and Kortchemski (2021).\nIn addition to the commonly used methods discussed above, more advanced concepts\nsuch as copulas can be applied to enhance the empirical results of the strategy.\nCompared to correlation or linear cointegration-based methods, the copula approach\nprovides more valuable information regarding the shape and characteristics of pairs\u2019\ndependency (Ferreira 2008). Xie and Wu (2013) demonstrated that the two commonly\nused pairs trading methods, namely the distance and cointegration methods, can be\ngeneralized as special cases of the copula method under certain conditions, and the\ndependency structure of assets in the copula approach is more robust and accurate.\nMoreover, Liew and Wu (2013) conducted a comparative study of a copula-based pairs\ntrading strategy with other conventional approaches such as the distance and\ncointegration approach.\nThey found that the copula approach for pairs trading has\nbetter empirical results than the others, as it provides more trading possibilities with\nhigher confidence in practice and does not entail any rigid assumptions, such as the\nlinearity association between assets\u2019 returns, in traditional approaches.\nHence, the\ncopula approach can provide closer estimations and predictions of reality. According\nto Stander, Marais, and Botha (2013), the copula approach can significantly\ndemonstrate the pairs\u2019 dependency, as it can capture the asymmetry and heavy-tail\ncharacteristics of asset returns to model the marginal distribution functions instead of\nmodeling them by Gaussian distribution.",
    "chunk_index": 2,
    "start_char": 5631,
    "end_char": 9149,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "trading has\nbetter empirical results than the others, as it provides more trading possibilities with\nhigher confidence in practice and does not entail any rigid assumptions, such as the\nlinearity association between assets\u2019 returns, in traditional approaches.\nHence, the\ncopula approach can provide closer estimations and predictions of reality. According\nto Stander, Marais, and Botha (2013), the copula approach can significantly\ndemonstrate the pairs\u2019 dependency, as it can capture the asymmetry and heavy-tail\ncharacteristics of asset returns to model the marginal distribution functions instead of\nmodeling them by Gaussian distribution. Additionally, their empirical analyses reveal\nthat there are more trading opportunities when the market is highly volatile, and the\nprofitability of the strategy strongly depends on the liquidity of the market.\nAccording to Krauss and St\u00a8ubinger (2017), the copula approach can be divided into\ntwo sub-streams: return-based and level-based. In the return-based copula method, the\nlog-returns of two assets are calculated, and their marginal distributions are estimated.\nThen, an appropriate copula is chosen to represent the dependency relationship\nbetween the two assets. To generate trading signals, a mispricing index is defined to\nindicate the degree of abnormal relationship between the assets. Unlike the distance\nand cointegration methods which use spread-based mispricing definition, the copula\nmethod defines mispricing based on the copula\u2019s conditional probability distribution of\nthe corresponding assets\u2019 log returns. The conditional distribution of copulas can be\n4\n\nobtained by taking the partial derivative of the copula function, as shown below1:\nh1|2 := h(u1|u2) = P(U1 \u2264u1|U2 = u2) = \u2202C(u1, u2)\n\u2202u2\nh2|1 := h(u2|u1) = P(U2 \u2264u2|U1 = u1) = \u2202C(u1, u2)\n\u2202u1\n(4)\nwhere C(u1, u2) is the copula distribution function, h1|2 and h2|1 are conditional copula\ndistribution functions, and U1 and U2 are the transformed uniform variables of the log\nreturns. The values of h1|2 and h2|1 are between 0 and 1, and as much as their values\nget away from 0.5, we can consider it as a deviance from the expected relationship\nbetween the two assets. Ferreira (2008), Liew and Wu (2013), Stander, Marais, and\nBotha (2013), and Keshavarz Haddad and Talebi (2023) deployed their strategy in that\nway.\nThe usage of the return-based method has a drawback in that entry and exit signals are\nlinked with only the previous period\u2019s return. To address this limitation, a new approach\n(hereafter referred to as the value-based method) was proposed by Xie and Wu (2013).\nThey defined a new mispricing index by aggregating the surplus value of conditional\nprobability in equation 4 from 0.5 across multiple periods to determine the extent to\nwhich assets are out of balance.\nCMI1|2\nt\n= CMI1|2\nt\u22121 +\n\u0010\nh1|2\nt\n\u22120.5\n\u0011\nCMI2|1\nt\n= CMI2|1\nt\u22121 +\n\u0010\nh2|1\nt\n\u22120.5\n\u0011\n(5)\nThe studies conducted by Xie, Liew, et al. (2016), Rad, Low, and Faff (2016), Krauss\nand St\u00a8ubinger (2017), and Silva, Ziegelmann, and Caldeira (2023). According to Xie,\nLiew, et al. (2016), the copula method outperforms the distance approach in terms of\ndescribing the dependency relationship between assets and identifying more statistical\narbitrage opportunities that generate greater profits.",
    "chunk_index": 3,
    "start_char": 8507,
    "end_char": 11788,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "to determine the extent to\nwhich assets are out of balance.\nCMI1|2\nt\n= CMI1|2\nt\u22121 +\n\u0010\nh1|2\nt\n\u22120.5\n\u0011\nCMI2|1\nt\n= CMI2|1\nt\u22121 +\n\u0010\nh2|1\nt\n\u22120.5\n\u0011\n(5)\nThe studies conducted by Xie, Liew, et al. (2016), Rad, Low, and Faff (2016), Krauss\nand St\u00a8ubinger (2017), and Silva, Ziegelmann, and Caldeira (2023). According to Xie,\nLiew, et al. (2016), the copula method outperforms the distance approach in terms of\ndescribing the dependency relationship between assets and identifying more statistical\narbitrage opportunities that generate greater profits. They also recommended utilizing\nthe copula approach for high-frequency pairs trading.\nRad, Low, and Faff (2016)\ncompared the performance of the copula approach with that of traditional methods\nusing daily US stocks. They found that while the profitability of the copula method\ncould be weaker, it is more reliable in capturing arbitrage opportunities in the US stock\nmarket. They determined that the Student-t copula is more appropriate for modeling\nthe dependence structure of pairs in the US stock market than other copulas.\nThe performance of a pairs trading strategy based on a weighted combination of\ncopulas was evaluated by Silva, Ziegelmann, and Caldeira (2023) against a distance\nmethodology, using a vast dataset of S&P500 stocks over 25 years.\nThe study\n1For more details, see Section 4.2.\n5\n\nexamined the effect of financial factors on profitability. The mixed copula approach\nyielded better results than the distance method, with higher alphas for fully invested\ncapital and superior performances overall. The approach was notably effective under\ncommitted capital during both crisis and non-crisis periods and fully invested during\nnon-crisis periods.\nHowever, in practice, the cumulative mispricing indices (CMI) in equation 5 may not\nnecessarily exhibit mean-reverting behavior,\nwhich can negatively impact the\nprofitability of the strategy. Hence, a new methodology is proposed in this study to\nreplace log-returns with stationary spread processes in order to overcome this issue2.\n3\nCryptocurrency Exchange and Data Source\nThe cryptocurrency market facilitates decentralized trading of cryptocurrencies across\nvarious exchanges.\nBinance, established in 2017, is the largest cryptocurrency\nexchange worldwide in terms of daily trading volume in both spot and derivatives\nmarkets.\nBinance provides three types of derivative contracts: Futures contracts,\nOptions, and Binance Leveraged Tokens (BLVT).\nFutures contracts are classified into two main categories: COIN-Margined contracts\nand USD s\u20dd-Margined contracts. COIN-Margined contracts are inverse Futures quoted\nin US dollars but denominated in an underlying cryptocurrency (e.g., Bitcoin). They\ninclude traditional quarterly Futures and perpetual Futures, also known as perpetual\nSwaps, which never expire or settle. Because they lack a settlement price, their price\ncan deviate significantly from their spot contract price. Binance uses funding fees to\naddress this issue for long and short sides.\nOn the other hand, USD s\u20dd-Margined\ncontracts are similar to COIN-Margined Futures, and they may have perpetual or\nquarterly expiration. However, they are quoted, denominated, and settled in stablecoins\nsuch as Tether (USDT) and Binance USD (BUSD), which are pegged to the US dollar\nvalue.",
    "chunk_index": 4,
    "start_char": 11248,
    "end_char": 14546,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "they lack a settlement price, their price\ncan deviate significantly from their spot contract price. Binance uses funding fees to\naddress this issue for long and short sides.\nOn the other hand, USD s\u20dd-Margined\ncontracts are similar to COIN-Margined Futures, and they may have perpetual or\nquarterly expiration. However, they are quoted, denominated, and settled in stablecoins\nsuch as Tether (USDT) and Binance USD (BUSD), which are pegged to the US dollar\nvalue. For this study, all nominated cryptocurrency coins are USDT-Margined Futures\n(Binance Crypto Derivatives accessed 2022-11-03).\nThe minimum trade amount, also known as the minimum price increment, is the\nsmallest possible change in the price of a contract at an exchange.\nThis value is\nspecific to each asset and can be adjusted over time. Assets with smaller increments\nhave narrower bid/ask spreads. When limit orders are not entirely disclosed to traders,\nthey tend to order contracts with smaller quote sizes to avoid slippage (Harris 1997).\nTo calculate profit and loss, all coins are valued in Tether (USDT), a stablecoin. Using\nthe Binance API, we collected historical hourly closed prices for twenty cryptocurrency\n2For more details, see Section 5.\n6\n\ncoins from 01/01/2021 to 19/01/2023. We used the linear Engle-Granger (EG) two-\nstep method and to identify cointegrated pairs and calculated Kendall\u2019s Tau. Then, we\ncalibrated the method\u2019s parameters using the copula approach and generated trading\nsignals for the strategy.\nTo calculate profit and loss, all coins are valued in Tether (USDT), a stablecoin. Using\nthe\nBinance\nAPI,\nwe\ncollected\nhistorical\nhourly\nclosed\nprices\nfor\ntwenty\ncryptocurrency coins from 01/01/2021 to 19/01/2023. We used Engle-Granger (EG)\nand Kapetanios-Shin-Shell (KSS) cointegration tests to identify cointegrated pairs and\ncalculated Kendall\u2019s Tau.\nThen, we calibrated the method\u2019s parameters using the\ncopula approach and generated trading signals for the strategy.\nA pairs trading cycle comprises formation and trading periods. In this study, the entire\nprocess of pairs trading is conducted within a month, with three weeks allocated for the\nformation step and the remaining week designated for the trading step. We carry out\n104 cycles that move dynamically over time, with each cycle sharing three-quarters of\nits data with its previous or subsequent cycle.\nFigure 1: The scheme of Formation and Trading Periods\nFurthermore, we implemented the entire methodology using Python and R, with\nPython being the preferred option for data handling and R for statistical tests and\ncopula modeling.\n7\n\nTable I: Python libraries and R packages used in this research\nPython\nR\nName\nType\nName\nType\nNumPy\nNumerical analysis\ncopula\nCopula modeling\nPandas\nTechnical computing\nVineCopula\nCopula modeling\nMatplotlib\nPlotting\nMASS\nStatistical package\nDatetime\nDate/time package\ndpylr\nData handling\nStatsmodels\nStatistical package\nlubridate\nDate/time package\nSciPy\nTechnical computing\nfUnitRoots\nUnit root analysis\n4\nTheoretical Framework\n4.1\nUnit-Root Test\nThe cointegration property can be employed to identify the most appropriate pair from\nmultiple combinations of coins. Both linear and non-linear cointegration tests can be\nutilized for evaluation. Initially, the pair spread value is defined without an intercept in\nthe following manner:\nSt = P 1\nt \u2212\u03b2P 2\nt\n(6)\nSuppose that P 1\nt and P 2\nt are non-stationary time series.",
    "chunk_index": 5,
    "start_char": 14084,
    "end_char": 17498,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "modeling\nPandas\nTechnical computing\nVineCopula\nCopula modeling\nMatplotlib\nPlotting\nMASS\nStatistical package\nDatetime\nDate/time package\ndpylr\nData handling\nStatsmodels\nStatistical package\nlubridate\nDate/time package\nSciPy\nTechnical computing\nfUnitRoots\nUnit root analysis\n4\nTheoretical Framework\n4.1\nUnit-Root Test\nThe cointegration property can be employed to identify the most appropriate pair from\nmultiple combinations of coins. Both linear and non-linear cointegration tests can be\nutilized for evaluation. Initially, the pair spread value is defined without an intercept in\nthe following manner:\nSt = P 1\nt \u2212\u03b2P 2\nt\n(6)\nSuppose that P 1\nt and P 2\nt are non-stationary time series. We use unit-root tests to study\nwhether the spread, St, is also a non-stationary process or not.\nThe augmented\nDickey-Fuller (ADF) unit-root test, as described by Dickey and Fuller (1979), uses a\ntest equation applied to the demeaned spread process St in the following form:\n\u2206St = \u03b2St\u22121 +\np\u22121\nX\ni=1\n\u03b3i\u2206St\u2212i + \u03f5t,\n(7)\nwhere \u03b2 is the coefficient of the lagged level of the series, \u03b3i are the coefficients of the\nlagged differences, \u03f5t is the error term, and p is the number of lags used in the test. The\nnull hypothesis of the ADF test is that the series has a unit root, i.e., \u03b2 = 0. If the\ntest statistic exceeds a critical value at a given significance level, the null hypothesis is\nrejected, indicating that the series is stationary and does not have a unit root. Conversely,\nif the test statistic is below the critical value, the null hypothesis cannot be rejected,\nimplying that the series has a unit root and is non-stationary.\nTraditional unit-root tests like the Augmented Dickey-Fuller (ADF) test assume that the\ndata-generating process is linear. However, non-linear unit-root tests are designed to\n8\n\naccount for non-linearities in time series data and provide more accurate assessments\nof unit roots. There are several non-linear unit-root tests available in the literature,\neach with its own assumptions, methodologies, and advantages. Examples include the\nTer\u00a8asvirta (1994) test, the Zivot and Andrews (2002) test, the Kapetanios, Shin, and\nSnell (2003) test, and the Kapetanios (2005) test. These tests often involve estimating\nnon-linear models like threshold auto-regressive (TAR) models, smooth transition auto-\nregressive (STAR) models, or other non-linear models, and computing test statistics to\ncompare estimated model parameters with critical values.\nThe general self-exciting threshold auto-regressive (SETAR) model with n regimes\napplied to the demeaned spread process St is in the following form:\nSt =\np\nX\ni=1\n \n\u03d5i11{St\u2212d\u2264c1} +\nn\u22121\nX\nj=1\n\u03d5ij1{cj<St\u2212d\u2264cj+1} + \u03d5in1{St\u2212d>cn}\n!\nSt\u2212i + \u03f5t,\n(8)\nwhere d denotes the transition\u2019s delay, cj represents the j-th threshold, and \u03f5t\nrepresents the error term. In special case Kapetanios, Shin, and Snell (2003) proposed\na test equation where the indicator function is replaced by an exponential smooth\ntransition function in the form:\nSt = St\u22121 +\np\nX\ni=1\n\u0010\n\u03b31i\n\u0010\n1 \u2212e\u2212\u03b8(St\u22121\u2212c)2\u0011\nSt\u2212i\n\u0011\n+ \u03f5t\n(9)\nWhen c is set to zero, and p is set to one, using Taylor approximation, equation 9 can be\nillustrated as\n\u2206St = \u03b4 (St\u22121)3 + \u03f5\n\u2032\nt\n(10)\nwhere \u03b4 = \u03b31\u03b8 and \u03f5\n\u2032\nt = f(\u03f5t).",
    "chunk_index": 6,
    "start_char": 16814,
    "end_char": 20025,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "!\nSt\u2212i + \u03f5t,\n(8)\nwhere d denotes the transition\u2019s delay, cj represents the j-th threshold, and \u03f5t\nrepresents the error term. In special case Kapetanios, Shin, and Snell (2003) proposed\na test equation where the indicator function is replaced by an exponential smooth\ntransition function in the form:\nSt = St\u22121 +\np\nX\ni=1\n\u0010\n\u03b31i\n\u0010\n1 \u2212e\u2212\u03b8(St\u22121\u2212c)2\u0011\nSt\u2212i\n\u0011\n+ \u03f5t\n(9)\nWhen c is set to zero, and p is set to one, using Taylor approximation, equation 9 can be\nillustrated as\n\u2206St = \u03b4 (St\u22121)3 + \u03f5\n\u2032\nt\n(10)\nwhere \u03b4 = \u03b31\u03b8 and \u03f5\n\u2032\nt = f(\u03f5t). The null hypothesis assumes that \u03b4 is equal to zero,\nwhile the alternative hypothesis posits that \u03b4 is less than zero. Note that the asymptotic\nstandard normal distribution of the t-statistic for \u03b4 = 0 against \u03b4 < 0 is not applicable.\nHowever, its asymptotic critical values have been determined through stochastic\nsimulations and are documented by Kapetanios, Shin, and Snell (2003).\n4.2\nCopula Concept\nSuppose that a continuous random variable X has a continuous distribution, and its\nprobability distribution function is defined as FX(x) := P(X \u2264x). If FX is strictly\nincreasing, then F \u22121\nX\nis defined by F \u22121\nX (u) = x \u21d4FX(x) = u. However, if FX is\nconstant on some interval, then the inverse function is not well defined by F \u22121\nX (u) = x.\nTo avoid this problem, we can define F \u22121\nX (u) for 0 < u < 1 by the generalized inverse\nfunction such that\nF \u22121\nX (u) = inf{x : FX(x) \u2265u},\n(11)\n9\n\nNow, in the same way, we define another continuous random variable Y with\ndistribution function FY and generalized inverse function F \u22121\nY\nsimilar to equation 11.\nGiven two continuous random variables X and Y , with distribution functions FX and\nFY respectively, the joint distribution function FX,Y can be written as:\nFX,Y (x, y) = P(X \u2264x, Y \u2264y) = P(FX(X) \u2264FX(x), FY (Y ) \u2264FY (y))\n(12)\nwhere the last equality follows from the fact that FX and FY are both increasing. Then\nwe define random variables U and V such that U := FX(X) and V\n:= FY (Y ).\nAccording to the probability integral transformation theorem,\nthe probability\ndistribution function of U, FU, and the probability distribution function of V , FV , are\nuniformly distributed on [0, 1]. (See Casella and Berger (2021, p. 54-55) for the proof).\nDefinition: A two-dimensional copula C is a function that maps the unit square [0, 1]2\ninto the unit interval [0, 1], satisfying the following requirements:\n1. C(0, v) = C(u, 0) = 0, for 0 \u2264u, v \u22641.\n2. C(u, 1) = u, and C(1, v) = v, for 0 \u2264u, v \u22641.\n3. C(u1, v1) \u2212C(u1, v2) \u2212C(u2, v1) + C(u2, v2) \u22650, for 1 \u2265u1 > u2 \u22650, and\n1 \u2265v1 > v2 \u22650.",
    "chunk_index": 7,
    "start_char": 19498,
    "end_char": 22062,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "C is a function that maps the unit square [0, 1]2\ninto the unit interval [0, 1], satisfying the following requirements:\n1. C(0, v) = C(u, 0) = 0, for 0 \u2264u, v \u22641.\n2. C(u, 1) = u, and C(1, v) = v, for 0 \u2264u, v \u22641.\n3. C(u1, v1) \u2212C(u1, v2) \u2212C(u2, v1) + C(u2, v2) \u22650, for 1 \u2265u1 > u2 \u22650, and\n1 \u2265v1 > v2 \u22650.\nWe can define several copula functions, but the three requirements above should be\nsatisfied by C(u, v) to have a well-defined joint distribution function (Cherubini et al.\n2011). According to Sklar\u2019s theorem, there exists a Copula function C which could\nconnect the uniform random variables U and V to the joint distribution function FX,Y\nas follows\nFX,Y (X, Y ) = FX,Y (F \u22121\nX (U), F \u22121\nY (V )) := C(U, V ),\n(13)\nHence, we can rewrite the joint distribution function of X and Y in terms of standard\nuniform random variables U and V such that\nFX,Y (x, y) = FX,Y (F \u22121\nX (u), F \u22121\nY (v)) := C(u, v) = C(FX(x), FY (y)),\n(14)\nwhere u = FX(x), and v = FY (y). Knowing FX,Y (x, y) = C(u, v), we can determine\nthe copula density function c(u, v) by\nc(u, v) = \u22022C(u, v)\n\u2202u \u2202v\n=\n\u22022FX,Y (x, y)\n\u2202FX(x) \u2202FY (y) =\n\u22022FX,Y (x,y)\n\u2202x \u2202y\n\u2202FX(x) \u2202FY (y)\n\u2202x \u2202y\n= fX,Y (x, y)\nfX(x)fY (y)\n(15)\nSklar\u2019s theorem enables us to separate the modeling of the marginal distributions FX(x)\nand FY (y) from the dependence relation represented in C. We can define the conditional\n10\n\ndistribution functions using copula function. The conditional distribution of Y |X = x\nis obtained by the first partial derivatives of the copula function as follows\nFY |X(y) = P(Y \u2264y|X = x) = P(FY (Y ) \u2264FY (y)|FX(X) = FX(x))\n= P(V \u2264v|U = u) = lim\n\u03b4\u21920+\nP(V \u2264v, U \u2208(u \u2212\u03b4, u + \u03b4)\nP(U \u2208(u \u2212\u03b4, u + \u03b4))\n= lim\n\u03b4\u21920+\nC(u + \u03b4, v) \u2212C(u \u2212\u03b4, v)\n2\u03b4\n= \u2202\n\u2202uC(u, v).\n(16)\nCopulas are invariant concerning strictly increasing transformations of the marginal\ndistributions.\nFor more details about the characterization of invariant copulas, see\nKlement, Mesiar, and Pap (2002). Also, we can increase the range of dependence\ncaptured by copulas by rotating them. The following equations show how to rotate a\ncopula by 90, 180, and 270 degrees:\nC90(u1, u2) := C(1 \u2212u2, u1)\nC180(u1, u2) := C(1 \u2212u1, 1 \u2212u2)\nC270(u1, u2) := C(u2, 1 \u2212u1)\n(17)\n4.3\nFamilies of copulas\nThere are several types of copulas. This research focuses on three popular families of\ncopulas: elliptical, Archimedean, and extreme value copulas. Each type of copula is\ncharacterized by its properties, such as its dependence structure and tail behavior, and\nis often used in different areas of statistics and finance.\n4.3.1\nElliptical Copulas\nElliptical copulas are widely used in finance and insurance applications due to their\nability to model dependence with different levels of tail dependence.",
    "chunk_index": 8,
    "start_char": 21763,
    "end_char": 24459,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "u2) := C(1 \u2212u1, 1 \u2212u2)\nC270(u1, u2) := C(u2, 1 \u2212u1)\n(17)\n4.3\nFamilies of copulas\nThere are several types of copulas. This research focuses on three popular families of\ncopulas: elliptical, Archimedean, and extreme value copulas. Each type of copula is\ncharacterized by its properties, such as its dependence structure and tail behavior, and\nis often used in different areas of statistics and finance.\n4.3.1\nElliptical Copulas\nElliptical copulas are widely used in finance and insurance applications due to their\nability to model dependence with different levels of tail dependence.\nAn elliptical\ncopula is constructed from a multivariate elliptical distribution. The most commonly\nused elliptical distributions are the multivariate Gaussian and Student-t. The density\nfunction of any elliptical distribution fX is in the form\nfX(x; \u00b5, \u03a3) = kn|\u03a3|\u22121\n2g\n\u0000\u0000x \u2212\u00b5)T\u03a3\u22121(x \u2212\u00b5\n\u0001\u0001\n,\n(18)\nwhere kn \u2208R is the normalizing constant and dependents on the dimension n, x is an n-\ndimensional random vector with mean vector \u00b5, and a positive definite matrix which is\n11\n\nproportional to the covariance matrix \u03a3, and some function g(.) which is independent\nof the dimension n (Czado 2019). In the the case of bivariate Gaussian distribution\ng(x) := e\u2212x/2, kn := 1/(2\u03c0), and the probability density function of X = (X1, X2) is\nfX(x; \u00b5, \u03a3) =\n1\n2\u03c0|\u03a3|1/2e\u22121\n2[(x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5)]\n\u00b5 =\n\u0012\u00b51\n\u00b52\n\u0013\n,\n\u03a3 =\n\u0012 \u03c32\n1\n\u03c1\u03c31\u03c32\n\u03c1\u03c31\u03c32\n\u03c32\n2\n\u0013\n,\n(19)\nwhere \u03c1 is the correlation between random variables X1 and X2, \u03c31 > 0 and \u03c32 > 0.\nIf \u00b51 = \u00b52 = 0 and \u03c31 = \u03c32 = 1, then the density and distribution functions of the\nstandard Bivariate Gaussian distribution are obtained by\n\u03d5X1X2(x1, x2; \u03c1) =\n1\n2\u03c0\np\n1 \u2212\u03c12e\n\u2212\nx2\n1\u22122\u03c1x1x2+x2\n2\n2(1\u2212\u03c12)\n\u03a6X1X2(u1, u2; \u03c1) =\nZ u1\n\u2212\u221e\nZ u2\n\u2212\u221e\n\u03d5X1X2(x1, x2; \u03c1) dx1 dx2\n(20)\nUsing Sklar\u2019s theorem in 13 and 20, the bivariate Gaussian copula is defined by\nC(u1, u2; \u03c1) := \u03a6X1X2(\u03a6\u22121\nX1(u1), \u03a6\u22121\nX2(u2); \u03c1)\n(21)\nIn the bivariate t distribution, g(.) and kn function in the formula 18 are defined by\ng(x) := (1 + x/\u03bd)\u2212(\u03bd+2)/2, kn := \u0393(\u03bd+2\n2 )/\n\u0000\u0393(\u03bd\n2)\u03bd\u03c0\n\u0001\n, and the probability density\nfunction of T = (T1, T2) is obtained by\nfT (t; \u03bd, \u00b5, \u03a3) =\n\u0393\n\u0000 \u03bd+2\n2\n\u0001\n\u0393\n\u0000 \u03bd\n2\n\u0001\n\u03bd\u03c0|\u03a3|1/2\n\u0014\n1 + 1\n\u03bd (t \u2212\u00b5)T\u03a3\u22121(t \u2212\u00b5)\n\u0015\u2212(\u03bd+2)/2\n(22)\nwhere \u03bd > 0 is the degree of freedom parameter and \u00b5 and \u03c3 are the same as 19. If\n\u00b51 = \u00b52 = 0 and \u03c31 = \u03c32 = 1, and knowing that \u0393\n\u0000 \u03bd+2\n2\n\u0001\n/\u0393\n\u0000 \u03bd\n2\n\u0001\n= \u03bd\n2, the density and\ndistribution function of the standard Bivariate Student-t distribution are obtained by\nfT1T2(t1, t2; \u03bd, \u03c1) = (1 \u2212\u03c12)\u22121/2\n2\u03c0\n\u0014\n1 + t2\n1 \u22122\u03c1t1t2 + t2\n2\n\u03bd(1 \u2212\u03c12)\n\u0015\u2212(\u03bd+2)/2\nFT1T2(u1, u2; \u03bd, \u03c1) =\nZ u1\n\u2212\u221e\nZ u2\n\u2212\u221e\nfT1T2(t1, t2;",
    "chunk_index": 9,
    "start_char": 23878,
    "end_char": 26464,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "\u00b5 and \u03c3 are the same as 19. If\n\u00b51 = \u00b52 = 0 and \u03c31 = \u03c32 = 1, and knowing that \u0393\n\u0000 \u03bd+2\n2\n\u0001\n/\u0393\n\u0000 \u03bd\n2\n\u0001\n= \u03bd\n2, the density and\ndistribution function of the standard Bivariate Student-t distribution are obtained by\nfT1T2(t1, t2; \u03bd, \u03c1) = (1 \u2212\u03c12)\u22121/2\n2\u03c0\n\u0014\n1 + t2\n1 \u22122\u03c1t1t2 + t2\n2\n\u03bd(1 \u2212\u03c12)\n\u0015\u2212(\u03bd+2)/2\nFT1T2(u1, u2; \u03bd, \u03c1) =\nZ u1\n\u2212\u221e\nZ u2\n\u2212\u221e\nfT1T2(t1, t2; \u03bd, \u03c1) dt1 dt2\n(23)\nThen the bivariate Student-t copula is defined by\nC(u1, u2; \u03bd, \u03c1) := FT1T2(F \u22121\nT1 (u1), F \u22121\nT2 (u2); \u03bd, \u03c1)\n(24)\n12\n\n4.3.2\nArchimedean Copulas\nArchimedean copulas are based on a generator function and can model dependence with\ntail dependence that decreases logarithmically or exponentially. According to Nelsen\n(2007), Archimedean copulas are defined as follows:\nC(u1, . . . , un) = \u03d5[\u22121](\u03d5(u1) + \u00b7 \u00b7 \u00b7 + \u03d5(un))\n(25)\nwhere \u03d5 : [0, 1] \u2192[0, \u221e] is called a generator which is continuous, strictly decreasing,\nand convex function such that \u03d5(1) = 0. In addition, \u03d5[\u22121] : [0, \u221e] \u2192[0, 1] is called the\npseudo-inverse of \u03d5 function and is defined by\n\u03d5[\u22121](t) =\n(\n\u03d5\u22121(t)\n, 0 \u2264t \u2264\u03d5(0)\n0\n, \u03d5(0) \u2264t \u2264\u221e\n(26)\nIf \u03d5(0) equals infinity, then the pseudo-inverse function \u03d5[\u22121] is equivalent to the\ninverse function \u03d5\u22121. Archimedean copulas allow for a range of generator functions to\nbe selected, which ultimately determines the type of tail dependence exhibited by the\ncopula. For this research, both one-parameter copulas (such as the Gumbel, Clayton,\nFrank, and Joe copulas) and two-parameter copulas (such as BB1, BB6, BB7, and\nBB8) were utilized.\nTable X in the appendix provides a list of the bivariate\nArchimedean copulas utilized in this study.\n4.3.3\nExtreme-Value Copulas\nExtreme value copulas are used to model dependence with strong tail dependence,\nmaking them suitable for modeling extreme events. Suppose that Xi = (Xi1, Xi2)T,\ni \u2208{1, 2, \u00b7 \u00b7 \u00b7 , n}, be independent and identically distributed random vectors with joint\ndistribution function F, and marginal distributions F1 and F2. According to Gudendorf\nand Segers (2010), we can define the bivariate vector of component-wise maxima\nMn = (Mn1, Mn2)T such that\nMnj :=\nmax\ni\u2208{1,2,\u00b7\u00b7\u00b7 ,n} (Xij) ,\nj = 1, 2.\n(27)\nThen, the bivariate copula Cn of Mn is obtained by\nCn(u1, u2) = CF\n\u0010\nu1/n\n1\n, u1/n\n2\n\u0011n\n,\n(u1, u2) \u2208[0, 1]2.\n(28)\nIn equation 28, if CF exists such that\nlim\nn\u2192\u221eCF(u1/n\n1\n, u1/n\n2\n)n = C(u1, u2),\n(u1, u2) \u2208[0, 1],\n(29)\n13\n\nthen the bivariate copula C in (29) is called an extreme-value copula. Bivariate extreme-\nvalue copulas can be demonstrated in terms of a function A(t) in this form:\nC(u1, u2) = (u1u2)A(ln(u2)/ ln(u1u2)),\n(u1, u2) \u2208(0, 1]2 \\ {(1, 1)},\n(30)\nwhere the function A : [0, 1] \u2192[1/2, 1], which is called the Pickands dependence\nfunction, is convex and satisfies max(1 \u2212t, t) \u2264A(t) \u22641 for all t \u2208[0, 1] (Gudendorf\nand Segers 2010).",
    "chunk_index": 10,
    "start_char": 26121,
    "end_char": 28888,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "CF exists such that\nlim\nn\u2192\u221eCF(u1/n\n1\n, u1/n\n2\n)n = C(u1, u2),\n(u1, u2) \u2208[0, 1],\n(29)\n13\n\nthen the bivariate copula C in (29) is called an extreme-value copula. Bivariate extreme-\nvalue copulas can be demonstrated in terms of a function A(t) in this form:\nC(u1, u2) = (u1u2)A(ln(u2)/ ln(u1u2)),\n(u1, u2) \u2208(0, 1]2 \\ {(1, 1)},\n(30)\nwhere the function A : [0, 1] \u2192[1/2, 1], which is called the Pickands dependence\nfunction, is convex and satisfies max(1 \u2212t, t) \u2264A(t) \u22641 for all t \u2208[0, 1] (Gudendorf\nand Segers 2010). Some Archimedean copulas, such as the Gumbel copula, can be\nexpressed by the extreme-value family. These special copulas create a hybrid category,\nincluding both the Archimedean and the extreme-value copulas, and are called\nArchimax copulas.\nTable II shows extreme-value copulas used in this research. Note that Tawn copula has\nthree parameters and its Pickands\u2019s dependence function is in the form\nA(t) = (1 \u2212\u03b2) + (\u03b2 \u2212\u03b1)t +\n\u0002\n(\u03b1(1 \u2212t))\u03b8 + (\u03b2t)\u03b8\n\u00031/\u03b8 ,\n(31)\nwhere \u03b8 \u22651 and \u03b1, \u03b2 \u2208[0, 1]1. The simplified Tawn copula cases with \u03b2 = 1 and \u03b1 = 1\nare respectively called Tawn Type 1 and Tawn Type 2 copula and have two parameters.\nTable II: Pickands dependence function of some extreme-value copulas\nName\nPickands function A(t)\nParameters\nGumbel\n\u0014\nt\u03b8 + (1 \u2212t)\u03b8\n\u00151/\u03b8\n\u03b8 \u22651\nTawn Type 1\n(1 \u2212\u03b1)t +\n\u0014\n(\u03b1(1 \u2212t))\u03b8 + t\u03b8\n\u00151/\u03b8\n\u03b8 \u22651, 0 \u2264\u03b1 \u22641\nTawn Type 2\n(1 \u2212\u03b2)(1 \u2212t)\n\u0014\n(1 \u2212t)\u03b8 + (\u03b2t)\u03b8\n\u00151/\u03b8\n\u03b8 \u22651, 0 \u2264\u03b2 \u22641\n4.4\nCopula Estimation\nWhen the marginal probability density of X1 and X2 and their corresponding copula\ndensity c(.) are given in their parametric with unknown parameters, we can estimate\nthe vector of parameter vector \u03b8 = (\u03b2, \u03b1)T with the maximum likelihood estimation\nmethod, where \u03b2 = (\u03b21, \u03b22)T represent the marginal parameters and \u03b1 represents the\ncopula\nparameters.\nThe\nlog-likelihood\nfunction\nfor\n(X1, X2),\nwhere\nXi = (xi1, . . . , xin), can be expressed as\nl(\u03b8) =\nn\nX\nj=1\n\u0002\nlog c\n\u0000F1(x1j; \u03b21), F2(x2j; \u03b22); \u03b1\n\u0001\n+log f1(x1j; \u03b21)+log f2(x2j; \u03b22)\n\u0003\n, (32)\n14\n\nand the maximum likelihood estimator of \u03b8 is\n\u02c6\u03b8ML = argmax\n\u03b8\nl(\u03b8).\n(33)\nHowever, since this approach is computationally expensive, an alternative method\ncalled the Inference for the Margins (IFM) two-step method can be used instead. This\nmethod is computationally easier to obtain compared to the full maximum likelihood\nestimation approach. First, we estimate the margins\u2019 parameters by performing the\nestimation of the univariate marginal distributions using the log-likelihood function\nwhere the maximum likelihood estimator of \u03b2i is\n\u02c6\u03b2i = argmax\n\u03b2i\nn\nX\nj=1\n[log fi(xij; \u03b2i)] .\n(34)\nThen given estimated marginal parameters, we transform data to the copula scale,\ndevelop the copula model, and estimate the copula parameters \u03b1 as follows\n\u02c6\u03b1ML = argmax\n\u03b1\nn\nX\nj=1\nh\nlog c\n\u0010\nF1(x1j; \u02c6\u03b21), F2(x2j; \u02c6\u03b22); \u03b1\n\u0011i\n.",
    "chunk_index": 11,
    "start_char": 28376,
    "end_char": 31150,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "the margins\u2019 parameters by performing the\nestimation of the univariate marginal distributions using the log-likelihood function\nwhere the maximum likelihood estimator of \u03b2i is\n\u02c6\u03b2i = argmax\n\u03b2i\nn\nX\nj=1\n[log fi(xij; \u03b2i)] .\n(34)\nThen given estimated marginal parameters, we transform data to the copula scale,\ndevelop the copula model, and estimate the copula parameters \u03b1 as follows\n\u02c6\u03b1ML = argmax\n\u03b1\nn\nX\nj=1\nh\nlog c\n\u0010\nF1(x1j; \u02c6\u03b21), F2(x2j; \u02c6\u03b22); \u03b1\n\u0011i\n.\n(35)\nWe can also employ a semiparametric approach known as canonical maximum\nlikelihood to estimate copula parameters without specifying the marginals.\nThe\nempirical cumulative distribution function of Xi = (xi1, . . . , xin) is\nc\nF i\nn(t) = #{Xi \u2264x}\nn + 1\n=\n1\nn + 1\nn\nX\nj=1\n1{xij\u2264t}.\n(36)\nThen, the copula parameters are estimated using maximum likelihood estimation as\nfollows:\n\u02c6\u03b1ML = argmax\n\u03b1\nn\nX\nj=1\nh\nlog c\n\u0010\nc\nF 1\nn(x1j), c\nF 2\nn(x2j); \u03b1\n\u0011i\n.\n(37)\n5\nImplementation Methodology\nThe study suggests a new approach to address the issue of cumulative mispricing\nindices (CMI) not exhibiting mean-reverting behavior in practice, which can negatively\naffect the profitability of trading strategies. The proposed methodology involves using\nstationary spread processes instead of log-returns.\nIn the pairs trading strategy\nemployed in this study, the spread process is defined as a linear combination of two\n15\n\nassets, with a reference asset (BTCUSDT) selected and other cryptocurrency coins\nidentified as cointegrated with it using a specific equation.\nSi\nt = BTCUSDTt \u2212\u02c6\u03b2iP i\nt\ni = 1, 2, . . . , 19\n(38)\nwhere \u02c6\u03b2 is the estimated linear regression coefficient between BTCUSDT and the\nsecond coin, chosen from the other 19 coins. This allows us to identify 19 pairs and\nselect the optimal pairs during the formation period for trading in the trading period.\nTo determine the optimal pairs, we use the linear Engle-Granger (EG) two-step method\nand the non-linear Kapetanios-Shin-Snell (KSS) cointegration tests to identify\ncointegrated coins. However, since there may be multiple cointegrated pairs, we need\nto add another criterion to rank them. To do this, we calculate Kendall\u2019s Tau (\u03c4), which\nis a measure of correlation for ranked data and defined\n\u03c4(Si, Sj) = \u03c4ij = Number of concordant pairs \u2212Number of discordant pairs\nTotal number of pairs\n(39)\nwhere\nNumber of concordant pairs =\nN\u22121\nX\nn=1\nN\nX\nm=n+1\nsgn(Si\n[n] \u2212Si\n[m])sgn(Sj\n[n] \u2212Sj\n[m]),\nNumber of discordant pairs =\nN\u22121\nX\nn=1\nN\nX\nm=n+1\nsgn(Si\n[n] \u2212Si\n[m])sgn(Sj\n[m] \u2212Sj\n[n]),\nand total number of pairs = N(N \u22121)/2. Here, N is the number of data points, Si\n[n]\nand Sj\n[n] are the rankings of the n-th data point in two different variables, and sgn(x) is\nthe sign function, which is 1 if x > 0, \u22121 if x < 0, and 0 if x = 0.",
    "chunk_index": 12,
    "start_char": 30702,
    "end_char": 33431,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "m=n+1\nsgn(Si\n[n] \u2212Si\n[m])sgn(Sj\n[m] \u2212Sj\n[n]),\nand total number of pairs = N(N \u22121)/2. Here, N is the number of data points, Si\n[n]\nand Sj\n[n] are the rankings of the n-th data point in two different variables, and sgn(x) is\nthe sign function, which is 1 if x > 0, \u22121 if x < 0, and 0 if x = 0.\nAfter calculating \u03c4 of BTCUSDT with each of the 19 altcoins, we select the two altcoins\nthat have the highest \u03c4 with BTCUSDT and create their corresponding pairs. During\nthe course of one week, the chosen pairs are traded and can be substituted with different\npairs at the start of each trading period. Instead of trading a pair of coins, we employ\na pair of spreads where each of them contains BTCUSDT. In this way, having a long\nposition in one spread and a short position in the other one implies that BTCUSDT will\nnot be traded at all and it only plays an intermediary role between two other coins.\nNow, we estimate the probability distribution function of spread processes (marginals).\nWe fit various distributions such as Gaussian, Student-t, and Cauchy to the data to\nidentify the best-fitting distribution. By employing statistical methods or maximum\nlikelihood estimation, we estimate the specific parameters associated with each\ndistribution.\nTo evaluate the goodness of fit, we calculate the AIC values for the\ncandidate distributions, ultimately selecting the distribution with the lowest AIC value\n16\n\nas the best-fitting option. Suppose that F i is the fitted cumulative distribution function\nof the spread process Si. The Probability Integral Transform is employed to convert\nthe spreads to random variables U1 := F 1(S1) and U2 := F 2(S2) with a standard\nuniform distribution. The next step is to determine a fitting copula model for U1 and\nU2. We select some potential copulas and estimate the corresponding parameters by\nthe maximum likelihood method.\nFinally, using the Akaike information criterion\n(AIC), we distinguish the most fitted copula model.\nDuring the trading period, using the hourly realizations of random variables U1 and U2,\nwhich are the transformed values of spread processes, we calculate the copula\nconditional probabilities h1|2 and h2|1 defined in equation 4, for the selected pairs of the\nweek. when h1|2 is higher (lower) than 0.5, the first coin can be considered to be\novervalued (undervalued) relative to the second one. Similarly, when h2|1 is higher\n(lower) than 0.5, the second coin can be considered to be overvalued (undervalued)\nrelative to the first one.\nTherefore, we can interpret mispricing as conditional\nprobabilities in equation 4 minus 0.5. We denote the trading thresholds to be \u03b11 and\n\u03b12. We study the optimal triggers via back-testing. The opening and closing signals are\ngenerated by the following rules:\nTable III: Trading rules in terms S1 and S2\nTrading Rule\nSignals\nIf h1|2 < \u03b11 and h2|1 > 1 \u2212\u03b11\nopen long S1 and short S2\nIf h1|2 > 1 \u2212\u03b11 and h2|1 < \u03b11\nopen",
    "chunk_index": 13,
    "start_char": 33140,
    "end_char": 36054,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "first one.\nTherefore, we can interpret mispricing as conditional\nprobabilities in equation 4 minus 0.5. We denote the trading thresholds to be \u03b11 and\n\u03b12. We study the optimal triggers via back-testing. The opening and closing signals are\ngenerated by the following rules:\nTable III: Trading rules in terms S1 and S2\nTrading Rule\nSignals\nIf h1|2 < \u03b11 and h2|1 > 1 \u2212\u03b11\nopen long S1 and short S2\nIf h1|2 > 1 \u2212\u03b11 and h2|1 < \u03b11\nopen short S1 and,long S2\nIf |h1|2 \u22120.5| < \u03b12 and |h2|1 \u22120.5| < \u03b12\nclose both positions\nTable IV: Trading rules in terms P 1 and P 2\nTrading Rule\nSignals\nIf h1|2 < \u03b11 and h2|1 > 1 \u2212\u03b11\nopen long \u03b22 \u00d7 P 2 and short \u03b21 \u00d7 P 1\nIf h1|2 > 1 \u2212\u03b11 and h2|1 < \u03b11\nopen short \u03b22 \u00d7 P 2 and long \u03b21 \u00d7 P 1\nIf |h1|2 \u22120.5| < \u03b12 and |h2|1 \u22120.5| < \u03b12\nclose both positions\n17\n\nFigure 2: Confidence bands of Gumbel copula (\u03b8 = 2) at \u03b11 = 5% and \u03b12 = 10%\nFigure 2 provides an illustrative depiction of the confidence bands in trading rules for\nthe Gumbel copula with a parameter value of \u03b8 = 2, under the conditions of \u03b11 = 5%\nand \u03b12 = 10%. If the data point (u1t, u2t) falls within the top green (down green) area, it\nsuggests that S1 is undervalued (overvalued) and S2 is overvalued (undervalued), which\nmay indicate an opportunity to open a position. Conversely, if (u1t, u2t) falls within the\nred area, it may signal the need to close the positions.\n6\nEmpirical Results\nTable V presents the occurrence rate of selected copulas and their rotations over 104\ntrading weeks. The results indicate that copulas of extreme value, such as Tawn type 1\nand 2, and certain two-parameter Archimedean copulas, particularly BB7, play a\n18\n\nsignificant role in the process of selecting the appropriate model.\nTable V: Occurrence rate of copulas in the study\nCopulas and their rotations\nStrategy with EG Test\nStrategy with KSS Test\nGaussian\n4.8%\n5.8%\nStudent-t\n6.7%\n4.8%\nClayton\n3.8%\n6.7%\nFrank\n2.9%\n3.8%\nGumbel\n1.0%\n4.8%\nJoe\n10.6%\n7.7%\nBB1\n5.8%\n2.9%\nBB6\n1.9%\n1.9%\nBB7\n15.4%\n15.4%\nBB8\n8.7%\n7.7%\nTawn type 1\n23.1%\n24.0%\nTawn type 2\n15.4%\n14.4%\nTables VIII and IX in the appendix display the coin pairs chosen for each trading week,\nwhich can vary from week to week. It is worth noting that there are instances where the\nselected coin pairs remain unchanged across consecutive weeks. The ADF unit-root test\nis conducted with a significance level of 10%. For the KSS unit-root test, the asymptotic\ncritical value at a 10% significance level is -1.92. If we fail to identify a minimum of\ntwo stationary spread processes within a specified formation period, we abstain from\ntrading during its corresponding trading period.\nThe results of our trading strategy\u2019s profit and loss calculations with varying entry\nthresholds (\u03b11) are illustrated in Figure 2.",
    "chunk_index": 14,
    "start_char": 35627,
    "end_char": 38363,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "where the\nselected coin pairs remain unchanged across consecutive weeks. The ADF unit-root test\nis conducted with a significance level of 10%. For the KSS unit-root test, the asymptotic\ncritical value at a 10% significance level is -1.92. If we fail to identify a minimum of\ntwo stationary spread processes within a specified formation period, we abstain from\ntrading during its corresponding trading period.\nThe results of our trading strategy\u2019s profit and loss calculations with varying entry\nthresholds (\u03b11) are illustrated in Figure 2. Note that the exit threshold (\u03b12) is fixed at\n10% in all cases.\nRealized profit and loss are determined by considering the\ncommission fees and the difference between the opening and closing prices of a\nposition. It should be noted that we initially invested 200,000 USDT, with the coins\u2019\nweights set to ensure that each side has a maximum initial capital of around 200,000\nUSDT. It is also assumed that all trades are executed using market orders, which\ntypically incur higher fees (known as taker fees) compared to the lower fees charged\nfor limit orders (known as maker fees). For instance, on Binance, the maker fee of\nUSDT-Margined Futures is set at 0.02%, while the taker fee is 0.04%.\n19\n\nFigure 3: Copula-based pairs trading strategy P&L\nWe assess the risk-adjusted performance of a strategy using the Sharpe ratio. Table VI3\nillustrates the yearly returns, volatility, and Sharpe ratios for different strategies, along\nwith the maximum drawdown and the returns obtained over the maximum drawdown\n3The Gross Profit and Loss (P&L) mentioned in table VI is the total profit and loss generated by\ntransactions before deducting any transaction fees associated with them.\n20\n\n(RoMaD), which serves as an alternative to the Sharpe Ratio4. In pairs trading with the\nEG cointegration test, the annualized return decreases with higher entry thresholds,\nranging from 76.2% at \u03b11 = 0.10 to 52.1% at \u03b11 = 0.20. In contrast, the annualized\nstandard deviation increases with higher levels of \u03b11, ranging from 38.2% at \u03b11 = 0.10\nto 52.7% at \u03b11\n=\n0.20.\nIn the optimal case, the annualized Sharpe Ratio is\napproximately 1 (\u03b11 = 0.10). The maximum drawdown increases with higher \u03b11,\nranging from \u221235.6% at \u03b11 = 0.10 to \u221243.9% at \u03b11 = 0.20.\nThe return over\nmaximum drawdown (RoMaD) decreases with higher \u03b11, ranging from 2.14 at\n\u03b11 = 0.10 to 1.19 at \u03b11 = 0.20.\nIn a similar manner, the annualized return in pairs trading utilizing the KSS\ncointegration test diminishes as the entry threshold levels increase, declining from\n35.3% when \u03b11 = 0.10 to 10.4% when \u03b11 = 0.20. As \u03b11 increases, the annualized\nstandard deviation increases, starting from 37.3% at \u03b11 = 0.10 and reaching 63.5% at\n\u03b11 = 0.20. Therefore, the annualized Sharpe Ratio exhibits a declining pattern as the\nthreshold levels increase. It ranges from 0.93 at \u03b11 = 0.10 to 0.16 at \u03b11 = 0.20. The\nmaximum drawdown increases with higher threshold levels, ranging from \u221234.0% at\n\u03b11 = 0.10 to \u221243.0% at \u03b11 = 0.20.",
    "chunk_index": 15,
    "start_char": 37824,
    "end_char": 40824,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "declining from\n35.3% when \u03b11 = 0.10 to 10.4% when \u03b11 = 0.20. As \u03b11 increases, the annualized\nstandard deviation increases, starting from 37.3% at \u03b11 = 0.10 and reaching 63.5% at\n\u03b11 = 0.20. Therefore, the annualized Sharpe Ratio exhibits a declining pattern as the\nthreshold levels increase. It ranges from 0.93 at \u03b11 = 0.10 to 0.16 at \u03b11 = 0.20. The\nmaximum drawdown increases with higher threshold levels, ranging from \u221234.0% at\n\u03b11 = 0.10 to \u221243.0% at \u03b11 = 0.20. The return over maximum drawdown shows a\nsimilar pattern as the annualized Sharpe Ratio, decreasing with higher thresholds.\nWhen the entry threshold \u03b11 increases, it results in an increase in the number of trading\nsignals. This, in turn, leads to higher volatility in both the EG and KSS strategies.\nHowever, despite the increased volatility, there is no corresponding increase in return.\nThis indicates that the optimal threshold is 0.10, and at this level, the risk-adjusted\nprofitability is similar between the strategy with the EG test and the KSS test.\nTo compare our strategy\u2019s results with the buy-and-hold strategy, we use a passive\ninvestment approach that involves holding a relatively steady portfolio for an extended\nperiod, despite short-term market fluctuations.\nThe Bitcoin buy-and-hold strategy\nshows a negative annualized return of \u221217.0% with a high annualized standard\ndeviation of 72.6%. The annualized Sharpe Ratio is negative (\u22120.23), indicating poor\nrisk-adjusted performance. The maximum drawdown is severe (\u221277.1%), indicating a\nhigh risk. On the other hand, the portfolio buy-and-hold strategy5 shows a positive\nannualized return of 14.4% with a high annualized standard deviation of 98.6%. The\nannualized Sharpe Ratio is positive (0.15), indicating better risk-adjusted performance\ncompared to the Bitcoin buy-and-hold strategy.\nThe maximum drawdown is also\nsevere (\u221282.2%), indicating a high risk.\nBoth tests of the pairs trading strategy\noutperform the Bitcoin buy-and-hold strategy and the portfolio buy-and-hold strategy.\n4Transaction fees are taken into account in all calculations\n5The buy-and-hold strategy in the portfolio involves investing in all twenty cryptocurrency coins with\nequal weights at the start of the study, retaining them throughout the trading periods, and ultimately\nselling them at the end of the study period.\n21\n\nOur pairs trading strategy outperforms the portfolio buy-and-hold by over six times\nwhen \u03b11 = 0.10.\nTable VI: Results of pairs trading strategies Utilizing hourly closed prices of twenty\ncryptocurrencies from 22/01/2021 to 19/01/2023\nPairs Trading with EG Test\n\u03b11 = 0.10\n\u03b11 = 0.15\n\u03b11 = 0.20\nTotal Return\n76.2%\n54.2%\n52.1%\nAnnualized Return\n37.1%\n26.4%\n25.4%\nAnnualized Standard Deviation\n38.2%\n47.4%\n52.7%\nAnnualized Sharpe Ratio\n0.97\n0.56\n0.48\nMaximum Drawdown\n-35.6%\n-41.8%\n-43.9%\nReturn over Maximum Drawdown\n2.14\n1.30\n1.19\nNumber of Transactions\n176\n200\n222\nTransaction Costs over Gross P&L\n11.7%\n19.2%\n21.9%\nPairs Trading with KSS Test\n\u03b11 = 0.10\n\u03b11 = 0.15\n\u03b11 = 0.20\nTotal Return\n72.3%\n44.8%\n21.3%\nAnnualized Return\n35.3%\n21.8%\n10.4%\nAnnualized Standard Deviation\n37.7%\n43.6%\n63.5%\nAnnualized Sharpe Ratio\n0.93\n0.50\n0.16\nMaximum Drawdown\n-34.0%\n-34.6%\n-43.0%\nReturn over Maximum",
    "chunk_index": 16,
    "start_char": 40361,
    "end_char": 43576,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "Deviation\n38.2%\n47.4%\n52.7%\nAnnualized Sharpe Ratio\n0.97\n0.56\n0.48\nMaximum Drawdown\n-35.6%\n-41.8%\n-43.9%\nReturn over Maximum Drawdown\n2.14\n1.30\n1.19\nNumber of Transactions\n176\n200\n222\nTransaction Costs over Gross P&L\n11.7%\n19.2%\n21.9%\nPairs Trading with KSS Test\n\u03b11 = 0.10\n\u03b11 = 0.15\n\u03b11 = 0.20\nTotal Return\n72.3%\n44.8%\n21.3%\nAnnualized Return\n35.3%\n21.8%\n10.4%\nAnnualized Standard Deviation\n37.7%\n43.6%\n63.5%\nAnnualized Sharpe Ratio\n0.93\n0.50\n0.16\nMaximum Drawdown\n-34.0%\n-34.6%\n-43.0%\nReturn over Maximum Drawdown\n2.13\n1.29\n0.50\nNumber of Transactions\n184\n222\n254\nTransaction Costs over Gross P&L\n12.9%\n25.2%\n47.7%\nBuy & Hold Strategy\nBitcoin Buy & Hold\nPortfolio Buy & Hold\nTotal Return\n-33.9%\n28.8%\nAnnualized Return\n-17.0%\n14.4%\nAnnualized Standard Deviation\n72.6%\n98.6%\nAnnualized Sharpe Ratio\n-0.23\n0.15\nMaximum Drawdown\n-77.1%\n-82.2%\nReturn over Maximum Drawdown\n-0.44\n0.35\n22\n\n7\nConclusion\nIn this paper, We develop a novel pairs trading framework for twenty Binance\nUSDT-Margined Futures cryptocurrency coins, which combines copula-based and\ncointegrated-based approaches. The methodology involves setting a reference asset (in\nthis case, BTCUSDT) and identifying other cryptocurrency coins that are cointegrated\nwith it. To investigate the presence of cointegration, we utilize both the EG two-step\nmethod and the KSS cointegration test. After ranking the cointegrated coins based on\nKendall\u2019s Tau correlation coefficients, we select the two assets with the highest\ncorrelation. These selected coins are then traded during the one-week trading period\nand are updated weekly. Trading rules are generated based on the copula conditional\nprobabilities of the spread processes corresponding to the selected assets. We establish\nvarious trading triggers and backtest the strategy.\nThe research findings show that the choice of an appropriate entry threshold in pairs\ntrading has an important impact on the outcomes. By using lower thresholds, both\nvolatility decreases and return increases concurrently. Our pairs trading strategy has\nshown a high risk-adjusted return and return over maximum drawdown when\ncompared to the buy-and-hold approach.\nThis suggests that our method can be\neffectively utilized for profitability. The study also emphasizes the effectiveness of\npairs trading in the cryptocurrency market and underscores the importance of\nmeticulous coin pair and copula model selection.\nReferences\nBinance Crypto Derivatives (accessed 2022-11-03). https://www.binance.com/en/\nsupport/faq/crypto-derivatives?c=4&navId=4.\nBroek, L van den and Zara Sharif (2018). \u201cCointegration-based pairs trading framework\nwith application to the Cryptocurrency market\u201d. Bachelor Thesis, Erasmus University\nRotterdam.\nCasella, George and Roger L Berger (2021). Statistical inference. Cengage Learning.\nChen, Huafeng et al. (2019). \u201cEmpirical investigation of an equity pairs trading\nstrategy\u201d. In: Management Science 65.1, pp. 370\u2013389.\nCherubini, Umberto et al. (2011). Dynamic copula methods in finance. John Wiley &\nSons.\nCzado, Claudia (2019). \u201cAnalyzing dependent data with vine copulas\u201d. In: Lecture\nNotes in Statistics, Springer.\nEnders, Walter and Pierre L Siklos (2001). \u201cCointegration and threshold adjustment\u201d.\nIn: Journal of Business & Economic Statistics 19.2, pp. 166\u2013176.\n23\n\nEngle, Robert F and Clive WJ Granger (1987). \u201cCo-integration and error correction:\nrepresentation,\nestimation,\nand\ntesting\u201d.\nIn:\nEconometrica:\njournal\nof\nthe\nEconometric Society, pp. 251\u2013276.\nFerreira, Luan (2008). \u201cNew tools for spread trading\u201d.",
    "chunk_index": 17,
    "start_char": 43072,
    "end_char": 46604,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "al. (2011). Dynamic copula methods in finance. John Wiley &\nSons.\nCzado, Claudia (2019). \u201cAnalyzing dependent data with vine copulas\u201d. In: Lecture\nNotes in Statistics, Springer.\nEnders, Walter and Pierre L Siklos (2001). \u201cCointegration and threshold adjustment\u201d.\nIn: Journal of Business & Economic Statistics 19.2, pp. 166\u2013176.\n23\n\nEngle, Robert F and Clive WJ Granger (1987). \u201cCo-integration and error correction:\nrepresentation,\nestimation,\nand\ntesting\u201d.\nIn:\nEconometrica:\njournal\nof\nthe\nEconometric Society, pp. 251\u2013276.\nFerreira, Luan (2008). \u201cNew tools for spread trading\u201d. In: Futures 37.12, pp. 38\u201341.\nGatev, Evan, William N Goetzmann, and K Geert Rouwenhorst (2006). \u201cPairs trading:\nPerformance of a relative-value arbitrage rule\u201d. In: The Review of Financial Studies\n19.3, pp. 797\u2013827.\nGudendorf, Gordon and Johan Segers (2010). \u201cExtreme-value copulas\u201d. In: Copula\ntheory and its applications. Springer, pp. 127\u2013145.\nHansen, Bruce E and Byeongseon Seo (2002). \u201cTesting for two-regime threshold\ncointegration in vector error-correction models\u201d. In: Journal of econometrics 110.2,\npp. 293\u2013318.\nHarris, Lawrence (1997). \u201cDecimalization: A review of the arguments and evidence\u201d.\nIn: Unpublished working paper, University of Southern California.\nHuck, Nicolas (2015). \u201cPairs trading: does volatility timing matter?\u201d In: Applied\nEconomics 47.57, pp. 6239\u20136256.\nJohansen, S\u00f8ren (1991). \u201cEstimation and hypothesis testing of cointegration vectors in\nGaussian\nvector\nautoregressive\nmodels\u201d.\nIn:\nEconometrica:\njournal\nof\nthe\nEconometric Society, pp. 1551\u20131580.\nKakushadze, Zura and Willie Yu (2019). \u201cAltcoin-Bitcoin Arbitrage\u201d. In: Bulletin of\nApplied Economics 6.1, pp. 87\u2013110.\nKapetanios, George (2005). \u201cUnit-root testing against the alternative hypothesis of up\nto m structural breaks\u201d. In: Journal of Time Series Analysis 26.1, pp. 123\u2013133.\nKapetanios, George, Yongcheol Shin, and Andy Snell (2003). \u201cTesting for a unit root in\nthe nonlinear STAR framework\u201d. In: Journal of Econometrics 112.2, pp. 359\u2013379.\n\u2013 (2006). \u201cTesting for cointegration in nonlinear smooth transition error correction\nmodels\u201d. In: Econometric Theory, pp. 279\u2013303.\nKeshavarz Haddad, GholamReza and Hassan Talebi (2023). \u201cThe profitability of pair\ntrading strategy in stock markets: Evidence from Toronto stock exchange\u201d. In:\nInternational Journal of Finance & Economics 28.1, pp. 193\u2013207.\nKlement, Erich Peter, Radko Mesiar, and Endre Pap (2002). \u201cInvariant copulas\u201d. In:\nKybernetika 38.3, pp. 275\u2013286.\nKrauss, Christopher (2017). \u201cStatistical arbitrage pairs trading strategies: Review and\noutlook\u201d. In: Journal of Economic Surveys 31.2, pp. 513\u2013545.\nKrauss, Christopher and Johannes St\u00a8ubinger (2017). \u201cNon-linear dependence modelling\nwith bivariate copulas: Statistical arbitrage pairs trading on the S&P 100\u201d. In: Applied\nEconomics 49.52, pp. 5352\u20135369.\nLeung, Tim and Hung Nguyen (2019). \u201cConstructing cointegrated cryptocurrency\nportfolios for statistical arbitrage\u201d. In: Studies in Economics and Finance 36.3,\npp. 581\u2013599.\n24\n\nLiew, Rong Qi and Yuan Wu (2013). \u201cPairs trading: A copula approach\u201d. In: Journal of\nDerivatives & Hedge Funds 19.1, pp. 12\u201330.\nNelsen, Roger B (2007). An introduction to copulas. Springer Science & Business\nMedia.\nPhillips, Peter CB and Sam Ouliaris (1990). \u201cAsymptotic properties of residual based\ntests for cointegration\u201d. In: Econometrica: journal of the Econometric Society,\npp. 165\u2013193.\nPritchard, Brendan Padraic Anson (2018). \u201cDigital asset arbitrage\u201d. PhD thesis.\nFundac\u00b8\u02dcao Getulio Vargas\u2019s Sao Paulo School of Business Administration.\nRad, Hossein, Rand Kwong Yew Low, and Robert Faff (2016).",
    "chunk_index": 18,
    "start_char": 46026,
    "end_char": 49641,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "In: Journal of\nDerivatives & Hedge Funds 19.1, pp. 12\u201330.\nNelsen, Roger B (2007). An introduction to copulas. Springer Science & Business\nMedia.\nPhillips, Peter CB and Sam Ouliaris (1990). \u201cAsymptotic properties of residual based\ntests for cointegration\u201d. In: Econometrica: journal of the Econometric Society,\npp. 165\u2013193.\nPritchard, Brendan Padraic Anson (2018). \u201cDigital asset arbitrage\u201d. PhD thesis.\nFundac\u00b8\u02dcao Getulio Vargas\u2019s Sao Paulo School of Business Administration.\nRad, Hossein, Rand Kwong Yew Low, and Robert Faff (2016). \u201cThe profitability of\npairs trading strategies: distance, cointegration and copula methods\u201d. In: Quantitative\nFinance 16.10, pp. 1541\u20131558.\nSilva, Fernando AB Sabino da, Flavio A Ziegelmann, and Jo\u02dcao F Caldeira (2023). \u201cA\npairs trading strategy based on mixed copulas\u201d. In: The Quarterly Review of\nEconomics and Finance 87, pp. 16\u201334.\nStander, Yolanda, Dani\u00a8el Marais, and Ilse Botha (2013). \u201cTrading strategies with\ncopulas\u201d. In: Journal of Economic and Financial Sciences 6.1, pp. 83\u2013107.\nTadi,\nMasood\nand\nIrina\nKortchemski\n(2021).\n\u201cEvaluation\nof\ndynamic\ncointegration-based pairs trading strategy in the cryptocurrency market\u201d. In: Studies\nin Economics and Finance.\nTer\u00a8asvirta, Timo (1994). \u201cSpecification, estimation, and evaluation of smooth transition\nautoregressive models\u201d. In: Journal of the american Statistical association 89.425,\npp. 208\u2013218.\nVidyamurthy, Ganapathy (2004). Pairs Trading: quantitative methods and analysis.\nVol. 217. John Wiley & Sons.\nXie, Wenjun, Rong Qi Liew, et al. (2016). \u201cPairs trading with copulas\u201d. In: The Journal\nof Trading 11.3, pp. 41\u201352.\nXie, Wenjun and Yuan Wu (2013). \u201cCopula-based pairs trading strategy\u201d. In: Asian\nFinance Association (AsFA).\nZivot, Eric and Donald W K Andrews (2002). \u201cFurther evidence on the great crash, the\noil-price shock, and the unit-root hypothesis\u201d. In: Journal of business & economic\nstatistics 20.1, pp. 25\u201344.\n25\n\nAppendix\nTable VII: Binance USDT-Margined Futures contracts used in the research\nSymbol\nUnderlying Crypto\nMin. Trade Amount\nMax. Leverage\nBTCUSDT\nBitcoin\n0.001 BTC\n125x\nETHUSDT\nEthereum\n0.001 ETH\n100x\nBCHUSDT\nBitcoin Cash\n0.001 BCH\n75x\nXRPUSDT\nRipple\n0.1 XRP\n75x\nEOSUSDT\nEOS.IO\n0.1 EOS\n75x\nLTCUSDT\nLitecoin\n0.001 LTC\n75x\nTRXUSDT\nTRON\n1 TRX\n50x\nETCUSDT\nEthereum Classic\n0.01 ETC\n75x\nLINKUSDT\nChainlink\n0.01 LINK\n75x\nXLMUSDT\nStellar\n1 XLM\n50x\nADAUSDT\nCardano\n1 ADA\n75x\nXMRUSDT\nMonero\n0.001 XMR\n50x\nDASHUSDT\nDash\n0.001 DASH\n50x\nZECUSDT\nZcash\n0.001 ZEC\n50x\nXTZUSDT\nTezos\n0.1 XTZ\n50x\nATOMUSDT\nCosmos\n0.01 ATOM\n25x\nBNBUSDT\nBinance Coin\n0.01 BNB\n75x\nONTUSDT\nOntology\n0.1 ONT\n50x\nIOTAUSDT\nIOTA\n0.1 IOTA\n25x\nBATUSDT\nBasic Attention Token\n0.1 BAT\n50x\n26\n\nTable VIII: Selected pairs using unit-root tests and Kendall\u2019s \u03c4 coefficient (part I)\nADF Unit-Root Test Result\nKSS Unit-Root Test Result\nWeek\nPair\nP-Value (S1)\nP-Value (S2)\nPair\nt-stat (S1)\nt-stat (S2)\n1\nETH-LTC\n0.075\n0.070\nBCH-ETC\n-2.72\n-1.96\n2\nLTC-BCH\n0.023\n0.010\nLTC-BCH\n-3.07\n-2.09\n3\nETH-LTC\n0.075\n0.037\nETH-LTC\n-2.10\n-2.90\n4\nETH-LTC\n0.095\n0.022\nETH-ETC\n-2.35\n-2.64\n5\nLTC-EOS\n0.096\n0.082\nLTC-BCH\n-2.48\n-2.66\n6\nLINK-TRX\n0.090\n0.069\nLTC-BCH\n-2.44\n-2.46\n7\nLINK-TRX\n0.097\n0.051\nLTC-BCH\n-2.44\n-2.32\n8\nETH-LTC\n0.010\n0.010\nETH-BCH\n-2.66\n-2.47\n9\nETH-LTC\n0.042\n0.022\nLTC-BNB\n-3.43\n-4.80\n10\nETH-BCH\n0.097\n0.073\nLTC-BCH\n-3.03",
    "chunk_index": 19,
    "start_char": 49108,
    "end_char": 52398,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "Week\nPair\nP-Value (S1)\nP-Value (S2)\nPair\nt-stat (S1)\nt-stat (S2)\n1\nETH-LTC\n0.075\n0.070\nBCH-ETC\n-2.72\n-1.96\n2\nLTC-BCH\n0.023\n0.010\nLTC-BCH\n-3.07\n-2.09\n3\nETH-LTC\n0.075\n0.037\nETH-LTC\n-2.10\n-2.90\n4\nETH-LTC\n0.095\n0.022\nETH-ETC\n-2.35\n-2.64\n5\nLTC-EOS\n0.096\n0.082\nLTC-BCH\n-2.48\n-2.66\n6\nLINK-TRX\n0.090\n0.069\nLTC-BCH\n-2.44\n-2.46\n7\nLINK-TRX\n0.097\n0.051\nLTC-BCH\n-2.44\n-2.32\n8\nETH-LTC\n0.010\n0.010\nETH-BCH\n-2.66\n-2.47\n9\nETH-LTC\n0.042\n0.022\nLTC-BNB\n-3.43\n-4.80\n10\nETH-BCH\n0.097\n0.073\nLTC-BCH\n-3.03\n-2.02\n11\nLTC-BCH\n0.079\n0.010\nLTC-BCH\n-2.58\n-2.63\n12\nATOM-ADA\n0.010\n0.010\nBCH-ATOM\n-1.96\n-3.02\n13\nBAT-ONT\n0.095\n0.021\nADA-XTZ\n-2.61\n-2.07\n14\nADA-EOS\n0.064\n0.010\nLTC-ADA\n-2.15\n-3.23\n15\nEOS-LTC\n0.044\n0.027\nEOS-LTC\n-2.33\n-2.14\n16\nBAT-IOTA\n0.022\n0.010\nTRX-BNB\n-2.56\n-2.06\n17\nBNB-TRX\n0.067\n0.036\nBCH-BNB\n-1.96\n-3.08\n18\nBCH-LTC\n0.082\n0.086\nTRX-IOTA\n-3.08\n-2.64\n19\nETH-TRX\n0.059\n0.023\nDASH-BCH\n-2.77\n-3.42\n20\nETH-LTC\n0.078\n0.088\nBCH-EOS\n-2.07\n-2.46\n21\nTRX-LTC\n0.010\n0.010\nTRX-LTC\n-2.17\n-2.05\n22\n-\n-\n-\nLTC-ETC\n-1.99\n-2.39\n23\n-\n-\n-\nBNB-LINK\n-2.10\n-2.06\n24\nETH-BNB\n0.072\n0.092\nETH-BNB\n-2.05\n-2.93\n25\nLTC-EOS\n0.018\n0.018\nLTC-BNB\n-1.97\n-2.32\n26\nETH-LTC\n0.085\n0.010\nLTC-XRP\n-2.50\n-3.91\n27\nBNB-DASH\n0.041\n0.043\nXRP-DASH\n-2.02\n-2.48\n28\nETH-BCH\n0.060\n0.096\nETH-BCH\n-1.95\n-2.35\n29\nBNB-LTC\n0.068\n0.015\nBNB-LTC\n-2.42\n-2.99\n30\nBNB-EOS\n0.044\n0.030\nBNB-EOS\n-2.22\n-3.77\n31\nETH-LINK\n0.030\n0.015\nLTC-LINK\n-2.52\n-2.72\n32\nETH-LINK\n0.058\n0.096\nETH-LTC\n-2.46\n-1.95\n33\nLTC-XRP\n0.063\n0.032\nLTC-BNB\n-2.73\n-1.93\n34\nEOS-XRP\n0.049\n0.041\nEOS-LTC\n-2.36\n-1.97\n35\nETH-EOS\n0.061\n0.041\nEOS-BNB\n-2.15\n-3.96\n36\nBNB-LINK\n0.090\n0.052\nLTC-ADA\n-3.91\n-2.53\n37\nETH-BNB\n0.046\n0.063\nETH-ETC\n-2.50\n-1.93\n38\nETH-LINK\n0.013\n0.063\nBCH-ONT\n-2.49\n-2.10\n39\nDASH-ONT\n0.079\n0.010\nLTC-DASH\n-2.08\n-2.20\n40\nLTC-BNB\n0.091\n0.090\nDASH-BNB\n-2.18\n-2.07\n41\nETC-DASH\n0.091\n0.035\nETC-DASH\n-1.95\n-2.52\n42\nBCH-LTC\n0.017\n0.027\nBCH-LTC\n-3.15\n-2.30\n43\nETH-ETC\n0.075\n0.010\nETC-EOS\n-3.81\n-2.44\n44\nETH-ETC\n0.045\n0.010\nETH-ETC\n-2.66\n-3.13\n45\nEOS-ETC\n0.021\n0.010\nEOS-ETC\n-4.45\n-3.32\n46\nEOS-ETC\n0.027\n0.010\nEOS-ETC\n-2.46\n-3.56\n47\nETC-XRP\n0.055\n0.069\nETC-XRP\n-3.07\n-2.75\n48\nETH-LTC\n0.089\n0.088\nLTC-DASH\n-3.29\n-4.01\n49\nETH-ETC\n0.026\n0.010\nETH-ETC\n-2.43\n-2.18\n50\nETH-ETC\n0.070\n0.010\nETH-ETC\n-2.96\n-2.75\n51\nETC-LTC\n0.013\n0.020\nETH-ETC\n-2.43\n-3.19\n52\nLTC-EOS\n0.010\n0.050\nLTC-EOS\n-3.39\n-2.61\n27\n\nTable IX: Selected pairs using unit-root tests and Kendall\u2019s \u03c4 coefficient (part II)\nADF Unit-Root Test Result\nKSS Unit-Root Test Result\nWeek\nPair\nP-Value (S1)\nP-Value (S2)\nPair\nt-stat (S1)\nt-stat (S2)\n53\nEOS-XRP\n0.032\n0.010\nEOS-XRP\n-2.43\n-3.51\n54\n-\n-\n-\nLTC-BNB\n-2.03\n-2.24\n55\nDASH-XLM\n0.041\n0.010\nEOS-LTC\n-2.20\n-2.01\n56\nETH-BNB\n0.049\n0.016\nETH-LTC\n-2.69\n-2.77\n57\nETH-BNB\n0.010\n0.018\nETH-BNB\n-2.83\n-3.20\n58\nETH-BNB\n0.028\n0.023\nETH-BNB\n-2.78\n-4.00\n59\nETH-BAT\n0.024\n0.068\nETH-BAT\n-3.41\n-1.96\n60\nETH-BNB\n0.020\n0.020\nETH-EOS\n-2.41\n-1.95\n61\nETH-BNB\n0.085\n0.010\nBNB-ADA\n-3.02\n-2.04\n62\nBNB-ONT\n0.010\n0.083\nETH-BNB\n-1.95\n-2.79\n63\nBNB-LTC\n0.060\n0.022\nLTC-LINK\n-3.47\n-2.32\n64\nLINK-ONT\n0.029\n0.095\nXRP-LINK\n-2.14\n-2.34\n65\nXRP-XLM\n0.010\n0.095\nXRP-ADA\n-3.35\n-2.16\n66",
    "chunk_index": 20,
    "start_char": 51917,
    "end_char": 54959,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "-2.03\n-2.24\n55\nDASH-XLM\n0.041\n0.010\nEOS-LTC\n-2.20\n-2.01\n56\nETH-BNB\n0.049\n0.016\nETH-LTC\n-2.69\n-2.77\n57\nETH-BNB\n0.010\n0.018\nETH-BNB\n-2.83\n-3.20\n58\nETH-BNB\n0.028\n0.023\nETH-BNB\n-2.78\n-4.00\n59\nETH-BAT\n0.024\n0.068\nETH-BAT\n-3.41\n-1.96\n60\nETH-BNB\n0.020\n0.020\nETH-EOS\n-2.41\n-1.95\n61\nETH-BNB\n0.085\n0.010\nBNB-ADA\n-3.02\n-2.04\n62\nBNB-ONT\n0.010\n0.083\nETH-BNB\n-1.95\n-2.79\n63\nBNB-LTC\n0.060\n0.022\nLTC-LINK\n-3.47\n-2.32\n64\nLINK-ONT\n0.029\n0.095\nXRP-LINK\n-2.14\n-2.34\n65\nXRP-XLM\n0.010\n0.095\nXRP-ADA\n-3.35\n-2.16\n66\nETH-BNB\n0.010\n0.047\nETH-BNB\n-3.15\n-3.10\n67\nETH-BNB\n0.041\n0.010\nETH-BNB\n-3.40\n-3.23\n68\nETH-BNB\n0.017\n0.010\nETH-BNB\n-3.56\n-3.42\n69\n-\n-\n-\nBCH-XLM\n-2.27\n-2.03\n70\nBNB-ETC\n0.031\n0.033\nLINK-BNB\n-2.07\n-2.39\n71\nLINK-ETC\n0.070\n0.062\nETC-BNB\n-2.29\n-2.42\n72\nLINK-EOS\n0.046\n0.025\nBNB-LINK\n-2.28\n-2.37\n73\nDASH-ETC\n0.037\n0.044\nEOS-DASH\n-1.96\n-3.22\n74\nONT-ZEC\n0.026\n0.010\nBNB-DASH\n-2.56\n-1.95\n75\nZEC-BCH\n0.047\n0.041\nETH-BNB\n-2.36\n-2.16\n76\nETH-XTZ\n0.010\n0.029\nETH-ZEC\n-2.83\n-2.52\n77\nETH-BNB\n0.057\n0.064\nETH-BNB\n-2.77\n-2.42\n78\nETH-BNB\n0.022\n0.010\nETH-BNB\n-3.17\n-2.19\n79\nBNB-LTC\n0.021\n0.037\nBNB-LTC\n-2.93\n-3.50\n80\nBNB-LTC\n0.017\n0.061\nBNB-LTC\n-3.08\n-2.09\n81\nETH-LTC\n0.010\n0.010\nLTC-LINK\n-3.25\n-2.52\n82\nLTC-XRP\n0.014\n0.017\nLTC-XRP\n-2.68\n-4.02\n83\nLTC-DASH\n0.010\n0.047\nLTC-DASH\n-4.27\n-2.64\n84\nETH-LTC\n0.044\n0.010\nETH-LTC\n-2.40\n-3.95\n85\nETH-IOTA\n0.090\n0.010\nIOTA-DASH\n-2.63\n-2.87\n86\nBAT-IOTA\n0.010\n0.010\nBAT-DASH\n-3.54\n-1.96\n87\nBNB-ETC\n0.023\n0.068\nBAT-ETC\n-2.49\n-2.85\n88\nBNB-TRX\n0.020\n0.084\nDASH-BAT\n-2.35\n-2.43\n89\nETH-DASH\n0.090\n0.010\nETH-DASH\n-2.71\n-4.19\n90\nETH-DASH\n0.010\n0.014\nETH-DASH\n-2.95\n-2.78\n91\nETH-BAT\n0.095\n0.032\nETH-DASH\n-2.54\n-2.26\n92\nETH-BAT\n0.010\n0.084\nETH-BAT\n-3.08\n-2.15\n93\nBNB-LTC\n0.014\n0.023\nBNB-LTC\n-2.22\n-3.47\n94\nBCH-DASH\n0.010\n0.046\nBCH-DASH\n-3.36\n-2.30\n95\nETH-BCH\n0.051\n0.034\nBCH-DASH\n-6.40\n-2.29\n96\nETH-ADA\n0.081\n0.010\nETH-ADA\n-2.44\n-2.53\n97\nETH-BAT\n0.042\n0.053\nETH-ADA\n-2.47\n-2.47\n98\nADA-BAT\n0.039\n0.010\nETH-ADA\n-1.97\n-3.08\n99\nXTZ-BAT\n0.010\n0.010\nETH-ATOM\n-1.98\n-2.15\n100\nETH-BCH\n0.010\n0.099\nETH-ONT\n-2.20\n-3.13\n101\n-\n-\n-\nLINK-BCH\n-2.47\n-2.00\n102\nETH-BNB\n0.057\n0.033\nETH-LTC\n-2.48\n-2.00\n103\nBNB-LINK\n0.010\n0.020\nBNB-LINK\n-3.85\n-3.27\n104\nLINK-DASH\n0.061\n0.100\nLINK-ONT\n-2.49\n-2.31\n28\n\nTable X: Bivariate Archimedean copulas distributions\nName\nBivariate Copula Distribution C(u1, u2)\n(ui = 1 \u2212ui)\nGenerator \u03d5(t)\nParameters\nClayton\n\"\nmax\n\u0012\nu\u2212\u03b8\n1\n+ u\u2212\u03b8\n2\n\u22121, 0\n\u0013#\u22121/\u03b8\n1\n\u03b8\n\u0010\nt\u2212\u03b8 \u22121\n\u0011\n\u03b8 > 0\nGumbel\nexp\n\"\n\u2212\n\u0014\n(\u2212ln u1)\u03b8 + (\u2212ln u2)\u03b8\n\u00151/\u03b8#\n(\u2212ln(t))\u03b8\n\u03b8 \u22651\nFrank\n\u2212\u03b8\u22121 ln\n\"\n1 +\n\u0000e\u2212\u03b8 \u22121\n\u0001\u22121\u0000e\u2212\u03b8u1 \u22121\n\u0001\u0000e\u2212\u03b8u2 \u22121\n\u0001\n#\n\u2212ln\n\u0010\ne\u2212\u03b8t\u22121\ne\u2212\u03b8\u22121\n\u0011\n\u03b8 \u2208R \\ {0}\nJoe\n1 \u2212\n\"\n\u00001 \u2212u1\n\u0001\u03b8 +\n\u00001 \u2212u2\n\u0001\u03b8 \u2212\n\u00001 \u2212u1\n\u0001\u03b8\u00001 \u2212u2\n\u0001\u03b8\n#1/\u03b8\n\u2212ln\n\u0010\n1 \u2212(1 \u2212t)\u03b8\u0011\n\u03b8 \u22651\nBB1\n\"\n1 +\n\u0014\u0010\nu\u2212\u03b8\n1\n\u22121\n\u0011\u03b4\n+\n\u0010\nu\u2212\u03b8\n2\n\u22121\n\u0011\u03b4\u00151/\u03b4#\u22121/\u03b8\n\u0010\nt\u2212\u03b8 \u22121\n\u0011\u03b4\n\u03b8 > 0, \u03b4 \u22651\nBB6\n1 \u2212\n\"\n1 \u2212exp\n\u0012\n\u2212\nh\u0002\n\u2212ln\n\u00001 \u2212u1 \u03b8\u0001\u0003\u03b4 +\n\u0002\n\u2212ln\n\u00001 \u2212u1 \u03b8\u0001\u0003\u03b4i1/\u03b4\u0013#1/\u03b8\n\u0010\nln",
    "chunk_index": 21,
    "start_char": 54468,
    "end_char": 57223,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "#\n\u2212ln\n\u0010\ne\u2212\u03b8t\u22121\ne\u2212\u03b8\u22121\n\u0011\n\u03b8 \u2208R \\ {0}\nJoe\n1 \u2212\n\"\n\u00001 \u2212u1\n\u0001\u03b8 +\n\u00001 \u2212u2\n\u0001\u03b8 \u2212\n\u00001 \u2212u1\n\u0001\u03b8\u00001 \u2212u2\n\u0001\u03b8\n#1/\u03b8\n\u2212ln\n\u0010\n1 \u2212(1 \u2212t)\u03b8\u0011\n\u03b8 \u22651\nBB1\n\"\n1 +\n\u0014\u0010\nu\u2212\u03b8\n1\n\u22121\n\u0011\u03b4\n+\n\u0010\nu\u2212\u03b8\n2\n\u22121\n\u0011\u03b4\u00151/\u03b4#\u22121/\u03b8\n\u0010\nt\u2212\u03b8 \u22121\n\u0011\u03b4\n\u03b8 > 0, \u03b4 \u22651\nBB6\n1 \u2212\n\"\n1 \u2212exp\n\u0012\n\u2212\nh\u0002\n\u2212ln\n\u00001 \u2212u1 \u03b8\u0001\u0003\u03b4 +\n\u0002\n\u2212ln\n\u00001 \u2212u1 \u03b8\u0001\u0003\u03b4i1/\u03b4\u0013#1/\u03b8\n\u0010\nln\n\u00001 \u2212(1 \u2212t)\u03b8\u0001 \u0011\u03b4\n\u03b8 \u22651, \u03b4 \u22651\nBB7\n1 \u2212\n\"\n1 \u2212\n\u0012\u0010\n1 \u2212u1 \u03b8\u0011\u2212\u03b4\n+\n\u0010\n1 \u2212u2 \u03b8\u0011\u2212\u03b4\n\u22121\n\u0013\u22121/\u03b4#1/\u03b8\n\u0010\n1 \u2212(1 \u2212t)\u03b8\u0011\u2212\u03b4\n\u22121\n\u03b8 \u22651, \u03b4 > 0\nBB8\n\u03b4\u22121\n\"\n1 \u2212\n\u0012\n1 \u2212\nh\n1 \u2212(1 \u2212\u03b4)\u03b8i\u22121 h\n1 \u2212(1 \u2212\u03b4u1)\u03b8i h\n1 \u2212(1 \u2212\u03b4u2)\u03b8i\u00131/\u03b8 #\n\u2212ln\n\u0012\n1\u2212(1\u2212\u03b4t)\u03b8\n1\u2212(1\u2212\u03b4)\u03b8\n\u0013\n\u03b8 \u22651, 0 < \u03b4 \u22641",
    "chunk_index": 22,
    "start_char": 56961,
    "end_char": 57453,
    "paper_title": "Copula-Based Trading of Cointegrated Cryptocurrenc",
    "paper_category": "q-fin.TR",
    "paper_filename": "Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Copula-Based_Trading_of_Cointegrated_Cryptocurrenc.pdf"
  },
  {
    "text": "Cross-Impact of Order Flow Imbalance\nin Equity Markets\nRama Cont1,5, Mihai Cucuringu1,2,3,4, and Chao Zhang\u22171,2,3\n1Mathematical Institute, University of Oxford, Oxford, UK\n2Department of Statistics, University of Oxford, Oxford, UK\n3Oxford-Man Institute of Quantitative Finance, University of Oxford, Oxford, UK\n4The Alan Turing Institute, London, UK\n5Oxford Suzhou Centre for Advanced Research, Suzhou, China\nMay 2023\nAbstract\nWe investigate the impact of order flow imbalance (OFI) on price movements in\nequity markets in a multi-asset setting. First, we propose a systematic approach for\ncombining OFIs at the top levels of the limit order book into an integrated OFI variable\nwhich better explains price impact, compared to the best-level OFI. We show that once\nthe information from multiple levels is integrated into OFI, multi-asset models with\ncross-impact do not provide additional explanatory power for contemporaneous impact\ncompared to a sparse model without cross-impact terms. On the other hand, we show\nthat lagged cross-asset OFIs do improve the forecasting of future returns.\nWe also\nestablish that this lagged cross-impact mainly manifests at short-term horizons and\ndecays rapidly in time.\nKeywords: Market impact, Cross-impact, Order flow imbalance, Return prediction.\nJEL Codes: C31, C53, G14\n\u2217Corresponding author.\nEmail: chao.zhang@stats.ox.co.uk.\nWe thank Robert Engle, \u00c1lvaro Cartea,\nSlavi Marinov and seminar participants at the 11th Bachelier World Congress 2022; the 14th Annual SoFiE\nConference; University of Oxford for helpful comments.\nWe also thank the Oxford Suzhou Centre for\nAdvanced Research for providing the computational facilities. Earlier versions of this paper circulated under\nthe titles \u201cPrice Impact of Order Flow Imbalance: Multi-level, Cross-asset and Forecasting\u201d and \u201cCross-\nImpact of Order Flow Imbalance: Contemporaneous and Predictive\u201d. First draft: December 2021.\n1\narXiv:2112.13213v4 [q-fin.TR] 13 Jun 2023\n\nContents\n1\nIntroduction\n3\n1.1\nMain contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nData and variables\n5\n2.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nSummary statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nContemporaneous cross-impact\n9\n3.1\nModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2\nEmpirical results\n. . . . . . .",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 2531,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nContemporaneous cross-impact\n9\n3.1\nModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2\nEmpirical results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2.1\nIn-sample performance . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2.2\nOut-of-sample performance . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.3\nDiscussion about contemporaneous cross-impact . . . . . . . . . . . . . . . .\n18\n3.3.1\nImpact on stocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.3.2\nImpact on portfolios\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4\nForecasting future returns\n21\n4.1\nPredictive models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2\nEmpirical results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.2.1\nStatistical performance . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.2.2\nEconomic gains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.3\nLonger forecasting horizons\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.4\nDiscussion about predictive cross-impact . . . . . . . . . . . . . . . . . . . .\n27\n5\nConclusion\n28\nA Aggregation of multi-level OFIs\n33\nB Contemporaneous price impact of multi-level OFIs\n35\nC Comparison with Capponi & Cont (2020)\n36\nD High-frequency updates of contemporaneous models\n37\nE Additional results of Section 4\n38\n2",
    "chunk_index": 1,
    "start_char": 2324,
    "end_char": 3808,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": ". . . . . . .\n27\n4.4\nDiscussion about predictive cross-impact . . . . . . . . . . . . . . . . . . . .\n27\n5\nConclusion\n28\nA Aggregation of multi-level OFIs\n33\nB Contemporaneous price impact of multi-level OFIs\n35\nC Comparison with Capponi & Cont (2020)\n36\nD High-frequency updates of contemporaneous models\n37\nE Additional results of Section 4\n38\n2\n\n1\nIntroduction\nAccurately estimating and forecasting the impact of trading behavior of market participants\non the price movements of assets carries practical implications for both practitioners and\nacademics, such as trading cost analysis (Frazzini et al. [23]) and optimal execution of trades.\nThe impact of trades on asset prices, known as price impact, has been the focus of many\nstudies and modeling efforts (Cont et al. [17], Lillo et al. [39]). In a multi-asset setting, several\nstudied have focused on the concept of cross-impact, which attempts to describe the impact\nof trading a given asset on the price of other assets (see Benzaquen et al. [5], Capponi and\nCont [8], Pasquariello and Vega [41]).\nSeveral studies have investigated contemporaneous cross-impact of order flow on returns\nby examining their cross-correlation structure.\nFor example, Hasbrouck and Seppi [28]\nrevealed that commonality in returns among Dow 30 stocks is mostly attributed to order\nflow commonality. Tomas et al. [47] built a principled approach to choosing a cross-impact\nmodel for various markets.\nCapponi and Cont [8] showed that the positive covariance\nbetween returns of a specific stock and order flow imbalances of other stocks does not\nnecessarily constitute evidence of cross-impact. They further demonstrated that, as long\nas the common factor in order flow imbalances is taken into account, adding cross-impact\nterms only marginally improves model performance, and thus may be disregarded. Our study\ncomplements Capponi and Cont [8] in several ways: unlike [8] which focuses on in-sample\nperformance, we also consider the forecasting power of cross-order flow using both single and\nmulti-level OFIs. To the best of our knowledge, there have been no studies that examine the\ninfluence of order flows on price movements in a multi-asset setting, while also taking into\naccount the deeper levels in the limit order book (LOB).1\nA more challenging problem than explaining contemporaneous returns is to examine\nthe impact of trade orders on prices over future horizons, which has received a lot less\nattention in the literature, despite its important economic implications. Some studies have\nexamined the relationship between order imbalances and future daily returns.2\nChordia\net al. [15] revealed that daily stock market returns are strongly related to contemporaneous\nand lagged order imbalances. Chordia and Subrahmanyam [14] further found that there\nexists a positive relation between lagged order imbalances and daily individual stock returns.\nThe authors also showed that imbalance-based trading strategies, i.e. buy if the previous\nday\u2019s imbalance is positive, and sell if the previous day\u2019s imbalance is negative, are able to\nyield statistically significant profits.Pasquariello and Vega [41] provided empirical",
    "chunk_index": 2,
    "start_char": 3461,
    "end_char": 6623,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "daily returns.2\nChordia\net al. [15] revealed that daily stock market returns are strongly related to contemporaneous\nand lagged order imbalances. Chordia and Subrahmanyam [14] further found that there\nexists a positive relation between lagged order imbalances and daily individual stock returns.\nThe authors also showed that imbalance-based trading strategies, i.e. buy if the previous\nday\u2019s imbalance is positive, and sell if the previous day\u2019s imbalance is negative, are able to\nyield statistically significant profits.Pasquariello and Vega [41] provided empirical evidence of\ncross-asset informational effects in NYSE and NASDAQ stocks between 1993 and 2004, and\ndemonstrated that the daily order flow imbalance in one stock, or across one industry, has a\nsignificant and persistent impact on daily returns of other stocks or industries. Rosenbaum\nand Tomas [43] provided a characterization of the class of cross-impact kernels for a market\nthat employs Hawkes processes to model trades and applied their method to two instruments\nfrom E-Mini Futures.\nGiven the recent progress in high-frequency trading (HFT), it is increasingly crucial\n1Xu et al. [53] studied the contemporaneous price impact (not cross-impact) model by extending the\nmodel of Cont et al. [17] to multi-level order flow imbalance.\n2Several studies, such as Hou [31], Menzly and Ozbas [40], Chinco et al. [12], Buccheri et al. [6], investigated\nthe lead-lag effect in equity returns across various assets, but did not take into account order flows.\n3\n\nto obtain accurate estimations of the cross-impact on future intraday returns. Benzaquen\net al. [5] introduced a multivariate linear model (see Kyle [37]) to describe the structure of\ncross-impact and found that a significant fraction of the covariance of stock returns can be\naccounted for by this model. Wang et al. [49, 51] empirically analyzed and discussed the\nimpact of trading a specific stock on the average price change of the whole market or of\nindividual sectors. Schneider and Lillo [44] derived theoretical limits for the size and form\nof cross-impact and verified them on sovereign bonds data. However, when modeling cross-\nimpact, these methods do not consider the possibility of high correlations between cross-asset\norder flows, which may result in overfitting issues. This is also evidenced by studies such\nas Benzaquen et al. [5] and Tomas et al. [47]. Moreover, these studies mainly investigated\nthe cross-impact coefficients for a fixed time period (i.e., in a static setting), ignoring the\ntemporal dynamics of cross-impact.\nIn recent years, machine learning models including deep neural networks, have achieved\nsubstantial developments, leading to their applications in financial markets, especially for the\ntask of modeling stock returns. For example, Huck [32] utilized state-of-the-art techniques,\nsuch as random forests, to construct a portfolio over a period of 22 years, and the results\ndemonstrated the power of machine learning models to produce profitable trading signals.\nKrauss et al. [36] applied a series of machine learning methods to forecast the probability\nof a stock outperforming the market index, and then constructed long-short portfolios from\nthe predicted one-day-ahead trading signals.\nGu et al.",
    "chunk_index": 3,
    "start_char": 6057,
    "end_char": 9322,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "of modeling stock returns. For example, Huck [32] utilized state-of-the-art techniques,\nsuch as random forests, to construct a portfolio over a period of 22 years, and the results\ndemonstrated the power of machine learning models to produce profitable trading signals.\nKrauss et al. [36] applied a series of machine learning methods to forecast the probability\nof a stock outperforming the market index, and then constructed long-short portfolios from\nthe predicted one-day-ahead trading signals.\nGu et al. [25] employed a set of machine\nlearning methods to make one-month-ahead return forecasts, and demonstrated the potential\nof machine learning approaches in empirical asset pricing, due to their ability to handle\nnonlinear interactions. Ait-Sahalia et al. [2] investigated the predictability of high-frequency\nstock returns and durations using LASSO and tree methods via many relevant predictors\nderived from returns and order flows. Tashiro et al. [46] and Kolm et al. [35] applied deep\nneural networks with LOB-based features to predict high-frequency returns. Nonetheless,\nto the best of our knowledge, cross-asset order flow imbalances have not been considered as\npredictors for forecasting future high-frequency returns in the literature, which is one of the\nmain directions we explore in the second half of this paper.\n1.1\nMain contributions\nThe present study makes two main contributions to the literature regarding the contemporaneous\nand predictive cross-impact of order flow imbalances on price returns.\nFirst, we revisit the significance of contemporaneous cross-impact by considering various\ndefinitions of order flow imbalance (OFI). Instead of only looking at the best-level orders,\nwe systematically examine the impact of multi-level order flows in a cross-asset setting.\nOur results show that, once information from multi-level order flow is incorporated in the\ndefinition of order flow imbalance, cross-impact terms do not provide additional explanatory\npower for contemporaneous impact, compared to a parsimonious model without cross-impact.\nTo the best of our knowledge, this is the first study to comprehensively analyze the relations\nbetween contemporaneous individual returns and multi-level orders in both single-asset and\nmulti-asset settings.\nFurthermore, we consider the associated forecasting problem and investigate the predictive\npower of the cross-asset order flows on future price returns. Our results suggest that cross-\n4\n\nimpact terms do provide significant information content for intraday forecasting of future\nreturns over a short horizon of up to several minutes, but their predictability decays quickly\nthrough time.\n1.2\nOutline\nSection 2 describes our dataset and defines the variables of interest. Section 3 discusses\nmodeling of contemporaneous cross-impact. In Section 4, we first discuss the out-of-sample\nforecasting performance of cross-impact models over one-minute-ahead horizon from two\nperspectives: R2 values and economic gains, and then examine the predictability over longer\nhorizons.\nFinally, we conclude the analysis in Section 5 and highlight potential future\nresearch directions.\n2\nData and variables\n2.1\nData\nWe use the Nasdaq ITCH data from LOBSTER to compute the independent and dependent\nvariables. Our data includes the top 100 components of S&P 500 index, existing from 2017-\n01-01 to 2019-12-31.3\nCont et al.",
    "chunk_index": 4,
    "start_char": 8816,
    "end_char": 12193,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "from two\nperspectives: R2 values and economic gains, and then examine the predictability over longer\nhorizons.\nFinally, we conclude the analysis in Section 5 and highlight potential future\nresearch directions.\n2\nData and variables\n2.1\nData\nWe use the Nasdaq ITCH data from LOBSTER to compute the independent and dependent\nvariables. Our data includes the top 100 components of S&P 500 index, existing from 2017-\n01-01 to 2019-12-31.3\nCont et al. [17] found that over short time intervals, price changes are mainly driven by\nthe Order Flow Imbalance (henceforth denoted as OFI). Kolm et al. [35] also demonstrated\nthat forecasting deep learning models trained on OFIs significantly outperform most models\ntrained directly on order books or returns. Therefore, we adopt the OFIs as features in our\nbelow analysis.\nDuring the interval (t\u2212h, t], we enumerate the observations of all order book updates by\nn. Given two consecutive order book states for a given stock i at n \u22121 and n, we compute\nthe bid order flows (OFm,b\ni,n ) and ask order flows (OFm,a\ni,n ) of stock i at level m at time n as\nOFm,b\ni,n :=\n\uf8f1\n\uf8f2\n\uf8f3\nqm,b\ni,n ,\nif P m,b\ni,n > P m,b\ni,n\u22121,\nqm,b\ni,n \u2212qm,b\ni,n\u22121,\nif P m,b\ni,n = P m,b\ni,n\u22121,\n\u2212qm,b\ni,n ,\nif P m,b\ni,n < P m,b\ni,n\u22121,\nOFm,a\ni,n :=\n\uf8f1\n\uf8f2\n\uf8f3\n\u2212qm,a\ni,n ,\nif P m,a\ni,n > P m,a\ni,n\u22121,\nqm,a\ni,n \u2212qm,a\ni,n\u22121,\nif P m,a\ni,n = P m,a\ni,n\u22121\nqm,a\ni,n ,\nif P m,a\ni,n < P m,a\ni,n\u22121,\nwhere, P m,b\ni,n and qm,b\ni,n denote the bid price and size (in number of shares) of stock i at level\nm, respectively.\nSimilarly, P m,a\ni,n\nand qm,a\ni,n\ndenote the ask price and ask size at level m,\nrespectively. Note that the variable OFm,b\ni,t\nis positive when (i) the bid price increase; (ii)\nthe bid price remains the same and the bid size increases. OFm,b\ni,t\nis negative when (i) the\nbid price decreases; (ii) the bid price remains the same and the bid size decreases. One can\nperform an analogous analysis and interpretation for the ask order flows OFm,a\ni,t .\n3We select the top 100 components based on their market capitalization as of the most recent market\nclose at the time of our analysis, i.e. 2019-12-31.\n5\n\nBest-level OFI.\nIt calculates the accumulative OFIs at the best bid/ask side during a\ngiven time interval (see Cont et al. [17], Kolm et al. [35]), and is defined as4\nOFI1,h\ni,t :=\nN(t)\nX\nn=N(t\u2212h)+1\nOF1,b\ni,n \u2212OF1,a\ni,n,\n(1)\nwhere N(t \u2212h) + 1 and N(t) are the indexes of the first and the last order book event in\nthe interval (t \u2212h, t].\nDeeper-level OFI.\nA natural extension of the best-level OFI defined in Eqn (1) is deeper-\nlevel OFI (see Xu et al. [53], Kolm et al. [35]).",
    "chunk_index": 5,
    "start_char": 11748,
    "end_char": 14334,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "[17], Kolm et al. [35]), and is defined as4\nOFI1,h\ni,t :=\nN(t)\nX\nn=N(t\u2212h)+1\nOF1,b\ni,n \u2212OF1,a\ni,n,\n(1)\nwhere N(t \u2212h) + 1 and N(t) are the indexes of the first and the last order book event in\nthe interval (t \u2212h, t].\nDeeper-level OFI.\nA natural extension of the best-level OFI defined in Eqn (1) is deeper-\nlevel OFI (see Xu et al. [53], Kolm et al. [35]). We define OFI at level m (m \u22651) as follows\nOFIm,h\ni,t\n:=\nN(t)\nX\nn=N(t\u2212h)+1\nOFm,b\ni,n \u2212OFm,a\ni,n .\n(2)\nDue to the intraday pattern in limit order depth, we use the average size to scale OFIs\nat the corresponding levels (consistent with Ahn et al. [1], Harris and Panchapagesan [26]),\nand consider\nofim,h\ni,t\n= OFIm,h\ni,t\nQM,h\ni,t\n,\n(3)\nwhere QM,h\ni,t\n=\n1\nM\nPM\nm=1\n1\n2\u2206N(t)\nPN(t)\nn=N(t\u2212h)+1\nh\nqm,b\ni,n + qm,a\ni,n\ni\nis the average order book depth\nacross the first M levels and \u2206N(t) = N(t) \u2212N(t \u2212h) is the number of events during\n(t \u2212h, t]. In this paper, we consider the top M = 10 levels of LOB and denote the multi-\nlevel OFI vector as ofi\nofi\nofi(h)\ni,t =\n\u0010\nofi1,h\ni,t , \u00b7 \u00b7 \u00b7 , ofi10,h\ni,t\n\u0011T\n.\nIntegrated OFI.\nOur following analysis in Section 2.2 will show that there exist strong\ncorrelations between multi-level OFIs, and that the first principal component can explain over\n89% of the total variance among multi-level OFIs. In order to make use of the information\nembedded in multiple LOB levels and avoid overfitting, we propose an integrated version of\nOFIs via Principal Components Analysis (PCA) as shown in Eqn (4), which only preserves\nthe first principal component.5\nWe further normalize the first principal component by\n4In Cont et al. [17], OFI was mathematically defined as OFI1,h\ni,t = L1,b\ni,h \u2212C1,b\ni,h \u2212M 1,b\ni,h \u2212L1,a\ni,h +C1,a\ni,h \u2212M 1,a\ni,h ,\nwhere L1,b\ni,h denotes the total size of buy orders that arrived to the current best bid during the time interval\n(t \u2212h, t]; C1,b\ni,h denotes the total size of buy orders that canceled from the current best bid during the time\ninterval (t\u2212h, t]; M 1,b\ni,h denotes the total size of marketable buy orders that arrived to current best ask during\nthe time interval (t \u2212h, t]. The quantities L1,a\ni,h, C1,a\ni,h , M 1,a\ni,h for sell orders are defined analogously. However,\nin the empirical study of Cont et al. [17], the OFI was computed from fluctuations in best bid/ask prices\nand their sizes according to Eqn (1). The reason is that information about individual orders is not available\nin the data set. For better comparison, we employ the same formula, i.e. Eqn (1), to compute OFI.\n5We would like to thank an anonymous reviewer for suggesting an analysis of various aggregations of\nmulti-level OFIs, as detailed in Appendix A.\n6",
    "chunk_index": 6,
    "start_char": 13980,
    "end_char": 16631,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "of Cont et al. [17], the OFI was computed from fluctuations in best bid/ask prices\nand their sizes according to Eqn (1). The reason is that information about individual orders is not available\nin the data set. For better comparison, we employ the same formula, i.e. Eqn (1), to compute OFI.\n5We would like to thank an anonymous reviewer for suggesting an analysis of various aggregations of\nmulti-level OFIs, as detailed in Appendix A.\n6\n\ndividing by its l1 norm so that the weights of multi-level OFIs in constructing integrated\nOFIs sum to 1, leading to\nofiI,h\ni,t = wT\n1 ofi\nofi\nofi(h)\ni,t\n\u2225w1\u22251\n,\n(4)\nwhere w1 is the first principal vector computed from historical data. To the best of our\nknowledge, this is the first work to aggregate multi-level OFIs into a single variable.\nLogarithmic returns.\nOur dependent variable is the logarithmic asset return. Specifically,\nwe define the returns over the interval (t \u2212h, t] as follows:\nr(h)\ni,t = log\n\u0012 Pi,t\nPi,t\u2212h\n\u0013\n,\n(5)\nwhere Pi,t is the mid-price at time t, i.e. Pi,t =\nP 1,b\ni,t +P 1,a\ni,t\n2\n.\n2.2\nSummary statistics\nTable 1 presents the summary statistics of multi-level OFIs, integrated OFIs, and returns\nfor the top 100 components of S&P 500 index. These descriptive statistics (e.g. mean, std,\netc) are computed at the minute level and aggregated across trading days and stocks.\nFigure 1 reveals that even though the correlation structure of multi-level OFIs may vary\nacross stocks, they all show strong relationships (above 75%). It is worth pointing out that\nthe best-level OFI exhibits the smallest correlation with any of the remaining nine levels, a\npattern that persists across different stocks. Table 2 further reveals that the first principal\ncomponent explains more than 89% of the total variance.\nTable 1: Summary statistics of OFIs and returns.\nMean (bp)\nStd (bp)\nSkewness\nKurtosis\n10% (bp)\n25% (bp)\n50% (bp)\n75% (bp)\n90% (bp)\nofi1,(1m)\n-0.01\n6.26\n-0.04\n1.89\n-7.97\n-3.45\n0.03\n3.47\n7.90\nofi2,(1m)\n0.01\n6.86\n-0.04\n1.04\n-8.86\n-3.88\n0.02\n3.95\n8.85\nofi3,(1m)\n-0.01\n7.05\n-0.04\n0.71\n-9.26\n-4.08\n0.01\n4.11\n9.19\nofi4,(1m)\n-0.02\n7.22\n-0.05\n0.68\n-9.50\n-4.21\n0.01\n4.24\n9.40\nofi5,(1m)\n-0.03\n7.14\n-0.05\n0.79\n-9.38\n-4.14\n0.01\n4.15\n9.25\nofi6,(1m)\n-0.03\n6.87\n-0.04\n0.96\n-8.98\n-3.94\n0.01\n3.95\n8.85\nofi7,(1m)\n-0.03\n6.39\n-0.05\n1.29\n-8.31\n-3.59\n0.01\n3.59\n8.16\nofi8,(1m)\n-0.03\n6.03\n-0.05\n1.59\n-7.80\n-3.37\n0.01\n3.36\n7.66\nofi9,(1m)\n-0.05\n5.71\n-0.05\n1.96\n-7.38\n-3.18\n0.01\n3.14\n7.19\nofi10,(1m)\n-0.05\n5.38\n-0.05\n2.52\n-6.92\n-2.97\n0.01\n2.91\n6.74\nofiI,(1m)\n0.01\n6.53\n-0.05\n0.76\n-8.52\n-3.81\n0.05\n3.89\n8.47\nr(1m)\n0.02\n4.81\n-0.04\n1.85\n-6.22\n-2.71\n0.00\n2.79\n6.23\nNote: These statistics are computed at the minute level across each stock and the full sample\nperiod. 1bp = 0.0001 = 0.01%.\n7\n\n(a) Average\n(b) AAPL\n(c) JPM\n(d) JNJ\nFigure 1: Correlation matrix of multi-level OFIs.",
    "chunk_index": 7,
    "start_char": 16194,
    "end_char": 19005,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "2.52\n-6.92\n-2.97\n0.01\n2.91\n6.74\nofiI,(1m)\n0.01\n6.53\n-0.05\n0.76\n-8.52\n-3.81\n0.05\n3.89\n8.47\nr(1m)\n0.02\n4.81\n-0.04\n1.85\n-6.22\n-2.71\n0.00\n2.79\n6.23\nNote: These statistics are computed at the minute level across each stock and the full sample\nperiod. 1bp = 0.0001 = 0.01%.\n7\n\n(a) Average\n(b) AAPL\n(c) JPM\n(d) JNJ\nFigure 1: Correlation matrix of multi-level OFIs.\nNote: Plot (a) is averaged across each stock and each trading day, Plots (b)-(d): correlation\nmatrix of Apple (AAPL), JPMorgan Chase (JPM), and Johnson & Johnson (JNJ) averaged\nacross each trading day. The x-axis and y-axis represent different levels of OFIs.\nTable 2:\nAverage percentage and the standard deviation (in parentheses) of variance\nattributed to each principal component.\nPrincipal Component\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nExplained Variance Ratio\n89.06\n4.99\n2.28\n1.28\n0.80\n0.54\n0.39\n0.29\n0.21\n0.15\n(6.12)\n(3.52)\n(1.26)\n(0.74)\n(0.48)\n(0.34)\n(0.25)\n(0.19)\n(0.15)\n(0.11)\nNote:\nThe table reports the ratio (in percentage points) between the variance of each\nprincipal component and the total variance averaged across each stock and trading day.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.06\n0.07\n0.08\n0.09\n0.10\n0.11\n0.12\n(a) Average\n(b) Volume\n(c) Volatility\n(d) Spread\nFigure 2: First principal component of multi-level OFIs, in quantile buckets for various stock\ncharacteristics.\nNote: The x-axis indexes the top 10 levels of the OFIs. Volume: trading volume on the\nprevious trading day. Volatility: volatility of 1-minute returns during the previous trading\nday. Spread: average bid-ask spread during the previous trading day. [0%, 25%), respectively\n[75%, 100%], denote the subset of stocks with the lowest, respectively highest, 25% values\nfor a given stock characteristic.\nIn Figure 2, we show statistics pertaining to the weights attributed to the top 10 levels\nin the first principal component. Plot 2(a) shows the average weights, and the one standard\n8\n\ndeviation bars, across all stocks in the universe. Plot 2(a) reveals that the best-level OFI\nhas the smallest weight in the first principal component, but the highest standard deviation,\nhinting that it fluctuates significantly across stocks. Plots (b-d) show various patterns for\nthe first principal component of multi-level OFIs, for each quantile bucket of various stock\ncharacteristics, in particular, for volume, volatility and spread. For instance, in Figure 2(b),\nthe red curve shows the average weights in the first principal component for each of the 10\nlevels, where the average is taken over all the top 25% largest volume stocks. A striking\npattern that emerges from this figure is that for high-volume (red line in 2(b)), and low-\nvolatility stocks (blue line in 2(c)), OFIs deeper in the LOB receive more weights in the first\ncomponent. However, for low-volume (blue line in 2(b)), and large-spread stocks (red line in\n2(d)), the best-level OFIs account more than the deeper-level OFIs.\n3\nContemporaneous cross-impact\nIn this section, we study the existence of contemporaneous cross-impact by comparing it\nwith the price impact model studied in Cont et al.",
    "chunk_index": 8,
    "start_char": 18648,
    "end_char": 21719,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "emerges from this figure is that for high-volume (red line in 2(b)), and low-\nvolatility stocks (blue line in 2(c)), OFIs deeper in the LOB receive more weights in the first\ncomponent. However, for low-volume (blue line in 2(b)), and large-spread stocks (red line in\n2(d)), the best-level OFIs account more than the deeper-level OFIs.\n3\nContemporaneous cross-impact\nIn this section, we study the existence of contemporaneous cross-impact by comparing it\nwith the price impact model studied in Cont et al. [17].\n3.1\nModels\nPrice impact of best-level OFIs.\nWe first pay attention to the price impact of best-level\nOFI (ofi1,h\ni,t ) on contemporaneous returns (r(h)\ni,t ) that materialize over the same time bucket\nas the OFI, via the model\nPI[1] :\nr(h)\ni,t = \u03b1[1]\ni + \u03b2[1]\ni ofi1,h\ni,t + \u03f5[1]\ni,t.\n(6)\nHere, \u03b1[1]\ni\nand \u03b2[1]\ni\nare the intercept and slope coefficients, respectively. \u03f5[1]\ni,t is a noise term\nsummarizing the influences of other factors, such as the OFIs at even deeper levels, and\npotentially the trading behaviors of other stocks. For the sake of simplicity, we refer to the\nabove regression model as PI[1] and use ordinary least squares (OLS) to estimate it.\nPrice impact of integrated OFIs.\nThe second model specification takes into account\nthe impact of multi-level OFIs by leveraging the integrated OFIs, which we set up as follows\nand use OLS for estimation.\nPII :\nr(h)\ni,t = \u03b1I\ni + \u03b2I\ni ofiI,h\ni,t + \u03f5I\ni,t.\n(7)\nCross-impact of best-level OFIs.\nAssuming there are N stocks in the studied universe,\nwe incorporate the multi-asset best-level OFIs, ofi1,h\nj,t (j = 1, . . . , N), as candidate features to\nhelp fit the returns of the i-th stock r(h)\ni,t . For simplicity, we denote the impact from itself\n(stock i) as Self and that from other stocks as Cross, as shown below,\nCI[1] :\nr(h)\ni,t = \u03b1[1]\ni + \u03b2[1]\ni,i ofi1,h\ni,t\n|\n{z\n}\nSelf\n+\nX\nj\u0338=i\n\u03b2[1]\ni,jofi1,h\nj,t\n|\n{z\n}\nCross\n+\u03b7[1]\ni,t.\n(8)\nTherefore, \u03b2[1]\ni,j represents the influence of the j-th stock\u2019s best-level OFIs on the returns of\nstock i.\n9\n\nCross-impact of integrated OFIs.\nFinally, we incorporate the cross-asset integrated\nOFIs to explore the impact of multi-level OFIs from other assets, resulting in the following\nCII model,\nCII :\nr(h)\ni,t = \u03b1I\ni + \u03b2I\ni,iofiI,h\ni,t\n| {z }\nSelf\n+\nX\nj\u0338=i\n\u03b2I\ni,jofiI,h\nj,t\n| {z }\nCross\n+\u03b7I\ni,t.\n(9)\nSparsity of cross-impact.\nAs we are aware, OLS regression becomes ill-posed when\nthere are fewer observations than parameters. Recall that we are now considering N \u2248100\nindependent variables in Eqns (8) and (9).\nAssuming the time interval is one minute\nand we are interested in estimating the intraday cross-impact models, e.g.\nrelying on\nthe 30-min estimation window and 1-min returns (as in Cont et al. [17]), then it seems\ninappropriate to estimate CI[1] and CII for intraday scenarios using the OLS regression\nwith more variables than observations.",
    "chunk_index": 9,
    "start_char": 21215,
    "end_char": 24079,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "Sparsity of cross-impact.\nAs we are aware, OLS regression becomes ill-posed when\nthere are fewer observations than parameters. Recall that we are now considering N \u2248100\nindependent variables in Eqns (8) and (9).\nAssuming the time interval is one minute\nand we are interested in estimating the intraday cross-impact models, e.g.\nrelying on\nthe 30-min estimation window and 1-min returns (as in Cont et al. [17]), then it seems\ninappropriate to estimate CI[1] and CII for intraday scenarios using the OLS regression\nwith more variables than observations.\nMoreover, the multicollinearity issue of features\ncontradicts the necessary condition for a well-posed OLS. As displayed in Figure 3, a\nsignificant portion of the cross-asset correlations based on the best-level OFIs cannot be\nignored. For example, approximately 10% of correlations are larger than 0.30. Last, Capponi\nand Cont [8] found that a certain number of cross-impact coefficients from their OLS\nregressions are not statistically significant at the 1% significance level.\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nFigure 3: Distribution of correlations based on the best-level OFIs.\nNote: The orange vertical line represents the average correlation.\nWith all the above considerations in mind, we assume that there is a small number of\nassets having a significant impact on a specific stock i, as opposed to the entire universe, in\nCI[1] and CII. To this end, we apply the Least Absolute Shrinkage and Selection Operator\n(LASSO)6 to solve Eqns (8) and (9). The sparsity of cross-impact terms also facilitates the\n6LASSO is a regression method that performs both variable selection and regularization, in order to\nenhance the prediction accuracy and interpretability of regression models (see more in Hastie et al. [29]).\nIt can be formulated as a linear regression model and the objective function consists of two parts, i.e. the\nsum of squared residuals, and the l1 constraint on the regression coefficients. In this work, we employ the\ncross-validation to choose the l1 penalty weight for each regression.\n10\n\nexplanation of coefficients. Note that even though the sparsity of the cross-impact terms is\nnot theoretically guaranteed, our empirical evidence confirms this modeling assumption.\n3.2\nEmpirical results\nFor a more representative and fair comparison with previous studies, we apply a similar\nprocedure described in Cont et al. [17] to our experiments.\nWe exclude the first and\nlast 30 minutes of the trading day due to the increased volatility near the opening and\nclosing sessions, in line with Hasbrouck and Saar [27], Chordia et al. [15], Chordia and\nSubrahmanyam [14], Cont et al. [17], Capponi and Cont [8]. In particular, we use each non-\noverlapping 30-minute estimation window during the intraday time interval 10:00 am - 3:30\npm to estimate the regressions, namely Eqns (6), (7), (8), and (9). Within each window,\nreturns and OFIs are computed for every minute.\n3.2.1\nIn-sample performance\nWe first measure the model performance via in-sample adjusted-R2, denoted as the in-\nsample R2 or IS R2.",
    "chunk_index": 10,
    "start_char": 23527,
    "end_char": 26621,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "[14], Cont et al. [17], Capponi and Cont [8]. In particular, we use each non-\noverlapping 30-minute estimation window during the intraday time interval 10:00 am - 3:30\npm to estimate the regressions, namely Eqns (6), (7), (8), and (9). Within each window,\nreturns and OFIs are computed for every minute.\n3.2.1\nIn-sample performance\nWe first measure the model performance via in-sample adjusted-R2, denoted as the in-\nsample R2 or IS R2. From Table 3, we first observe that PI[1] can explain 71.16% of the\nin-sample variation of a stock\u2019s contemporaneous returns, consistent with the findings of\nCont et al. [17]. Meanwhile, PII displays higher and more consistent explanation power,\nwith an average adjusted R2 value of 87.14% and a standard deviation of 9.16%, indicating\nthe effectiveness of our integrated OFIs.7\nTable 3 also shows that the in-sample R2 values increase as cross-asset OFIs are included\nas additional features, which is not surprising given that PI[1] (respectively, PII) is a nested\nmodel of CI[1] (respectively, CII). However, the increments of the in-sample R2 are smaller\nwhen using integrated OFIs (87.85%-87.14%=0.71%), compared to the counterpart using\nbest-level OFIs (73.87%-71.16%=2.71%). This indicates that cross-asset multi-level OFIs\nmay not provide additional information on the variance in returns compared to the price\nimpact model with integrated OFIs.\nTable 3: In-sample performance for contemporaneous returns.\nBest-level OFIs\nIntegrated OFIs\nPI[1]\nCI[1]\nPII\nCII\nIS R2\n71.16\n73.87\n87.14\n87.85\n(13.80)\n(12.23)\n(9.16)\n(8.58)\nNote: The table reports the mean values and standard deviations (in parentheses) of in-\nsample R2 (in percentage points) of various models when modeling contemporaneous returns.\nThe models include PI[1] (Eqn (6)), CI[1] (Eqn (8)), PII (Eqn (7)), and CII (Eqn (9)). These\nstatistics are averaged across each stock and each regression window.\n7We also investigate price impact with multi-level OFIs in Appendix B. The results demonstrate that the\nprice impact model using integrated OFIs outperforms those using multi-level OFIs in out-of-sample tests.\n11\n\nNext, we take a closer look at the cross-impact coefficients based on either the best-\nlevel or integrated OFIs, i.e. \u03b2[1]\ni,j and \u03b2I\ni,j (i, j = 1, . . . , N). Table 4 reveals the frequency\nof self-impact and cross-impact variables selected by LASSO, i.e. the frequency of \u03b2[1]\ni,j \u0338= 0\n(respectively, \u03b2I\ni,j \u0338= 0). We observe that self-impact variables are consistently chosen in both\nCI[1] and CII, as found in Cont et al. [17]. However, another interesting observation is that\nthe frequency of a cross-asset integrated OFI variable selected by CII is around 1/2 of its\ncounterpart in CI[1]. When we turn to the size of the average regression coefficients as shown\nin Table 4, we obtain reasonably consistent results. The self-impact is much higher than the\ncross-impact in both the CI[1] and CII models, while the cross-impact coefficients in CII\nare about 1/3 in scale of their counterparts in CI[1]. This difference in scale may suggest\nthat the cross-impact terms are less important in the CII model, however, it is worth noting\nthat even small cross-term coefficients can have a non-negligeable effect when aggregated at\nportfolio level.",
    "chunk_index": 11,
    "start_char": 26185,
    "end_char": 29447,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "around 1/2 of its\ncounterpart in CI[1]. When we turn to the size of the average regression coefficients as shown\nin Table 4, we obtain reasonably consistent results. The self-impact is much higher than the\ncross-impact in both the CI[1] and CII models, while the cross-impact coefficients in CII\nare about 1/3 in scale of their counterparts in CI[1]. This difference in scale may suggest\nthat the cross-impact terms are less important in the CII model, however, it is worth noting\nthat even small cross-term coefficients can have a non-negligeable effect when aggregated at\nportfolio level.\nTable 4: Summary statistics of coefficients in the cross-impact models CI[1] and CII.\nFrequency (%)\nMagnitude\nCI[1]\nCII\nCI[1]\nCII\nSelf\n99.85\n99.96\n1.02\n1.24\n(0.34)\n(0.18)\n(0.31)\n(0.34)\nCross\n17.34\n8.29\n4.5e\u22123\n1.6e\u22123\n(2.78)\n(2.56)\n(1.3e\u22123)\n(0.7e\u22123)\nNote: The table is calculated over each stock and each regression window. The first two\ncolumns describe the frequency of Self and Cross variables chosen by the corresponding\nmodel with a standard deviation (in parentheses); The last two columns describe the\nmagnitude of Self and Cross coefficients in the corresponding model with a standard\ndeviation (in parentheses).\nIn addition, cross-impact being large/small is a statement about a matrix, more related\nwith its singular values and relative magnitudes, rather than the individual value of the\ncoefficients. Figure 4 shows a comparison of the top 20 singular values of the coefficient\nmatrices given by the best-level and integrated OFIs.8 The relatively large singular values\nof the best-level OFI matrix are a consequence of the higher edge density, and thus average\ndegree, of the network. Note that both networks exhibit a large top singular value of the\nadjacency matrix (akin to the usual market mode in Laloux et al. [38]), and the integrated\nOFI network has a faster decay of the spectrum, thus revealing its low-rank structure.\n8Here we only use the off-diagonal elements, i.e.\n\u0002\n\u03b2[1]\ni,j\n\u0003\ni\u0338=j and\n\u0002\n\u03b2I\ni,j\n\u0003\ni\u0338=j.\n12\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n0\n1\n2\n3\n4\nBest-level OFIs\nIntegrated OFIs\nFigure 4: Barplot of singular values for the coefficient matrix in contemporaneous cross-\nimpact models.\nNote: We perform Singular Value Decomposition (SVD) on the coefficient matrix to obtain\nthe singular values. Singular values are in descending order and the coefficients are averaged\nover each regression window between 2017\u20132019. The x-axis represents the singular value\nrank, and the y-axis represents the singular values.\nWe visualize a network for each coefficient matrix, which only preserves the edges larger\nthan a given threshold (following Kenett et al. [34], Curme et al. [20]), as shown in Figure\n5. We color stocks according to the GICS sector division, and sort them by their market\ncapitalization within each sector.9\nAs one can see from Figure 5(a), the cross-impact\ncoefficient matrix\nh\n\u03b2[1]\ni,j\ni\nj\u0338=i displays a sectorial structure, in accordance with previous studies\n(e.g. Benzaquen et al. [5]).",
    "chunk_index": 12,
    "start_char": 28857,
    "end_char": 31893,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "visualize a network for each coefficient matrix, which only preserves the edges larger\nthan a given threshold (following Kenett et al. [34], Curme et al. [20]), as shown in Figure\n5. We color stocks according to the GICS sector division, and sort them by their market\ncapitalization within each sector.9\nAs one can see from Figure 5(a), the cross-impact\ncoefficient matrix\nh\n\u03b2[1]\ni,j\ni\nj\u0338=i displays a sectorial structure, in accordance with previous studies\n(e.g. Benzaquen et al. [5]). This behavior could be fueled by index arbitrage strategies,\nwhere traders may, for example, trade an entire basket of stocks coming from the same\nsector against an index.\nFigure 5(b) presents the network of cross-impact coefficients based on integrated OFIs,\ni.e.\n\u0002\n\u03b2I\ni,j\n\u0003\nj\u0338=i. Compared with Figure 5(a), the connections in Figure 5(b) are much weaker,\nimplying that the cross-impact from stocks can be potentially explained by a stock\u2019s own\nmulti-level OFIs, to a large extent. Note that there is only one connection from GOOGL to\nGOOG, as pointed out at the top of Figure 5(b). This stems from the fact that both stock\nticker symbols pertain to Alphabet (Google). Our study also reveals that OFIs of GOOGL\nhave more influence on the returns of GOOG, not the other way around. The main reason\nmight be that GOOGL shares have voting rights, while GOOG shares do not.\nIn Figures 5(c) and 5(d), we set lower threshold values (75-th, respectively, 25-th percentile\nof coefficients) in order to promote more edges in the networks based on integrated OFIs.\nInterestingly, we observe only four connections in Figure 5(c). Except from bidirectional\nlinks between GOOGL and GOOG, there exists a one-way link from Cigna (CI) to Anthem\n(ANTM), and another one-way link from Duke Energy (DUK) to NextEra Energy (NEE).\nAnthem announced to acquire Cigna in 2015.\nAfter a prolonged breakup, this merger\nfinally failed in 2020. Therefore, it is unsurprising that the OFIs of Cigna can affect the\nprice movements of Anthem. Conversely, Anthem\u2019s OFIs also have an impact on the price\nmovements of Cigna, but to a lesser extent.\nFurther research should be undertaken to\n9The Global Industry Classification Standard (GICS) is an industry taxonomy developed in 1999 by\nMSCI and Standard & Poor\u2019s (S&P) for use by the global financial community.\n13\n\ninvestigate this phenomenon. In terms of the second pair, Duke Energy rebuffed NextEra\u2019s\nacquisition interest in 2020. Note that 2020 is not in our sample period. This finding hints\nthat certain market participants may have noticed the special relationship between Duke\nEnergy and NextEra Energy before this mega-merger was proposed.\n14\n\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB\nAXP",
    "chunk_index": 13,
    "start_char": 31406,
    "end_char": 34344,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "special relationship between Duke\nEnergy and NextEra Energy before this mega-merger was proposed.\n14\n\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(a) Threshold = 95-th percentile, based on best-\nlevel OFIs\n(b) Threshold\n=\n95-th\npercentile,\nbased\non\nintegrated OFIs\n(c) Threshold\n=\n75-th\npercentile,\nbased\non\nintegrated OFIs\nAAP\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(d) Threshold\n=\n25-th\npercentile,\nbased\non\nintegrated OFIs\nFigure 5: Illustrations of the coefficient networks constructed from contemporaneous cross-\nimpact models.\nNote: To render the networks more interpretable and for ease of visualization, we only\nplot the top 5% largest (a-b), or top 25% largest (c), or top 75% largest (d), in magnitude\ncoefficients. The coefficients are averaged over each regression window between 2017\u20132019.\nNodes are colored by the GICS structure and sorted by market capitalization. Green links\nrepresent positive values while black links represent negative values. The width of edges is\nproportional to the absolute values of their respective coefficients.\n15\n\n3.2.2\nOut-of-sample performance\nAlthough the in-sample estimation yields interesting findings, practitioners are eventually\nconcerned about the out-of-sample estimation. Therefore, we propose to perform the following\nout-of-sample tests. We use the above fitted models to estimate returns on the following 30-\nminute data and compute the corresponding R2, denoted as out-of-sample R2 or OS R2.10\nTable 5 reports the average values and their standard deviations of out-of-sample R2 of\nPI[1], CI[1], PII, and CII. We first focus on the models using best-level OFIs. It appears\nCI[1] has a slight advantage compared with PI[1] for out-of-sample tests with an improvement\nof 1.39% (=66.03%-64.64%). However, when involving multi-level or integrated OFIs, the\nperformance of CII is slightly worse than PII, indicating that the cross-impact model with\nintegrated OFIs cannot provide extra explanatory power to the price impact model with\nintegrated OFIs. Overall, we observe that the models using integrated OFIs unveil significant\nand consistent improvements over those using only best-level OFIs.\nTable 5: Out-of-sample performance for contemporaneous returns.",
    "chunk_index": 14,
    "start_char": 33963,
    "end_char": 36804,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "However, when involving multi-level or integrated OFIs, the\nperformance of CII is slightly worse than PII, indicating that the cross-impact model with\nintegrated OFIs cannot provide extra explanatory power to the price impact model with\nintegrated OFIs. Overall, we observe that the models using integrated OFIs unveil significant\nand consistent improvements over those using only best-level OFIs.\nTable 5: Out-of-sample performance for contemporaneous returns.\nBest-level OFIs\nIntegrated OFIs\nPI[1]\nCI[1]\nPII\nCII\nOS R2\n64.64\n66.03\n83.83\n83.62\n(21.82)\n(19.51)\n(16.90)\n(14.53)\nNote: The table reports the mean values and standard deviations (in parentheses) of out-of-\nsample R2 (in percentage points) of various models when modeling contemporaneous returns.\nThe models include PI[1] (Eqn (6)), CI[1] (Eqn (8)), PII (Eqn (7)), and CII (Eqn (9)). These\nstatistics are averaged across each stock and each regression window.\nIn general, we observe strong evidence implying CI[1] provides a better out-of-sample\nestimate than PI[1], while for PII and CII, the evidence is opposite. However, it is important\nto note that these conclusions are based on a point estimate and do not necessarily indicate\nstatistical significance. Therefore, we perform the following hypothesis test for each stock\non the out-of-sample data to assess statistical significance,\nH0 : E\nh\nR2\nOS\n\u0010\nCI[1]\u0011\n\u2212R2\nOS\n\u0010\nPI[1]\u0011i\n\u22640 vs. H1 : E\nh\nR2\nOS\n\u0010\nCI[1]\u0011\n\u2212R2\nOS\n\u0010\nPI[1]\u0011i\n> 0.\nWe employ the approach from Giacomini and White [24] and Chinco et al. [12] to assess\nstatistical significance through a Wald-type test (see Ward and Ahlquist [52]). Theorem\n1 in Giacomini and White [24] implies that we can use a standard t-test to evaluate the\nstatistical significance of changes in R2. A p-value less than a given significance level \u03b1\nrejects the null hypothesis in favor of the alternative at the 1 \u2212\u03b1 confidence level, implying\n10Previous studies either investigated the in-sample R2 (including Cont et al. [17], Capponi and Cont\n[8]), or adopted a cross-validation method (e.g. Xu et al. [53]). However, these works failed to consider the\ngeneralization error of their models or damaged the chronological order of the time-series data. In contrast,\nwe obey the temporal ordering in our study. These matters are vital to practitioners, as only the historical\ndata are accessible for the model fit in practice.\n16\n\nCI[1] has significantly better estimation than PI[1]. We also implement this test for the\ncomparison between PII and CII.\nGE\nT\nMO\nCMCSA\nMDLZ\nKO\nPFE\nSBUX\nCVS\nSO\nPG\nCSCO\nXOM\nWMT\nBSX\nTGT\nBMY\nQCOM\nGILD\nTJX\nWFC\nMRK\nORCL\nAMT\nNKE\nC\nIBM\nD\nPM\nABBV\nUSB\nJNJ\nUPS\nMDT\nAAPL\nMSFT\nMS\nMCD\nABT\nFISV\nCAT\nPEP\nLLY\nFB\nCVX\nDUK\nLOW\nAXP\nCME\nUTX\nJPM\nDHR\nPYPL\nUNP\nANTM\nCB\nCI\nAMGN\nMMM\nFIS\nHD\nNVDA\nCRM\nRTN\nACN\nBA\nTXN\nADP\nSYK\nHON\nCHTR\nGS\nV\nNFLX\nAVGO\nCOST\nLMT\nNEE\nPNC\nSPGI\nBDX\nTMO\nBRK.B\nINTU\nADBE\nISRG\nAMZN\nGOOGL\nGOOG\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(a) R2\nOS\n\u0010\nCI[1]\u0011",
    "chunk_index": 15,
    "start_char": 36343,
    "end_char": 39262,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "MRK\nORCL\nAMT\nNKE\nC\nIBM\nD\nPM\nABBV\nUSB\nJNJ\nUPS\nMDT\nAAPL\nMSFT\nMS\nMCD\nABT\nFISV\nCAT\nPEP\nLLY\nFB\nCVX\nDUK\nLOW\nAXP\nCME\nUTX\nJPM\nDHR\nPYPL\nUNP\nANTM\nCB\nCI\nAMGN\nMMM\nFIS\nHD\nNVDA\nCRM\nRTN\nACN\nBA\nTXN\nADP\nSYK\nHON\nCHTR\nGS\nV\nNFLX\nAVGO\nCOST\nLMT\nNEE\nPNC\nSPGI\nBDX\nTMO\nBRK.B\nINTU\nADBE\nISRG\nAMZN\nGOOGL\nGOOG\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(a) R2\nOS\n\u0010\nCI[1]\u0011\n\u2212R2\nOS\n\u0010\nPI[1]\u0011\nPYPL\nXOM\nC\nGILD\nJPM\nWMT\nFB\nNKE\nMRK\nPG\nMSFT\nSO\nAAPL\nMS\nSBUX\nBSX\nMO\nTGT\nUSB\nCVX\nT\nCMCSA\nD\nDUK\nBMY\nPEP\nV\nPFE\nTXN\nWFC\nGE\nMDLZ\nCSCO\nMDT\nTJX\nORCL\nABT\nKO\nAXP\nCVS\nJNJ\nQCOM\nMCD\nCAT\nIBM\nPM\nNVDA\nUTX\nLOW\nHD\nLLY\nUPS\nDHR\nCRM\nFISV\nADP\nHON\nABBV\nUNP\nNEE\nAMGN\nNFLX\nGS\nCOST\nCME\nMMM\nPNC\nBA\nADBE\nFIS\nBRK.B\nAMT\nACN\nAVGO\nRTN\nCB\nAMZN\nCHTR\nCI\nANTM\nSYK\nLMT\nTMO\nINTU\nBDX\nSPGI\nGOOGL\nISRG\nGOOG\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n(b) R2\nOS\n\u0010\nCII\u0011\n\u2212R2\nOS\n\u0010\nPII\u0011\nFigure 6: Mean differences of out-of-sample R2 between CI and PI models.\nNote: A positive (negative) number indicates superiority for the CI (PI) model. The y-axis\nrepresents the average difference of OS R2 between CI and PI, while the x-axis lists the stock\nsymbols. Stars indicate the p-values, with orange, green, and blue representing significance\nat the 1%, 5%, and 10% levels, respectively.\nFigure 6 illustrates the main results from the above hypothesis tests. When using only\nthe best-level OFIs, the cross-impact model is superior to the price impact model for 91.0%\n(94.4%) of stocks, at the 1% (5%) confidence level. However, when examining the models\nusing integrated OFIs, we reject the null hypothesis (i.e., in favor of the cross-impact model)\n17\n\nonly for 28.1% (33.7%) of stocks at the 1% (5%) confidence level. As expected, cross-impact\nterms can significantly improve the explanatory power of the price impact model for GOOG\nand GOOGL.\nDynamics of limit order book may depend on the tick-to-price ratio, or alternatively, the\nfraction of time that the bid-ask spread is equal to one tick for a given stock (Curato and\nLillo [19]). We examine whether this dependence also extends to cross-asset OFIs.11 Our\nfindings, presented in Table 6, suggest that cross-asset OFIs can better explain the price\ndynamics of stocks with a larger tick-to-price ratio.\nTable 6: Out-of-sample R2 of various contemporaneous models sorted by tick-to-price ratio.\n[0%, 25%)\n[25%, 50%)\n[50%, 75%)\n[75%, 100%]\nPI[1]\n44.38\n62.51\n77.55\n70.70\nCI[1]\n53.01\n66.32\n72.50\n78.34\nPII\n68.14\n84.58\n88.14\n89.86\nCII\n72.01\n84.80\n88.51\n91.01\nNote: [0%, 25%), respectively [75%, 100%], denote the subset of stocks with the lowest,\nrespectively highest, 25% values according to the tick-to-price ratio.\n3.3\nDiscussion about contemporaneous cross-impact\n3.3.1\nImpact on stocks\nIn summary, our previous results mainly show that when considering only the best-level\nOFI of a single stock, the addition of the best-level OFI from other stocks slightly increases\nthe explanatory power.",
    "chunk_index": 16,
    "start_char": 38928,
    "end_char": 41762,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "[0%, 25%)\n[25%, 50%)\n[50%, 75%)\n[75%, 100%]\nPI[1]\n44.38\n62.51\n77.55\n70.70\nCI[1]\n53.01\n66.32\n72.50\n78.34\nPII\n68.14\n84.58\n88.14\n89.86\nCII\n72.01\n84.80\n88.51\n91.01\nNote: [0%, 25%), respectively [75%, 100%], denote the subset of stocks with the lowest,\nrespectively highest, 25% values according to the tick-to-price ratio.\n3.3\nDiscussion about contemporaneous cross-impact\n3.3.1\nImpact on stocks\nIn summary, our previous results mainly show that when considering only the best-level\nOFI of a single stock, the addition of the best-level OFI from other stocks slightly increases\nthe explanatory power. On the other hand, when the information from multiple levels is\nintegrated into the OFI, the improvement is negligible. In the meantime, it is unsurprising\nthat taking into account more levels in the LOB (PII) could better explain price changes,\ncompared to only considering best-level orders (PI[1]).\nAfter observing these results, several natural questions may arise: How can the above\nfacts be reconciled?12 How do the cross-asset best-level OFIs interact with the multi-level\nOFIs, when modeling contemporaneous returns?\nTo address these questions, we consider the following scenario, also depicted in Figure\n7. For simplicity, we denote the order from trading strategy A on stock i (respectively, j)\nas Ai (respectively, Aj). Analogously, we define orders from strategy B and S. Let us next\nconsider the orders of stock i. There are three orders from different portfolios, given by Ai,\nBi and Si. Ai is at the third bid level of stock i and linked to an order at the best ask level\n11We would like to thank an anonymous reviewer for suggesting this analysis.\n12One possible explanation for those facts is that the duration of the cross-impact terms might be shorter\nthan the current time interval (30 minutes) used in our experiments, rendering the cross-impact terms\nvanish in out-of-sample tests. To verify this assertion, we implement additional experiments where models\nare updated more frequently. The results (deferred to Appendix D) reveal that even under higher-frequency\nupdates (1-min) of the models, there is no benefit from introducing cross-impact terms to the price impact\nmodel with integrated OFIs.\n18\n\nof stock j, i.e. Aj. Also, Bi is at the best ask level of stock i and linked to an order at the\nbest bid level of stock j, i.e. Bj. Finally, Si is an individual bid order at the best level of\nstock i.\nWe observe that the best-level limit orders from stock j may be linked to price movements\nof stock i through paths Bj \u2192Bi \u2192ofi1\ni \u2192ri and Aj \u2192Ai \u2192ofi3\ni \u2192ri. Thus the\nprice impact model for stock i which only utilizes its own best-level orders will ignore the\ninformation of Ai, while the cross-impact model can partially collect it along the path Aj \u2192\nAi. This might illustrate why the best-level OFIs of multiple assets can provide slightly\nadditional explanatory power to the price impact model using only the best-level OFIs.\nMulti-Asset \nTrading B\nMulti-Asset \nTrading A\nStock \nStock\nSingle-Asset \nTrading S\nPrices\nPrices\nDepth\nDepth\nFigure 7: Illustration of the cross-impact model.",
    "chunk_index": 17,
    "start_char": 41166,
    "end_char": 44276,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "and Aj \u2192Ai \u2192ofi3\ni \u2192ri. Thus the\nprice impact model for stock i which only utilizes its own best-level orders will ignore the\ninformation of Ai, while the cross-impact model can partially collect it along the path Aj \u2192\nAi. This might illustrate why the best-level OFIs of multiple assets can provide slightly\nadditional explanatory power to the price impact model using only the best-level OFIs.\nMulti-Asset \nTrading B\nMulti-Asset \nTrading A\nStock \nStock\nSingle-Asset \nTrading S\nPrices\nPrices\nDepth\nDepth\nFigure 7: Illustration of the cross-impact model.\nNote: The orders at different levels of each stock may come from single-asset and multi-\nasset trading strategies. The returns of stock i are potentially influenced by orders of stock j\nthrough the connections Bj \u2192Bi \u2192ofi1\ni \u2192ri and Aj \u2192Ai \u2192ofi3\ni \u2192ri. Information along\nthe path Aj \u2192Ai \u2192ofi3\ni \u2192ri can be collected by the price impact model with integrated\nOFIs but not by the price impact model with only best-level OFIs.\nNonetheless, if we can integrate multi-level OFIs in an efficient way (in our example,\naggregate order imbalances caused by orders Ai, Bi and Si), then there is no need to consider\nOFIs from other stocks for modeling price dynamics. For example, information hidden in the\npath Aj \u2192Ai \u2192ofi3\ni \u2192ri can be leveraged as long as Ai is well absorbed into new integrated\nOFIs. In this sense, for stock i, cross-asset best-level OFIs (including Aj) are surrogates of its\nown OFIs at different levels (here Ai), to a certain extent. The likelihood of this relationship\nis attributed to massive portfolio trades that submit or cancel limit orders across a variety of\nassets at different levels.13 We put forward this mechanism which potentially explains why\n13Cao et al. [7], Hautsch and Huang [30], Sirignano [45], Chakrabarty et al. [11] showed that the depth of\nsome deeper levels (such 2-3) is higher than the best level depth.\n19\n\nthe cross-impact model with integrated OFIs cannot provide additional explanatory power\ncompared to the price impact model with integrated OFIs.14\n3.3.2\nImpact on portfolios\nA related question is about the aggregation of cross-impact at portfolio level. Let us consider\nthe OFI of portfolio p as ofi1,h\np,t := PN\ni=1 wiofi1,h\ni,t , where wi is the weight of asset i in a portfolio.\nThen the price impact for portfolio p is\nr(h)\np,t = a[1]\np + \u03b2[1]\np ofi1,h\np,t + e[1]\np,t\n(10)\n= a[1]\np + \u03b2[1]\np\nN\nX\ni=1\nwiofi1,h\ni,t + e[1]\np,t,\nwhere a[1]\np is the intercept and e[1]\np,t is the noise term.\nOn the other hand, the cross-impact for portfolio p is\nr(h)\np,t =\nN\nX\ni=1\nwir(h)\np,t\n(11)\n=\nN\nX\ni=1\nwi\n\u0010\n\u03b1[1]\ni + \u03b2[1]\ni ofi1,h\ni,t + \u03f5[1]\ni,t\n\u0011\n= \u03b1[1]\np +\nN\nX\ni=1\n\u03b2[1]\ni wiofi1,h\ni,t + \u03f5[1]\np,t,\nwhere \u03b1[1]\np = PN\ni=1 wi\u03b1[1]\ni\nis the intercept.",
    "chunk_index": 18,
    "start_char": 43722,
    "end_char": 46461,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "e[1]\np,t,\nwhere a[1]\np is the intercept and e[1]\np,t is the noise term.\nOn the other hand, the cross-impact for portfolio p is\nr(h)\np,t =\nN\nX\ni=1\nwir(h)\np,t\n(11)\n=\nN\nX\ni=1\nwi\n\u0010\n\u03b1[1]\ni + \u03b2[1]\ni ofi1,h\ni,t + \u03f5[1]\ni,t\n\u0011\n= \u03b1[1]\np +\nN\nX\ni=1\n\u03b2[1]\ni wiofi1,h\ni,t + \u03f5[1]\np,t,\nwhere \u03b1[1]\np = PN\ni=1 wi\u03b1[1]\ni\nis the intercept.\nComparing (10) and (11) shows that cross-impact at portfolio level depends on the angles\nof \u20d7\u03b2 = (\u03b2[1]\n1 , . . . , \u03b2[1]\nN ) and \u20d7w = (w1, . . . , wN).\nOn one hand, if individual assets exhibit\na universal pattern of price dynamics, i.e. \u03b2[1]\ni\n\u2248\u03b2[1]\nj , \u2200i \u0338= j, we may conclude that the\nportfolio return is driven by its own order flows, to a large extent. On the other hand, if the\nportfolio places most of the weight on a specific stock or a set of stocks with similar patterns\nof price dynamics, then the portfolio returns are also driven by its own order flows, rather\nthan by cross-impact. In other scenarios, it is necessary to consider the cross-impact. One\ncan perform an analogous analysis for models with integrated OFIs.\nIn fact, the mechanism depicted in Figure 7 aligns with this analysis. For example, orders\nplaced on stock i represented by Ai and Si have the potential to affect the price movements\nof that stock, which subsequently affect the returns of portfolio B, even if Ai and Si have no\ndirect association with B. Additionally, Aj may have a different influence on the performance\nof B because of the (potentially) different price dynamics of stock j. Therefore, it may be\nnecessary to take into account cross-asset OFIs when developing models for portfolio returns.\n14It would be a very interesting research direction to derive testable predictions for this mechanism in\nfuture work. However, so far it hinges on the availability of client-ID based LOB data, i.e. knowing that\nthe same market participant is behind the orders for two related instruments, released at approximately the\nsame time, such as (Ai, Aj), (Bi, Bj).\n20\n\nTo examine the potential of cross-asset OFIs in explaining portfolio returns, we choose\ntwo widely-used portfolio construction methods: the equal-weighted portfolio (EW), and\nthe eigenportfolio15 using the 1st principal component (Eig1).\nTable 7 summarizes the\nout-of-sample R2 of various models on these two portfolios. As Table 7 shows, there is a\nsignificant difference between PI[1] and CI[1], PII and CII, indicating that cross-impact is a\ncrucial factor when modeling portfolio returns. Again, the models using the integrated OFIs\noutperform their counterparts using the best-level OFIs.\nTable 7: Out-of-sample performance on portfolio returns.\nBest-level OFIs\nIntegrated OFIs\nPI[1]\nCI[1]\nPII\nCII\nEW\n79.29\n81.03\n85.26\n87.97\n(6.24)\n(6.16)\n(3.54)\n(2.78)\nEig1\n80.73\n81.95\n84.69\n87.70\n(6.75)\n(6.36)\n(3.70)\n(2.84)\nNote: EW: equal-weighted portfolio. Eig1: eigenportfolio using the 1st principal component\nof multi-asset returns.",
    "chunk_index": 19,
    "start_char": 46145,
    "end_char": 49044,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "difference between PI[1] and CI[1], PII and CII, indicating that cross-impact is a\ncrucial factor when modeling portfolio returns. Again, the models using the integrated OFIs\noutperform their counterparts using the best-level OFIs.\nTable 7: Out-of-sample performance on portfolio returns.\nBest-level OFIs\nIntegrated OFIs\nPI[1]\nCI[1]\nPII\nCII\nEW\n79.29\n81.03\n85.26\n87.97\n(6.24)\n(6.16)\n(3.54)\n(2.78)\nEig1\n80.73\n81.95\n84.69\n87.70\n(6.75)\n(6.36)\n(3.70)\n(2.84)\nNote: EW: equal-weighted portfolio. Eig1: eigenportfolio using the 1st principal component\nof multi-asset returns.\n4\nForecasting future returns\nIn the previous section, the definitions of price impact and cross-impact are based on\ncontemporaneous OFIs and returns, meaning that both quantities pertain to the same bucket\nof time. In this section, we extend the above studies to future returns, and probe into the\nforward-looking price impact and cross-impact models.\n4.1\nPredictive models\nWe first propose the following forward-looking price impact and cross-impact models, denoted\nas FPI[1], FPII, FCI[1], and FCII, respectively. FPI[1] (FPII) uses the lagged best-level\n(integrated) OFIs of stock i to predict its own future return r(f)\ni,t+f during (t, t + f], while\nFCI[1] (FCII) involves the lagged multi-asset best-level (integrated) OFIs. We employ OLS\n15See Avellaneda and Lee [3].\n21\n\nto fit the forward-looking price impact models and LASSO to fit the cross-impact models.\nFPI[1] :\nr(f)\ni,t+f = \u03b1[1]\ni +\nX\nk\u2208L\n\u03b2[1],k\ni\nofi1,(kh)\ni,t\n+ \u03f5[1]\ni,t+f,\n(12)\nFCI[1] :\nr(f)\ni,t+f = \u03b1[1]\ni +\nN\nX\nj=1\nX\nk\u2208L\n\u03b2[1],k\ni,j ofi1,(kh)\ni,t\n+ \u03b7[1]\ni,t+f,\n(13)\nFPII :\nr(f)\ni,t+f = \u03b1I\ni +\nX\nk\u2208L\n\u03b2I,k\ni ofiI,(kh)\ni,t\n+ \u03f5I\ni,t+f,\n(14)\nFCII :\nr(f)\ni,t+f = \u03b1I\ni +\nN\nX\nj=1\nX\nk\u2208L\n\u03b2I,k\ni,j ofiI,(kh)\ni,t\n+ \u03b7I\ni,t+f,\n(15)\nwhere f is the forecasting horizon of future returns and L = {1, 2, 3, 5, 10, 20, 30} represents\nthe set of lags.\nFurthermore, we compare OFI-based models with return-based models studied in previous\nworks, where lagged returns are involved as predictors. AR (Eqn (16)) is an autoregressive\n(AR) model using various returns over different time horizons, inspired by Corsi [18], Ait-\nSahalia et al. [2]. CAR (Chinco et al. [12]) uses the entire cross-section lagged returns as\ncandidate predictors, as detailed in Eqn (17). We employ OLS to fit the ARs and LASSO\nto fit the CARs.\nAR :\nr(f)\ni,t+f = \u03b1i +\nX\nk\u2208L\n\u03b2r,k\ni r(kh)\ni,t\n+ \u03f5i,t+f\n(16)\nCAR :\nr(f)\ni,t+f = \u03b1i +\nN\nX\nj=1\nX\nk\u2208L\n\u03b2r,k\ni,j r(kh)\ni,t\n+ \u03b7i,t+f\n(17)\n4.2\nEmpirical results\nIn this experiment, observations associated with returns and OFIs are computed minutely,\ni.e.\nh = 1 minute.16\nFollowing Chinco et al. [12], we use data from the previous 30\nminutes to estimate the model parameters and apply the fitted model to forecast future\nf-minute returns. We then move one minute forward and repeat this procedure to compute\nthe rolling f-minute-ahead return forecasts.\nFor all models, we initially focus on the 1-\nminute forecasting horizon.",
    "chunk_index": 20,
    "start_char": 48477,
    "end_char": 51429,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "r(kh)\ni,t\n+ \u03b7i,t+f\n(17)\n4.2\nEmpirical results\nIn this experiment, observations associated with returns and OFIs are computed minutely,\ni.e.\nh = 1 minute.16\nFollowing Chinco et al. [12], we use data from the previous 30\nminutes to estimate the model parameters and apply the fitted model to forecast future\nf-minute returns. We then move one minute forward and repeat this procedure to compute\nthe rolling f-minute-ahead return forecasts.\nFor all models, we initially focus on the 1-\nminute forecasting horizon. In Section 4.3, we consider return forecasts over longer horizons,\nincluding f \u2208{2, 3, 5, 10, 20, 30} minutes, to assess the strength and duration of price impact\nand cross-impact.\nFollowing the analysis of Cartea et al. [9], Chinco et al. [12], we demonstrate the effectiveness\nof the forward-looking price impact and cross-impact models from two perspectives: (1)\nstatistical performance, and (2) economic gain.\n16Note that we choose to use the physical time as opposed to the trading time. This is because each stock\nhas its own specific trading time, which is asynchronous with that of others. Thus it is difficult to work\nout the cross\u2013impact between stocks on a trading time scale, also see Wang et al. [49, 50]. The choice of\na 1-minute bin size allows us to abstract away from microstructure effects which are not the focus of the\npresent mesoscopic study, as is the case in Benzaquen et al. [5], Chinco et al. [12].\n22\n\n4.2.1\nStatistical performance\nTable 8 summarizes the out-of-sample R2 values of the aforementioned predictive models\nwhen predicting the subsequent 1-minute returns, i.e. f = 1. It appears the cross-impact\nmodels FCI[1] (respectively, FCII, CAR) achieve higher out-of-sample R2 statistics compared\nto the price impact models FPI[1] (respectively, FPII, AR). We also implement the same\nhypothesis test described in Section 3 to investigate the statistical significance (unreported)\nof these results.\nWe observe that the cross-impact models exhibit significantly superior\nperformance than the price impact models across all stocks, at the 1% confidence level.\nTable 8: Out-of-sample performance for one-minute-ahead returns.\nBest-level OFIs\nIntegrated OFIs\nReturns\nFPI[1]\nFCI[1]\nFPII\nFCII\nAR\nCAR\nOS R2\n-0.37\n-0.10\n-0.36\n-0.10\n-0.36\n-0.10\n(0.10)\n(0.05)\n(0.08)\n(0.05)\n(0.11)\n(0.05)\nNote: The table reports the mean values and standard deviations (in parentheses) of out-\nof-sample R2 of various models when modeling one-minute-ahead returns. The predictive\nmodels include FPI[1] (Eqn (12)), FCI[1] (Eqn (13)), FPII (Eqn (14)), FCII (Eqn (15)),\nAR (Eqn (16)) and CAR (Eqn (17)). These statistics are averaged across each stock and\neach regression window.\nMost of the empirical literature in return prediction focuses its evaluations on out-of-\nsample R2. However, we remark that negative R2 values do not imply that the forecasts are\neconomically meaningless (see more discussions in Kelly et al. [33], Choi et al. [13]).17 To\nemphasize this point, we will incorporate these return forecasts into a forecast-based trading\nstrategy, and showcase their profitability in the following subsection.",
    "chunk_index": 21,
    "start_char": 50919,
    "end_char": 54047,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "regression window.\nMost of the empirical literature in return prediction focuses its evaluations on out-of-\nsample R2. However, we remark that negative R2 values do not imply that the forecasts are\neconomically meaningless (see more discussions in Kelly et al. [33], Choi et al. [13]).17 To\nemphasize this point, we will incorporate these return forecasts into a forecast-based trading\nstrategy, and showcase their profitability in the following subsection.\nConsidering the different magnitudes of the OFIs and returns, we first normalize the\ncoefficient matrix of each model by dividing by the average of the absolute coefficients.\nFigure E.1 (deferred to Appendix E) shows the average coefficient matrices of FCI[1], FCII,\nand CAR. For example, as revealed in Figure E.1(a) (FCI[1]), for a specific stock, the\nmain influence comes from its own OFI, i.e. the absolute values of diagonal elements are\nsignificantly larger than the off-diagonal ones. We observe that cross-impact is often negative,\nconsistent with Pasquariello and Vega [41]. Except for the self-impact, most stocks are also\ninfluenced by stocks in Communication Services, Consumer Discretionary and Information\nTechnology.\nTo better illustrate the interactions between different stocks, we construct a network for\neach normalized coefficient matrix and only preserve the cross-asset edges (i.e. off-diagonal\n17A simple example can be framed as follows. Consider a model with one predictor and suppose that\nthe estimated predictive coefficient is a significantly large multiple of the actual value. In this case, the R2\nwill become negative. However, the predictions will be perfectly correlated with the true expected return,\nresulting in a positive expected return for our strategy. Proposition 4 in Kelly et al. [33] further proposed\nthat we can worry less about the positivity of out-of-sample R2 from a prediction model and focus more on\nthe out-of-sample performance of specific trading strategies based on predicted returns.\n23\n\nelements) larger than the 95-th percentile of coefficients. Figure 8 illustrates some of the main\ncharacteristics of the coefficient networks for FCI[1], FCII, and CAR. For example, we again\nobserve that there are more edges from Communication Services, Consumer Discretionary\nand Information Technology, indicating they may contain more predictive power for others.\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(a) Based on best-level OFIs\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM",
    "chunk_index": 22,
    "start_char": 53590,
    "end_char": 56580,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "CME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(a) Based on best-level OFIs\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB AXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(b) Based on integrated OFIs\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM WFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\n(c) Based on returns\nFigure 8: Network structure of the coefficient matrix constructed from forward-looking cross-\nimpact models.\nNote:\nThe coefficients are averaged over 2017\u20132019.\nTo render the networks more\ninterpretable and for ease of visualization, we only plot the top 5% largest in magnitude\ncoefficients. Nodes are colored by the GICS structure and sorted by market capitalization.\nGreen links represent positive values while black links represent negative values. The width\nof edges is proportional to the absolute values of their respective coefficients.\nTable 9: Group degree centrality for each GICS sector.\nGroup In-degree Centrality\nGroup Out-degree Centrality\nBest-level OFIs\nIntegrated OFIs\nReturns\nBest-level OFIs\nIntegrated OFIs\nReturns\nInformation Technology\n0.12\n0.36\n0.26\n0.46\n0.62\n0.59\nCommunication Services\n0.06\n0.24\n0.20\n0.85\n0.74\n0.60\nConsumer Discretionary\n0.09\n0.20\n0.15\n0.86\n0.51\n0.17\nConsumer Staples\n0.03\n0.15\n0.09\n0.00\n0.11\n0.01\nHealth Care\n0.10\n0.37\n0.19\n0.12\n0.22\n0.59\nFinancials\n0.12\n0.21\n0.17\n0.03\n0.41\n0.08\nIndustrials\n0.10\n0.19\n0.20\n0.00\n0.39\n0.27\nUtilities\n0.00\n0.07\n0.04\n0.00\n0.06\n0.00\nEnergy\n0.06\n0.07\n0.04\n0.00\n0.14\n0.00\nReal Estate\n0.00\n0.05\n0.02\n0.00\n0.00\n0.01\nNote: According to the threshold networks as shown in Figure 8, we compute the fraction\nof stocks outside of a specific sector connected to stocks in this specific sector. The color of\neach sector in this table corresponds to the color in Figure 8.\nTo gain a better understanding of the structural properties of the resulting network,\nwe aggregate node centrality measures (see Everett and Borgatti [22]) at the sector level,\n24\n\nand also perform a spectral analysis of the adjacency matrix. From Table 9, we observe\nthat the out-degree centrality of Communication Services, Consumer Discretionary and\nInformation Technology is significantly larger than that of others, consistent with previous\nfindings.",
    "chunk_index": 23,
    "start_char": 56242,
    "end_char": 58998,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "each sector in this table corresponds to the color in Figure 8.\nTo gain a better understanding of the structural properties of the resulting network,\nwe aggregate node centrality measures (see Everett and Borgatti [22]) at the sector level,\n24\n\nand also perform a spectral analysis of the adjacency matrix. From Table 9, we observe\nthat the out-degree centrality of Communication Services, Consumer Discretionary and\nInformation Technology is significantly larger than that of others, consistent with previous\nfindings. Figure 8(c) also shows that the network based on returns contains more inner-sector\nconnections than the other two counterparts, thus implying a sectorial structure. Table 10\npresents the top five most significant stocks in terms of out-degree centrality in each network,\nwhich exhibit more impact on the prices of other stocks.\nTable 10: Top 5 stocks according to node out-degree centrality in threshold networks.\nBest-level OFIs\nIntegrated OFIs\nReturns\nAMZN\nNFLX\nNVDA\nGOOG\nAMZN\nNFLX\nGOOGL\nNVDA\nISRG\nNVDA\nGS\nAVGO\nNFLX\nFB\nGE\nNote: The out-degree centrality for a node is the fraction of nodes its outgoing edges are\nconnected to.\nFigure 9 shows a barplot with the average value for the top 20 largest singular values of\nthe network adjacency matrix, for best-level OFIs, integrated OFIs, and returns, where the\naverage is performed over all constructed networks. For ease of visualization and comparison,\nwe first normalize the adjacency matrix before computing the top singular values, which\nexhibit a fast decay. In addition to the significantly large top singular value revealing that the\nnetworks have a strong rank-1 structure, the next 6-8 singular values are likely to correspond\nto the more prominent industry sectors.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nBest-level OFIs\nIntegrated OFIs\nReturns\nFigure 9: Barplot of normalized singular values for the average coefficient matrix in forward-\nlooking cross-impact models.\nNote: We perform Singular Value Decomposition (SVD) on the coefficient matrix and obtain\nthe singular values. The x-axis represents the singular value rank, and the y-axis shows the\nnormalized singular values. The coefficients are averaged over 2017\u20132019.\n25\n\n4.2.2\nEconomic gains\nOn the basis of return forecasts, we employ a portfolio construction method, proposed by\nChinco et al. [12], to evaluate the economic gains of the aforementioned predictive models.\nForecast-implied portfolio.\nFor a specific forecasting model F, the motivations of portfolio\nconstruction can be summarized as follows.\n\u2022 It only executes an order when the one-minute-ahead return forecast exceeds the bid-\nask spread.\n\u2022 It buys/sells more shares of the i-th stock when the absolute value of one-minute-ahead\nreturn forecast for i-th stock is higher.\n\u2022 It buys/sells more shares of the i-th stock when the one-minute-ahead return forecasts\nfor the i-th stock tend to be less volatile throughout the trading day.\nThis strategy allocates a fraction wi,t of its capital to the i-th stock\nwi,t\ndef\n=\n1{|fF\ni,t|>sprdi,t} \u00b7 f F\ni,t / \u03c3F\ni,t\nPN",
    "chunk_index": 24,
    "start_char": 58479,
    "end_char": 61591,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "exceeds the bid-\nask spread.\n\u2022 It buys/sells more shares of the i-th stock when the absolute value of one-minute-ahead\nreturn forecast for i-th stock is higher.\n\u2022 It buys/sells more shares of the i-th stock when the one-minute-ahead return forecasts\nfor the i-th stock tend to be less volatile throughout the trading day.\nThis strategy allocates a fraction wi,t of its capital to the i-th stock\nwi,t\ndef\n=\n1{|fF\ni,t|>sprdi,t} \u00b7 f F\ni,t / \u03c3F\ni,t\nPN\nn=1 1{|fF\nn,t|>sprdn,t} \u00b7\n\f\ff F\nn,t\n\f\f / \u03c3F\nn,t\n,\n(18)\nwhere f F\ni,t represents the one-minute-ahead return forecast according to model F for minute\n(t + 1), sprdi,t represents the relative bid-ask spread at time t, \u03c3F\ni,t represents the standard\ndeviation of the model\u2019s one-minute-ahead return forecasts for the i-th stock during the\nprevious 30 minutes of trading, i.e. the standard deviation of in-sample fits. The denominator\nis the total investment so that the strategy is self-financed.\nIf there are no stocks with\nforecasts that exceed the spread in a given minute, then we set wi,t = 0, \u2200i.\nFinally, we compute the profit and loss (PnL) of the resulting portfolios on each trading\nday by summing the strategy\u2019s minutely returns as in Chinco et al. [12].\nTable 11: Economic performance of forecast-implied trading strategy.\nBest-level OFIs\nIntegrated OFIs\nReturns\nFPI[1]\nFCI[1]\nFPII\nFCII\nAR\nCAR\nPnL\n0.21\n0.43\n0.23\n0.39\n0.23\n0.40\n(0.12)\n(0.17)\n(0.13)\n(0.19)\n(0.13)\n(0.18)\nNote:\nThe table reports the mean values and standard deviations (in parentheses) of\nannualized PnLs of forecast-implied trading strategy of various models for forecasting one-\nminute-ahead returns. The predictive models include FPI[1] (Eqn (12)), FCI[1] (Eqn (13)),\nFPII (Eqn (14)), FCII (Eqn (15)), AR (Eqn (16)) and CAR (Eqn (17)). These statistics\nare averaged over 2017-2019.\nTable 11 compares the performance (annualized PnL) of the forecast-implied strategies,\nbased on forecast returns from various predictive models. It is worth noting that in the\n26\n\nfollowing analysis, the strategy ignores trading costs, as this is not the focus of our paper.\nTable 11 shows that portfolios based on forecasts of the forward-looking cross-impact model\noutperform those based on forecasts of the forward-looking price impact model.\n4.3\nLonger forecasting horizons\nOne-minute-ahead return forecasts are not the only time horizon of interest to practitioners\nand academics. Additionally, we evaluate the performance of the above models and examine\nthe forecasting ability of cross-impact terms over longer prediction horizons.\nFigure 10 illustrates the model predictability from the perspective of raw annualized PnL\nacross multiple horizons.18 Due to the similar performance of FPI[1] and FPII (respectively,\nFCI[1] and FCII) over longer horizons, we only show the curves of FPI[1], FCII, AR,\nCAR, and a benchmark (S&P100 ETF). It appears that superior forecasting ability arises\nfrom cross-asset terms at short horizons. However, the PnL of cross-asset models declines\nmore quickly over longer horizons. A further study with more focus on the reasons for the\npredictability of cross-asset OFIs over multiple horizons is therefore suggested.",
    "chunk_index": 25,
    "start_char": 61144,
    "end_char": 64305,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "multiple horizons.18 Due to the similar performance of FPI[1] and FPII (respectively,\nFCI[1] and FCII) over longer horizons, we only show the curves of FPI[1], FCII, AR,\nCAR, and a benchmark (S&P100 ETF). It appears that superior forecasting ability arises\nfrom cross-asset terms at short horizons. However, the PnL of cross-asset models declines\nmore quickly over longer horizons. A further study with more focus on the reasons for the\npredictability of cross-asset OFIs over multiple horizons is therefore suggested. Finally, the\nmodels in which each stock only relies on its own returns/OFIs marginally outperform their\ncounterparts which use the entire cross-sectional predictors.\n1\n3\n10\n30\nFuture horizon (in minutes)\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nPnL\nFPI[1]\nFCI[1]\nAR\nCAR\nSP100\nFigure 10: Annualized PnL as a function of the forecasting horizon.\nNote: The x-axis represents the prediction horizon (in minutes), while the y-axis represents\nthe annualized PnL. The grey horizontal line is the performance of the S&P100 ETF index.\n4.4\nDiscussion about predictive cross-impact\nTables 8 and 11 reveal that, in contrast to the price impact model, multi-asset OFIs can\nprovide considerably more additional explanatory power for future returns compared to\n18To plot this figure, we only accumulate the PnLs between [10:31, 15:30], which is the shared trading\nperiod for the studied forecasting horizons.\n27\n\ncontemporaneous returns. A possible explanation for this asymmetric phenomenon is that\nthere exists a time lag between when the OFIs of a given stock are formed (a so-called flow\nformation period) and the actual time when traders notice this change of flow and incorporate\nit into their trading model (see Buccheri et al. [6]).19 For example, assume a trader submitted\nan unexpectedly large amount of buy limit orders of Apple (AAPL) at 10:00 am, at either\nthe first level or potentially deeper in the book. Other traders may notice this anomaly and\nadjust their portfolios (including Apple) at a later time, for example, 10:01 am. In this case,\nthe OFIs of Apple may indicate future price changes of other stocks.\nConsistent with our explanation, Hou [31] argued that the gradual diffusion of industry\ninformation is a leading cause of the lead-lag effect in stock returns. Cohen and Frazzini [16]\nfound that certain stock prices do not promptly incorporate news pertaining to economically\nrelated firms, due to the presence of investors subject to attention constraints.\nFurther\nresearch should be undertaken to investigate the origins of the success of multi-asset OFIs\nin predicting future returns.\nIt is also interesting to note that forward-looking models using integrated OFIs cannot\nsignificantly outperform models using the best-level OFIs. This phenomenon might stem\nfrom the fact that the integrated OFIs do not explicitly take into account the level information\n(distance of a given level to the best bid/ask) of multi-level OFIs, and are agnostic to\ndifferent sizes resting at different levels on the bid and ask sides of the book.\nPrevious\nstudies (such as Hasbrouck and Saar [27], Cao et al. [7], Cenesizoglu et al.",
    "chunk_index": 26,
    "start_char": 63787,
    "end_char": 66923,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "integrated OFIs cannot\nsignificantly outperform models using the best-level OFIs. This phenomenon might stem\nfrom the fact that the integrated OFIs do not explicitly take into account the level information\n(distance of a given level to the best bid/ask) of multi-level OFIs, and are agnostic to\ndifferent sizes resting at different levels on the bid and ask sides of the book.\nPrevious\nstudies (such as Hasbrouck and Saar [27], Cao et al. [7], Cenesizoglu et al. [10]) demonstrated\nthat traders might strategically choose to place their orders in different levels of the book\ndepending on various factors, therefore limit orders at different price levels may contain\ndifferent information content with respect to predicting future returns. A further study with\nmore focus on the impact of multi-level OFIs over different time horizons is suggested.\n5\nConclusion\nWe have systematically examined the impact of OFIs from multiple perspectives. The main\ncontributions can be summarized as follows.\nFirst, we verify the effects of multi-level and cross-asset OFIs on contemporaneous price\ndynamics. We introduce a new procedure to examine the cross-impact on contemporaneous\nreturns.\nUnder the sparsity assumption of cross-impact coefficients, we use LASSO to\ndescribe such a structure and compare the performances with the price impact model which\nonly utilizes a stock\u2019s own OFIs.\nWe implement models with the best-level OFIs and\nintegrated OFIs, respectively. The results first demonstrate that our integrated OFIs provide\na higher explanatory power for price movements than the widely-used best-level OFIs. More\ninterestingly, in comparison with the price impact model using best-level OFIs, the cross-\nimpact model exhibits additional explanatory power. However, the cross-impact model with\n19A closely-related phenomenon is the Epps effect documented by Epps [21], which showed that the\nempirical correlation estimates tend to decrease when the sampling is done at high frequencies. Previous\nresearch (including Ren\u00f2 [42], T\u00f3th and Kert\u00e9sz [48], Zhang [54]) demonstrated that the Epps effect might\nbe explained by the non-synchronicity, the possible lead-lag relationship between stock returns, etc. T\u00f3th\nand Kert\u00e9sz [48] also described that the Epps effect might be caused by the reaction time of traders to news\nand events, which is usually spread out over a time interval of a few minutes.\n28\n\nintegrated OFIs cannot provide extra explanatory power to the price impact model with\nintegrated OFIs, indicating the effectiveness of our integrated OFIs.\nIn addition, we apply the price impact and cross-impact models to the challenging task of\npredicting future returns. The results reveal that involving cross-asset OFIs can increase out-\nof-sample R2. We subsequently demonstrate that this increase in out-of-sample R2 leads to\nadditional economic profits, when incorporated in common trading strategies, thus providing\nevidence of cross-impact over short future horizons. We also find that predictability of cross-\nimpact terms vanishes quickly over longer horizons.\nFuture research directions.\nThere are a number of interesting avenues to explore in\nfuture research. One such direction pertains to the assessment of whether cross-asset multi-\nlevel OFIs can improve the forecast of future returns (in the present work, we only considered\nthe best-level OFI and integrated OFI due to limited computing power).",
    "chunk_index": 27,
    "start_char": 66461,
    "end_char": 69873,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "this increase in out-of-sample R2 leads to\nadditional economic profits, when incorporated in common trading strategies, thus providing\nevidence of cross-impact over short future horizons. We also find that predictability of cross-\nimpact terms vanishes quickly over longer horizons.\nFuture research directions.\nThere are a number of interesting avenues to explore in\nfuture research. One such direction pertains to the assessment of whether cross-asset multi-\nlevel OFIs can improve the forecast of future returns (in the present work, we only considered\nthe best-level OFI and integrated OFI due to limited computing power). Another interesting\ndirection pertains to performing a similar analysis as in the present paper, but for the last 15-\n30 minutes of the trading day, where a significant fraction of the total daily trading volume\noccurs. For example, for the first few months of 2020 in the US equity market, about 23%\nof trading volume in the 3,000 largest stocks by market value has taken place after 3:30 pm,\ncompared with about 4% from 12:30 pm to 1 pm (Banerji [4]). It would be an interesting\nstudy to explore the interplay between the OFI dynamics and this surge of trading activity\nat the end of U.S. market hours.\nReferences\n[1] Hee-Joon Ahn, Kee-Hong Bae, and Kalok Chan. Limit orders, depth, and volatility:\nEvidence from the stock exchange of Hong Kong. Journal of Finance, 56(2):767\u2013788,\n2001.\n[2] Yacine Ait-Sahalia, Jianqing Fan, Lirong Xue, and Yifeng Zhou. How and when are\nhigh-frequency stock returns predictable? Available at SSRN 4095405, 2022.\n[3] Marco Avellaneda and Jeong-Hyun Lee. Statistical arbitrage in the US equities market.\nQuantitative Finance, 10(7):761\u2013782, 2010.\n[4] Gunjan\nBanerji.\nThe\n30\nMinutes\nthat\nCan\nMake\nor\nBreak\nthe\nTrading\nDay,\n2020.\nURL\nhttps://www.wsj.com/articles/\nthe-30-minutes-that-can-make-or-break-the-trading-day-11583886131?\nreflink=desktopwebshare_permalink.\n[5] Michael Benzaquen, Iacopo Mastromatteo, Zoltan Eisler, and Jean-Philippe Bouchaud.\nDissecting cross-impact on stock markets: An empirical analysis. Journal of Statistical\nMechanics: Theory and Experiment, 2017(2):023406, 2017.\n[6] Giuseppe Buccheri, Fulvio Corsi, and Stefano Peluso. High-frequency lead-lag effects\nand cross-asset linkages: a multi-asset lagged adjustment model. Journal of Business\n& Economic Statistics, 39(3):605\u2013621, 2021.\n29\n\n[7] Charles Cao, Oliver Hansch, and Xiaoxin Wang. The information content of an open\nlimit-order book. Journal of Futures Markets: Futures, Options, and Other Derivative\nProducts, 29(1):16\u201341, 2009.\n[8] Francesco Capponi and Rama Cont.\nMulti-asset market impact and order flow\ncommonality.\nSSRN, 2020.\nURL https://papers.ssrn.com/sol3/papers.cfm?\nabstract_id=3706390.\n[9] \u00c1lvaro Cartea, Ryan Donnelly, and Sebastian Jaimungal. Enhancing trading strategies\nwith order book signals. Applied Mathematical Finance, 25(1):1\u201335, 2018.\n[10] Tolga Cenesizoglu, Georges Dionne, and Xiaozhou Zhou. Asymmetric effects of the\nlimit order book on price dynamics. Journal of Empirical Finance, 65:77\u201398, 2022.\n[11] Bidisha Chakrabarty, Terrence Hendershott, Samarpan Nawn, and Roberto Pascual.\nOrder exposure in high frequency markets. Available at SSRN 3074049, 2022.\n[12] Alex Chinco, Adam D Clark-Joseph, and Mao Ye. Sparse signals in the cross-section of\nreturns. Journal of Finance, 74(1):449\u2013492, 2019.\n[13] Darwin Choi, Wenxi Jiang, and Chao Zhang. Alpha go everywhere: Machine learning\nand international stock returns. Available at SSRN 3489679, 2022.\n[14] Tarun Chordia and Avanidhar Subrahmanyam.",
    "chunk_index": 28,
    "start_char": 69248,
    "end_char": 72817,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "order book on price dynamics. Journal of Empirical Finance, 65:77\u201398, 2022.\n[11] Bidisha Chakrabarty, Terrence Hendershott, Samarpan Nawn, and Roberto Pascual.\nOrder exposure in high frequency markets. Available at SSRN 3074049, 2022.\n[12] Alex Chinco, Adam D Clark-Joseph, and Mao Ye. Sparse signals in the cross-section of\nreturns. Journal of Finance, 74(1):449\u2013492, 2019.\n[13] Darwin Choi, Wenxi Jiang, and Chao Zhang. Alpha go everywhere: Machine learning\nand international stock returns. Available at SSRN 3489679, 2022.\n[14] Tarun Chordia and Avanidhar Subrahmanyam. Order imbalance and individual stock\nreturns: Theory and evidence. Journal of Financial Economics, 72(3):485\u2013518, 2004.\n[15] Tarun Chordia, Richard Roll, and Avanidhar Subrahmanyam.\nOrder imbalance,\nliquidity, and market returns. Journal of Financial Economics, 65(1):111\u2013130, 2002.\n[16] Lauren Cohen and Andrea Frazzini. Economic links and predictable returns. Journal\nof Finance, 63(4):1977\u20132011, 2008.\n[17] Rama Cont, Arseniy Kukanov, and Sasha Stoikov. The price impact of order book\nevents. Journal of Financial Econometrics, 12(1):47\u201388, 2014.\n[18] Fulvio Corsi. A simple approximate long-memory model of realized volatility. Journal\nof Financial Econometrics, 7(2):174\u2013196, 2009.\n[19] Gianbiagio Curato and Fabrizio Lillo.\nHow tick size affects the high frequency\nscaling of stock return distributions. Financial Econometrics and Empirical Market\nMicrostructure, pages 55\u201376, 2015.\n[20] Chester Curme, Michele Tumminello, Rosario N Mantegna, H Eugene Stanley, and\nDror Y Kenett.\nEmergence of statistically validated financial intraday lead-lag\nrelationships. Quantitative Finance, 15(8):1375\u20131386, 2015.\n[21] Thomas W Epps. Comovements in stock prices in the very short run. Journal of the\nAmerican Statistical Association, 74(366a):291\u2013298, 1979.\n30\n\n[22] Martin G Everett and Stephen P Borgatti. The centrality of groups and classes. The\nJournal of Mathematical Sociology, 23(3):181\u2013201, 1999.\n[23] Andrea Frazzini, Ronen Israel, and Tobias J Moskowitz. Trading costs of asset pricing\nanomalies. Fama-Miller Working Paper, Chicago Booth Research Paper, (14-05), 2012.\n[24] Raffaella Giacomini and Halbert White.\nTests of conditional predictive ability.\nEconometrica, 74(6):1545\u20131578, 2006.\n[25] Shihao Gu, Bryan Kelly, and Dacheng Xiu. Empirical asset pricing via machine learning.\nReview of Financial Studies, 33(5):2223\u20132273, 2020.\n[26] Lawrence E Harris and Venkatesh Panchapagesan. The information content of the limit\norder book: evidence from NYSE specialist trading decisions. Journal of Financial\nMarkets, 8(1):25\u201367, 2005.\n[27] Joel Hasbrouck and Gideon Saar. Limit orders and volatility in a hybrid market: The\nIsland ECN. Stern School of Business Dept. of Finance Working Paper FIN-01-025,\n2002.\n[28] Joel Hasbrouck and Duane J Seppi. Common factors in prices, order flows, and liquidity.\nJournal of Financial Economics, 59(3):383\u2013411, 2001.\n[29] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical\nlearning: data mining, inference, and prediction. Springer Science & Business Media,\n2009.\n[30] Nikolaus Hautsch and Ruihong Huang. The market impact of a limit order. Journal of\nEconomic Dynamics and Control, 36(4):501\u2013522, 2012.\n[31] Kewei Hou.\nIndustry information diffusion and the lead-lag effect in stock returns.\nReview of Financial Studies, 20(4):1113\u20131138, 2007.\n[32] Nicolas Huck.\nLarge data sets and machine learning:\nApplications to statistical\narbitrage. European Journal of Operational Research, 278(1):330\u2013342, 2019.\n[33] Bryan T Kelly, Semyon Malamud, and Kangying Zhou. The virtue of complexity in\nreturn prediction.",
    "chunk_index": 29,
    "start_char": 72245,
    "end_char": 75901,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "Science & Business Media,\n2009.\n[30] Nikolaus Hautsch and Ruihong Huang. The market impact of a limit order. Journal of\nEconomic Dynamics and Control, 36(4):501\u2013522, 2012.\n[31] Kewei Hou.\nIndustry information diffusion and the lead-lag effect in stock returns.\nReview of Financial Studies, 20(4):1113\u20131138, 2007.\n[32] Nicolas Huck.\nLarge data sets and machine learning:\nApplications to statistical\narbitrage. European Journal of Operational Research, 278(1):330\u2013342, 2019.\n[33] Bryan T Kelly, Semyon Malamud, and Kangying Zhou. The virtue of complexity in\nreturn prediction. Journal of Finance, forthcoming, 2022.\n[34] Dror Y Kenett, Michele Tumminello, Asaf Madi, Gitit Gur-Gershgoren, Rosario N\nMantegna, and Eshel Ben-Jacob. Dominating clasp of the financial sector revealed by\npartial correlation analysis of the stock market. PloS One, 5(12):e15032, 2010.\n[35] Petter N Kolm, Jeremy Turiel, and Nicholas Westray.\nDeep order flow imbalance:\nExtracting alpha at multiple horizons from the limit order book. Mathematical Finance,\nto appear, 2023.\n[36] Christopher Krauss, Xuan Anh Do, and Nicolas Huck. Deep neural networks, gradient-\nboosted trees, random forests: Statistical arbitrage on the S&P 500. European Journal\nof Operational Research, 259(2):689\u2013702, 2017.\n31\n\n[37] Albert S Kyle. Continuous auctions and insider trading. Econometrica: Journal of the\nEconometric Society, pages 1315\u20131335, 1985.\n[38] Laurent Laloux, Pierre Cizeau, Marc Potters, and Jean-Philippe Bouchaud. Random\nmatrix theory and financial correlations.\nInternational Journal of Theoretical and\nApplied Finance, 3(03):391\u2013397, 2000.\n[39] Fabrizio Lillo, J Doyne Farmer, and Rosario N Mantegna. Master curve for price-impact\nfunction. Nature, 421(6919):129\u2013130, 2003.\n[40] Lior Menzly and Oguzhan Ozbas.\nMarket segmentation and cross-predictability of\nreturns. Journal of Finance, 65(4):1555\u20131580, 2010.\n[41] Paolo Pasquariello and Clara Vega. Strategic cross-trading in the US stock market.\nReview of Finance, 19(1):229\u2013282, 2015.\n[42] Roberto Ren\u00f2. A closer look at the epps effect. International Journal of Theoretical\nand Applied Finance, 6(01):87\u2013102, 2003.\n[43] Mathieu Rosenbaum and Mehdi Tomas.\nA characterisation of cross-impact kernels.\narXiv preprint arXiv:2107.08684, 2021.\n[44] Michael Schneider and Fabrizio Lillo.\nCross-impact and no-dynamic-arbitrage.\nQuantitative Finance, 19(1):137\u2013154, 2019.\n[45] Justin A Sirignano. Deep learning for limit order books. Quantitative Finance, 19(4):\n549\u2013570, 2019.\n[46] Daigo Tashiro, Hiroyasu Matsushima, Kiyoshi Izumi, and Hiroki Sakaji.\nEncoding\nof high-frequency order information and prediction of short-term stock price by deep\nlearning. Quantitative Finance, 19(9):1499\u20131506, 2019.\n[47] Mehdi Tomas, Iacopo Mastromatteo, and Michael Benzaquen. How to build a cross-\nimpact model from first principles: Theoretical requirements and empirical results.\nQuantitative Finance, 22(6):1017\u20131036, 2022.\n[48] Bence T\u00f3th and J\u00e1nos Kert\u00e9sz. The epps effect revisited. Quantitative Finance, 9(7):\n793\u2013802, 2009.\n[49] Shanshan Wang, Rudi Sch\u00e4fer, and Thomas Guhr. Average cross-responses in correlated\nfinancial markets. European Physical Journal B, 89(9):207, 2016.\n[50] Shanshan Wang, Rudi Sch\u00e4fer, and Thomas Guhr. Cross-response in correlated financial\nmarkets: individual stocks. European Physical Journal B, 89(4):105, 2016.\n[51] Shanshan Wang, Sebastian Neus\u00fc\u00df, and Thomas Guhr. Statistical properties of market\ncollective responses. European Physical Journal B, 91:1\u201311, 2018.\n[52] Michael D Ward and John S Ahlquist. Maximum likelihood for social science: strategies\nfor analysis. Cambridge University Press, 2018.\n32",
    "chunk_index": 30,
    "start_char": 75327,
    "end_char": 78979,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "financial markets. European Physical Journal B, 89(9):207, 2016.\n[50] Shanshan Wang, Rudi Sch\u00e4fer, and Thomas Guhr. Cross-response in correlated financial\nmarkets: individual stocks. European Physical Journal B, 89(4):105, 2016.\n[51] Shanshan Wang, Sebastian Neus\u00fc\u00df, and Thomas Guhr. Statistical properties of market\ncollective responses. European Physical Journal B, 91:1\u201311, 2018.\n[52] Michael D Ward and John S Ahlquist. Maximum likelihood for social science: strategies\nfor analysis. Cambridge University Press, 2018.\n32\n\n[53] Ke Xu, Martin D Gould, and Sam D Howison. Multi-level order-flow imbalance in a\nlimit order book. Market Microstructure and Liquidity, 4(3-4):1950011, 2018.\n[54] Lan Zhang.\nEstimating covariation: Epps effect, microstructure noise.\nJournal of\nEconometrics, 160(1):33\u201347, 2011.\nA\nAggregation of multi-level OFIs\nTable 2 presents evidence of the effectiveness of PCA in selecting weights for combining multi-\nlevel OFIs. However, Figure 2 shows that the weights derived from PCA are not extremely\ndifferent. This prompts us to consider a simpler method, namely the simple average (SA),\nto achieve similar performance. Table A.1 reveals that the explained variance of the SA\nincreases as more levels are included. Nonetheless, the SA across 10 levels is inferior to the\nfirst principal component (PC) in terms of EVR, i.e. 85.07% of SA vs 89.06% of PC in Table\n2. Additionally, Table A.2 provides further evidence that PC consistently performs better\nthan SA across different subsets grouped by stock-specific characteristics.\nTable A.1: Average percentage and the standard deviation (in parentheses) of variance\nattributed to SA across multiple levels.\nAverage across\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nExplained Variance Ratio\n13.82\n22.81\n32.53\n42.73\n52.58\n61.45\n68.82\n75.10\n80.49\n85.07\n(9.04)\n(10.09)\n(10.35)\n(10.17)\n(9.58)\n(8.69)\n(7.71)\n(7.10)\n(7.23)\n(8.14)\nNotes: The table reports the ratio (in percentage points) between the variance of SA across\nmultiple levels and the total variance averaged across each stock and trading day.\n33\n\nTable A.2: Explained variance ratio by different integration ways of multi-level OFIs, sorted\nby stock characteristics.\nVolume\nVolatility\nSpread\n[0%, 25%)\nSA\n83.02\n85.09\n81.84\nPC\n85.79\n89.64\n89.90\n[25%, 50%)\nSA\n87.33\n84.75\n87.97\nPC\n89.93\n89.12\n91.17\n[50%, 75%)\nSA\n86.93\n85.23\n87.57\nPC\n90.76\n89.03\n89.81\n[75%, 100%]\nSA\n83.55\n85.23\n83.03\nPC\n90.23\n88.48\n85.53\nNotes: SA: taking the simple average of OFIs across 10 levels. PC: taking the first principal\ncomponent of OFIs across 10 levels. Volume: trading volume on the previous trading day.\nVolatility: volatility of one-minute returns during the previous trading day. Spread: average\nbid-ask spread during the previous trading day. [0%, 25%), respectively [75%, 100%], denote\nthe subset of stocks with the lowest, respectively highest, 25% values for a given stock\ncharacteristic.\nWe then perform the price impact and cross-impact analysis with simple average multi-\nlevel OFIs, denoted as PISA and CISA, respectively. As shown in Table A.3, PISA has\na similar performance with CISA, consistent with our main analysis. This again confirms\nthat as long as multi-level orders are taken into account, adding cross-impact terms cannot\nsignificantly improve model performance.",
    "chunk_index": 31,
    "start_char": 78455,
    "end_char": 81730,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "100%], denote\nthe subset of stocks with the lowest, respectively highest, 25% values for a given stock\ncharacteristic.\nWe then perform the price impact and cross-impact analysis with simple average multi-\nlevel OFIs, denoted as PISA and CISA, respectively. As shown in Table A.3, PISA has\na similar performance with CISA, consistent with our main analysis. This again confirms\nthat as long as multi-level orders are taken into account, adding cross-impact terms cannot\nsignificantly improve model performance. On the other hand, PII is slightly better than\nPISA. A future research direction might be to devise various weighting schemes that average\nthe OFI information across the multiple levels, where the weights could be given, for example,\nby an inverse function of the distance of each price level to the mid-price or applying tensor-\nSVD/PCA on this data.\nTable A.3: Out-of-sample performance of models based on different aggregations of multi-\nlevel OFIs.\nPISA\nCISA\nPII\nCII\nOS R2\n82.34\n82.51\n83.83\n83.62\n(18.02)\n(14.27)\n(16.90)\n(14.53)\nNotes: PISA (resp. CISA): price impact (resp. cross-impact) model using the simple average\nof OFIs across 10 levels.\n34\n\nB\nContemporaneous price impact of multi-level OFIs\nTo explicitly identify the impact of deeper-level OFIs, we also consider an extended version\nof PI[1] by incorporating multi-level OFIs as features in the model\nPI[m] :\nr(h)\ni,t = \u03b1[m]\ni\n+\nm\nX\nk=1\n\u03b2[m],k\ni\nofik,(h)\ni,t\n+ \u03f5[m]\ni,t .\n(19)\nRecall that ofik,(h)\ni,t\nis the OFI at level k. We refer to this model as PI[m], and use OLS to\nestimate it.\nThe top panel of Table B.1 shows that the in-sample R2 values increase as more multi-level\nOFIs are included as features, which is not surprising given that PI[m] is a nested model\nof PI[m+1]. However the increments of the in-sample R2 are descending, indicating that\nmuch deeper LOB data might be unable to provide additional information. This argument\nis confirmed by the models\u2019 performance on out-of-sample data, as shown at the bottom\npanel of Table B.1. Out-of-sample R2 reaches a peak at PI[8].\nTable B.1: Performance of price impact models with multi-level OFIs.\nPI[1]\nPI[2]\nPI[3]\nPI[4]\nPI[5]\nPI[6]\nPI[7]\nPI[8]\nPI[9]\nPI[10]\nIS R2\n71.16\n81.61\n85.07\n86.69\n87.66\n88.30\n88.74\n89.04\n89.24\n89.38\n(13.80)\n(11.80)\n(10.76)\n(10.30)\n(10.05)\n(9.86)\n(9.71)\n(9.57)\n(9.45)\n(9.34)\nOS R2\n64.64\n75.81\n79.47\n81.13\n82.05\n82.65\n83.01\n83.16\n83.15\n83.11\n(21.82)\n(19.83)\n(18.87)\n(18.61)\n(18.58)\n(18.65)\n(18.78)\n(18.93)\n(19.49)\n(20.93)\nNote: The table reports the mean values and standard deviations (in parentheses) of both in-\nsample and out-of-sample R2 (in percentage points) of PI[m] (m = 1, . . . , 10) when modeling\ncontemporaneous returns. These statistics are averaged across each stock and each regression\nwindow.\nImpact comparison between multi-level OFIs.\nAn interesting question is whether the\nOFIs at different price levels contribute evenly in terms of price impact. Based on Figure\n1(a), we conclude that multi-level OFIs have different contributions to price movements.",
    "chunk_index": 32,
    "start_char": 81221,
    "end_char": 84251,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "sample and out-of-sample R2 (in percentage points) of PI[m] (m = 1, . . . , 10) when modeling\ncontemporaneous returns. These statistics are averaged across each stock and each regression\nwindow.\nImpact comparison between multi-level OFIs.\nAn interesting question is whether the\nOFIs at different price levels contribute evenly in terms of price impact. Based on Figure\n1(a), we conclude that multi-level OFIs have different contributions to price movements.\nGenerally, OFIs at the second-best level manifest greater influence than OFIs at the best\nlevel in model PI[10], which is perhaps counter-intuitive, at first sight.\nWe further investigate how the coefficients vary across stocks with different characteristics,\nsuch as volume, volatility, and bid-ask spread. Figure B.1 (b)-(d) reveals that for stocks with\nhigh-volume and small-spread, order flow posted deeper in the LOB has more influence on\nprice movements. The results regarding spread are in line with Xu et al. [53], where it is\nobserved that for large-spread stocks (AMZN, TSLA, and NFLX), the coefficients of ofim\n(OFIs at the m-th level) tend to get smaller as the LOB level m increases, while for small-\nspread stocks (ORCL, CSCO, and MU), the coefficients of ofim may become larger as m\nincreases.\nCont et al. [17] concluded that the effect of ofim(m \u22652) on price changes is only second-\norder or null. There are two likely causes for the differences between their findings and ours.\n35\n\nFirst, the data used in Cont et al. [17] includes 50 stocks (randomly picked from S&P500\nconstituents) for a single month in 2010, while we use the top 100 large-cap stocks for 36\nmonths during 2017-2019. Second, Cont et al. [17] considered the average of the coefficients\nacross 50 stocks. In our work, we first group 100 stocks by firm characteristics, and then\nstudy the average coefficients of each subset. Therefore, our results are based on a more\ngranular analysis, across a significantly longer period of time.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nCoefficients\n(a) Average\n(b) Volume\n(c) Volatility\n(d) Spread\nFigure B.1: Coefficients of the model PI[10].\nNote: Plot (a) reports average coefficients and one standard deviation (error bars); Plots\n(b)-(d) show coefficients sorted by stock characteristics. Volume: trading volume on the\nprevious trading day. Volatility: volatility of one-minute returns during the previous trading\nday. Spread: average bid-ask spread during the previous trading day. [0%, 25%), respectively\n[75%, 100%], denote the subset of stocks with the lowest, respectively highest, 25% values\nfor a given stock characteristic. The x-axis represents different levels of OFIs and the y-axis\nrepresents the coefficients.\nC\nComparison with Capponi & Cont (2020)\nOne closely related work is Capponi and Cont [8] (CC hereafter), where the authors propose\na two-step procedure to justify the significance of cross-impact terms and render a different\nconclusion about cross-impact.\nIn the first step, the authors use OLS to decompose each stock\u2019s OFIs (ofi1,(h)\ni,t\n) into\nthe common factor of OFIs (F (h)\nofi,t), that",
    "chunk_index": 33,
    "start_char": 83794,
    "end_char": 86905,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "stock characteristic. The x-axis represents different levels of OFIs and the y-axis\nrepresents the coefficients.\nC\nComparison with Capponi & Cont (2020)\nOne closely related work is Capponi and Cont [8] (CC hereafter), where the authors propose\na two-step procedure to justify the significance of cross-impact terms and render a different\nconclusion about cross-impact.\nIn the first step, the authors use OLS to decompose each stock\u2019s OFIs (ofi1,(h)\ni,t\n) into\nthe common factor of OFIs (F (h)\nofi,t), that is the first principal component of the multi-asset\norder flow imbalances, and obtain the idiosyncratic components (\u03c4 (h)\ni,t ) of the OFIs, for each\nindividual stock.\nofi1,(h)\ni,t\n= \u00b5i + \u03b3iF (h)\nofi,t + \u03c4 (h)\ni,t .\n(20)\nIn the second step, they regress returns (r(h)\ni,t ) of stock i against (i) the common factor\nof OFIs (F (h)\nofi,t), (ii) the idiosyncratic components of its own OFIs (\u03c4 (h)\ni,t ), and (iii) the\nidiosyncratic components of the OFIs of other stocks (\u03c4 (h)\nj,t , j \u0338= i). Finally, we arrive at\nthe cross-impact model proposed by Capponi and Cont [8] in Eqn (21), denoted as CICC.\nCICC :\nr(h)\ni,t = \u03b1CC\ni\n+ \u03b2CC\ni0 F (h)\nofi,t +\nN\nX\nj=1\n\u03b2CC\nij \u03c4 (h)\nj,t + \u03b7CC\ni,t .\n(21)\n36\n\nCICC is compared with a parsimonious model PICC (Eqn (22)), in which only the\ncommon order flow factor and a stock\u2019s own idiosyncratic OFI are utilized.\nPICC :\nr(h)\ni,t = \u03b1CC\ni\n+ \u03b2CC\ni0 F (h)\nofi,t + \u03b2CC\nii \u03c4 (h)\ni,t + \u03f5CC\ni,t .\n(22)\nWe estimate the PICC and CICC models on historical data, under the same setting as\nin Section 3.2. Given that there are more features than observations, we employ LASSO in\nthe second step to testify the intraday cross-impact of the idiosyncratic OFIs.\nSimilarly, we present the both in-sample and out-of-sample R2 values of PICC and CICC\nin Table C.1. We observe small improvements (1.37% in in-sample tests, 0.58% in out-of-\nsample tests) from PICC to CICC. From considering Tables 3, 5, and C.1, we also observe\nthat introducing the common factor leads to quite small changes in the model\u2019s explanatory\npower of price dynamics in the in-sample and out-of-sample tests. Moreover, the models\nemploying integrated OFIs continually outperform others.\nTable C.1: Performance of CC\u2019s models.\nPICC\nCICC\nIS R2\n72.58\n73.95\n(13.22)\n(12.56)\nOS R2\n64.78\n65.36\n(19.95)\n(18.68)\nNote: The table reports the mean values and standard deviations (in parentheses) of both\nin-sample and out-of-sample R2 (in percentage points) of PICC and CICC when modeling\ncontemporaneous returns. These statistics are averaged across each stock and each regression\nwindow.\nCapponi and Cont [8] claim that the main determinants of impact is from idiosyncratic\norder flow imbalance as well as a market order flow factor common across stocks; we\nconclude that as long as the multi-level OFIs are included, additional cross-impact terms\nare not necessary.",
    "chunk_index": 34,
    "start_char": 86400,
    "end_char": 89251,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "(in parentheses) of both\nin-sample and out-of-sample R2 (in percentage points) of PICC and CICC when modeling\ncontemporaneous returns. These statistics are averaged across each stock and each regression\nwindow.\nCapponi and Cont [8] claim that the main determinants of impact is from idiosyncratic\norder flow imbalance as well as a market order flow factor common across stocks; we\nconclude that as long as the multi-level OFIs are included, additional cross-impact terms\nare not necessary. The results also reveal that the sparse price impact model with integrated\n(or multi-level) OFIs can explain the price dynamics better than the models proposed by\nCapponi and Cont [8].\nD\nHigh-frequency updates of contemporaneous models\nIn this experiment, we use a 30-minute window to estimate contemporaneous models. We\nthen apply the estimated coefficients to fit data in the next one minute, and repeat this\nprocedure every minute. The results summarized in Table D.1 reveal similar conclusions as\nin Section 3, illustrating the robustness of our findings.\n37\n\nTable D.1: Performance of various models under one-minute update frequency.\nBest-level OFIs\nIntegrated OFIs\nPI[1]\nCI[1]\nPII\nCII\nIS R2\n70.80\n73.55\n86.10\n86.84\n(13.10)\n(12.73)\n(9.64)\n(8.79)\nOS R2\n59.67\n61.46\n78.88\n78.91\n(23.15)\n(18.96)\n(16.78)\n(15.02)\nNote: The table reports the mean values and standard deviations (in parentheses) of both\nin-sample and out-of-sample R2 (in percentage points) of various models when modeling\ncontemporaneous returns in one-minute update frequency. The models include PI[1] (Eqn\n(6)), CI[1] (Eqn (8)), PII (Eqn (7)), and CII (Eqn (9)). These statistics are averaged across\neach stock and each regression window.\nE\nAdditional results of Section 4\nOne interesting question to consider when examining the predictability of cross-impact is\nwhether the lead-lag effect is linked to the frequency of LOB updates. As shown in Table\nE.1, the results indicate that assets with a higher frequency of book updates tend to lead\n\u201cslower\u201d assets, which aligns with the findings reported by Kolm et al. [35].\nTable E.1: Out-of-sample R2 of various predictive models sorted by book updates.\n[0%, 25%)\n[25%, 50%)\n[50%, 75%)\n[75%, 100%]\nFPI[1]\n-0.38\n-0.37\n-0.36\n-0.34\nFCI[1]\n-0.12\n-0.11\n-0.10\n-0.09\nFPII\n-0.37\n-0.35\n-0.35\n-0.33\nFCII\n-0.12\n-0.11\n-0.10\n-0.09\nNotes: [0%, 25%), respectively [75%, 100%], denote the subset of stocks with the lowest,\nrespectively highest, 25% values according to the frequencies of book updates.\n38\n\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nFuture Return\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT",
    "chunk_index": 35,
    "start_char": 88762,
    "end_char": 91726,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "TJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nFuture Return\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nPredictor\nInformation Technology\nCommunication Services\nConsumer Discretionary\nConsumer Staples\nHealth Care\nFinancials\nIndustrials\nUtilities\nEnergy\nReal Estate\n25\n20\n15\n10\n5\n0\n(a) Coefficient matrix of FCI[1]\n39\n\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nFuture Return\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nPredictor\nInformation Technology\nCommunication Services\nConsumer Discretionary\nConsumer Staples\nHealth Care\nFinancials\nIndustrials\nUtilities\nEnergy\nReal Estate\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n(b) Coefficient matrix of FCII\n40\n\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nFuture Return\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX",
    "chunk_index": 36,
    "start_char": 91397,
    "end_char": 93660,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "MO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nFuture Return\nAAPL\nMSFT\nV\nCSCO\nADBE\nCRM\nNVDA\nACN\nPYPL\nAVGO\nORCL\nIBM\nTXN\nQCOM\nFIS\nADP\nINTU\nFISV\nFB\nGOOG\nGOOGL\nT\nCMCSA\nNFLX\nCHTR\nAMZN\nHD\nMCD\nNKE\nSBUX\nLOW\nTJX\nTGT\nPG\nKO\nPEP\nWMT\nPM\nCOST\nMO\nMDLZ\nJNJ\nMRK\nPFE\nMDT\nABT\nBMY\nAMGN\nABBV\nTMO\nLLY\nCVS\nDHR\nGILD\nANTM\nCI\nBDX\nISRG\nSYK\nBSX\nBRK.B\nJPM\nWFC\nC\nUSB\nAXP\nGS\nCME\nCB\nPNC\nSPGI\nMS\nBA\nHON\nUNP\nMMM\nGE\nLMT\nUPS\nCAT\nNEE\nD\nDUK\nSO\nXOM\nCVX\nAMT\nPredictor\nInformation Technology\nCommunication Services\nConsumer Discretionary\nConsumer Staples\nHealth Care\nFinancials\nIndustrials\nUtilities\nEnergy\nReal Estate\n30\n20\n10\n0\n(c) Coefficient matrix of CAR\nFigure E.1:\nAverage coefficient matrices constructed from forward-looking cross-impact\nmodels.\n41",
    "chunk_index": 37,
    "start_char": 93324,
    "end_char": 94182,
    "paper_title": "Cross-Impact of Order Flow Imbalance in Equity Mar",
    "paper_category": "q-fin.TR",
    "paper_filename": "Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Cross-Impact_of_Order_Flow_Imbalance_in_Equity_Mar.pdf"
  },
  {
    "text": "Emergence of stylized facts during the opening of stock markets\nSebastian M. Krause,1 Jonas A. Fiegen,1 and Thomas Guhr1\n1Faculty of Physics, University of Duisburg-Essen, Duisburg, Germany\nFinancial markets show a number of non-stationarities, ranging from volatility \ufb02uctuations over\never changing technical and regulatory market conditions to seasonalities.\nOn the other hand,\n\ufb01nancial markets show various stylized facts which are remarkably stable. It is thus an intriguing\nquestion to \ufb01nd out how these stylized facts emerge. As a \ufb01rst example, we here investigate how\nthe bid-ask-spread between best sell and best buy o\ufb00er for stocks develops during the trading day.\nFor rescaled and properly smoothed data we observe collapsing curves for many di\ufb00erent NASDAQ\nstocks, with a slow power law decline of the spread during the whole trading day.\nThis e\ufb00ect\nemerges robustly after a highly \ufb02uctuating opening period. Some so called large-tick stocks behave\ndi\ufb00erently because of technical boundaries. Their spread closes to one tick shortly after the market\nopening. We use our \ufb01ndings for identifying the duration of the market opening which we \ufb01nd to\nvary largely from stock to stock.\nI.\nINTRODUCTION\nFinancial markets are highly non-stationary [1\u20133].\nMarket conditions change constantly due to technical in-\nnovations and regulatory changes [4]. The correlations\nbetween stock price returns of di\ufb00erent stocks change\nover time\n[3, 5]. Correlations are especially strong in\ntimes of crisis and correlated losses imply systemic risk.\nFurther the volatility of stock prices varies strongly, with\nsmall volatility during calm periods and large volatility\nduring times of crisis [6]. Even though system proper-\nties as volatility vary strongly over time, they can be de-\nscribed with universal scaling laws [7\u20139], establishing dif-\nferent \ufb01nancial stylized facts as the inverse cubic power-\nlaw distribution of returns [10, 11] and slowly decaying\nauto-correlation of absolute returns caused by volatility\nclustering [6, 12]. Stylized facts apply robustly even for\nmarkets under drastically changing conditions from early\nindustrialization to highly digitalized markets.\nAnother sign of non-stationarity in \ufb01nancial markets\nis seasonality. The time of the year [13, 14], o\ufb03cial re-\nports [14, 15], holidays [16], the day of the week [17] and\nthe time of the day in\ufb02uence the market dynamics. Par-\nticularly strong is intra-day seasonality, with pronounced\nover-night e\ufb00ects [15], a special character of the market\nopening period [18], and frozen dynamics of the price\nafter the market closes [19]. Intra-day volatility was in-\ntensely investigated [20]. Possible reasons for seasonal-\nities were discussed, such as arrival of new information\nover night or during the day, habits of traders, and the\nneed to close positions before the market closes. A pro-\nfound understanding of intra-day seasonality is impor-\ntant for modeling volatility on longer time scales\n[21].\nSeasonality-adjusted agent based modeling [6, 22\u201324] of\nthe order book mechanics can shed light on mini \ufb02ash-\ncrashes [25, 26] and seasonal impact e\ufb00ects [27].\nA fundamental open question is how stylized facts\nemerge in highly non-stationary \ufb01nancial markets. Here\nwe investigate as a \ufb01rst example the intra-day season-\nality of the bid-ask-spread of stocks.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3327,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "of traders, and the\nneed to close positions before the market closes. A pro-\nfound understanding of intra-day seasonality is impor-\ntant for modeling volatility on longer time scales\n[21].\nSeasonality-adjusted agent based modeling [6, 22\u201324] of\nthe order book mechanics can shed light on mini \ufb02ash-\ncrashes [25, 26] and seasonal impact e\ufb00ects [27].\nA fundamental open question is how stylized facts\nemerge in highly non-stationary \ufb01nancial markets. Here\nwe investigate as a \ufb01rst example the intra-day season-\nality of the bid-ask-spread of stocks. The spread is the\nprice di\ufb00erence between best sell and best buy o\ufb00er. It\nis an important measure for market liquidity and trading\ncosts, and its average value over the trading day strongly\nimpacts the order\ufb02ow and the structure of the order\nbook [28\u201330]. We analyze order \ufb02ow data of 96 NASDAQ\nstocks in early 2016. The spread performs large \ufb02uctu-\nations during the whole trading day, and there are large\ndi\ufb00erences between di\ufb00erent stocks concerning activity\nand sensitivity to technical bounds as the tick size [30].\nTo overcome these problems we design a robust method\nfor calculating moving averages which adapts to individ-\nual activity and smooths \ufb02uctuations. This allows us to\nuncover how the market organizes to a non-stationary\nstate with universal time dependence of the spread, in-\nstead of approaching a constant stationary-state value.\nOnly if the spread reaches the lower technical bound of\none tick it converges to this value. In both cases we ob-\nserve an opening period with di\ufb00erent behavior compared\nto the rest of the trading day and also with larger \ufb02uctu-\nations. We quantify the duration of this opening period\nindividually for every stock.\nII.\nDATA\nWe analyze Historical TotalView-ITCH data from\nNASDAQ US, downloaded from [31].\nOur data span\nover \ufb01ve consecutive working days from March, 7 2016 to\nMarch, 11 2016. Out of the 100 stocks listed in the NAS-\nDAQ 100 in this period [32], four stocks are not available\nin [31], and therefore cannot be included in our analysis.\nThe data provide information about limit orders being\nplaced, deleted, partially canceled, partially traded and\nfully traded. Moreover, they contain information about\ntrades against hidden orders. A detailed description of\nthe data can be found in [33].\nWe analyze data from times between the market open-\ning time t0 = 9:30 am and the market closing te = 4:00\npm (New York time). All events have a time stamp in\nmilliseconds. Events happening in the same millisecond\nhave the same time stamp. Incoming orders are processed\nby the market in the same order as in which they arrive,\neven if the time stamp is the same.\narXiv:1812.07369v1 [q-fin.TR] 18 Dec 2018\n\n2\nFor every stock and day individually we reconstruct the\norderbook of all visible orders. This allows us to identify\nfor every time t the best available ask price bestask(t),\nand the best available bid price bestbid(t). For charac-\nterizing the price of the stock irrespective of selling or\nbuying, the midpoint price\nm(t) = bestbid(t) + bestask(t)\n2\n(1)\nis a good measure.",
    "chunk_index": 1,
    "start_char": 2780,
    "end_char": 5863,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "are processed\nby the market in the same order as in which they arrive,\neven if the time stamp is the same.\narXiv:1812.07369v1 [q-fin.TR] 18 Dec 2018\n\n2\nFor every stock and day individually we reconstruct the\norderbook of all visible orders. This allows us to identify\nfor every time t the best available ask price bestask(t),\nand the best available bid price bestbid(t). For charac-\nterizing the price of the stock irrespective of selling or\nbuying, the midpoint price\nm(t) = bestbid(t) + bestask(t)\n2\n(1)\nis a good measure. The di\ufb00erence between best bid and\nbest ask price is the spread\ns(t) = bestask(t) \u2212bestbid(t).\n(2)\nThis is an important measure for the market liquidity.\nIf the spread is small, trading is possible with low costs.\nThe structure of the orderbook beyond the quotes of best\nbid and best ask depends strongly on the spread s [30]. If\nthe spread is as small as one tick, a stock is characterized\nas large-tick stock [28, 29, 34]. In this case, the quotes\ncarry large volumes, the orderbook close to the quotes\nis densely \ufb01lled, but far from the quotes the orderbook\nis sparse.\nA spread size of many ticks usually results\nin gaps behind the quotes, but limit orders far from the\nquotes are more likely. As the spread s is an important\ncharacteristic of the whole orderbook, we will concentrate\non this quantity in the following.\nIII.\nEMERGENCE OF SCALING\nIn the upper panel of Fig. 1 the spread s(t \u2212t0) of\nCelgene Corporation on Monday is shown as a function\nof elapsed time t \u2212t0 since market opening t0. The inset\nwith a zoom into the \ufb01rst minutes demonstrates bursty\nchanges of the spread. Sometimes the spread stays con-\nstant for almost a minute, then it \ufb02uctuates strongly on\nmuch shorter time scales. During the whole trading day\nwith a total length of 390 minutes the spread performs\nstrong \ufb02uctuations between the smallest possible value of\none cent and larger values. To see the trend of the spread\nduring the day, the data is also repeated on a logarith-\nmic scale. The curve begins directly after the \ufb01rst change\nof the spread during the trading hours. Hence, the last\nspread of the pre-market hours is ignored.\nFor calcu-\nlating the moving average, we divide the trading hours\ninto time windows of varying size and calculate for every\ntime window the arithmetic mean. The \ufb01rst two inter-\nvals each span 64 spread changes. Each following interval\nspans two times more spread changes than its predeces-\nsor (the n-th interval with n > 2 spans 64 \u00b7 2n\u22122 spread\nchanges). The variable duration of time intervals helps\nadapting to the logarithmic scale and to largely varying\nbursty activity in di\ufb00erent stocks. The moving average\nilluminates that the spread follows a decreasing trend\nthroughout the whole trading day. Further, the moving\naverage stays far above the smallest possible value of one\ncent.",
    "chunk_index": 2,
    "start_char": 5339,
    "end_char": 8171,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "times more spread changes than its predeces-\nsor (the n-th interval with n > 2 spans 64 \u00b7 2n\u22122 spread\nchanges). The variable duration of time intervals helps\nadapting to the logarithmic scale and to largely varying\nbursty activity in di\ufb00erent stocks. The moving average\nilluminates that the spread follows a decreasing trend\nthroughout the whole trading day. Further, the moving\naverage stays far above the smallest possible value of one\ncent. We are dealing with a so called small-tick stock,\n0\n100\n200\n300\n400\nt\u2212t0 [min]\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ns [$]\n0\n1\n2\n3\n4\n5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n10\n-2\n10\n-1\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n-2\n10\n-1\ns [$]\nFIG. 1: Blue curves show the spread of Celgene Corporation\non Monday.\nThe inset in the upper panel is a zoom into\nthe \ufb01rst minutes. The lower panel shows the same data on\ndouble logarithmic scales, with black diamonds representing\nthe moving average.\nbecause the tick size of one cent is relatively small com-\npared with the spread.\nThe spread of some other stocks closes so much that\nthe tick size becomes a relevant lower bound, therefore\nthey are called large-tick stocks. As an example Fig. 2\nshows the spread of the stock of Comcast Corporation.\nThe moving average approaches the smallest possible\nvalue of one cent after about ten minutes. The upper\npanel of Fig. 3 displays moving averages of the spread\non Tuesday for di\ufb00erent large-tick stocks. By comparing\nwith the dashed line at one cent, we see that all stocks\nconverge to one tick during the trading day. This behav-\nior is observed for 30 out of the 96 stocks under consider-\nation. We marked the tickers of these stocks with a star\nin Tab. 1 in the Supplementary Material.\nThe spread\nof the large-tick stock with ticker SIRI (not listed in the\ntable) is special, because it closes so fast, that the \ufb01rst\nvalue of the moving average is already very close one tick.\nThe lower panel of Fig. 3 displays moving averages\nof the normalized spread s(t)/\u27e8s\u27e9for di\ufb00erent small-tick\nstocks on Tuesday. Here, \u27e8s\u27e9denotes the average of the\n\n3\n0\n100\n200\n300\n400\nt\u2212t0 [min]\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ns [$]\n0.0\n0.5\n1.0\n1.5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n10\n-3\n10\n-2\n10\n-1\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n-2\n10\n-1\ns [$]\nFIG. 2: As in Fig. 2 for Comcast Corporation, where the\nspread at later times approaches the smallest possible value\nof one cent.\nspread over the whole trading day. After a highly \ufb02uc-\ntuating opening period of about 20 minutes, the curves\nstart to collapse after around 10 minutes with a scaling\nbehavior \u221d(t \u2212t0)\u2212\u03b1 for the rest of the trading day.\nCompared with the dashed lines with a scaling\ns \u221d(t \u2212t0)\u22120.4\u00b10.15,\n(3)\nthe exponent \u03b1 is around 0.4 for all displayed stocks.",
    "chunk_index": 3,
    "start_char": 7728,
    "end_char": 10421,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "2: As in Fig. 2 for Comcast Corporation, where the\nspread at later times approaches the smallest possible value\nof one cent.\nspread over the whole trading day. After a highly \ufb02uc-\ntuating opening period of about 20 minutes, the curves\nstart to collapse after around 10 minutes with a scaling\nbehavior \u221d(t \u2212t0)\u2212\u03b1 for the rest of the trading day.\nCompared with the dashed lines with a scaling\ns \u221d(t \u2212t0)\u22120.4\u00b10.15,\n(3)\nthe exponent \u03b1 is around 0.4 for all displayed stocks. All\n66 small-tick stocks behave like this.\nThis \ufb01nding can be understood as emergence of a uni-\nversal law for the intra-day seasonality of stock spreads.\nWhile sparse actions of traders directly after market\nopening imply an erratic behavior with large \ufb02uctuations,\naccumulating more and more actions over time helps the\nmarket to converge to a non-equilibrium non-stationary\nemergent state with stylized time dependence. Only if\nthe spread reaches its technical lower bound of one tick\nit stays constant for the rest of the trading day, implying\na quite di\ufb00erent kind of market situation.\nFor a better understanding of the rescaling with \u27e8s\u27e9, it\nis worth discussing an idealized spread\ns(id)(t \u2212t0) = c(t \u2212t0)\u2212\u03b1\n(4)\n10\n-1\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n-2\n10\n-1\ns [$]\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n0\n10\n1\ns/\n\u00ad\ns\n\u00ae\nFIG. 3: Upper panel: Moving average of the spread for large-\ntick stocks on double-logarithmic scales (Activision Blizzard\nin blue, Ebay in green, Liberty Global PLC Class A in red, Mi-\ncron Technology in cyan, Starbucks in magenta and VIP In-\ndustries in yellow). The black dashed line indicates the small-\nest possible value of one cent. Lower panel: Moving average\nof the normalized spread for small-tick stocks on double log-\narithmic scaling (Tripadvisor in blue, Net\ufb02ix in green, Henry\nSchein in red, Dollar Tree in cyan, Biogen in magenta and\nAdobe Systems in yellow. The average of the last time win-\ndow is omitted, as it can contain only a few spread changes).\nThe black dashed lines are power laws \u221d(t \u2212t0)\u2212\u03b1 with ex-\nponents \u03b1 = 0.4 \u00b1 0.15.\nwith 0 < \u03b1 < 1. Because of the pole at t = t0 we de\ufb01ne\ns(id) for times t with t0 < tl \u2264t \u2264te where te marks\nthe end of the trading day. The ratio s(id)(te \u2212t0)/\u27e8s(id)\u27e9\ndescribes how much the spread at the end of the trading\nday lies below the average. With\n\u27e8s(id)\u27e9=\n1\nte \u2212tl\nte\nZ\ntl\nc(t \u2212t0)\u2212\u03b1dt \u2248s(id)(te \u2212t0)\n1 \u2212\u03b1\n,\n(5)\nwhere we use tl \u2212t0 \u226ate \u2212t0, we \ufb01nd\ns(id)(te \u2212t0)/\u27e8s(id)\u27e9\u2248(1 \u2212\u03b1).\n(6)\nProjecting the rescaled moving average of the spread in\nthe lower panel of Fig. 3 to te \u2212t0 = 390 min, we \ufb01nd a\n\n4\nvalue around 0.6. This result is compatible with our \ufb01nd-\ning for an idealized spread with \u03b1 = 0.4.",
    "chunk_index": 4,
    "start_char": 9951,
    "end_char": 12594,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "te \u2212tl\nte\nZ\ntl\nc(t \u2212t0)\u2212\u03b1dt \u2248s(id)(te \u2212t0)\n1 \u2212\u03b1\n,\n(5)\nwhere we use tl \u2212t0 \u226ate \u2212t0, we \ufb01nd\ns(id)(te \u2212t0)/\u27e8s(id)\u27e9\u2248(1 \u2212\u03b1).\n(6)\nProjecting the rescaled moving average of the spread in\nthe lower panel of Fig. 3 to te \u2212t0 = 390 min, we \ufb01nd a\n\n4\nvalue around 0.6. This result is compatible with our \ufb01nd-\ning for an idealized spread with \u03b1 = 0.4. For the spread\nat the starting time tl shortly after market opening we\n\ufb01nd s(id)(tl\u2212t0)/\u27e8s(id)\u27e9\u2248(1\u2212\u03b1)[(te\u2212t0)/(tl\u2212t0)]\u03b1. Due\nto the pole of s(id), the idealized spread at the market\nopening depends strongly on tl. Furthermore, as there\nare large \ufb02uctuations of real data s(t) for the spread in\nthe period after market opening, the comparison with the\nidealized curve of s(id)(t) at this early elapsed time yields\nlimited information. The scale free time dependence of\nthe spread s(id) also makes it di\ufb03cult to determine a typ-\nical time ts for the closing of the spread, because of the\nscaling\ns(id)(t \u2212t0) = c1\n\u0012 t \u2212t0\nts \u2212t0\n\u0013\u2212\u03b1\n= c2\n\u0014\nt \u2212t0\nk(ts \u2212t0)\n\u0015\u2212\u03b1\n(7)\nwith any positive k, making the choice of ts arbitrary.\nIV.\nDURATION OF THE MARKET OPENING\nPERIOD\nOur \ufb01ndings so far imply that the spread converges\neither to a slowly decaying power law, or to a con-\nstant value of one tick, if it reaches this technical lower\nbound. In both cases the convergence takes place during\na strongly \ufb02uctuating opening period.\nWe now intro-\nduce a method for identifying the time t1 which marks\nthe end of this opening period, based on the moving av-\nerage. We calculate the moving average in a modi\ufb01ed\nway with increased time resolution. As before, the \ufb01rst\ntwo non-overlapping time windows each span 64 spread\nchanges. The third window overlaps the second window,\nas it starts at spread change number 1.1 \u00b7 64 (the closest\ninteger) and ends at spread change number 1.1\u00b7128. The\nnext windows are found in the same way, by increasing\nthe number of spread changes with a factor of 1.1 in each\nstep. The moving average is called \u00afs(t). The time of the\nend of the opening period t1 is de\ufb01ned as the largest time\nt for which \u00afs(t) > 3\u27e8s\u27e9/2. The market opening duration\nis\nT = t1 \u2212t0,\n(8)\naccording to this interpretation.\nThis procedure is illustrated in Fig. 4 for Illumina Inc.\non Monday. The strong \ufb02uctuations of the spread are\nreduced in the moving average. The moving average ends\nbefore the trading day ends, because the largest possible\nvalue of the upper limit of the time window is the end\nof the trading day.\nIf this value is reached, we allow\nthe lower limit to further increas, as long as the time\nwindow spans at least one quarter of all spread changes.\nThe horizontal black line marks the value of 3\u27e8s\u27e9/2, and\nthe vertical line marks t1, where the \u00afs is above 3\u27e8s\u27e9/2\nfor the last time.",
    "chunk_index": 5,
    "start_char": 12256,
    "end_char": 14981,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "before the trading day ends, because the largest possible\nvalue of the upper limit of the time window is the end\nof the trading day.\nIf this value is reached, we allow\nthe lower limit to further increas, as long as the time\nwindow spans at least one quarter of all spread changes.\nThe horizontal black line marks the value of 3\u27e8s\u27e9/2, and\nthe vertical line marks t1, where the \u00afs is above 3\u27e8s\u27e9/2\nfor the last time. We see that the spread still changes\nconsiderably after this time, but slowly.\n0\n50\n100\n150\nt\u2212t0 [min]\n0.0\n0.5\n1.0\n1.5\n2.0\ns [$]\nFIG. 4: Spread of Illumina Inc. on Monday in blue. The black\nsolid curve illustrates the moving average and the dashed hori-\nzontal line is at 3/2 of the average spread. The vertical dashed\nline highlights the time t1.\nIn the upper panel of Fig. 5 the rescaled moving aver-\nage of the spread \u00afs/\u27e8s\u27e9is shown for Activion Blizard, Ya-\nhoo and Autodesk. For Activition Blizard and Yahoo the\nrescaled spread saturates to a value slightly below one.\nThese are large-tick stocks, their spread saturates to one\ntick at the end of the trading day. The vertical dashed\nlines indicate short opening durations T = t1 \u2212t0 of less\nthan \ufb01ve minutes for Activation Blizard and about twelve\nminutes for Yahoo. Afterwards, the spread only changes\nby a small factor below two and saturates fast. The reas-\ncaled spread of Autodesk behaves di\ufb00erently with a late\nt1 about 37 minutes after market opening and a decreas-\ning spread during the whole rest of the trading day. Here\nthe moving average of the spread reduces by more than\na factor of two after t1 which can be understood with\nEq. (6). The longer lasting opening period and the sub-\nstantial change of the spread during the rest of the trad-\ning day is typical for small-tick stocks with a large spread.\nComparing the three stocks, the opening duration varies\nby a factor of eight. In the lower panel of Fig. 5 data is\nprovided for Lam Research Corporation on three di\ufb00er-\nent trading days (Monday, Wednesday and Friday). All\ncurves are similar, and accordingly the times t1 vary by\nless than a factor 3/2 between 31 minutes and 45 min-\nutes. The largest di\ufb00erences between the curves appear\nin the opening period due to large \ufb02uctuations.\nV.\nCOMPARISON ACROSS STOCKS\nWe proceed by calculating market opening durations\nT = t1 \u2212t0 for all stocks, except for Sirius XM Holdings\nInc (ticker SIRI) as the spread of this stock starts close\nto one tick and has a \ufb02at evolution during the trading\nday. By ignoring this stock, we are left with 95 stocks for\nall further investigations. All stocks are listed in Tab. 1",
    "chunk_index": 6,
    "start_char": 14568,
    "end_char": 17159,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "XM Holdings\nInc (ticker SIRI) as the spread of this stock starts close\nto one tick and has a \ufb02at evolution during the trading\nday. By ignoring this stock, we are left with 95 stocks for\nall further investigations. All stocks are listed in Tab. 1\n\n5\n10\n-1\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n0\n10\n1\ns/\n\u00ad\ns\n\u00ae\n10\n-1\n10\n0\n10\n1\n10\n2\nt\u2212t0 [min]\n10\n0\n10\n1\ns/\n\u00ad\ns\n\u00ae\nFIG. 5: Upper panel: Rescaled moving average of the spread\non Friday on a double logarithmic scale (Activion Blizard in\ngreen, Yahoo in blue, Autodesk in red). Vertical dashed lines\nmark the end of the market opening period t1, the horizontal\ndashed line is at 3/2. Lower panel: Rescaled moving aver-\nage of the spread of Lam Research Corporation (Monday in\nyellow, Wednesday in magenta, Friday in cyan).\nin the Supplementary Material.\nIn the upper panel of Fig. 6 we display the probability\ndensity function (PDF) of the opening durations T for\nall considered stocks and weekdays. Opening durations\nspan a surprisingly broad range from about a minute\nto more than one hour.\nWe conclude that the open-\ning phases are very di\ufb00erent for di\ufb00erent stocks.\nThe\ndotted black line indicates the fraction of the PDF orig-\ninating from large-tick stocks with a particularly small\naverage spread (\u27e8s\u27e9< 0.0115 $ with the spread being av-\neraged over all \ufb01ve trading days). These 12 stocks (tick-\ners AMAT, CSCO, EBAY, FOXA, INTC, MSFT, MU,\nSPLS, SYMC, VIP, VOD and YHOO) lead to small open-\ning durations. This result can be understood from Fig. 5:\nFor such stocks the opening period is short because the\nspread saturates fast to a value close to one tick. Still,\nthe stocks di\ufb00er with respect to the time scale, on which\nthe saturation takes place. With the red dashed line in\nFig. 6 we learn that the 83 stocks with larger spread have\n10\n0\n10\n1\n10\n2\nT\n0.00\n0.01\n0.02\n0.03\n0.04\nPDF\n10\n0\n10\n1\n10\n2\nT\n0.00\n0.01\n0.02\n0.03\n0.04\nPDF\nFIG. 6: Upper panel: PDF of market opening durations on a\nlogarithmic scale with the solid line. The fraction of opening\ndurations connected to large-tick stocks with smallest spread\nis highlighted with the dotted black line, the fraction con-\nnected to all other stocks with the red dashed line. Lower\npanel: Same for opening durations averaged over \ufb01ve trading\ndays for every stock individually.\nlarger opening durations. The area under the red dashed\ncurve is more than six times larger than the area under\nthe black dotted curve. The di\ufb00erent optical appearance\noriginates from logarithmic scaling.\nAveraged over the \ufb01ve working days, Fig. 6 (lower\npanel) shows that even averaged durations span a large\nrange of values, and the distinction between large-tick\nstocks and small-tick stocks stays the same. Averaged\ndurations for individual stocks are listed in Tab. 1 in the\nSupplementary Material.\nVI.",
    "chunk_index": 7,
    "start_char": 16914,
    "end_char": 19688,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "larger opening durations. The area under the red dashed\ncurve is more than six times larger than the area under\nthe black dotted curve. The di\ufb00erent optical appearance\noriginates from logarithmic scaling.\nAveraged over the \ufb01ve working days, Fig. 6 (lower\npanel) shows that even averaged durations span a large\nrange of values, and the distinction between large-tick\nstocks and small-tick stocks stays the same. Averaged\ndurations for individual stocks are listed in Tab. 1 in the\nSupplementary Material.\nVI.\nCOMPARISON WITH THE REST OF THE\nTRADING DAY\nWe now analyze, how the order \ufb02ow during the market\nopening period di\ufb00ers from the order \ufb02ow during the rest\nof the trading day. For a given stock and trading day,\nwe use the opening duration T and de\ufb01ne three time\nintervals, each of duration T. The \ufb01rst interval spans the\nopening period from t0 = 9:30 am to t1 = t0 + T. The\n\n6\nsecond interval starts at noon. It spans the time interval\nfrom t2 = 12 am to t3 = t2 + T. The third interval spans\nthe time from t4 = 2 pm to t5 = t4 + T.\nAs a price\nvolatility measure, we consider the standard deviation \u03c3\nof one minute returns\n\u03c3 =\nq\n\u27e8r2\n1 min\u27e9\u2212\u27e8r1 min\u27e92,\n(9)\nr1 min = m(t + 1 min) \u2212m(t)\nm(t)\n,\n(10)\nwith the time average \u27e8r1 min\u27e9ranging over the time in-\nterval under consideration.\nBased on the values \u03c3 for\nall stocks, trading days and time intervals, we compute\nseparate PDFs for the opening period, the time inter-\nval starting at noon and the time interval starting at 2\npm. As Fig. 7 displays, the price volatility is much larger\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\n\u03c3\n0\n1000\n2000\n3000\n4000\nPDF\nFIG. 7: PDF of one minute return standard deviation dur-\ning the opening (green), after noon (red) and after 2:00 pm\n(black).\nfor the opening period than for the later time intervals.\nA possible explanation are news which arrive over night\nand have to be included in the price during the opening\nperiod.\nAs can be inferred from Fig. 8, the order \ufb02ow dur-\ning the opening period di\ufb00ers from the rest of the day.\nWe calculate for every stock, weekday and time inter-\nval the share of in-spread limit orders among all limit\norders. In-spread limit orders are limit orders that are\nplaced between best ask price and best bid price and\ntherefore change the quote. For the opening period, we\ncalculate the PDF of in-spread limit order shares across\nall stocks and weekdays. Analog results are shown for the\nsecond and third time window. The trading at opening\nhas larger shares of limit orders that change the quotes\nthan during the rest of the trading day.\nVII.\nDISCUSSION\nWe compared properly smoothed and rescaled time se-\nries of the spread of stocks traded on NASDAQ. We found\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nshare\n0\n10\n20\n30\nPDF\nFIG.",
    "chunk_index": 8,
    "start_char": 19181,
    "end_char": 21917,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "the PDF of in-spread limit order shares across\nall stocks and weekdays. Analog results are shown for the\nsecond and third time window. The trading at opening\nhas larger shares of limit orders that change the quotes\nthan during the rest of the trading day.\nVII.\nDISCUSSION\nWe compared properly smoothed and rescaled time se-\nries of the spread of stocks traded on NASDAQ. We found\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nshare\n0\n10\n20\n30\nPDF\nFIG. 8: PDF of the share of in-spread limit orders among all\nlimit orders during the opening (green), after noon (red) and\nafter 2:00 pm (black).\nthat the data collapses to a power law with small neg-\native exponent for the majority of the stocks, namely\nall 66 large-tick stocks. This means that the spread is\nnon-stationary during the whole trading day and closes\nslowly. We conclude that the market converges to a non-\nstationary emergent state which follows a stylized time\ndependence.\nOnly if the spread reaches the technical lower bound of\none tick, it stays constant for the rest of the trading day.\nThis behavior was found for a minority of 30 small-tick\nstocks. As was known already from other studies, the\norder \ufb02ow of small-tick stocks and large-tick stocks has\nmajor di\ufb00erences [28, 29]. Here we found a complemen-\ntary aspect of this fact. The technical constraint keeps\nthe market for large-tick stocks from further adjusting\nthe spread during the trading day, which would be the\nnatural non-stationary emergent behavior otherwise.\nOur \ufb01ndings have implications for data analysis of the\norder \ufb02ow. If the market opening is excluded from the\ndata analysis, this should be done individually for every\nstock. There are also implications on agent based mod-\neling. The order \ufb02ow should be modeled with a distinct\nopening phase, during which the order book is \ufb01lled and\nthe spread stabilizes. Further the non-stationarity during\nthe rest of the trading day has to be accounted for. These\nresults have to be included when setting up agent-based\nmodels for stock markets.\nWe further elaborated on the highly \ufb02uctuating open-\ning period during which the spread is large and the scal-\ning behavior emerges. The end of this period was iden-\nti\ufb01ed as the time where the smoothed spread falls below\ntwo thirds of its average.\nSurprisingly, we found that\nopening durations vary from a few minutes to more than\none hour. The largest di\ufb00erences appear between large-\ntick stocks with particularly small spread, which reach\nthe lower bound of one tick very quickly, and small-tick\nstocks. In a second step, we used the individual durations\nfor characterizing the order \ufb02ow during the opening pe-\n\n7\nriod across the whole market. We compared with later\ntrading periods around noon and in the early afternoon.\nThe price and order \ufb02ow behaves distinctly di\ufb00erent dur-\ning the opening period than later. The volatility is much\nlarger, and the share of limit orders being placed be-\ntween best buy and best sell o\ufb00er is much larger during\nthe opening period.",
    "chunk_index": 9,
    "start_char": 21477,
    "end_char": 24464,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "one tick very quickly, and small-tick\nstocks. In a second step, we used the individual durations\nfor characterizing the order \ufb02ow during the opening pe-\n\n7\nriod across the whole market. We compared with later\ntrading periods around noon and in the early afternoon.\nThe price and order \ufb02ow behaves distinctly di\ufb00erent dur-\ning the opening period than later. The volatility is much\nlarger, and the share of limit orders being placed be-\ntween best buy and best sell o\ufb00er is much larger during\nthe opening period. In Summary, these results give a \ufb01rst\nexample of how stylized facts emerge during the opening\nof stock markets.\n[1] B. B. Mandelbrot, in Fractals and scaling in \ufb01nance\n(Springer, 1997), pp. 371\u2013418.\n[2] R. N. Mantegna and H. E. Stanley, Physica A: Statistical\nMechanics and its Applications 239, 255 (1997).\n[3] M. C. M\u00a8unnix, T. Shimada, R. Sch\u00a8afer, F. Leyvraz, T. H.\nSeligman, T. Guhr, and H. E. Stanley, Scienti\ufb01c reports\n2, 644 (2012).\n[4] K. H. Chung and A. J. Lee, Asia-Paci\ufb01c Journal of Fi-\nnancial Studies 45, 7 (2016).\n[5] B. Rosenow, P. Gopikrishnan, V. Plerou, and H. E. Stan-\nley, Physica A: Statistical Mechanics and its Applications\n324, 241 (2003).\n[6] R. Cont, in Long memory in economics (Springer, 2007),\npp. 289\u2013309.\n[7] R. N. Mantegna and H. E. Stanley, Nature 376, 46\n(1995).\n[8] X. Gabaix, P. Gopikrishnan, V. Plerou, and H. E. Stan-\nley, Nature 423, 267 (2003).\n[9] J.-P. Bouchaud, Quantitative Finance 1, 105 (2001).\n[10] V. Plerou, P. Gopikrishnan, L. A. N. Amaral, M. Meyer,\nand H. E. Stanley, Physical review e 60, 6519 (1999).\n[11] S. Dro\u02d9zd\u02d9z, M. Forczek, J. Kwapie\u00b4n, P. O\u00b4swie, R. Rak,\net al., Physica A: Statistical Mechanics and its Applica-\ntions 383, 59 (2007).\n[12] Y. Liu, P. Gopikrishnan, H. E. Stanley, et al., Physical\nreview e 60, 1390 (1999).\n[13] M. S. Roze\ufb00and W. R. Kinney Jr, Journal of \ufb01nancial\neconomics 3, 379 (1976).\n[14] T. Y. Chang, S. M. Hartzmark, D. H. Solomon, and E. F.\nSoltes, The Review of Financial Studies 30, 281 (2016).\n[15] J. Francis, D. Pagach, and J. Stephan, Journal of Ac-\ncounting Research pp. 165\u2013184 (1992).\n[16] R. A. Ariel, The Journal of Finance 45, 1611 (1990).\n[17] C. Brooks and G. Persand, Applied Economics Letters\n8, 155 (2001).\n[18] W. A. Brock and A. W. Kleidon, Journal of Economic\nDynamics and Control 16, 451 (1992).\n[19] J. Clara-Rahola, A. M. Puertas, M. A. Sanchez-Granero,\nJ. E. Trinidad-Segovia, and F. J. de las Nieves, Physical\nreview letters 118, 068301 (2017).\n[20] L. Bauwens and P. Giot, Econometric modelling of stock\nmarket intraday activity, vol. 38 (Springer Science &\nBusiness Media, 2013).\n[21] R. Gen\u00b8cay, F. Sel\u00b8cuk, and B. Whitcher, Physica A: Sta-\ntistical Mechanics and its Applications 289, 543 (2001).\n[22] I. Giardina, J.-P. Bouchaud, and M. M\u00b4ezard, Physica\nA: Statistical Mechanics and its Applications 299, 28\n(2001).\n[23] S. M. Krause and S. Bornholdt, Physica A: Statistical\nMechanics and its Applications 392, 4048 (2013).",
    "chunk_index": 10,
    "start_char": 23954,
    "end_char": 26905,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "Nieves, Physical\nreview letters 118, 068301 (2017).\n[20] L. Bauwens and P. Giot, Econometric modelling of stock\nmarket intraday activity, vol. 38 (Springer Science &\nBusiness Media, 2013).\n[21] R. Gen\u00b8cay, F. Sel\u00b8cuk, and B. Whitcher, Physica A: Sta-\ntistical Mechanics and its Applications 289, 543 (2001).\n[22] I. Giardina, J.-P. Bouchaud, and M. M\u00b4ezard, Physica\nA: Statistical Mechanics and its Applications 299, 28\n(2001).\n[23] S. M. Krause and S. Bornholdt, Physica A: Statistical\nMechanics and its Applications 392, 4048 (2013).\n[24] S. M. Krause and S. Bornholdt, Physical Review E 86,\n056106 (2012).\n[25] V. Filimonov and D. Sornette, Physical Review E 85,\n056108 (2012).\n[26] T. Braun, J. A. Fiegen, D. C. Wagner, S. M. Krause, and\nT. Guhr, PloS one 13, e0196920 (2018).\n[27] R. I. Webb, D. Ryu, D. Ryu, and J. Han, Emerging Mar-\nkets Review 26, 80 (2016).\n[28] A. Gareche,\nG. Disdier,\nJ. Kockelkoren,\nand J.-P.\nBouchaud, Physical Review E 88, 032809 (2013).\n[29] K. Dayri and M. Rosenbaum, Market Microstructure and\nLiquidity 1, 1550003 (2015).\n[30] M. Theissen, S. M. Krause, and T. Guhr, The European\nPhysical Journal B 90, 218 (2017).\n[31] http://tradingphysics.com/, accessed:\nSeptember 14,\n2016.\n[32] https://en.wikipedia.org/wiki/NASDAQ 100, accessed:\nSeptember 28, 2016.\n[33] R. Huang and T. Polak, Available at SSRN 1977207\n(2011).\n[34] Z. Eisler, J.-P. Bouchaud, and J. Kockelkoren, Quantita-\ntive Finance 12, 1395 (2012).\n\n8\nSupplementary marerial to\nEmergence of stylized facts during the opening of stock markets\nSebastian M. Krause, Jonas A. Fiegen and Thomas Guhr\nTicker\nT [min] Q\nAAL*\n19.7\n1.2\nAAPL*\n10.0\n2.2\nADBE\n27.4\n2.2\nADI\n28.1\n1.5\nADP\n32.8\n2.3\nADSK\n32.7\n1.4\nAKAM\n35.0\n2.1\nALXN\n25.2\n2.2\nAMAT*\n3.1\n3.5\nAMGN\n36.1\n2.4\nAMZN\n21.5\n2.0\nATVI*\n14.4\n1.5\nAVGO\n39.0\n1.9\nBBBY\n24.1\n2.1\nBIDU\n28.3\n2.6\nBIIB\n32.6\n1.7\nBMRN\n29.4\n1.7\nCA*\n12.7\n1.4\nCELG\n36.5\n1.7\nCERN\n31.9\n5.1\nCHKP\n31.9\n1.7\nCHRW\n30.5\n2.9\nCHTR\n34.4\n1.7\nCMCSA*\n10.3\n2.0\nCOST\n30.1\n2.1\nCSCO*\n1.3\n5.8\nCTSH\n22.8\n1.3\nCTXS\n42.0\n1.7\nDISCA*\n17.8\n1.3\nDISH\n28.8\n1.7\nDLTR\n33.6\n1.7\nEA\n30.7\n2.0\nTicker\nT [min] Q\nEBAY*\n4.1\n3.8\nEQIX\n27.4\n2.5\nESRX\n28.1\n3.1\nEXPD\n35.7\n2.6\nFAST\n42.6\n2.2\nFB\n19.2\n2.0\nFISV\n37.4\n2.0\nFOXA*\n6.7\n5.1\nGILD\n23.9\n2.5\nGOOG\n22.1\n3.0\nGRMN\n33.7\n2.3\nHSIC\n34.7\n4.1\nILMN\n24.4\n2.3\nINTC*\n2.5\n2.3\nINTU\n32.0\n2.6\nISRG\n21.4\n1.9\nJD*\n10.9\n2.7\nKHC\n42.2\n4.4\nKLAC*\n28.3\n1.7\nLBTYA*\n15.0\n1.9\nLLTC\n19.4\n1.4\nLMCA\n34.5\n2.0\nLRCX\n34.3\n1.7\nLVNTA\n34.7\n1.8\nMAR\n31.1\n2.3\nMAT*\n14.6\n1.2\nMDLZ*\n14.0\n2.4\nMNST\n32.9\n2.1\nMSFT*\n4.0\n4.8\nMU*\n4.3\n4.5\nMYL\n20.9\n1.9\nNFLX\n30.9\n1.2\nTicker\nT [min]\nQ\nNTAP*\n13.5\n2.5\nNVDA*\n12.9\n1.6\nNXPI\n35.4\n2.8\nORLY\n49.0\n1.7\nPAYX\n35.5\n2.5\nPCAR\n35.8\n1.7\nPCLN\n35.2\n10.2\nQCOM*\n13.9\n2.4\nREGN\n19.7\n3.9\nROST\n28.3\n1.6\nSBAC\n31.4\n1.9\nSBUX*\n16.8\n1.4\nSNDK\n22.4\n2.0\nSPLS*\n4.9\n4.1\nSRCL\n38.6",
    "chunk_index": 11,
    "start_char": 26370,
    "end_char": 29114,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "34.5\n2.0\nLRCX\n34.3\n1.7\nLVNTA\n34.7\n1.8\nMAR\n31.1\n2.3\nMAT*\n14.6\n1.2\nMDLZ*\n14.0\n2.4\nMNST\n32.9\n2.1\nMSFT*\n4.0\n4.8\nMU*\n4.3\n4.5\nMYL\n20.9\n1.9\nNFLX\n30.9\n1.2\nTicker\nT [min]\nQ\nNTAP*\n13.5\n2.5\nNVDA*\n12.9\n1.6\nNXPI\n35.4\n2.8\nORLY\n49.0\n1.7\nPAYX\n35.5\n2.5\nPCAR\n35.8\n1.7\nPCLN\n35.2\n10.2\nQCOM*\n13.9\n2.4\nREGN\n19.7\n3.9\nROST\n28.3\n1.6\nSBAC\n31.4\n1.9\nSBUX*\n16.8\n1.4\nSNDK\n22.4\n2.0\nSPLS*\n4.9\n4.1\nSRCL\n38.6\n1.8\nSTX\n23.4\n1.8\nSYMC*\n5.3\n4.7\nTRIP\n35.4\n1.6\nTSCO\n34.7\n1.7\nTSLA\n37.8\n6.7\nTXN*\n18.9\n1.3\nVIAB\n30.7\n2.2\nVIP*\n3.8\n3.2\nVOD*\n2.3\n4.2\nVRSK\n40.2\n2.0\nVRTX\n35.8\n1.7\nWDC\n30.2\n1.4\nWFM*\n23.0\n1.8\nWYNN\n41.9\n1.5\nXLNX\n24.7\n2.5\nYHOO*\n6.6\n5.0\nTABLE I: Stocks analyzed. Tickers of large-tick stocks, where the moving average of the spread approaches one tick, are marked\nwith a star. Opening period duration T averaged over \ufb01ve trading days and quotient Q between largest opening duration and\nsmallest opening duration of a stock.\nIn Tab. I the opening durations T averaged for every stock over all \ufb01ve working days are listed for all considered\nstocks. The quotient Q between the maximal opening duration and the minimal opening duration is also listed. For\nPriceline Group Inc. (ticker PCLN) this quotient is larger than ten, meaning that opening durations are very diverse\nfor this stock. For most other stocks, the quotient Q is much smaller.",
    "chunk_index": 12,
    "start_char": 28740,
    "end_char": 30041,
    "paper_title": "Emergence of stylized facts during the opening of ",
    "paper_category": "q-fin.TR",
    "paper_filename": "Emergence_of_stylized_facts_during_the_opening_of_.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Emergence_of_stylized_facts_during_the_opening_of_.pdf"
  },
  {
    "text": "arXiv:0904.4131v2 [q-fin.TR] 11 Jan 2010\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET\nMODEL\nALEXANDER WEISS\nAbstract. In a recent paper, Alfonsi, Fruth and Schied (AFS) propose a\nsimple order book based model for the impact of large orders on stock prices.\nThey use this model to derive optimal strategies for the execution of large\norders.\nWe apply these strategies to an agent-based stochastic order book\nmodel that was recently proposed by Bovier, \u02c7Cern\u00b4y and Hryniv, but already\nthe calibration fails. In particular, from our simulations the recovery speed of\nthe market after a large order is clearly dependent on the order size, whereas\nthe AFS model assumes a constant speed.\nFor this reason, we propose a\ngeneralization of the AFS model, the GAFS model, that incorporates this\ndependency, and prove the optimal investment strategies. As a corollary, we\n\ufb01nd that we can derive the \u201ccorrect\u201d constant resilience speed for the AFS\nmodel from the GAFS model such that the optimal strategies of the AFS and\nthe GAFS model coincide.\nFinally, we show that the costs of applying the\noptimal strategies of the GAFS model to the arti\ufb01cial market environment\nstill di\ufb00er signi\ufb01cantly from the model predictions, indicating that even the\nimproved model does not capture all of the relevant details of a real market.\n1. Introduction\nFor a long time, \ufb01nancial mathematics mainly focused on asset pricing, but the\nscope has been extended in the last years. One of the current topics of interest is\nthe theory of optimal trading strategies for the execution of large orders. Here, a\ntrader would like to purchase1 a huge volume of shares up to time T . Since the\nsupply of limit orders for a certain price is limited, the trader will not be able to\ntrade the whole order for the current price, but he or she will su\ufb00er from an adverse\nprice movement. This additional price impact, induced by the trader\u2019s own trading,\ncan be lessened if he or she gives the market time to recover; the best price returns\nto previous levels. However, the time interval [0, T ] is assumed to be too short in\norder to wait for a full recovery of the market. The optimal execution problem asks\nfor the optimal splitting and the optimal trading times to minimize the expected\nprice impact.\nThere have been several models to solve the optimal execution problem, moti-\nvated by empirical \ufb01ndings (for references see next paragraph); yet, since we do\nnot know if these models capture all relevant features of real markets, we cannot\nbe sure that the strategies work in reality, and tests on real markets would be an\nDate: March 13, 2022.\n2000 Mathematics Subject Classi\ufb01cation. 91B24 62P05.\nKey words and phrases. market micro structure, illiquid markets, optimal trading strategies.\nWork supported by the DFG Research Center MATHEON.\n1In this article, we focus on a trader purchasing shares, since all models mentioned here either\nconsider only this part of the problem or work symmetrically for buyers and sellers.\n1",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 2987,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "of real markets, we cannot\nbe sure that the strategies work in reality, and tests on real markets would be an\nDate: March 13, 2022.\n2000 Mathematics Subject Classi\ufb01cation. 91B24 62P05.\nKey words and phrases. market micro structure, illiquid markets, optimal trading strategies.\nWork supported by the DFG Research Center MATHEON.\n1In this article, we focus on a trader purchasing shares, since all models mentioned here either\nconsider only this part of the problem or work symmetrically for buyers and sellers.\n1\n\n2\nALEXANDER WEISS\nexpensive experiment. For this reason, microscopic market models are an excellent\ntool for testing theoretical models of optimal trading strategies. Based on assump-\ntions about the market participants\u2019 behavior, these models simulate the trading\nof \ufb01nancial assets on the level of single traders or orders [GB03, SFGK03, B\u02c7CH06].\nThe emerging price processes show typical features of real markets [Con01, PB03,\nATHL05]. Hence, microscopic models provide arti\ufb01cial, yet reasonable, market en-\nvironments that allow for applying optimal trading strategies without costs or risk,\ncomparing the numerical results with the theoretical expectations and resolving de-\nviations by an improvement of the underlying market assumptions with respect to\nthe empirical \ufb01ndings. In this paper, we exemplify how this approach can improve\na solution for the optimal execution problem.\nAll approaches to the optimal execution problem rely on two empirical \ufb01ndings\nthat have been validated in many studies (see [Sch08], pp. 3, for a list of references):\nFirst, a large order has an impact on its price; second, this impact decreases in time,\nbut it does not vanish completely. That implies the costs of all subsequent orders\nare in\ufb02uenced by the impact of a large order. These two e\ufb00ects are called tempo-\nrary and permanent impact. Many models implement these observations straightly\n[BL98, AC01, HS05]: They consider a stochastic process that simulates the current\nbest price evolving independently from the large trader\u2019s action in time, and two\nfunctions mapping the volume of a large order to the temporary or, respectively,\npermanent impact. When a large order is executed, the corresponding impacts are\njust added to the price. Yet, it is doubtful if the complex dynamics of limit order\nbooks (LOB), which underlie most modern markets, can be captured by looking at\nthe best price only. Therefore, recent models attempt to take the dynamics of the\nwhole order book into account. Obizhaeva and Wang introduced a model with an\nunderlying block shaped LOB and calculated the optimal trading strategy in terms\nof a recursive formula by applying Bellman equations [OW05]. Alfonsi, Fruth and\nSchied introduced a generalization of this model for general order book shapes and\ngave an explicit solution for the optimal trading strategy with respect to their mar-\nket model (introduced in [AFS09] and revisited in [AS09]); this model is the one\nwe will test in a microscopic market environment, and we refer to it as the AFS\nmodel.\nThe AFS model describes the underlying market by two parameters: The shape\nof the (continuous) LOB given by a shape function f and a positive constant \u03c1\nexpressing the resilience speed of the order book.",
    "chunk_index": 1,
    "start_char": 2475,
    "end_char": 5722,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "Alfonsi, Fruth and\nSchied introduced a generalization of this model for general order book shapes and\ngave an explicit solution for the optimal trading strategy with respect to their mar-\nket model (introduced in [AFS09] and revisited in [AS09]); this model is the one\nwe will test in a microscopic market environment, and we refer to it as the AFS\nmodel.\nThe AFS model describes the underlying market by two parameters: The shape\nof the (continuous) LOB given by a shape function f and a positive constant \u03c1\nexpressing the resilience speed of the order book. There are two versions of the\nmodel: In the \ufb01rst one, the consumed volume recovers exponentially fast; in the\nsecond version, the best price recovers in this way. The shape of the order book is\nstatic such that there is a bijection between the impact on the best price and on\nthe volume. Thus, the response of the order book to the execution of a large order\ndepends on the current price impact only, but not on possible executions before.\nTo test the optimal AFS strategies, we need to select a microscopic market\nmodel. The model that serves best as virtual market environment was introduced\nby Bovier, \u02c7Cerny and Hryniv and is called the Opinion Game [B\u02c7CH06]. It simulates\na family of traders on the level of a generalized order book. The underlying idea of\nthe generalized order book is that every trader has an individual, subjective opinion\nabout the current fair price. Instead of orders, the generalized order book records\nthese opinions. Thus, it also captures traders who are willing to trade for a price\nclose to the best quotes but have not placed public orders (in some markets it is also\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n3\npossible to place hidden or partially visible orders [FS09, BPV09]). These traders\no\ufb00er hidden liquidity; they will in\ufb02uence the price impact when an order is executed\nbut do not appear in the order book [WR05]. Thus, the Opinion Game provides a\nmore realistic market response to orders than classical order book models.\nIn order to apply the AFS strategies in the Opinion Game, we have to determine\nthe correct values for f and \u03c1. There are several problems to \ufb01nd the value for \u03c1.\nFirst, the AFS model does not assume a permanent impact; second, the market\nrecovery is only poorly approximated by an exponential function; third, \u03c1 does not\nexist as a constant value but depends on the traded volume. While the \ufb01rst two\nitems can be bypassed, the third item strongly con\ufb02icts with the assumptions of the\nAFS model. For this reason we introduce a generalization of the AFS model that\nwe call the generalized AFS model or GAFS model. The GAFS model substitutes\n\u03c1 by \u00af\u03c1 that is a function of an order\u2019s price impact or volume impact, depending\non the model version. Furthermore, we extend the results of the AFS model by\nproving that there exists a unique, deterministic optimal trading strategy for the\nGAFS model.",
    "chunk_index": 2,
    "start_char": 5163,
    "end_char": 8095,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "the \ufb01rst two\nitems can be bypassed, the third item strongly con\ufb02icts with the assumptions of the\nAFS model. For this reason we introduce a generalization of the AFS model that\nwe call the generalized AFS model or GAFS model. The GAFS model substitutes\n\u03c1 by \u00af\u03c1 that is a function of an order\u2019s price impact or volume impact, depending\non the model version. Furthermore, we extend the results of the AFS model by\nproving that there exists a unique, deterministic optimal trading strategy for the\nGAFS model. It turns out that, although \u00af\u03c1 is a function, the optimal strategy\nevaluates it for one value only. Consequently, the optimal strategies of the AFS\nand the GAFS models coincide when \u03c1 is chosen to be this value. In this sense, the\nAFS model is also su\ufb03cient for the order impact dependent case, but the GAFS\nmodel is needed to calibrate it correctly.\nAfter calibrating the (G)AFS model to the Opinion Game, we calculate the op-\ntimal strategies for several parameter sets, apply these strategies to the Opinion\nGame, and sample their impact costs. On a general level, the sampled costs show\nthe expected natural behavior; for instance, the costs decrease if the available trad-\ning time T or the number of trading opportunities within [0, T ] become larger.\nFurthermore, the simulations reinforce the advantages of the GAFS model com-\npared to the AFS model. We show that the AFS model performs worse than the\nGAFS model for a bad, yet reasonable, choice of the value for \u03c1. On the other hand,\nwe \ufb01nd that, in comparison to the predicted costs, the sampled costs of the GAFS\nstrategies are up to four times higher. This shows that the (G)AFS model does not\ncapture all relevant details of the Opinion Game\u2019s order book dynamics, indicating\nthat the optimal (G)AFS strategies could also perform worse than theoretically\nexpected on real markets.\nIn Section 2, we introduce the AFS model and restate its optimal trading strate-\ngies. In Section 3, we present this version of the Opinion Game that we used to\nanalyze the AFS model. In Section 4, we determine f and \u03c1 in the Opinion Game,\nwhich leads to the GAFS model. Finally, in Section 5, we apply the GAFS optimal\nstrategies in the Opinion Game, and compare the resulting costs for several param-\neter sets. Furthermore, we show that the GAFS strategies perform better than the\nAFS strategies with an suboptimal choice of \u03c1 in the Opinion Game.\n2. The market model of Alfonsi, Fruth and Schied and its optimal\nexecution strategies\nA trader would like to purchase X0 > 0 shares within a time period [0, T ], T > 0.\nX0 is assumed to be large such that the trader\u2019s order has an impact on the price\nand the underlying limit order book. We will refer to this trader as large trader\nin the following. Because we consider a buy order, we \ufb01rst de\ufb01ne how the upper\npart of the LOB, which contains the sell limit orders, is modeled. As long as the",
    "chunk_index": 3,
    "start_char": 7590,
    "end_char": 10485,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "model of Alfonsi, Fruth and Schied and its optimal\nexecution strategies\nA trader would like to purchase X0 > 0 shares within a time period [0, T ], T > 0.\nX0 is assumed to be large such that the trader\u2019s order has an impact on the price\nand the underlying limit order book. We will refer to this trader as large trader\nin the following. Because we consider a buy order, we \ufb01rst de\ufb01ne how the upper\npart of the LOB, which contains the sell limit orders, is modeled. As long as the\n\n4\nALEXANDER WEISS\nFigure 1. The order book of the AFS model. For simplicity, we\nhave left o\ufb00the time index t.\nlarge trader does not take action, the LOB is described by the una\ufb00ected best ask\nprice A0 := (A0\nt )t\u22650 and by a shape function f : R \u2192(0, \u221e) (see Figure 1). A0\nis a martingale on a given \ufb01ltered probability space (\u2126, (Ft)t\u22650, F, P) satisfying\nA0\n0 = A0 for some A0 \u2208R; f is a continuous function.\nThe amount of shares\navailable for a price A0\nt + x, x \u22650, at time t is then given by f(x)dx. Notice that\nthe shape of the order book with respect to the best ask price is static.\nNow, assume the large trader acts for the \ufb01rst time and purchases x0 shares at\ntime t0; he or she consumes all shares between A0\nt0 and A0\nt0 + DA\nt0+, DA\nt0+ being\nuniquely determined by\n(2.1)\nZ DA\nt0+\n0\nf(x)dx = x0.\nDA := (DA)t\u22650 is called the extra spread caused by the large trader. In general, if\nwe know DA\ntn for a trading time tn, DA\ntn+ is given by\n(2.2)\nZ DA\ntn+\nDA\ntn\nf(x)dx = xn\nwhereby xn is the amount of shares traded at time tn. The large trader is inactive\nbetween two trading times, tn and tn+1, and the extra spread recovers. For the\nexact way of recovery there are two versions considered. To conform to the notation\nof [AFS09], we \ufb01rst state Version 2. In this case, DA\nt is de\ufb01ned for t \u2208(tn, tn+1] by\n(2.3)\nDA\nt := e\u2212\u03c1(t\u2212tn)DA\ntn+.\nThe parameter \u03c1 is a positive constant called the resilience speed. To complete the\nde\ufb01nition, we set DA\nt := 0 for t \u2264t0. Now, we can introduce the best ask price\nA := (At)t\u22650 by\n(2.4)\nAt := A0\nt + DA\nt .\nIn contrast to A0, A includes the large trader\u2019s impact. In particular, the amount\nof shares available for a price A0\nt + x at time t is given by\n(2.5)\n\u001a f(x)dx\nfor x \u2265At \u2212A0\nt\n0\notherwise\n.\nIn other words, every trader in the market experiences the large trader\u2019s impact\nafter time t0.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n5\nThe price impact DA can also be expressed in terms of the impact on the volume\nEA := (EA\nt )t\u22650.",
    "chunk_index": 4,
    "start_char": 10006,
    "end_char": 12476,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "includes the large trader\u2019s impact. In particular, the amount\nof shares available for a price A0\nt + x at time t is given by\n(2.5)\n\u001a f(x)dx\nfor x \u2265At \u2212A0\nt\n0\notherwise\n.\nIn other words, every trader in the market experiences the large trader\u2019s impact\nafter time t0.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n5\nThe price impact DA can also be expressed in terms of the impact on the volume\nEA := (EA\nt )t\u22650. Because the shape function f is strictly positive, there is a one-\nto-one relation between EA and DA. Given DA, the process EA is de\ufb01ned by\n(2.6)\nEA\nt :=\nZ DA\nt\n0\nf(x)dx.\nWe can generally introduce the antiderivative of f,\n(2.7)\nF(x) :=\nZ x\n0\nf(x)dx,\nto get the relations\n(2.8)\nEA\nt = F(DA\nt )\nand\nDA\nt = F \u22121(EA\nt ).\nBy (2.2) and (2.8), we easily conclude\n(2.9)\nEA\ntn+ = EA\ntn + xn.\nThis motivates to de\ufb01ne Version 1, in which we \ufb01rst de\ufb01ne EA and then derive DA\nby relation (2.8). We set EA\nt := 0 for t \u2208[0, t0] and\n(2.10)\nEA\nt := e\u2212\u03c1(t\u2212tn)EA\ntn+, t \u2208(tn, tn+1].\nThe equations (2.9) and (2.10) de\ufb01ne EA completely.\nSummarizing, we have introduced two versions of the AFS model: In Version\n1, we de\ufb01ne the volume impact EA and assume that it recovers exponentially fast\nbetween the large trader\u2019s orders. DA is then derived from EA by relation (2.8); in\nVersion 2, we \ufb01rst de\ufb01ne the price impact DA, assume an exponentially fast recovery\nand derive EA from it. Observe that the AFS model recovers completely as the time\ntends to in\ufb01nity (and if no more large orders are executed after some \ufb01nite time);\nthere is no permanent impact. Furthermore, the assumption of an exponential decay\nof the price impact is under discussion in the scienti\ufb01c community. An empirical\nstudy in [BGPW04] suggests a power-law decay. On a theoretic level, Gatheral\nproved for a market model similar to the AFS model that an exponential decay\ncan easily imply arbitrage opportunities while power-law decays do not show this\nundesired property [Gat09].\nAlfonsi and Schied, however, were able to show in\n[AS09] that, despite of the similarity to Gatheral\u2019s model, the AFS model does not\ngive arbitrage opportunities (under mild assumptions concerning the shape of the\norder book). The \ufb01nal answer to the question how the decay of the price impact\nis modeled best has not been given yet. As far as we know, the same question for\nthe volume impact has not been treated.\nWe cannot exclude a priori that it is reasonable to sell shares and to buy them\nback later. Thus, we also have to model the impact of (large) sell orders on the\nLOB. Such orders will be written as orders with negative sign. Let B0 = (B0\nt )t\u22650\nbe the una\ufb00ected best bid price with\n(2.11)\nB0\nt \u2264A0\nt for all t \u22650\nas only constraint for its dynamics.",
    "chunk_index": 5,
    "start_char": 12058,
    "end_char": 14770,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "yet. As far as we know, the same question for\nthe volume impact has not been treated.\nWe cannot exclude a priori that it is reasonable to sell shares and to buy them\nback later. Thus, we also have to model the impact of (large) sell orders on the\nLOB. Such orders will be written as orders with negative sign. Let B0 = (B0\nt )t\u22650\nbe the una\ufb00ected best bid price with\n(2.11)\nB0\nt \u2264A0\nt for all t \u22650\nas only constraint for its dynamics. The lower part of the LOB is modeled by the\nshape function f on the negative part of its domain. More precisely, the number\nof bids for the price B0\nt + x, x < 0, is given by f(x)dx. As before, we can now\n\n6\nALEXANDER WEISS\nintroduce the extra spread DB := (DB\nt )t\u22650. Given a sell order xn < 0, a trading\ntime tn, and DB\ntn, DB\ntn+ is implicitly de\ufb01ned by\n(2.12)\nZ DB\ntn+\nDB\ntn\nf(x)dx = xn.\nNote that DB is non-positive. We equivalently de\ufb01ne the impact on the volume\nEB := (EB\nt )t\u22650 by\n(2.13)\nEB\ntn+ := EB\ntn + xn.\nEB is also non-positive, and its connection to DB is again given by (2.8).\nTo\ncomplete the de\ufb01nitions for sell orders, we set DB\nt := 0 and EB\nt := 0 for all t \u2264t0,\nand\n(2.14)\n\uf8f1\n\uf8f2\n\uf8f3\nEB\nt := e\u2212\u03c1(t\u2212tn)EB\ntn+\nfor Version 1\nDB\nt := e\u2212\u03c1(t\u2212tn)DB\ntn+\nfor Version 2\nfor t \u2208(tn, tn+1],\nwhereby tn and tn+1 are two successive trading times of the large trader.\nNow that all orders are well de\ufb01ned, we introduce the cost of a large order xtn\nat some trading time tn by\n(2.15)\n\u03c0tn(xtn) :=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nR DA\ntn+\nDA\ntn\n(A0\ntn + x)f(x)dx\nfor a buy market order xtn \u22650\nR DB\ntn+\nDB\ntn\n(B0\ntn + x)f(x)dx\nfor a sell market order xtn < 0\n.\nWe assume that the large trader needs to purchase the X0 shares in N + 1 steps\nat equidistant points in time 0 =: t0 < \u00b7 \u00b7 \u00b7 < tN := T .\nHis or her admissible\nstrategies are sequences \u03be = (\u03be0, . . . , \u03beN) of random variables such that\n\u2022 PN\nn=0 \u03ben = X0,\n\u2022 \u03ben is Ftn-measurable for all n, and\n\u2022 all \u03ben are bounded from below.\nWe denote the set of all admissible strategies by \u02c6\u039e. The goal is to \ufb01nd an admissible\nstrategy \u03be\u2217that minimizes the average cost C (\u03be) given by the sum of the single\ntrades\u2019 costs:\n(2.16)\nC (\u03be) := E\n N\nX\nn=0\n\u03c0tn(\u03ben)\n!\n.\nUnder the technical assumption that\n(2.17)\nlim\nx\u2192\u221eF(X) = \u221eand\nlim\nx\u2192\u2212\u221eF(x) = \u2212\u221e,\nAlfonsi, Fruth and Schied give the unique optimal strategies for both versions ex-\nplicitly.",
    "chunk_index": 6,
    "start_char": 14336,
    "end_char": 16628,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "of all admissible strategies by \u02c6\u039e. The goal is to \ufb01nd an admissible\nstrategy \u03be\u2217that minimizes the average cost C (\u03be) given by the sum of the single\ntrades\u2019 costs:\n(2.16)\nC (\u03be) := E\n N\nX\nn=0\n\u03c0tn(\u03ben)\n!\n.\nUnder the technical assumption that\n(2.17)\nlim\nx\u2192\u221eF(X) = \u221eand\nlim\nx\u2192\u2212\u221eF(x) = \u2212\u221e,\nAlfonsi, Fruth and Schied give the unique optimal strategies for both versions ex-\nplicitly. We restate them here to give the reader the opportunity to compare them\nto our theorems for the GAFS model in Section 4. For the sake of convenience, we\nset \u03c4 := T/(N + 1) = tn+1 \u2212tn.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n7\nOptimal strategy for Version 1, Theorem 4.1 in [AFS09]. Suppose that the function\n(2.18)\nh1(x) := F \u22121(x) \u2212e\u2212\u03c1\u03c4F \u22121(e\u2212\u03c1\u03c4x)\nis one-to-one. Then there exists a unique optimal strategy \u03be(1) = (\u03be(1)\n0 , . . . , \u03be(1)\nN ).\nThe initial market order \u03be(1)\n0\nis the unique solution of the equation\n(2.19)\nF \u22121 \u0010\nX0 \u2212N\u03be(1)\n0 (1 \u2212e\u2212\u03c1\u03c4)\n\u0011\n= h1(\u03be(1)\n0 )\n1 \u2212e\u03c1\u03c4 ,\nthe intermediate orders are given by\n(2.20)\n\u03be(1)\n1\n= \u00b7 \u00b7 \u00b7 = \u03be(1)\nN\u22121 = \u03be(1)\n0 (1 \u2212e\u2212\u03c1\u03c4),\nand the \ufb01nal order is determined by\n(2.21)\n\u03be(1)\nN = X0 \u2212\nN\u22121\nX\nn=0\n\u03be(1)\nn .\nIn particular, the optimal strategy is deterministic. Moreover, it consists only of\nnontrivial buy orders, that is \u03ben > 0 for all n.\nOptimal strategy for Version 2, Theorem 5.1 in [AFS09]. Suppose that the function\n(2.22)\nh2(x) := xf(x) \u2212e\u22122\u03c1\u03c4f(e\u2212\u03c1\u03c4x)\nf(x) \u2212e\u2212\u03c1\u03c4f(e\u2212\u03c1\u03c4x)\nis one-to-one and that the shape function satis\ufb01es\n(2.23)\nlim\n|x|\u2192\u221ex2\ninf\ny\u2208[e\u2212\u03c1\u03c4 x,x] f(y) = \u221e.\nThen there exists a unique optimal strategy \u03be(2) = (\u03be(2)\n0 , . . . , \u03be(2)\nN ).\nThe initial\nmarket order \u03be(2)\n0\nis the unique solution of the equation\n(2.24)\nF \u22121 \u0010\nX0 \u2212N[\u03be(2)\n0\n\u2212F(e\u2212\u03c1\u03c4F \u22121(\u03be(2)\n0 ))]\n\u0011\n= h(F \u22121(\u03be(2)\n0 )),\nthe intermediate orders are given by\n(2.25)\n\u03be(2)\n1\n= \u00b7 \u00b7 \u00b7 = \u03be(2)\nN\u22121 = \u03be(2)\n0\n\u2212F(e\u2212\u03c1\u03c4F \u22121(\u03be(2)\n0 )),\nand the \ufb01nal order is determined by\n(2.26)\n\u03be(2)\nN = X0 \u2212\nN\u22121\nX\nn=0\n\u03be(2)\nn .\nIn particular, the optimal strategy is deterministic. Moreover, it consists only of\nnontrivial buy orders, that is \u03ben > 0 for all n.\nOne can easily check that the orders \u03be(\u00b7)\n1 , . . . , \u03be(\u00b7)\nN\u22121 have exactly the volume that\nhas recovered since the last trade. In this sense, the theorems just give the right\nbalance between the \ufb01rst and the last order. This balance is found by solving the\nparticular equations, (2.19) and (2.24), given in both theorems.\n\n8\nALEXANDER WEISS\n3. The Opinion Game\nNext, we focus on the Opinion Game. In Section 3.1, we recapitulate the original\nmodel as introduced by Bovier, \u02c7Cern\u00b4y and Hryniv in [B\u02c7CH06].",
    "chunk_index": 7,
    "start_char": 16252,
    "end_char": 18790,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "volume that\nhas recovered since the last trade. In this sense, the theorems just give the right\nbalance between the \ufb01rst and the last order. This balance is found by solving the\nparticular equations, (2.19) and (2.24), given in both theorems.\n\n8\nALEXANDER WEISS\n3. The Opinion Game\nNext, we focus on the Opinion Game. In Section 3.1, we recapitulate the original\nmodel as introduced by Bovier, \u02c7Cern\u00b4y and Hryniv in [B\u02c7CH06]. We have already\ndiscussed in the introduction why the underlying generalized order book of this\nmodel provides even more information about the market behavior than a classical\norder book. Yet, the Opinion Game has no explicit notion of orders and, conse-\nquently, also large orders and their executions are not de\ufb01ned. However, we argue\nin Section 3.2 that the generalized order book contains an implicit notion of or-\nders. Furthermore, we state the algorithm that we use to simulate the execution of\nlarge orders and show on a qualitative level that this extension leads to a realistic\nresponse of the Opinion Game to large orders.\n3.1. The model. We consider a \ufb01xed number of traders N \u2208N and a \ufb01xed number\nof tradable shares M < N. Every trader is described by the pair (pi, ni), whereby pi\nis the opinion of trader i about the right logarithmic price; the opinion is individual\nand subjective. For numerical reasons, pi \u2208Z. The number of shares that trader i\nposseses is given by ni. In the most general setting, ni can take values form 0 to\nM; however, we just divide traders in buyers and sellers by setting ni \u2208{0, 1}. We\nde\ufb01ne the best bid price by\n(3.1)\npb := max\ni:ni=0 pi,\nand the best ask price by\n(3.2)\npa := min\ni:ni=1 pi;\nthe price p is given by\n(3.3)\np := pb + pa\n2\n.\nThe market is said to be in a stable state if pb < pa; no buyer is then willing to pay\nthe lowest asked price and vice versa.\nFor numerical reasons, the dynamics is de\ufb01ned in discrete time. Every round\nconsists of three steps:\n(1) A trader is chosen\nWe de\ufb01ne\n(3.4)\ng(i, t) :=\n\u001a\n(1 + pb(t) \u2212pi(t))\u2212\u03b3\nif trader i is buyer\n(1 + pi(t) \u2212pa(t))\u2212\u03b3\nif trader i is seller\n,\nand set\n(3.5)\nP(trader i is chosen at time t) :=\ng(i, t)\nPN\nj=1 g(j, t)\n.\nThe parameter \u03b3 > 0 can be chosen arbitrarily. Observe that the de\ufb01ned\nmeasure prefers traders close to the price. The larger \u03b3 is the greater is this\npreference. Here, we assume that a trader close to the current price reacts\nfaster to price \ufb02uctuations than a long time investor with an opinion being\ncompletely di\ufb00erent from the current price.\n(2) The trader\u2019s change of opinion\nIf a trader is chosen, he or she changes her opinion to p\u2032\ni(t+1) := pi(t)+d(t).",
    "chunk_index": 8,
    "start_char": 18365,
    "end_char": 20977,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "to the price. The larger \u03b3 is the greater is this\npreference. Here, we assume that a trader close to the current price reacts\nfaster to price \ufb02uctuations than a long time investor with an opinion being\ncompletely di\ufb00erent from the current price.\n(2) The trader\u2019s change of opinion\nIf a trader is chosen, he or she changes her opinion to p\u2032\ni(t+1) := pi(t)+d(t).\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n9\nThe random variable d(t) takes values in {\u2212l, \u2212l + 1, . . . , l \u22121, l}, l \u2208N,\nand is independently sampled for all t. The measure of d(t) is given by\n(3.6)\nP(d(t) = m) =\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2l+1 ((\u00b5ext(t)\u00b5\u00b7)m \u22271)\nfor m \u0338= 0\n1 \u2212Pl\nm=1 P(d(t) = \u00b1m)\nelse\n, whereby\n(3.7)\n\u00b5\u00b7 :=\n\u001a\n\u00b5B\nif trader i is a buyer\n\u00b5S\nif trader i is a seller\n.\nWe assume that \u00b5S < 1 < \u00b5B to implement the idea that all traders have a\ntendency to move into the direction of the price. The \u00b5ext(t) introduces a\ndrift that changes randomly in time and acts on all traders in the same way,\nmodeling news, rumors and events in\ufb02uencing the price. This drift process\nis of paramount importance for the stylized facts, statistical features of the\nprice process on large time scales; however, as we want to concentrate on\nthe large orders\u2019 impact, which happens on shorter time scales, we assume\n\u00b5ext \u22611 in the remainder of this article.\n(3) Trading (if necessary)\nIf the market with the changed opinion is stable again, that is\n(3.8)\npb((p1(t), . . . , p\u2032\ni(t), . . . , pN(t))) < pa((p1(t), . . . , p\u2032\ni(t), . . . , pN(t))),\nwe set pi(t+1) := p\u2032\ni(t+1), else a trade happens. Let us assume that trader\ni is a buyer, the other case is symmetric. We uniformly choose a trading\npartner j with pj(t) = pa(t) and set ni(t + 1) = 0 and nj(t + 1) = 1. After\nthe trade, both traders move away from the best price:\n(3.9)\npi(t + 1) := pa(t + 1) + \u00afg\nand\npj(t + 1) := pa(t + 1) \u2212g\nwhereby \u00afg and g can be \ufb01xed or random numbers in N. This last step\nis justi\ufb01ed by the idea that the traders want to make pro\ufb01t and are only\nwilling to trade for a better price than they have paid.\n3.2. An extension for large orders. For the existence of orders in the Opinion\nGame, let us consider a buyer and a seller with matching opinions such that a\ntrade happens. In order book driven markets, trades can only come about if both\ntraders have placed some kind of orders. From this point of view, the Opinion\nGame has an implicit notion of orders, at least when trades are happening.",
    "chunk_index": 9,
    "start_char": 20616,
    "end_char": 23041,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "For the existence of orders in the Opinion\nGame, let us consider a buyer and a seller with matching opinions such that a\ntrade happens. In order book driven markets, trades can only come about if both\ntraders have placed some kind of orders. From this point of view, the Opinion\nGame has an implicit notion of orders, at least when trades are happening. This\nobservation motivates a change of our point of view on the Opinion Game: In the\nremainder of this article, we rather think about (maybe hidden or unplaced) buy\nor sell orders instead of traders with opinion. For the sake of convenience, we omit\nthe word generalized in the following when we talk about the order book of the\nOpinion Game.\nTo test the AFS model, we have to introduce large orders to the Opinion Game.\nAssume we would like to purchase X stocks at time t. Then, we do not apply the\nstandard dynamics explained above at time t; instead, we use the following algo-\nrithm:\nset p(1)\nk\n:= pk(t) for all k \u2208{1, . . . , N}\nset n(1)\nk\n:= nk(t) for all k \u2208{1, . . . , N}\nlet pa(1) be the best ask price of the con\ufb01guration\n\u0010\np(1)\nk , n(1)\nk\n\u0011\nk\u2208{1,...,N}\n\n10\nALEXANDER WEISS\nfrom x := 1 to X do {\n\ufb01nd i s.th. p(x)\ni\n\u2264p(x)\nj\nfor all j \u2208{1, . . . , N}\np(x+1)\ni\n:= pa(x)\nchoose uniformly trading partner j s.th. i \u0338= j and p(x)\nj\n= pa(x)\nn(x+1)\ni\n:= 1 and n(x+1)\nj\n:= 0\np(x+1)\nj\n:= pa(x) \u2212g\np(x+1)\ni\n:= pa(x) + \u02c6g(x)\nset p(x+1)\nk\n:= p(x+1)\nk\nfor all k \u2208{1, . . . , N}\\{i, j}\nset n(x+1)\nk\n:= n(x+1)\nk\nfor all k \u2208{1, . . . , N}\\{i, j}\nlet pa(x+1) be the best ask price of the con\ufb01guration\n\u0010\np(x+1)\ni\n, n(x+1)\ni\n\u0011\ni\u2208{1,...,N}\n}\nset pk(t + 1) := p(X+1)\nk\nfor all k \u2208{1, . . . , N}\\{i, j}\nset nk(t + 1) := n(X+1)\nk\nfor all k \u2208{1, . . . , N}\\{i, j}\nThe value g is the same random or deterministic value as in the original dynamics.\nThe random variables \u02c6g(x) are independently distributed with measure\n(3.10)\nP(\u02c6g(x) = k) = 1\nM\nN\nX\nn=1\n1{p(x)\nn \u2212pa(x)=k} for k \u2208N0.\nIn other words, we execute a large buy order of volume X by taking the lowest\nX orders one by one and putting them directly to the ask price such that a trade\nis enforced. The number of market participants is constant in the Opinion Game,\nthus taking orders from the tail is an obvious method to simulate a large order that\nis placed out of the blue. After each single trade, we adjust the order prices; the\nprice of the (new) buy order is decreased by g, the price of the sell order is increased\nby \u02c6g.",
    "chunk_index": 10,
    "start_char": 22688,
    "end_char": 25111,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "taking the lowest\nX orders one by one and putting them directly to the ask price such that a trade\nis enforced. The number of market participants is constant in the Opinion Game,\nthus taking orders from the tail is an obvious method to simulate a large order that\nis placed out of the blue. After each single trade, we adjust the order prices; the\nprice of the (new) buy order is decreased by g, the price of the sell order is increased\nby \u02c6g. The density function of \u02c6g is given by the order book\u2019s current shape.\nThis choice of \u02c6g leads to a realistic response of the order book to the execution\nof large orders (see Figure 2). While the large order is executed, the new sell orders\nhave a great probability to be placed in vicinity to the peak of the order book\u2019s\nseller part; thus the peak grows, and the order book provides more liquidity for\nprices in this region. Here, we implement the idea that the execution of a large buy\norder leads to a conspicuous rise in the price that attracts more traders to place sell\norders close to the current best ask price; these traders hope that the price increase\ncontinues such that their orders are executed. At the same time, these additional\no\ufb00ers provide more liquidity that slows down the price increase. If we consider\nthe immediate price impact of the large order as function of the executed volume,\nthe additional liquidity leads to a sublinear function shape. Sublinear behavior of\nan order\u2019s immediate price impact has also been observed for real world markets\nin several empirical studies [BGPW04, ATHL05]. After the execution of the large\norder, the price increase stops and some traders realize quickly that orders for higher\nprices will probably not be executed in the near future; they place new orders for\nlower prices. However, most traders need more time to acknowledge that their price\nclaims are probably too high. In result, the best ask price decreases, but the order\nbook volume in proximity to the new best quote is low. It takes more time until the\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n11\nFigure 2. Sketch of the order book shape in the Opinion Game\nwhen a large order is executed. Before the execution, the order\nbook is in equilibrium (upper left \ufb01gure); directly afterwards, the\nbest ask price is increased, and there is more liquidity close to it\n(upper right \ufb01gure). When the LOB recovers from the order, the\nbest ask price decreases, but the best quotes have a low volume\nonly (lower \ufb01gure); it takes more time until the order book is in\nequilibrium again.\nLOB is back in equilibrium. This recovery behavior of the order book is technically\nimplemented by the preference for traders close to the best quotes in (3.5) when\nwe update opinions.",
    "chunk_index": 11,
    "start_char": 24668,
    "end_char": 27402,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "from the order, the\nbest ask price decreases, but the best quotes have a low volume\nonly (lower \ufb01gure); it takes more time until the order book is in\nequilibrium again.\nLOB is back in equilibrium. This recovery behavior of the order book is technically\nimplemented by the preference for traders close to the best quotes in (3.5) when\nwe update opinions. As another feature that is known from real world markets,\nthe best ask price does not return to the value it has had before the execution, but\nit stabilizes at higher values after the order book has returned to equilibrium. We\ndiscuss this permanent impact on the best price in Section 4.2.\nSince the dynamics are symmetric, the algorithm applies to large sell orders in\nthe same way.\n4. Determining the parameters\nThe Opinion Game provides a variety of parameters to in\ufb02uence the character-\nistics of the modeled market. For instance, it is possible to change the size of the\nmarket or the volatility in the Opinion Game to simulate di\ufb00erent markets. Never-\ntheless, we restrict ourselves in the following to one parameter set, which is stated\nin Subsection 4.1. Although the variation of parameters surely leads to additional\ninsight, our choice already gives a sound understanding of the problems that occur\nwhen applying the AFS model. In the same subsection, we also describe the aver-\naged shape of the Opinion Game\u2019s order book that will serve as shape function f\nfor the AFS model.\nHaving set up the Opinion Game, we try to determine the AFS model\u2019s resilience\nspeed, \u03c1. It turns out that the assumption of a constant \u03c1 is not valid in the Opinion\nGame. Thus, we substitute \u03c1 by a function \u00af\u03c1 that maps both the order\u2019s impact and\nthe time elapsed since the last trade to the resilience speed. We describe how we\ncan extract the function values from the sampled data, and argue that it is su\ufb03cient\nto know the impact-dependent function \u00af\u03c1(\u00b7) := \u00af\u03c1(\u00b7, \u03c4) only; recall that \u03c4 = T/N\n\n12\nALEXANDER WEISS\nwas the recovery time between two successive trades. Finally, we introduce the\ngeneralized AFS theorems that assume the resilience speed to be a function of the\nprice impact (in Version 2) or the volume impact (in Version 1).\n4.1. The parameters of the Opinion Game and the shape of its order\nbook. There is a high degree of freedom in the parameters for the Opinion Game.\nNevertheless, certain parameter sets have been shown to be more reasonable choices\nthan others. Calibrated with these parameter values, the Opinion Game results in\na realistic price process in terms of stylized facts. However, not all choices can be\njusti\ufb01ed rigorously. For an extensive discussion about the choice of parameter \u03b3,\nfor instance, we refer to [Wei09b]. We used the following values in all simulations\nthroughout this article, since those ones have been shown to generate price processes\nwith realistic statistical features [B\u02c7CH06, Wei09a]:\n\u266fof traders N\n2000\n\u266fof shares M\n1000\nspeed of adaption \u03b3\n1.5\njump range {\u2212l, .",
    "chunk_index": 12,
    "start_char": 27049,
    "end_char": 30018,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "choices\nthan others. Calibrated with these parameter values, the Opinion Game results in\na realistic price process in terms of stylized facts. However, not all choices can be\njusti\ufb01ed rigorously. For an extensive discussion about the choice of parameter \u03b3,\nfor instance, we refer to [Wei09b]. We used the following values in all simulations\nthroughout this article, since those ones have been shown to generate price processes\nwith realistic statistical features [B\u02c7CH06, Wei09a]:\n\u266fof traders N\n2000\n\u266fof shares M\n1000\nspeed of adaption \u03b3\n1.5\njump range {\u2212l, . . ., l}\n{\u22124, . . ., 4}\ndrift of buyers \u00b5B\ne0.1\ndrift of sellers \u00b5S\ne\u22120.1\njump ranges \u00afg, g\nrandom variables, uniformly distributed on {5, . . ., 20},\nsampled idependently every time they are used\nAll sample runs that we did in the Opinion Game, either to extract necessary pa-\nrameters or to test execution strategies, were started independently with a random\nseed for the random number generator. Furthermore, the recording of data or the\nexecution of large orders was started after 1 000 000 simulation steps only, such that\nthe model had su\ufb03cient time to get close to a stable state.\nTo determine f, we recorded 500 times the Opinion Game\u2019s LOB relative to the\nbest prices. Figure 3 shows the resulting upper part of the order book. The lower\npart is symmetric up to small deviations caused by the object\u2019s random nature.\nEven if the shape is not static as assumed in the AFS model, an averaged shape\nis clearly visible.\nWe use these mean values to de\ufb01ne the shape function f for\nthe Opinion Game.\nFor non-integer values, we interpolate f by assuming that\nthe function is a right-continuous step function. This means that we violate the\nassumption of the AFS model about f being continuous. Yet, this choice for f has\nthe advantage that the integral of f from 0 to an integer n is equal to the sum of\nthe integer function values from 0 to n\u22121. Furthermore, for all parameter sets that\nwe considered, we were still able to \ufb01nd unique solutions for the optimal trading\nstrategies.\nRecall that the price scale in the Opinion Game is logarithmic, whereas the AFS\nmodel assumes a linear scale. However, it is possible to scale the grid of the Opinion\nGame with a factor \u01eb, and the di\ufb00erence between logarithmic and linear scale is\nnegligible if \u01eb is small. To determine the order of \u01eb, we consider an order of 200\nunits of shares, 20% of the market volume in the Opinion Game; it is mentioned\nin [AFS09] that the size of large orders can amount up to twenty percent of the\ndaily traded volume. We assume that the shape of the LOB, f, is determined as\ndescribed above, and the best ask price before our trade is denoted by A0. Then",
    "chunk_index": 13,
    "start_char": 29459,
    "end_char": 32147,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "shares, 20% of the market volume in the Opinion Game; it is mentioned\nin [AFS09] that the size of large orders can amount up to twenty percent of the\ndaily traded volume. We assume that the shape of the LOB, f, is determined as\ndescribed above, and the best ask price before our trade is denoted by A0. Then\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n13\nFigure 3. The seller part of the LOB relative to the best ask price.\nThe solid line marks the mean values, the dashed lines illustrate\nthe quartiles. The minimal and maximal values are illustrated by\nthe dotted lines.\nthe relative impact costs are given by\n(4.1)\n1\n200e\u01ebA0\nZ D0+\n0\ne\u01eb(A0+x)f(x)dx \u22121 \u2248\n\u01eb\n200\nZ D0+\n0\nxf(x)dx\n|\n{z\n}\n\u22482039.47\n\u224810.20\u01eb.\nAn empirical study of the US stock market shows that large orders can cause\nrelative costs up to 3.55% [ATHL05]. If we assume that \u01eb \u22640.0355/10.2, then \u01eb is\nof order 10\u22123 at most. Thus, it is reasonable to assume \u01eb to be small. However, we\nare interested in qualitative results; thus, and for the sake of convenince, we will\nsimply assume that the Opinion Game operates on Z.\n4.2. Determining \u03c1 for the AFS model. In the following, we present our ap-\nproach to calibrate \u03c1 for the Opinion Game. We describe our simulation approach\nand the corresponding results for Version 2 of the AFS model only. Recall that, in\nthis version, \u03c1 determines the recovery speed of the price impact. Our ansatz and\nthe observations are qualitatively the same for the other version. Nevertheless, we\nintroduce the GAFS model for the price impact dependent as well as for the volume\nimpact dependent case in the end of this section.We \ufb01rst describe how we sampled\nthe necessary data. Afterwards, we focus on the main problems of extracting \u03c1 from\nthose data. Possible solutions are discussed and culminate in this section\u2019s main\nresult: The GAFS theorems, which assume that the resilience speed is a function\n\u00af\u03c1 depending on the order\u2019s impact.\n\n14\nALEXANDER WEISS\nWe \ufb01xed a price impact D \u2208{1, . . . , 20} and ran 2500 simulations for each value\nof D, resulting in 50 000 simulations. Since every run had an initialization period\nof 1 000 000 steps, each simulation took several seconds. Observe that a simulation\ntime of one second per run already results in a total computing time of almost\n14 hours. As we ran several simulations parallel, we were able to \ufb01nish the data\ncollecting within a few days.\nEach run consisted of a trading part in which a large sell order was executed at\nonce. The particular order volume was determined by its price impact: The trading\npart was \ufb01nished as soon as the impact was equal to D. In a second experiment\u2019s\npart, we recorded the relaxiation of the price. In particular, the large execution\ntook place at time \u00aft := 1 000 000;",
    "chunk_index": 14,
    "start_char": 31840,
    "end_char": 34598,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "to \ufb01nish the data\ncollecting within a few days.\nEach run consisted of a trading part in which a large sell order was executed at\nonce. The particular order volume was determined by its price impact: The trading\npart was \ufb01nished as soon as the impact was equal to D. In a second experiment\u2019s\npart, we recorded the relaxiation of the price. In particular, the large execution\ntook place at time \u00aft := 1 000 000; we recorded\n(4.2)\n\u00afp(t) := pa(t + 1 + \u00aft) \u2212pa(\u00aft)\nfor t \u2208{0, . . ., 50 000}. The process (\u00afp(t))t\u2208N0 is the discrete counterpart of the\nAFS model\u2019s process DA.\nTo avoid problems caused by random \ufb02uctuations in \u00afp, we consider the pointwise\naverage of the samples denoted by \u27e8\u00afp\u27e9and de\ufb01ned by\n(4.3)\n\u27e8\u00afp\u27e9t :=\n1\n2500\n2500\nX\ni=0\n\u00afpi\nt\nfor all t \u2208{0, . . . , 50 000}, \u00afpi denoting the ith sample. For a clear distinction, we\ndenote the value for \u03c1 that we extract from \u27e8\u00afp\u27e9by \u00af\u03c1num. The AFS model assumes\n\u27e8\u00afp\u27e9to be of the form\n(4.4)\n\u27e8\u00afp\u27e9t = De\u2212\u00af\u03c1numt\nwith a static value \u00af\u03c1num; this follows from equation (2.3). Thus we should be able\nto determine \u00af\u03c1num by\n(4.5)\n\u00af\u03c1num = ln D \u2212ln\u27e8\u00afp\u27e9t\nt\nfor an arbitrary t. However, the right hand side of the equation depends on D and\nt; thus, we would like to consider \u00af\u03c1num(D, t) as a function.\nGiven \u27e8\u00afp\u27e9, let \u02c6p : [0, \u221e) \u2192R the corresponding regression function of the form\n(4.6)\n\u02c6pt := A + Be\u02c6\u03c1t,\nfor t \u2208[0, \u221e). It is determined by a Newton-Gau\u00df algorithm with three degrees of\nfreedom: A, B, \u02c6\u03c1. Observe that all three values can depend on D. The form of\nthe regression function is motivated by assumption (4.4), which also leads to the\nexpectation that A = 0 and B = D. Figure 4 shows the statistical behavior of \u00afp\nfor D = 8, the corresponding \u27e8\u00afp\u27e9and \u02c6p. Furthermore, we compare \u27e8\u00afp\u27e9for di\ufb00erent\nD values. The three main problems are visible:\n(1) The AFS model assumes AD to be 0; this is not the case.\n(2) The measured data is only well-approximated by an exponential function for\nlarge times. For small t, it is doubtful that the assumption of an exponential\ndecay is the right choice at all.\n(3) If \u00af\u03c1num was constant the \u27e8\u00afp\u27e9should be approximately parallel on a loga-\nrithmic scale; instead, \u00af\u03c1num depends D.\nThese problems occured for all tested values of D. Next, we discuss the problems\nand their consequences for the determination of \u00af\u03c1num one by one.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n15\nFigure 4. The two upper graphs show quartiles and extremal val-\nues of 2500 samples of \u00afp for D = 8, and the corresponding \u27e8\u00afp\u27e9and\n\u02c6p (red). The graph on top illustrates the long time behavior on the\ndomain t \u2208[0, 50 000]. Clearly, \u02c6p converges to a level AD > 0. The\nmiddle graph displays t \u2208[0, 2000] showing the poor approximation\nby \u02c6p.",
    "chunk_index": 15,
    "start_char": 34189,
    "end_char": 36895,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "problems\nand their consequences for the determination of \u00af\u03c1num one by one.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n15\nFigure 4. The two upper graphs show quartiles and extremal val-\nues of 2500 samples of \u00afp for D = 8, and the corresponding \u27e8\u00afp\u27e9and\n\u02c6p (red). The graph on top illustrates the long time behavior on the\ndomain t \u2208[0, 50 000]. Clearly, \u02c6p converges to a level AD > 0. The\nmiddle graph displays t \u2208[0, 2000] showing the poor approximation\nby \u02c6p. The lower graph shows \u27e8\u00afp\u27e9(black) for D = 16, D = 12 and\nD = 8 (top down) as well as their regression functions \u02c6p (red) on\na logarithmic scale and with respect to the new asymptotic levels\nAD. If \u00af\u03c1num was constant the \u27e8\u00afp\u27e9should be approximately parallel.\n\n16\nALEXANDER WEISS\nFigure 5. Mean, quartiles and extremal values of 500 samples\nof the permanent impact in dependence on the purchased volume\nV \u2208{25, 50, . . ., 275, 300}. For every volume V , we recorded the\nbest ask price before the trade and the averaged best ask price\n500 000 steps after the trade. Here, the averaged best ask price\nis the mean of the best ask price sampled all 100 steps over a\ntime interval of 100 000 steps. The linear regression of the mean is\ndisplayed in red.\n4.2.1. Existence of a permanent price impact. The reason for problem (1) is a per-\nmament impact on the order book that a large trade causes. After having recovered,\nthe LOB is shifted by Iper(X), whereby Iper : R \u2192R is assumed to be increasing\nand Iper(0) = 0. Huberman and Stanzl [HS04] argued on a theoretic level that\nlinearity of Iper is equivalent to the absence of arbitrage opportunities. Empirical\nstudies by Almgren et al. [ATHL05] reinforce the conjecture of a linear perma-\nnent impact: The authors state that the permanent impact is well described by\nthe power law x0.9\u00b10.1 with respect to a Gaussian error model; the assumption of\nlinearity cannot be rejected by this result. Figure 5 shows the permanent impact\nfor the Opinion Game. The mean is well aproximated by a linear function with\ncoe\ufb03cient 0.02738.\nConcerning the problems in determining \u00af\u03c1num, caused by the positive AD, we\nhave two possibilities: First, we could ignore the permanent impact such that \u00af\u03c1num\nwould be given by (4.5). This would be an appropriate solution for small t, but it\nwould cause the AFS model to assume that even for large t the LOB is still not\nclose to equilibrium; \u00af\u03c1num could become arbitrarily small. Second, we could assume\nthat the whole model has been shifted by AD such that AD is the new zero line. In\nthis case, \u00af\u03c1num would be given by\n(4.7)\n\u00af\u03c1num(D, t) = ln D \u2212ln(\u27e8\u00afp\u27e9t \u2212AD)\nt\n,\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n17\nwhich is \ufb01ne for large t but grows to in\ufb01nity as t goes to zero.",
    "chunk_index": 16,
    "start_char": 36423,
    "end_char": 39153,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "the LOB is still not\nclose to equilibrium; \u00af\u03c1num could become arbitrarily small. Second, we could assume\nthat the whole model has been shifted by AD such that AD is the new zero line. In\nthis case, \u00af\u03c1num would be given by\n(4.7)\n\u00af\u03c1num(D, t) = ln D \u2212ln(\u27e8\u00afp\u27e9t \u2212AD)\nt\n,\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n17\nwhich is \ufb01ne for large t but grows to in\ufb01nity as t goes to zero. To avoid this problem,\nwe de\ufb01ne\n(4.8)\n\u00af\u03c1num(D, t) := ln D \u2212ln(\u27e8\u00afp\u27e9t \u2212(1 \u2212e\u2212t)AD)\nt\n.\nFurthermore, let us point out that there is no special reason to choose 1 \u2212exp(\u2212t).\nHowever, at this point, it becomes clear that the complex dynamics within the LOB\nare poorly described by an added permanent impact function.\n4.2.2. \u27e8\u00afp\u27e9is poorly approximated by an exponential function. Since \u27e8\u00afp\u27e9should decay\nexponentially fast, \u00af\u03c1num should be a constant. However, the existence of a perma-\nnent impact and the consequential de\ufb01nition of \u00af\u03c1num in (4.8) makes the validity of\nthis assumption unlikely here. Yet, even without the permanent impact, the de-\nscription of \u27e8\u00afp\u27e9by an exponential function is poor as the upper right graph of Figure\n4 shows. As mentioned in Section 2, the rejection of an exponential decay does not\ncontradict former research results. If we nevertheless try to calibrate \u03c1, it becomes\ntime-dependent. A time-dependent resilience speed seems to be incompatible with\nthe AFS theorems at \ufb01rst, but a closer look at the theorem\u2019s statement reveals that\n\u03c1 is only needed to determine the order book state before the next trade, given the\nstate after the current trade. The time between two succeeding trades is given by\n\u03c4. Thus, we focus on \u00af\u03c1num(\u00b7, \u03c4) and use the notation\n(4.9)\n\u00af\u03c1num(D) := \u00af\u03c1num(D, \u03c4)\nassuming that \u03c4, which is given by the input parameters N and T , is \ufb01xed. Figure\n6 shows the function \u00af\u03c1num(\u00b7, \u03c4) for several values of \u03c4.\n4.2.3. \u00af\u03c1num is a function of D. In contrast to the time dependence, the dependence\non the order\u2019s price impact requires a generalization of the AFS theorem, stated in\nSection 2. Now, the resilience speed \u00af\u03c1 : R \u2192(0, \u221e) is a continously di\ufb00erentiable\nfunction of DA. In particular, the equations (2.3) and (2.14), which describe the\nprice recovery in the AFS model, change to\n(4.10)\nDA\nt := e\u2212\u00af\u03c1(DA\ntn+)(t\u2212tn)DA\ntn+ for t \u2208(tn, tn+1],\n(4.11)\nDB\nt := e\u2212\u00af\u03c1(DB\ntn+)(t\u2212tn)DB\ntn+ for t \u2208(tn, tn+1].\nWe denote this modi\ufb01ed model as Version 2 of the generalized AFS model.\nFor the following theorem concerning the optimal trading strategy for the GAFS\nmodel, we need two technical assumptions:\n(4.12)\nThe range of \u00af\u03c1 is a subset of [k, K], 0 < k < K < \u221e, and\n(4.13)\n1 \u2212\u03c4 \u00af\u03c1\u2032(x)x > 0 for all x \u2208R.\nThe \ufb01rst assumption bounds the relaxation speed, the second assumption ensures\nthat a larger impact cannot overtake a smaller one in the recovery phase as we will\nsee in Lemma A.1.",
    "chunk_index": 17,
    "start_char": 38766,
    "end_char": 41576,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "generalized AFS model.\nFor the following theorem concerning the optimal trading strategy for the GAFS\nmodel, we need two technical assumptions:\n(4.12)\nThe range of \u00af\u03c1 is a subset of [k, K], 0 < k < K < \u221e, and\n(4.13)\n1 \u2212\u03c4 \u00af\u03c1\u2032(x)x > 0 for all x \u2208R.\nThe \ufb01rst assumption bounds the relaxation speed, the second assumption ensures\nthat a larger impact cannot overtake a smaller one in the recovery phase as we will\nsee in Lemma A.1.\nTheorem 4.1 (Optimal stratey for the generalized AFS model, Version 2). Suppose\nthat \u00af\u03c1 ful\ufb01ls (4.12) and (4.13), and that f satis\ufb01es\n(4.14)\nlim\n|x|\u2192\u221ex2\ninf\ny\u2208[e\u03c4 \u00af\n\u03c1(x)x,x]f(y) = \u221e.\n\n18\nALEXANDER WEISS\nFigure 6. We show the graphs of \u03c1num(D, \u03c4) in dependence on\nD for \u03c4 = 70, 700, 7000 (top down). Observe that the x-axis only\nbegin in 5 due to the fact that small price impacts cannot be dis-\ntinguished from the noise contained in the signal.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n19\nFurthermore, let the function\n(4.15)\nh2(x) := xf(x) \u2212e\u22122\u03c4 \u00af\u03c1(x)f(e\u2212\u03c4 \u00af\u03c1(x)x)(1 \u2212\u03c4 \u00af\u03c1\u2032(x)x)\nf(x) \u2212e\u2212\u03c4 \u00af\u03c1(x)f(e\u2212\u03c4 \u00af\u03c1(x)x)(1 \u2212\u03c4 \u00af\u03c1\u2032(x)x)\nbe one-to-one. Then there exists a unique optimal strategy \u03be(2) = (\u03be(2)\n0 , . . . , \u03be(2)\nN ) \u2208\n\u02c6\u039e. The initial market order \u03be(2)\n0\nis the unique solution of the equation\n(4.16)\nF \u22121 \u0010\nX0 \u2212N\nh\n\u03be(2)\n0\n\u2212F\n\u0010\ne\u2212\u03c4 \u00af\u03c1(F \u22121(\u03be(2)\n0\n))F \u22121(\u03be(2)\n0 )\n\u0011i\u0011\n= h2(F \u22121(\u03be(2)\n0 )),\nthe intermediate orders are given by\n(4.17)\n\u03be(2)\n1\n= \u00b7 \u00b7 \u00b7 = \u03be(2)\nN\u22121 = \u03be(2)\n0\n\u2212F\n\u0010\ne\u2212\u03c4 \u00af\u03c1(F \u22121(\u03be(2)\n0\n))F \u22121(\u03be(2)\n0 )\n\u0011\n,\nand the \ufb01nal order is determined by\n(4.18)\n\u03be(2)\nN = X0 \u2212\nN\nX\nn=0\n\u03be(2)\nn .\nIn particular, the optimal stratey is deterministic. Moreover, it consists only of\nnontrivial buy orders, that is \u03be(2)\nn\n> 0 for all n.\nProof. See Appendix A.2.\n\u25a1\nObserve that the intermediate orders of the optimal strategy, de\ufb01ned in (4.17),\nhave the same size. Furthermore, they suggest to purchase exactly that volume\nthat has recovered since the last trade. The GAFS model has inherited this feature\nfrom the AFS model. Yet, this observation means that also the DA\ntn+ are equal\nto each other for all n \u2208{0, . . . , N \u22121}, and thus, \u00af\u03c1 is only evaluated for one\nvalue. In other words, although \u00af\u03c1 is a function, the optimal strategy uses only one\nvalue. Of course, if \u00af\u03c1 \u2261\u03c1 for some constant \u03c1 in the GAFS model both models, the\nGAFS and the AFS, coincide. This is the main advantage of the GAFS theorem:\nIt determines the right resilience speed from \u00af\u03c1; a manual calibration, as in the AFS\nmodel, is not needed anymore.\nAs already mentioned, our simulations for the calibration of \u03c1 in Version 1 of the\nAFS model result in the same problems as described for Version 2.",
    "chunk_index": 18,
    "start_char": 41149,
    "end_char": 43748,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "value. Of course, if \u00af\u03c1 \u2261\u03c1 for some constant \u03c1 in the GAFS model both models, the\nGAFS and the AFS, coincide. This is the main advantage of the GAFS theorem:\nIt determines the right resilience speed from \u00af\u03c1; a manual calibration, as in the AFS\nmodel, is not needed anymore.\nAs already mentioned, our simulations for the calibration of \u03c1 in Version 1 of the\nAFS model result in the same problems as described for Version 2. Especially, \u03c1\nbecomes volume impact dependent, motivating the GAFS model, Version 1: Now,\nthe resilience speed \u00af\u03c1 : [0, \u221e) \u2192(0, \u221e) is a twice di\ufb00erentiable function of EA. In\nparticular, the equations (2.10) and (2.14) from the AFS model become\n(4.19)\nEA\nt := e\u2212\u00af\u03c1(EA\ntn+)(t\u2212tn)EA\ntn+, t \u2208(tn, tn+1],\n(4.20)\nEB\nt := e\u2212\u00af\u03c1(EB\ntn+)(t\u2212tn)EB\ntn+, t \u2208(tn, tn+1].\nin the GAFS model. Then, the following theorem determines the optimal trading\nstrategy in the set of all admissible strategies \u02c6\u039e:\nTheorem 4.2 (Optimal stratey for the generalized AFS model, Version 1). Suppose\nthat \u00af\u03c1 ful\ufb01ls the assumptions (4.12) and (4.13), and additionally\n(4.21)\ne\u2212\u00af\u03c1(x)\u03c4 (1 \u2212\u03c4 \u00af\u03c1\u2032(x)x) < 1 for all x \u2208R.\nFurthermore, let the function\n(4.22)\nh1(x) := F \u22121(x) \u2212e\u2212\u00af\u03c1(x)\u03c4 (1 \u2212\u03c4 \u00af\u03c1\u2032(x)x) F \u22121(e\u2212\u00af\u03c1(x)\u03c4x)\n1 \u2212e\u2212\u00af\u03c1(x)\u03c4 (1 \u2212\u03c4 \u00af\u03c1\u2032(x)x)\n\n20\nALEXANDER WEISS\nN\nT\n\u03be(2)\n0\n\u03be(2)\n1\n\u03be(2)\nN\nPredicted\nSampled\nSamp/Pred\n40\n400\n8.95\n4.74\n6.38\n701.47\n1867.74\n266%\n40\n4000\n6.13\n4.81\n6.15\n500.24\n1573.50\n315%\n40\n40 000\n5.16\n4.86\n5.40\n392.42\n1076.89\n274%\n50\n400\n8.29\n3.80\n5.29\n691.94\n1853.37\n268%\n50\n4000\n5.20\n3.88\n4.94\n462.51\n1535.96\n332%\n50\n40 000\n4.26\n3.90\n4.82\n349.26\n1014.42\n290%\n80\n400\n7.55\n2.37\n5.63\n691.65\n1832.69\n265%\n80\n4000\n3.73\n2.44\n3.37\n387.98\n1464.03\n377%\n80\n40 000\n2.65\n2.46\n2.91\n231.67\n914.17\n395%\nTable 1. The optimal strategies according to the GAFS model,\nVersion 2, for X = 200 and several values for N and T .\nbe one-to-one. Then there exists a unique optimal strategy \u03be(1) = (\u03be(1)\n0 , . . . , \u03be(1)\nN ) \u2208\n\u02c6\u039e. The initial market order \u03be(1)\n0\nis the unique solution of the equation\n(4.23)\nF \u22121 \u0010\nX0 \u2212N\u03be(1)\n0 (1 \u2212e\u2212\u00af\u03c1(\u03be(1)\n0\n)\u03c4)\n\u0011\n= h1(\u03be(1)\n0 ),\nthe intermediate orders are given by\n(4.24)\n\u03be(1)\n1\n= \u00b7 \u00b7 \u00b7 = \u03be(1)\nN\u22121 = \u03be(1)\n0 (1 \u2212e\u2212\u00af\u03c1(\u03be(1)\n0\n)\u03c4),\nand the \ufb01nal order is determined by\n(4.25)\n\u03be(1)\nN = X0 \u2212\nN\nX\nn=0\n\u03be(1)\nn .\nIn particular, the optimal stratey is deterministic. Moreover, it consists only of\nnontrivial buy orders, that is \u03ben > 0 for all n.\nProof. See Appendix A.1.\n\u25a1\nAs in Version 2 of the GAFS model, \u00af\u03c1 is only evaluated in one value, and if \u00af\u03c1 \u2261\u03c1\nthe best strategies of the GAFS and the AFS models coincide.\n5. Numerical results\nLet us turn to the numerical results of this paper.",
    "chunk_index": 19,
    "start_char": 43326,
    "end_char": 45916,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "In particular, the optimal stratey is deterministic. Moreover, it consists only of\nnontrivial buy orders, that is \u03ben > 0 for all n.\nProof. See Appendix A.1.\n\u25a1\nAs in Version 2 of the GAFS model, \u00af\u03c1 is only evaluated in one value, and if \u00af\u03c1 \u2261\u03c1\nthe best strategies of the GAFS and the AFS models coincide.\n5. Numerical results\nLet us turn to the numerical results of this paper. Again, we focus on Version 2\nand use the parameter values determined in the last section to calculate the GAFS\noptimal strategies and to apply them in the Opinion Game. We show \ufb01rst that\nthe resulting costs show an expected behavior on a general level, and that the AFS\nmodel with a suboptimal value for \u03c1 suggests a strategy that produces signi\ufb01cantly\nhigher costs than the corresponding GAFS strategy. Afterwards, we compare the\ncosts sampled in the Opinion Game to the costs predicted by the GAFS model,\nand \ufb01nd large di\ufb00erences. We refer to the values for f and \u00af\u03c1, \u00af\u03c1num, as determined\nin the Sections 4.1 and 4.2.\nTable 1 shows the GAFS optimal strategies and their costs for di\ufb00erent values\nof T and N. We consider two kinds of costs. The predicted costs are the impact\ncosts that are theoretically predicted by the (G)AFS model. Here, we assume that\nthe market behaves as described in Section 2. The sampled costs are the average\nof 500 samples with the given strategy in the Opinion Game. Observe \ufb01rst that\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n21\nStrategy\n\u03be(2)\n0\n\u03be(2)\n1\n\u03be(2)\nN\nPredicted\nSampled\nGAFS\n3.73\n2.44\n3.37\n387.98\n1464.03\nAFS\n21.02\n0.97\n102.26\n979.97\n1584.12\nTable 2. The optimal strategies and their costs for the AFS\nmodel with \u03c1 = \u02c6\u03c1 and the GAFS model with \u00af\u03c1 from Section 4.2.\n(X, T, N) = (200, 4000, 80).\nthe predicted and the sampled costs decrease if the trading time or the number of\ntrading opportunities increase. Of course, this is no special feature of the (G)AFS\nstrategies; every \ufb01xed strategy bene\ufb01ts from a larger \u03c4, which is implied by a\ngreater T , and additional trading opportunities can be used, but do not have to be\nused. Thus, every reasonable strategy can only perform better with larger T or N.\nNevertheless, the costs of the GAFS strategies show a reasonable behavior.\nFurthermore, the GAFS strategies perform better than the AFS strategies: Re-\ncall that the AFS model with the right value for \u03c1 results in the same optimal\nstrategy as the GAFS model. Moreover, the (G)AFS model assumes an exponen-\ntial decay of the price impact (see (4.4)).\nWe have taken this assumption into\naccount by introducing \u27e8\u00af\u03c1\u27e9\u2019s regression function \u02c6p in (4.6), which was of the form\n(5.1)\n\u02c6pt := A + Be\u2212\u02c6\u03c1t.\nTable 2 shows the optimal strategies and their costs for (X, T, N) = (200, 4000, 80)\nwith respect to the AFS model with \u03c1 = \u02c6\u03c1 and the GAFS model with \u00af\u03c1 = \u00af\u03c1num.",
    "chunk_index": 20,
    "start_char": 45541,
    "end_char": 48330,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "same optimal\nstrategy as the GAFS model. Moreover, the (G)AFS model assumes an exponen-\ntial decay of the price impact (see (4.4)).\nWe have taken this assumption into\naccount by introducing \u27e8\u00af\u03c1\u27e9\u2019s regression function \u02c6p in (4.6), which was of the form\n(5.1)\n\u02c6pt := A + Be\u2212\u02c6\u03c1t.\nTable 2 shows the optimal strategies and their costs for (X, T, N) = (200, 4000, 80)\nwith respect to the AFS model with \u03c1 = \u02c6\u03c1 and the GAFS model with \u00af\u03c1 = \u00af\u03c1num.\nThe example shows that a naive guess of a good \u03c1 can lead to much higher costs:\nThe AFS costs amount 253% of the GAFS costs in prediction, and still 108% in\nthe samples.\nThe last two paragraphs have shown that the GAFS strategies are reasonable\nand superior to the AFS stratgies. However, returning to Table 1, we see that the\npredicted and the sampled costs for the individual parameter sets di\ufb00er strongly.\nThe last column shows both kinds of costs in relation to each other. Obviously,\nthe sampled costs are multiple times higher. This observation is a strong evidence\nthat the assumptions of the (G)AFS model are insu\ufb03cient to capture the whole\ncomplexity of the order book dynamics in the Opinion Game. It is doubtful if the\n(G)AFS model really suggests optimal trading strategies for this arti\ufb01cial market\nenvironment. With regard to the Opinion Game features concerning the order book\nbehavior that we have discussed in Section 3, it is highly unlikely that the (G)AFS\nstrategies minimize the costs in real world markets.\n6. Conclusions\nIn this paper, we have tried to apply the AFS model to an arti\ufb01cial market\nenvironment. The elegance of the AFS model, the order book approach and the\nexplicit results for the optimal trading strategies, cannot be denied. Yet, the prob-\nlems we faced in calibrating the model to our market pose the question if the AFS\nmodel assumptions are oversimpli\ufb01ed. We point out again that the problems we\nhad to handle, the permanent impact and the non-exponential decay of the impact,\nare not arti\ufb01cial. It is well known that those e\ufb00ects are also characteristic for real\nmarkets. Even if it is possible to bypass some problems or to extend the model in\na suitable way as we did by introducing the GAFS model, the question remains if\n\n22\nALEXANDER WEISS\na next generation of large order models is necessary. In [GSS10], the authors call\nthe large order market models with an underlying order book models of the second\ngeneration dissociation of (\ufb01rst generation) large order models working with \ufb01xed\nprice impact functions as described in the introduction. Here, a third generation\nof models is conceivable taking into account that the order book shape is not con-\nstant such that there is no one-to-one correspondence between the price and the\nvolume and that the market adapts to periodically executed large orders. Yet, it is\nalso obvious those models would be of much higher complexity and analytic results\nwould be hard to get;",
    "chunk_index": 21,
    "start_char": 47891,
    "end_char": 50799,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "of (\ufb01rst generation) large order models working with \ufb01xed\nprice impact functions as described in the introduction. Here, a third generation\nof models is conceivable taking into account that the order book shape is not con-\nstant such that there is no one-to-one correspondence between the price and the\nvolume and that the market adapts to periodically executed large orders. Yet, it is\nalso obvious those models would be of much higher complexity and analytic results\nwould be hard to get; a wide subject for future research.\nAcknowledgement. We thank Anton Bovier (University of Bonn), who sug-\ngested the article\u2019s subject to us and supported our research with many remarks,\nand Antje Fruth (QPL Berlin) for the discussions concerning the AFS model. Es-\npecially, we thank Evangelia Petrou (University of Bonn), who contributed to the\narticle\u2019s quality with a countless number of critical comments, suggestions and\nqueries.\nAppendix A. Proofs of the Theorems 4.1 and 4.2\nThe structure of the proofs remains the same as in the proofs of the corresponding\nAFS theorems (see Appendices A to C in [AFS09]). Nevertheless, we need to justify\nthe constraints on \u00af\u03c1; furthermore, the calculations become more complicated by our\ngeneralization. For simplicity, we assume t0 = 0 in this section.\nWe start with the introduction of slightly changed dynamics for the GAFS model\nand the reduction of the admissible strategies to deterministic ones. For any ad-\nmissible strategy \u03be, the new dynamics is de\ufb01ned by the processes D := (Dt)t\u22650 and\nE := (Et)t\u22650. We set D0 = Dt0 := 0 =: Et0 = E0 and\n(A.1)\nEtn+ := Etn + \u03ben and Dtn+ := F \u22121(F(Etn) + \u03ben)\nfor the trading times t0, . . . , tN. The processes\u2019 values between two successive trad-\ning times t \u2208(tn, tn+1) are given by\n(A.2)\nEt := e\u2212\u00af\u03c1(Etn+)(t\u2212tn)Etn+\nfor Version 1;\nDt := e\u2212\u00af\u03c1(Dtn+)(t\u2212tn)Dtn+\nfor Version 2.\nGiven one process, we can recover the other one by the equations (2.8):\n(A.3)\nEt = F(Dt)\nand\nDt = F \u22121(Et).\nLemma A.1. Under assumption (4.13),\n(A.4)\nEB\nt \u2264Et \u2264EA\nt and DB\nt \u2264Dt \u2264DA\nt\nfor all t \u22650. In the special case that all \u03ben are non-negative, we have DA = D and\nEA = E.\nProof. To see that DA = D and EA = E if \u03be consists of buy orders only, observe\nthat the new dynamics matches exactly the original ones for such a \u03be.\nFor the general case, we consider EB\nt \u2264Et; the other inequalities follow equiva-\nlently. Observe that it is su\ufb03cient to prove\n(A.5)\nEB\ntn+1 \u2264Etn+1\nfor\n(A.6)\nEB\ntn+ \u2264Etn+,\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n23\nsince both function are exponentially decreasing on (tn, tn+1], and the relative order\nof EB and E cannot be reversed from tn to tn+. Furthermore, we can restrict\nourselves to the case that EB\ntn+ and Etn+ have the same sign, as the signs canno\nchange in the considered time interval, (tn, tn+1].",
    "chunk_index": 22,
    "start_char": 50309,
    "end_char": 53110,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "follow equiva-\nlently. Observe that it is su\ufb03cient to prove\n(A.5)\nEB\ntn+1 \u2264Etn+1\nfor\n(A.6)\nEB\ntn+ \u2264Etn+,\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n23\nsince both function are exponentially decreasing on (tn, tn+1], and the relative order\nof EB and E cannot be reversed from tn to tn+. Furthermore, we can restrict\nourselves to the case that EB\ntn+ and Etn+ have the same sign, as the signs canno\nchange in the considered time interval, (tn, tn+1]. We are consequently done if we\ncan show that the inequality\n(A.7)\n|x + y|e\u2212\u00af\u03c1(x+y)\u03c4 > |x|e\u2212\u00af\u03c1(x)\u03c4\nfor all (x, y) \u2208R2 with sgn(x + y)=sgn(x) and |x + y| > |x|. Observe that we have\nequality in the equation above if we consider the trivial case that y = 0. We de\ufb01ne\na function ux : R \u2192R by\n(A.8)\nux(y) := (x + y)e\u2212\u00af\u03c1(x+y)\u03c4.\nDi\ufb00erentiation yields\n(A.9)\nu\u2032\nx(y) = e\u2212\u00af\u03c1(x+y)\u03c4(1 \u2212\u03c4 \u00af\u03c1\u2032(x)x).\nThe right hand side of this equation is positive by assumption (4.13), thus ux is\nstrictly increasing. Since ux(0) = xe\u2212\u00af\u03c1(x)\u03c4, (A.7) is proven.\n\u25a1\nIt remains to de\ufb01ne the simpli\ufb01ed price of \u03ben under the new dynamics by\n(A.10)\n\u00af\u03c0tn(\u03ben) :=\nZ Dtn+\nDtn\n(A0\ntn + x)f(x)dx = A0\ntn\u03ben +\nZ Dtn+\nDtn\nxf(x)dx.\nObserve that\n(A.11)\n\u00af\u03c0tn(\u03ben) \u2264\u03c0tn(\u03ben)\nfor all admissible strategies \u03be because of Lemma A.1. In particular, if \u03be consists of\nbuy orders only, we have equality.\nWe show in the next two sections that, the strategies given in the Theorems 4.2\nand 4.1, \u03be(1) and \u03be(2), are the unique minimizers of the price functional\n(A.12)\n\u00af\nC (\u03be) := E\n\" N\nX\nn=0\n\u00af\u03c0tn(\u03ben)\n#\nfor the corresponding version of the model. As \u03be(1) and \u03be(2) consist of buy orders\nonly, (A.11) and the remark afterwards imply that these strategies are also the\nminimizers of the original price functional C .\nWe turn to the reduction of \u02c6\u039e to deterministic strategies.\nLet us de\ufb01ne the\nremaining trading volume X = (Xt)t\u2208[0,T ] by\n(A.13)\nXt :=\n\u001a X0 \u2212P\ntn<t \u03ben\nfor t \u2264T\n0\nfor t > T\n.\nFurthermore, we set XtN+1 := 0. We can transform the price of a strategy \u03be \u2208\u02c6\u039e\nby\n(A.14)\nN\nX\nn=0\n\u00af\u03c0tn(\u03ben) =\nN\nX\nn=0\nA0\ntn\u03ben +\nN\nX\nn=0\nZ Dtn+\nDtn\nxf(x)dx,\nand use de\ufb01nition (A.13) as well as integration by parts to rewrite the \ufb01rst term on\nthe right hand side:\n(A.15)\nN\nX\nn=0\nA0\ntn\u03ben = \u2212\nN\nX\nn=0\nA0\ntn\n\u0000Xtn+1 \u2212Xtn\n\u0001\n= X0A0 +\nN\nX\nn=0\nXtn\n\u0010\nA0\ntn \u2212A0\ntn+1\n\u0011\n.\n\n24\nALEXANDER WEISS\nSince \u03be is admissible, X is a bounded process and Xtn is Ftn\u22121-measurable, A0 is\na martingale. Thus, the expectation of (A.15) must be X0A0. The second term on\nthe right hand side of (A.14) is deterministic for a given realization of a strategy\n\u03be(\u03c9).",
    "chunk_index": 23,
    "start_char": 52652,
    "end_char": 55162,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "n=0\nA0\ntn\n\u0000Xtn+1 \u2212Xtn\n\u0001\n= X0A0 +\nN\nX\nn=0\nXtn\n\u0010\nA0\ntn \u2212A0\ntn+1\n\u0011\n.\n\n24\nALEXANDER WEISS\nSince \u03be is admissible, X is a bounded process and Xtn is Ftn\u22121-measurable, A0 is\na martingale. Thus, the expectation of (A.15) must be X0A0. The second term on\nthe right hand side of (A.14) is deterministic for a given realization of a strategy\n\u03be(\u03c9). We denote this term by\n(A.16)\nC(i)(\u03be) :\nRN+1\n\u2192\nR\n\u03be\n7\u2192\nPN\nn=0\nR Dtn+\nDtn\nxf(x)dx\nfor Version i, i \u2208{1, 2}. Now, we can express \u00af\nC by\n(A.17)\n\u00af\nC (\u03be) = A0X0 + E(C(i)(\u03be)).\nWe spend the next two section to show C(i) has a unique minimizer in the set\n(A.18)\n\u039e :=\n(\nx := (x0, . . . , xN) \u2208RN+1 :\nN\nX\nn=0\nxn = X0\n)\nand this minimizer is determined by the formula given in Theorem 4.2 or 4.1 re-\nspectively.\nFor the sake of convenience, we introduce some more notation:\n(A.19)\n\u00afax := exp(\u2212\u03c4 \u00af\u03c1(x)) for x \u2208R,\n(A.20)\nan :=\n\u001a exp(\u2212\u03c4 \u00af\u03c1(Etn+))\nin Section A.1\nexp(\u2212\u03c4 \u00af\u03c1(Dtn+))\nin Section A.2\nfor n \u2208{0, . . ., N}.\nBecause the range of \u00af\u03c1 is assumed to be [k, K], 0 < k < K < \u221e, by (4.12),\n(A.21)\ne\u2212\u03c4K \u2264\u00afax \u2264e\u2212\u03c4k\nand\ne\u2212\u03c4K \u2264an \u2264e\u2212\u03c4k.\nAdditionally, we will need these functions:\n(A.22)\n\u02dcF(x) :=\nR x\n0 xf(x)dx\nand\nG(x) := \u02dcF(F \u22121(x)).\nObserve that\n(A.23)\nG\u2032(x) = \u02dcF \u2032(F \u22121(x))(F \u22121)\u2032(x) = F \u22121f(F \u22121(x))\n1\nf(F \u22121(x)) = F \u22121(x),\nand thus, G is twice continuously di\ufb00erentiable, non-negative, convex and has a\n\ufb01xed point in 0.\nA.1. The optimal strategy for Version 1. In this section, we calculate the\nunique minimizer of C(1) in \u039e. For any \u03be = (x0, . . . , xN) \u2208\u039e, we have\nC(1)(\u03be)\n=\nN\nX\nn=0\nZ Dtn+\nDtn\nxf(x)dx\n(A.24)\n=\nN\nX\nn=0\nh\n\u02dcF(F \u22121(Etn+)) \u2212\u02dcF(F \u22121(Etn))\ni\n=\nN\nX\nn=0\n[G(Etn + xn) \u2212G(Etn)]\nLemma A.2. The function C(1) has at least one local minimum in \u039e.\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n25\nProof. The statement will follow from\n(A.25)\nC(1)(\u03be) \u2192\u221efor ||\u03be||\u221e\u2192\u221e\nbecause C(1) is continuous. First, we use the properties of G to \ufb01nd a lower bound\nfor G(x) \u2212G(cx), x \u2208R and c \u2208[0, 1]:\nG(x) \u2212G(cx)\n\u2265\nG(cx) + (x \u2212cx)G\u2032(cx) \u2212G(cx)\n(A.26)\n=\n(1 \u2212c)|F \u22121(cx)||x|.\nThe inequality (A.26) applied to (A.24) leads to a lower bound for C(1):\nC(1)(\u03be)\n=\nG(EtN + xN) \u2212G(Et0) +\nN\u22121\nX\nn=0\n\u0002\nG(Etn + xn) \u2212G(Etn+1)\n\u0003\n=\nG\n\u0000\u0000\u03a0N\nn=0an\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + aN\u22121xN\u22121 + xN\n\u0001\n\u2212G(0)\n+\nN\u22121\nX\nn=0\nh\nG\n\u0000\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0001\n(A.27)\n\u2212G\n\u0000an\n\u0002\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0003\u0001 i\n\u2265\nG\n\u0000\u0000\u03a0N\nn=0an\n\u0001\nx0 + \u00b7",
    "chunk_index": 24,
    "start_char": 54826,
    "end_char": 57157,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "lower bound for C(1):\nC(1)(\u03be)\n=\nG(EtN + xN) \u2212G(Et0) +\nN\u22121\nX\nn=0\n\u0002\nG(Etn + xn) \u2212G(Etn+1)\n\u0003\n=\nG\n\u0000\u0000\u03a0N\nn=0an\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + aN\u22121xN\u22121 + xN\n\u0001\n\u2212G(0)\n+\nN\u22121\nX\nn=0\nh\nG\n\u0000\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0001\n(A.27)\n\u2212G\n\u0000an\n\u0002\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0003\u0001 i\n\u2265\nG\n\u0000\u0000\u03a0N\nn=0an\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + aN\u22121xN\u22121 + xN\n\u0001\n\u2212G(0)\n+\nN\u22121\nX\nn=0\nh\n(1 \u2212an)\n\f\fF \u22121 \u0000an\n\u0002\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0003\u0001\f\f\n\f\fan\n\u0002\u0000\u03a0n\u22121\nm=0am\n\u0001\nx0 + \u00b7 \u00b7 \u00b7 + an1xn\u22121 + xn\n\u0003\f\f\ni\n.\nWe de\ufb01ne a linear mapping T : RN+1 \u2192RN+1 by\n(A.28)\nT (\u03be) :=\n\u0000x0, a0x0 + x1, . . . ,\n\u0002\n\u03a0N\u22121\nn=0 an\n\u0003\nx0 + \u00b7 \u00b7 \u00b7 + aN\u22121xN\u22121 + xN\n\u0001\n,\nand the smallest an by\n(A.29)\na := min{an : n \u2208{0, . . ., N}}.\nObserve that\n(A.30)\n||T (\u03be)||\u221e\u2265||(x0, ax0 + x1, . . . , anx0 + \u00b7 \u00b7 \u00b7 + axN\u22121 + xN)||\u221e\u2192\u221e\nfor ||\u03be||\u221e\u2192\u221eas well as G(x) \u2192\u221eand |F \u22121(ax)||x| \u2192\u221efor |x| \u2192\u221e. The last\nstatement follows, because F is unbounded. Finally, we de\ufb01ne\n(A.31)\nH(x) := min\n\u0000G(x), |F \u22121(ax)||x|\n\u0001\n.\nAlso H(x) \u2192\u221efor x \u2192\u221e, and consequently,\n(A.32)\nC(1)(\u03be) \u2265H(||T (\u03be)||\u221e) \u2212G(0) \u2192\u221e.\n\u25a1\nOne has to determine \u03be(1)\n0\nby solving\n(A.33)\nF \u22121 \u0010\nX0 \u2212N\u03be(1)\n0 (1 \u2212a0)\n\u0011\n= h1(\u03be(1)\n0 )\nin Theorem 4.2. We de\ufb01ne the function\n(A.34)\n\u02c6h1(x) := h1(x) \u2212F \u22121 (X0 \u2212N(1 \u2212\u00afax)x)\n\n26\nALEXANDER WEISS\nfor which \u03be(1)\n0\nis a zero.\nLemma A.3. Given that the assumptions of Theorem 4.2 hold, function \u02c6h1 has at\nmost one zero, which is positive if it exists.\nProof. For the existence of at most one zero, it is su\ufb03cient to show that \u02c6h1 is\nstrictly increasing. The function h1 has a \ufb01xed point in 0, is positive for positive\narguments and continuous as well as bijective, thus it must be strictly increasing\nor, equivalently, its slope must be strictly positive. Consequently, the slope of \u02c6h1 is\nalso positive, because\n\u02c6h\u2032\n1(x)\n=\nh\u2032\n1(x) \u2212d\ndx\n\u0002\nF \u22121 (X0 \u2212Nx(1 \u2212\u00afax))\n\u0003\n(A.35)\n=\nh\u2032\n1(x) + N\n1 \u2212\u00afax (1 \u2212\u03c4 \u00af\u03c1\u2032(x)x)\nf(F \u22121(X0 \u2212Nx(1 \u2212\u00afax))),\n(A.36)\nand the numerator of the second term is positive by assumption (4.21). The posi-\ntivity of the zero (if existing) follows simply from\n(A.37)\n\u02c6h1(0) = \u2212F \u22121(X0) < 0.\n\u25a1\nNow, we are prepared to prove Theorem 4.2.\nLemma A.4. Strategy \u03be(1) is the unique minimizer of function C(1) and all com-\nponents of \u03be(1) are positive.\nProof. We showed in Lemma A.2 that there is an optimal strategy \u03be\u2217= (x\u2217\n0, . . . , x\u2217\nN) \u2208\n\u039e.",
    "chunk_index": 25,
    "start_char": 56873,
    "end_char": 59105,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "assumption (4.21). The posi-\ntivity of the zero (if existing) follows simply from\n(A.37)\n\u02c6h1(0) = \u2212F \u22121(X0) < 0.\n\u25a1\nNow, we are prepared to prove Theorem 4.2.\nLemma A.4. Strategy \u03be(1) is the unique minimizer of function C(1) and all com-\nponents of \u03be(1) are positive.\nProof. We showed in Lemma A.2 that there is an optimal strategy \u03be\u2217= (x\u2217\n0, . . . , x\u2217\nN) \u2208\n\u039e. Thus, there must be a Lagrange multiplier \u03bd \u2208R such that\n(A.38)\n\u2202\n\u2202x\u2217n\nC(1)(\u03be\u2217) = \u03bd\nfor n \u2208{0, . . ., N}.\nUsing representation (A.24) of C(1), one gets\n(A.39)\n\u2202\n\u2202xn\nC(1)(x) = F \u22121(Etn+) + an (1 \u2212\u00af\u03c1\u2032(Etn+)Etn+)\n\u0014\n\u2202\n\u2202xn+1\nC(1)(x) \u2212F \u22121(Etn+1)\n\u0015\nfor n \u2208{0, . . . , N \u22121}. In combination with the Langrange multiplier, the recursive\nformula yields\n(A.40)\nF \u22121(Etn+) + an (1 \u2212\u00af\u03c1\u2032(Etn+)Etn+)\n\u0002\n\u03bd \u2212F \u22121(Etn+1)\n\u0003\n= \u03bd\n(A.41)\n\u21d4\u03bd = F \u22121(Etn+) \u2212an (1 \u2212\u00af\u03c1\u2032(Etn+)Etn+) F \u22121(anEtn+)\n1 \u2212an (1 \u2212\u00af\u03c1\u2032(Etn+)Etn+)\n= h1(Etn+)\nfor n \u2208{0, . . . , N \u22121}. The function h1 is bijective by assumption, and thus,\nx\u2217\n0\n=\nh\u22121\n1 (\u03bd)\n(A.42)\nx\u2217\nn\n=\n(1 \u2212a0)x\u2217\n0 for n \u2208{1, . . ., N \u22121}\n(A.43)\nx\u2217\nN\n=\nX0 \u2212x\u2217\n0 \u2212(N \u22121)x\u2217\n0(1 \u2212a0).\n(A.44)\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n27\nTherefore, the optimal strategy \u03be\u2217is completely de\ufb01ned if we can determine x\u2217\n0.\nBy (A.24),\nC(1)(x\u2217)\n=\nG(x\u2217\n0) \u2212G(0) + (N \u22121) [G(a0x\u2217\n0 + (1 \u2212a0)x\u2217\n0) \u2212G(a0x\u2217\n0)]\n+G(a0x\u2217\n0 + X0 \u2212x\u2217\n0 \u2212(N \u22121)(1 \u2212a0)x\u2217\n0) \u2212G(a0x\u2217\n0)\n=\nN [G(x\u2217\n0) \u2212G(a0x\u2217\n0)] + G(X0 \u2212N(1 \u2212a0)x\u2217\n0) \u2212G(0)\n(A.45)\n=:\nC(1)\n0 (x\u2217\n0).\nWe know that C(1)\n0\nhas a minimum because of Lemma A.2. We can \ufb01nd it by\ndi\ufb00erentiation:\nd\ndxC(1)\n0 (x)\n(A.46)\n=\nN\n\u0002\nF \u22121(x) \u2212\u00afax[1 \u2212\u03c4 \u00af\u03c1\u2032(x)x]F \u22121(\u00afaxx)\n\u2212(1 \u2212\u00afax[1 \u2212\u03c4 \u00af\u03c1\u2032(x)x]) F \u22121(X0 \u2212N(1 \u2212\u00afax)x)\n\u0003\n=\nN (1 \u2212\u00afax[1 \u2212\u03c4 \u00af\u03c1\u2032(x)x]) \u02c6h1(x).\nAssumption (4.21) and Lemma A.3 tell us C(1) has exactly one minimum, and this\nminimum is positive. We have established the uniqueness and representation of the\noptimal strategy.\nIt remains to show that all components of x\u2217are positive. We already know that\nx\u2217\n0 > 0. The positivity of x\u2217\nn follows from (A.43) for all n \u2208{1, . . ., N \u22121}. For\nthe last order, x\u2217\nN, observe that (A.46) vanishes in x\u2217\n0. Furthermore, F \u22121 is strictly\nincreasing, and thus,\n0\n=\nF \u22121(x\u2217\n0) \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)F \u22121(a0x\u2217\n0)\n\u2212[1 \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)] F \u22121(X0 \u2212N(1 \u2212a0)x\u2217\n0\n|\n{z\n}\n=x\u2217\nN+a0x\u2217\n0\n)\n(A.47)\n>\n[1 \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)]\n\u0002\nF \u22121(a0x\u2217\n0) \u2212F \u22121(a0x\u2217\n0 + x\u2217\nN)\n\u0003\n,\n(A.48)\nwhich, indeed, implies the positivity of x\u2217\nN.\n\u25a1\nA.2. The optimal strategy for Version 2. In this section, we determine the\nunique minimizer of C(2) in \u039e.",
    "chunk_index": 26,
    "start_char": 58745,
    "end_char": 61211,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "and thus,\n0\n=\nF \u22121(x\u2217\n0) \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)F \u22121(a0x\u2217\n0)\n\u2212[1 \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)] F \u22121(X0 \u2212N(1 \u2212a0)x\u2217\n0\n|\n{z\n}\n=x\u2217\nN+a0x\u2217\n0\n)\n(A.47)\n>\n[1 \u2212a0(1 \u2212\u03c4 \u00af\u03c1\u2032(x\u2217\n0)x\u2217\n0)]\n\u0002\nF \u22121(a0x\u2217\n0) \u2212F \u22121(a0x\u2217\n0 + x\u2217\nN)\n\u0003\n,\n(A.48)\nwhich, indeed, implies the positivity of x\u2217\nN.\n\u25a1\nA.2. The optimal strategy for Version 2. In this section, we determine the\nunique minimizer of C(2) in \u039e. For \u03be = (x0, . . . , xN) \u2208\u039e, we have\nC(2)(\u03be)\n=\nN\nX\nn=0\nZ Dtn+\nDtn\nxf(x)dx\n(A.49)\n=\nN\nX\nn=0\n\u0010\nG(xn + F(Dtn)) \u2212\u02dcF(Dtn)\n\u0011\nLemma A.5. The function C(2) has a local minimum in \u039e.\nProof. Again, it su\ufb03ces to show\n(A.50)\nC(2)(\u03be) \u2192\u221efor ||\u03be||\u221e\u2192\u221e.\n\n28\nALEXANDER WEISS\nWe rearrange (A.49) and get\nC(2)(\u03be)\n(A.51)\n=\nN\nX\nn=0\n\u0010\n\u02dcF(F \u22121(xn + F(Dtn))) \u2212\u02dcF(Dtn)\n\u0011\n=\n\u02dcF(aNF \u22121(xN + F(DtN)))\n+\nN\nX\nn=0\n\u0010\n\u02dcF(F \u22121(xn + F(Dtn))) \u2212\u02dcF(anF \u22121(xn + F(Dtn)))\n\u0011\n\u2265\nN\nX\nn=0\n\u0010\n\u02dcF(F \u22121(xn + F(Dtn))) \u2212\u02dcF(anF \u22121(xn + F(Dtn)))\n\u0011\nA lower bound for the last line of (A.51) is given by\n\u02dcF(x) \u2212\u02dcF(\u00afaxx)\n=\n\f\f\f\f\nZ x\n\u00afaxx\nzf(z)dz\n\f\f\f\f\n(A.52)\n\u2265\ninf\ny\u2208[\u00afaxx,x] f(y)\n\f\f\f\f\nZ x\n\u00afaxx\nzdz\n\f\f\f\f\n=\n1\n2(1 \u2212\u00afa2\nx)x2\ninf\ny\u2208[\u00afaxx,x]f(y) \u22650.\nBecause of the assumptions (4.14) and (2.17), we know\n(A.53)\nH(x) := 1\n2(1 \u2212\u00afa2\nF \u22121(x))(F \u22121(x))2\ninf\ny\u2208[\u00afaF \u22121(x)F \u22121(x),F \u22121(x)] f(y).\ntends to in\ufb01nity for |x| \u2192\u221e. Finally, we introduce the mapping\n(A.54)\nT (x) := (x0, x1 + F(Dt1), . . . , xN + F(DtN)),\nfor which C(2)(x) \u2265H(||T (x)||\u221e) holds. It remains to show that ||T (x)||\u221e\u2192\u221e\nfor |x| \u2192\u221e. Let us assume there is a sequence xk such that ||xk||\u221e\u2192\u221ebut\n||T (xk)||\u221eremains bounded. This implies especially the boundedness of (xk\n0). But\nthen again, Dk\nt1 = ak\n0F \u22121(xk\n0) remains bounded. We can continue the argumen-\ntation for all coordinates of T (x) and conclude that (xk\nn) is a bounded sequence\nfor all n \u2208{0, . . . , N}. This contradicts the assumption, and thus the lemma is\nproven.\n\u25a1\nLemma A.6. Under the assumptions of Theorem 4.1, equation (4.16) has at most\none solution, which is positive if existing. Furthermore, g(x) := f(x)\u2212\u00afaxf(\u00afaxx)(1\u2212\n\u03c4 \u00af\u03c1\u2032(x)x) is positive.\nProof. We show that both h2 \u25e6F \u22121 and\n(A.55)\n\u02c6h2(x) := \u2212F \u22121 \u0000X0 \u2212N\n\u0002\nx \u2212F\n\u0000\u00afaF \u22121(x)F \u22121(x)\n\u0001\u0003\u0001\nare strictly increasing. In this case, at most one zero can exist, and its positivity\nis guaranteed by h2(F \u22121(0)) = 0 and \u02c6h2(0) = \u2212F(X0) < 0. The function h2 is\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n29\nstrictly increasing because it is continuous, bijective, has a \ufb01xed point at zero and\nlim\n\u01eb\u21920\nh2(\u01eb) \u2212h2(0)\n\u01eb\n=\nlim\n\u01eb\u21920\nf(\u01eb) \u2212\u00afa2\n\u01ebf(\u00afa\u01eb\u01eb)(1 \u2212\u03c4 \u00af\u03c1\u2032(\u01eb)\u01eb)\nf(\u01eb) \u2212\u00afa\u01ebf(\u00afa\u01eb\u01eb)(1 \u2212\u03c4 \u00af\u03c1\u2032(\u01eb)\u01eb)\n(A.56)\n=\n1 \u2212\u00afa2\n0\n1 \u2212\u00afa0\n> 0.",
    "chunk_index": 27,
    "start_char": 60839,
    "end_char": 63357,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "this case, at most one zero can exist, and its positivity\nis guaranteed by h2(F \u22121(0)) = 0 and \u02c6h2(0) = \u2212F(X0) < 0. The function h2 is\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n29\nstrictly increasing because it is continuous, bijective, has a \ufb01xed point at zero and\nlim\n\u01eb\u21920\nh2(\u01eb) \u2212h2(0)\n\u01eb\n=\nlim\n\u01eb\u21920\nf(\u01eb) \u2212\u00afa2\n\u01ebf(\u00afa\u01eb\u01eb)(1 \u2212\u03c4 \u00af\u03c1\u2032(\u01eb)\u01eb)\nf(\u01eb) \u2212\u00afa\u01ebf(\u00afa\u01eb\u01eb)(1 \u2212\u03c4 \u00af\u03c1\u2032(\u01eb)\u01eb)\n(A.56)\n=\n1 \u2212\u00afa2\n0\n1 \u2212\u00afa0\n> 0.\n(A.57)\nSince F \u22121 is also strictly increasing, we have proven the same property for h2\u25e6F \u22121.\nWe di\ufb00erentiate \u02c6h2:\n(A.58)\n\u02c6h\u2032\n2(x) = N\n\"\nf(F \u22121(x)) \u2212\u00afaF \u22121(x)f(\u00afaF \u22121(x)F \u22121(x))(1 \u2212\u03c4 \u00af\u03c1\u2032(F \u22121(x))F \u22121(x))\nf(F \u22121(x))f\n\u0000F \u22121 \u0000X0 \u2212N\n\u0002\nx \u2212F(\u00afaF \u22121(x)F \u22121(x))\n\u0003\u0001\u0001\n#\nThis expression is strictly positive because the numerator is strictly positive as we\nshow next. We de\ufb01ne both\n(A.59)\nk(x)\n:=\nf(x) \u2212\u00afaxf(\u00afaxx)(1 \u2212\u03c4 \u00af\u03c1\u2032(x)x) and\nk2(x)\n:=\nf(x) \u2212\u00afa2\nxf(\u00afaxx)(1 \u2212\u03c4 \u00af\u03c1\u2032(x)x).\nThe numerater of (A.58) can be expressed by k(F \u22121(x)), and furthermore, h2(x) =\nxk2(x)/k(x). Both functions k and k2 are continuous, and due to the properties of\nh explained in the beginning of the proof, the functions must have the same sign\nfor all x \u2208R. The function k2 is greater than k for all x \u2208R; thus, there can be\nno change of signs and we have either k(x) > 0 and k2(x) > 0 or k(x) < 0 and\nk2(x) < 0 for all x. Because k(0) = f(0)(1 \u2212\u00afa0) > 0, positivity is proven.\n\u25a1\nLemma A.7. For all n \u2208{0, . . . , N \u22121}, the partial derivatives of C(1) can be\nexpressed by\n\u2202\n\u2202xn\nC(2)(x)\n=\nDtn+ + anf(Dtn+1)(1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+)\nf(Dtn+)\n\u0014\n\u2202\n\u2202xn+1\nC(2)(x) \u2212Dtn+1\n\u0015\n.\nProof. First, observe\n(A.60)\n\u2202\n\u2202xn\nDtm = anf(Dtn+1)\nf(Dtn+)\n(1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+)\n\u2202\n\u2202xn+1\nDtm\nfor n \u2208{0, . . . , m \u22122}. This follows from\n\u2202\n\u2202xn\nDtm (A.61)\n=\n\u2202\n\u2202xn\n\u0002\nam\u22121F \u22121(xm\u22121 + F(. . . (anF \u22121(xn + F(Dtn))) . . . ))\n\u0003\n=\n\" m\u22121\nY\nk=n+1\n\u0014 d\ndx\u00afaF \u22121(xk+F (x))F \u22121(xk + F(x))\n\u0015\nx=Dtk\n# \u0014 \u2202\n\u2202xn\n\u00afaF \u22121(xn+F (Dtn ))F \u22121(xn + F(Dtn))\n\u0015\n=\n\u0014\nf(Dtn+1)\n\u2202\n\u2202xn+1\nDtm\n\u0015 \u0014an (1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+)\nf(Dtn+)\n\u0015\n.\n\n30\nALEXANDER WEISS\nWe use (A.60) and (A.49) for the transformation\n\u2202\n\u2202xn\nC(2)(x)\n(A.62)\n=\nF \u22121(xn + F(Dtn)) +\nN\nX\nm=n+1\n\u2202\n\u2202xn\nh\nG(xm + F(Dtm)) \u2212\u02dcF(Dtm)\ni\n=\nDtn+ +\nN\nX\nm=n+1\nf(Dtm)\n\u0014 \u2202\n\u2202xn\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003\n=\nDtn+ + anf(Dtn+1)\nf(Dtn+)\n[1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+]\n\u0010\nDtn+1+ \u2212Dtn+1\n+\nN\nX\nm=n+2\nf(Dtm)\n\u0014\n\u2202\n\u2202xn+1\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003 \u0011\n.\nNow, the same calculation for \u2202C(2)(x)/\u2202xn+1 results in\n(A.63)\n\u2202\n\u2202xn\nDtm = Dtn+1+ +\nN\nX\nm=n+2\nf(Dtm)\n\u0014\n\u2202\n\u2202xn+1\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003\n,\nand combining (A.62) and (A.63) yields the desired result.",
    "chunk_index": 28,
    "start_char": 62953,
    "end_char": 65438,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "=\nDtn+ +\nN\nX\nm=n+1\nf(Dtm)\n\u0014 \u2202\n\u2202xn\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003\n=\nDtn+ + anf(Dtn+1)\nf(Dtn+)\n[1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+]\n\u0010\nDtn+1+ \u2212Dtn+1\n+\nN\nX\nm=n+2\nf(Dtm)\n\u0014\n\u2202\n\u2202xn+1\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003 \u0011\n.\nNow, the same calculation for \u2202C(2)(x)/\u2202xn+1 results in\n(A.63)\n\u2202\n\u2202xn\nDtm = Dtn+1+ +\nN\nX\nm=n+2\nf(Dtm)\n\u0014\n\u2202\n\u2202xn+1\nDtm\n\u0015 \u0002\nF \u22121(xm + F(Dtm)) \u2212Dtm\n\u0003\n,\nand combining (A.62) and (A.63) yields the desired result.\n\u25a1\nFinally, we are prepared to prove Theorem 4.1.\nLemma A.8. Strategy \u03be(2) is the unique minimizer of function C(2) and all com-\nponents of \u03be(2) are positive.\nProof. Lemma A.5 guarantees the existence of at least one optimal strategy \u03be\u2217\u2208\u039e.\nBy standard arguments, there is a Lagrange multiplier \u03bd \u2208R such that\n(A.64)\n\u2202\n\u2202x\u2217n\nC(2)(\u03be\u2217) = \u03bd\nfor n \u2208{0, . . ., N}.\nWe use Lemma A.7 to get\n(A.65)\n\u03bd = Dtn+ + anf(anDtn+)\nf(Dtn+)\n(1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+) [\u03bd \u2212anDtn+]\n(A.66)\n\u21d4\u03bd = Dtn+\nf(Dtn+) \u2212a2\nnf(anDtn+)(1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+)\nf(Dtn+) \u2212anf(anDtn+)(1 \u2212\u03c4 \u00af\u03c1\u2032(Dtn+)Dtn+) = h2(Dtn+)\nfor n \u2208{0, . . . , N \u22121}. Function h2 is one-to-one, and thus,\n(A.67)\n\u03bd = h2(F \u22121(x\u2217\nn + F(Dtn)))\nimplies that x\u2217\nn + F(Dtn) does not depend on n \u2208{0, . . . , N \u22121}. Consequently,\nDtn+ = F \u22121(x\u2217\nn + F(Dtn)) is constant in n such that we can conclude\nx\u2217\n0\n=\nF(h\u22121\n2 (\u03bd)),\n(A.68)\nx\u2217\nn\n=\nx\u2217\n0 \u2212F(Dtn) = x\u2217\n0 \u2212F(a0F \u22121(x\u2217\n0)) for n \u2208{1, . . . , N \u22121},\n(A.69)\nx\u2217\nN\n=\nX0 \u2212x\u2217\n0 \u2212(N \u22121)\n\u0002\nx\u2217\n0 \u2212F(a0F \u22121(x\u2217\n0))\n\u0003\n.\n(A.70)\n\nEXECUTING LARGE ORDERS IN A MICROSCOPIC MARKET MODEL\n31\nThe value x\u2217\n0 determines the optimal solution completely, and thus, it must mini-\nmize\nC(2)\n0 (x0)\n:=\nC(2) \u0000x0, x0 \u2212F\n\u0000a0F \u22121(x0)\n\u0001\n, . . . , X0 \u2212x0 \u2212(N \u22121)\n\u0002\nx0 \u2212F(a0F \u22121(x0))\n\u0003\u0001\n(A.49)\n=\nG(x0) +\nN\u22121\nX\nn=1\nh\nG(x0) \u2212\u02dcF(a0F \u22121(x0))\ni\n+ G(X0 \u2212N[x0 \u2212F(a0F \u22121(x0))]) \u2212\u02dcF(a0F \u22121(x0))\n=\nN[G(x0) \u2212\u02dcF(a0F \u22121(x0))] + G(X0 \u2212N[x0 \u2212F(a0F \u22121(x0))]).\nDi\ufb00erentiation results in\ndC(2)\n0 (x0)\ndx0\n=\nN\n\"\nD0+ \u2212a2\n0D0+\nf(Dt1)\nf(D0+) (1 \u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+)\n(A.71)\n+ Dtn+\n\u0012\na0\nf(Dt1)\nf(D0+) (1 \u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+) \u22121\n\u0013 #\nsuch that we can calculate the minimizer by\n(A.72)\nd\ndx\u2217\n0 C(2)\n0 (x\u2217\n0)\n=\n0\n\u21d4\nDtN+\n=\nD0+\nf(D0+)\u2212a2\n0f(Dt1 )(1\u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+)\nf(D0+)\u2212a0f(Dt1 )(1\u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+).\nThe left hand side of the last line can be rewritten as\nDtN+\n=\nF \u22121(F(DtN ) + x\u2217\nN)\n(A.73)\n=\nF \u22121(F(Dt1) + X0 \u2212x\u2217\n0 \u2212(N \u22121)(x\u2217\n0 \u2212F(Dt1)))\n=\nF \u22121(X0 \u2212N(x\u2217\n0 \u2212F(Dt1))\nand the right hand side is just h2(F \u22121(x\u2217\n0)). We know by Lemma A.6 that equation\n(A.72) has at most one zero such that we are \ufb01nished with the existence, uniqueness\nand representation of the optimal strategy.",
    "chunk_index": 29,
    "start_char": 65036,
    "end_char": 67503,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "DtN+\n=\nD0+\nf(D0+)\u2212a2\n0f(Dt1 )(1\u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+)\nf(D0+)\u2212a0f(Dt1 )(1\u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+).\nThe left hand side of the last line can be rewritten as\nDtN+\n=\nF \u22121(F(DtN ) + x\u2217\nN)\n(A.73)\n=\nF \u22121(F(Dt1) + X0 \u2212x\u2217\n0 \u2212(N \u22121)(x\u2217\n0 \u2212F(Dt1)))\n=\nF \u22121(X0 \u2212N(x\u2217\n0 \u2212F(Dt1))\nand the right hand side is just h2(F \u22121(x\u2217\n0)). We know by Lemma A.6 that equation\n(A.72) has at most one zero such that we are \ufb01nished with the existence, uniqueness\nand representation of the optimal strategy.\nAt last, we show that all components of this strategy are positive. We already\nknow x\u2217\n0 > 0 and thus also x\u2217\nn > 0 for all n \u2208{1, . . . , N \u22121} by (A.69). For the\npositivity of x\u2217\nN, we transform (A.72) into\n(A.74)\nDtN+ = D0+\n\u0014\n1 +\na0f(a0D0+) \u2212a2\n0f(a0D0+)\nf(D0+) \u2212a0f(a0Dt0+)(1 \u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+) (1 \u2212\u03c4 \u00af\u03c1\u2032(D0+)D0+)\n\u0015\n.\nThe fraction on the right hand side is strictly positive by Lemma A.6; positivity of\nx\u2217\nN follows from\n(A.75)\nDtN+ > D0+ = DtN\na0\n> DtN.\n\u25a1\nReferences\n[AC01]\nRobert Almgren and Neil Chriss. Optimal execution of portfolio transactions. J. Risk,\n3(2):5\u201339, January 2001.\n\n32\nALEXANDER WEISS\n[AFS09]\nAur\u00b4elien Alfonsi, Antje Fruth, and Alexander Schied. Optimal execution strate-\ngies\nin\nlimit\norder\nbooks\nwith\ngeneral\nshape\nfunctions.\nQuant.\nFinance,\ndoi:10.1080/14697680802595700, June 2009.\n[AS09]\nAur\u00b4elien Alfonsi and Alexander Schied. Optimal execution and absence of price ma-\nnipulations in limit order book models. SSRN Working Paper 1499209, SSRN, July\n2009.\n[ATHL05]\nRobert Almgren, Chee Thum, Emmanuel Hauptmann, and Hong Li. Equity market\nimpact. Risk, pages 21\u201328, July 2005. slightly shortened version of \u2018Direct estimation\nof equity market impact\u2019.\n[B\u02c7CH06]\nAnton Bovier, Ji\u02c7r\u00b4\u0131 \u02c7Cern\u00b4y, and Ostap Hryniv. The opinion game: Stock price evolution\nfrom microscopic market modeling. Int. J. Theoretical Appl. Finance, 9(1):91\u2013111,\nFebruary 2006.\n[BGPW04] Jean-Philippe Bouchaud, Yuval Gefen, Marc Potters, and Matthieu Wyart. Fluctua-\ntions and response in \ufb01nancial markets: the subtle nature of random price changes.\nQuant. Finance, 4(2):176\u2013190, April 2004.\n[BL98]\nDmitris Bertsimas and Andrew Lo. Optimal control of execution costs. J. Financ.\nMark., 1(1):1\u201350, April 1998.\n[BPV09]\nHendrik Bessembinder, Marios Panayides, and Kumar Venkataraman. Hidden liquid-\nity: An analysis of order exposure strategies in electronic stock markets. J. Finan.\nEcon., 94(3):361\u2013383, December 2009.\n[Con01]\nRama Cont. Empirical properties of asset returns: stylized facts and statistical issues.\nQuant. Finance, 1(2):223\u2013236, February 2001.\n[FS09]\nStefan Frey and Patrik Sandas. The impact of iceberg orders in limit order books.\nCFR Working Paper 09-06, Centre for Financial Research Cologne, May 2009.\n[Gat09]\nJim Gatheral. No-dynamic-arbitrage and market impact. SSRN Working Paper\n1292353, SSRN, September 2009. accepted by Quant. Finance.\n[GB03]\nIrene Giardina and Jean-Philippe Bouchaud. Bubbles, crashes and intermittency in\nagent based market models. Eur. Phys. J. B, 31(3):421\u2013437, February 2003.\n[GSS10]\nJim Gatheral, Alexander Schied, and Alla Slynko. Transient linear price impact and\nFredholm integral equations. SSRN Working Paper 1531466, SSRN, January 2010.\n[HS04]\nGur Huberman and Werner Stanzl. Price manipulation and quasi-arbitrage. Econo-\nmetrica, 72(4):1247\u20131275, July 2004.",
    "chunk_index": 30,
    "start_char": 67045,
    "end_char": 70315,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "Research Cologne, May 2009.\n[Gat09]\nJim Gatheral. No-dynamic-arbitrage and market impact. SSRN Working Paper\n1292353, SSRN, September 2009. accepted by Quant. Finance.\n[GB03]\nIrene Giardina and Jean-Philippe Bouchaud. Bubbles, crashes and intermittency in\nagent based market models. Eur. Phys. J. B, 31(3):421\u2013437, February 2003.\n[GSS10]\nJim Gatheral, Alexander Schied, and Alla Slynko. Transient linear price impact and\nFredholm integral equations. SSRN Working Paper 1531466, SSRN, January 2010.\n[HS04]\nGur Huberman and Werner Stanzl. Price manipulation and quasi-arbitrage. Econo-\nmetrica, 72(4):1247\u20131275, July 2004.\n[HS05]\nGur Huberman and Werner Stanzl. Optimal liquidity trading. Rev. Finance, 9(2):165\u2013\n200, June 2005.\n[OW05]\nAnna Obizhaeva and Jiang Wang. Optimal trading strategy and supply/demand dy-\nnamics. revised and resubmitted, J. Financ. Mark., 2005.\n[PB03]\nMarc Potters and Jean-Philippe Bouchaud. More statistical properties of order books\nand price impact. Phys. A, 324(1\u20132):133\u2013140, June 2003.\n[Sch08]\nTorsten Sch\u00a8oneborn. Trade execution in illiquid markets: Optimal stochastic control\nand multi-agent equilibria. dissertation, TU Berlin, May 2008.\n[SFGK03]\nEric Smith, J Doyne Farmer, L\u00b4aszl\u00b4o Gillemot, and Supriya Krishnamurthy. Statistical\ntheory of the continuous double auction. Quant. Finance, 3(6):481\u2013514, December\n2003.\n[Wei09a]\nAlexander Wei\u00df. Aspects of microscopic modelling in \ufb01nance. dissertation, TU Berlin,\nOctober 2009.\n[Wei09b]\nAlexander Wei\u00df. Escaping the Brownian stalkers. Electron. J. Probab., 14(7):139\u2013160,\nJanuary 2009.\n[WR05]\nPhilipp Weber and Bernd Rosenow. Order book approach to price impact. Quant.\nFinance, 5(4):357\u2013364, August 2005.\nWeierstrass Institute for Applied Analysis and Stochastics, Mohrenstrasse 39, 10117\nBerlin, Germany\nE-mail address: alexander.weiss@wias-berlin.de",
    "chunk_index": 31,
    "start_char": 69695,
    "end_char": 71530,
    "paper_title": "Executing large orders in a microscopic market mod",
    "paper_category": "q-fin.TR",
    "paper_filename": "Executing_large_orders_in_a_microscopic_market_mod.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Executing_large_orders_in_a_microscopic_market_mod.pdf"
  },
  {
    "text": "Forecast-to-Fill: Benchmark-Neutral Alpha and Billion-Dollar\nCapacity in Gold Futures (2015\u20132025)\nMainak Singha\nNASA, Goddard Space Flight Center, Greenbelt, MD, USA\nDepartment of Physics, The Catholic University of America, Washington, DC, USA\nEmail:\nmainak.singha@nasa.gov, singham@cua.edu\nJose Aguilera-Toste\nMassachusetts Institute of Technology, Cambridge, MA, USA\nPolytechnic University of Madrid, Madrid, Spain\nVinayak Lahiri\nPanth\u00b4eon-Sorbonne University, Paris, France\nNovember 12, 2025\nAbstract\nWe test whether simple, interpretable state variables\u2014trend and momentum\u2014can produce\ndurable, out-of-sample alpha in one of the world\u2019s most liquid assets, gold. Using a rolling\n10-year train \u21926-month test walk-forward from 2015\u20132025 (2,793 trading days), we convert\na smoothed trend\u2013momentum regime signal into volatility-targeted, friction-aware positions\nvia fractional, impact-adjusted Kelly sizing and ATR-based exits. Out-of-sample, the strategy\ndelivers a Sharpe ratio of 2.88, and maximum drawdown 0.52%, net of a 0.7 bps cost and a\nsquare-root impact term (\u03b3 = 0.02). A regression on spot-gold returns yields a 43% annualized\nreturn , a CAGR of 43%, and a 37% alpha (Sharpe 2.88, IR 2.09), at a targeted volatility of 15%)\nand \u03b2 = 0.03, confirming benchmark-neutral performance. A friction-adjusted growth curve\ng(L) = \u00b5uL \u22121\n2(\u03c3uL)2 \u2212nkL \u2212\u03b3(nL)3/2 defines a positive-growth frontier up to \u223c$1 billion\nAUM\u2014only 0.07% of daily CME gold volume\u2014beyond which impact concavely limits returns.\nStatistical tests (bootstrap CI [2.49, 3.27]; SPA p = 0.000) confirm significance and robustness\nto latency, reversal, and cost stress. We conclude that forecast-to-fill engineering\u2014linking\ntransparent signals to executable trades with explicit risk, cost, and impact control\u2014can turn\nmodest predictability into allocator-grade, billion-dollar-scalable alpha.\n1\nIntroduction\nWe start with a simple question. If we build a clear, easy-to-understand signal that looks at whether\ngold prices are trending up or down, can we actually turn that signal into real profits once we\naccount for the messy details of trading\u2014like how risky each day is, how much trading costs eat\ninto returns, when to enter and exit, and how the strategy would behave if we tested it as if it were\nrunning live? This question matters because many trading ideas look great on paper but fall apart\nwhen tested in real life. It is easy to design a back-test that seems to \u201cpredict\u201d prices if we ignore\ntrading costs or assume perfect timing. In reality, markets are noisy, trades slip, and every action\n1\narXiv:2511.08571v1 [q-fin.TR] 11 Nov 2025\n\ncarries a cost. The real challenge is not forecasting prices, but seeing whether a simple and honest\nrule still holds up after all those frictions are added. If a clear rule like \u201cfollow the trend when\nit is strong\u201d continues to work once we include risk, costs, and execution limits, then the edge is\nlikely genuine rather than luck or overfitting. This matters because it turns theory into something\npractical. Many models look brilliant in hindsight, but few survive live trading. Our question asks\nwhether careful, transparent engineering can turn small, consistent patterns in the data into real,\ndurable performance.\nWe are not trying to predict exact price movements\u2014that is nearly impossible.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3320,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "it is strong\u201d continues to work once we include risk, costs, and execution limits, then the edge is\nlikely genuine rather than luck or overfitting. This matters because it turns theory into something\npractical. Many models look brilliant in hindsight, but few survive live trading. Our question asks\nwhether careful, transparent engineering can turn small, consistent patterns in the data into real,\ndurable performance.\nWe are not trying to predict exact price movements\u2014that is nearly impossible. Instead, we ask\nwhether markets offer small, repeatable rewards that appear under certain conditions and disappear\nunder others. For example, when gold is trending strongly, traders who hedge or rebalance may\ntemporarily push prices in one direction, creating a short-lived edge. If we manage risk carefully\nand trade with discipline, we can capture some of that reward without needing perfect forecasts. In\nshort, Our goal is not to guess the future, but to earn a consistent premium from how the market\nbehaves in different states\u2014using engineering and validation strong enough to survive skeptical\nscrutiny.\nGold provides an ideal proving ground for systematic research because its price dynamics are\nalmost entirely information-driven rather than dependent on cash flows or credit risk. Historically,\ngold has moved inversely to the U.S. dollar and real yields, making it a natural hedge against\nmonetary debasement and inflation shocks. Its deep liquidity and long price history allow robust\nstatistical testing, while its role as a \u201cflight-to-quality\u201d asset in risk-off regimes adds an additional\nbehavioral dimension. These features make gold a clean laboratory for studying how information,\nsentiment, and macro conditions translate into price movement\u2014without the confounding effects of\nearnings or balance-sheet fundamentals.\nWe make four contributions.\n1. From forecast to actual trades. We build a full pipeline that connects a simple market\nsignal to real, executable trades. It starts with smoothing prices to see the overall direction,\nmeasures how confident we are in that trend, adjusts position size based on market volatility,\nand finally decides how much to risk using a version of Kelly sizing that accounts for trading\ncosts. We also include clear stop and exit rules so trades are never left open without control.\n2. Testing the right way. We use a strict \u201cwalk-forward\u201d setup that mimics live trading. The\nmodel trains on ten years of past data, then is tested on the next six months\u2014never looking\nahead. We repeat this process month by month and only report results from the test periods.\nThis prevents any \u201ccheating\u201d from future information and makes our results realistic.\n3. Showing the alpha is real. We prove the gains are not random or tied to the gold market\nitself.\nThe strategy produces returns that are mostly independent of overall gold prices\n(\u03b2 \u22480.03) and are statistically significant in multiple tests. It still works when we add delays,\nreverse the signal, or remove pieces of the model\u2014meaning the edge is stable and not just a\nlucky backtest.\n4. Understanding how big it can scale. We calculate how trading costs and market impact\nreduce growth as position size increases.",
    "chunk_index": 1,
    "start_char": 2822,
    "end_char": 6036,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "alpha is real. We prove the gains are not random or tied to the gold market\nitself.\nThe strategy produces returns that are mostly independent of overall gold prices\n(\u03b2 \u22480.03) and are statistically significant in multiple tests. It still works when we add delays,\nreverse the signal, or remove pieces of the model\u2014meaning the edge is stable and not just a\nlucky backtest.\n4. Understanding how big it can scale. We calculate how trading costs and market impact\nreduce growth as position size increases. Using a friction-adjusted version of Kelly\u2019s formula,\nwe map out where returns start to level off. This gives an honest estimate of how much capital\nthe strategy could handle before performance fades.\nWe foreshadow our answer. In short, the strategy works the way it should. Out-of-sample, it\ndelivers a strong Sharpe ratio of about 2.9 with very small drawdowns and steady profits on the days\n2\n\nit is active. It remains profitable even after adding realistic trading costs and small execution delays.\nWhen we intentionally flip the signal or remove key parts like trend or momentum, performance\ncollapses\u2014as it should if the edge is genuine. Statistical tests confirm the results are not due to luck,\nand our estimates of capacity show that the strategy could realistically scale to hundreds of millions\nof dollars before impact starts to matter. In other words, the system behaves exactly as a robust,\nwell-engineered trading model should: clear rules, stable performance, and no hidden curve-fitting.\n2\nData, preprocessing, and scope\n2.1\nCoverage and alignment\nWe use daily settlement prices for gold from the London Bullion Market Association (LBMA) and\nCOMEX. These are the most reliable and liquid data sources available, meaning they reflect real\nprices that major traders and funds actually transact on. Using this dataset avoids the noise and\ninconsistencies that appear in smaller feeds or intraday ticks.\nThe data we use runs through September 16, 2025. Our out-of-sample backtest is extended to\nOctober 31, 2025, to align with the New York Stock Exchange business-day calendar. On holidays or\nmissing days, we simply carry forward the last known price\u2014never the return\u2014so that the sequence\nof daily moves remains realistic and continuous. Returns are computed as simple close-to-close\npercentage changes:\nrt =\nPt\nPt\u22121\n\u22121.\nThis ensures the results reflect actual price evolution, not artifacts from missing data. Reliable data\nis the foundation of any backtest. If the prices are off by even a few basis points, the small edge we\nare trying to measure could vanish or flip sign. Using standardized LBMA/COMEX data eliminates\nthat risk and keeps the analysis grounded in market reality.\n2.2\nEffective sample used by the walk-forward\nOur walk-forward process requires a long history to train on before we begin testing. Each model\nlooks back ten full years of daily data to learn its parameters, then it is tested on the next six\nmonths. Because our first out-of-sample window starts in January 2015, the first training window\nmust begin around January 2005.",
    "chunk_index": 2,
    "start_char": 5536,
    "end_char": 8605,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "Using standardized LBMA/COMEX data eliminates\nthat risk and keeps the analysis grounded in market reality.\n2.2\nEffective sample used by the walk-forward\nOur walk-forward process requires a long history to train on before we begin testing. Each model\nlooks back ten full years of daily data to learn its parameters, then it is tested on the next six\nmonths. Because our first out-of-sample window starts in January 2015, the first training window\nmust begin around January 2005.\nThis means that although raw data go all the way back to 1980, only the 2005\u20132025 period is\nactively used. The earlier data are not needed and would add little informational value because\nmarket microstructure and liquidity conditions before 2005 were very different. The goal is not to\nuse as much data as possible, but to use data that actually resemble today\u2019s market. Ten years is\nlong enough to cover multiple economic cycles but short enough that the model still represents the\ncurrent trading environment.\n2.3\nFrictions\nEvery trade has a cost, so we model that cost directly. There are two main parts:\n1. Linear cost: we assume a round-trip cost of k = 0.7 basis points (0.007%) per complete\nbuy\u2013sell cycle. This approximates the bid\u2013ask spread and broker fees in highly liquid gold\nfutures.\n3\n\n2. Market impact: large trades move prices against the trader. We represent this with a\n\u201csquare-root\u201d impact function, where the cost increases with the square root of trade size. We\nset the impact parameter to \u03b3 = 0.02, which is conservative for gold liquidity.\nWe test cost multipliers (0.5\u00d7, 1\u00d7, 1.5\u00d7, 2\u00d7) later to stress the model under heavier friction.\nMany backtests ignore transaction costs, but even a small underestimation can turn a good strategy\ninto a losing one. By including both spread and impact, we ensure that any observed alpha could\nactually survive live execution.\n2.4\nRisk targets and constraints\nTo keep returns stable, we target a fixed volatility of 15% per year. This means the strategy\nautomatically takes smaller positions when the market is volatile and larger ones when it is calm.\nLeverage is capped at Wmax = 2.0, so the system can never double its exposure.\nPosition size is determined by a fractional Kelly multiplier of \u03bbKelly = 0.40, which means we\nonly risk 40% of the mathematically optimal Kelly fraction. When the Kelly estimate is near\nzero\u2014meaning the edge is barely above costs\u2014we still take a small baseline position equal to 25% of\nthe usual size. This keeps the model from sitting idle in uncertain periods. Volatility targeting keeps\nrisk consistent across time, which makes results easier to interpret and prevents sudden blowups.\nThe capped leverage and fractional Kelly scaling keep the strategy realistic and psychologically\ntradable for a real fund.\n2.5\nWhat we do not do in FAST mode\nIn the headline configuration, called FAST, we intentionally avoid adding slow safety filters such as\nlong-term moving-average (DMA) filters, daily loss caps, or turnover throttles. Those belong in the\nSTRESSED configuration, which is used later for operational testing.",
    "chunk_index": 3,
    "start_char": 8128,
    "end_char": 11216,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "in uncertain periods. Volatility targeting keeps\nrisk consistent across time, which makes results easier to interpret and prevents sudden blowups.\nThe capped leverage and fractional Kelly scaling keep the strategy realistic and psychologically\ntradable for a real fund.\n2.5\nWhat we do not do in FAST mode\nIn the headline configuration, called FAST, we intentionally avoid adding slow safety filters such as\nlong-term moving-average (DMA) filters, daily loss caps, or turnover throttles. Those belong in the\nSTRESSED configuration, which is used later for operational testing.\nThe goal of FAST mode is to isolate the raw performance of the core signal under fair but\nminimal constraints. If it performs well here, we can be confident it will still hold up once operational\nsafety layers are added. It\u2019s the cleanest way to test whether the signal itself has genuine predictive\npower.\n3\nSignal construction and economic intuition\nThis section turns raw daily prices into an implementable trading intent. Every transformation is\ndefined precisely so the entire pipeline is reproducible from first principles.\n3.1\nPrices, returns, calendar, and information\nLet Pt > 0 denote the gold settlement price on trading day t (LBMA/COMEX close). We compute\nsimple close-to-close returns\nrt \u2261\nPt\nPt\u22121\n\u22121.\nAll decisions at day t are measurable with respect to the historical sigma-field Ft \u2261\u03c3({P\u03c4}\u03c4\u2264t); no\nquantity uses information from t+1 or later within the same test segment.\n4\n\n3.2\nSmoothing the price and extracting a slope\nWe smooth log-prices to suppress high-frequency noise while preserving drift. Define yt \u2261log Pt.\nThe exponentially weighted moving average (EMA) with smoothing parameter \u03bb \u2208(0, 1) is\n\u02dcyt = \u03bb \u02dcyt\u22121 + (1 \u2212\u03bb) yt,\n\u02dcy0 = y0.\nThe one-step smoothed slope (a proxy for trend intensity) is the first difference\n\u2206\u02dcyt \u2261\u02dcyt \u2212\u02dcyt\u22121.\nAll statistics used to normalize this slope are computed on the training window only. Let \u00b5train and\n\u03c3train > 0 denote, respectively, the sample mean and standard deviation of \u2206\u02dcyt across the preceding\n10-year training window. The standardized slope is\nzt \u2261\u2206\u02dcyt \u2212\u00b5train\n\u03c3train\n.\nStandardization stabilizes units so that thresholds are comparable across windows with different\nvolatility levels.\n3.3\nMapping slope to a bounded trend confidence\nRaw zt is unbounded. We clip and affine-transform it into a probability-like score in [0, 1],\n\u00afzt \u2261min{ max(zt, \u22123), 3 },\nptrend(t) \u2261\u00afzt + 3\n6\n\u2208[0, 1].\nValues near 0.5 correspond to neutral slope; values near 1 (resp. 0) indicate strongly positive (resp.\nnegative) slopes. This monotone transform is fixed (frozen) before the out-of-sample (OOS) segment\nbegins.\n3.4\nMomentum confirmation and blended regime probability\nTo prevent slope noise from triggering trades, we add a direction check via a K-day momentum\nindicator. With K = 50,\nmt \u2261\u22ae\n\u001a Pt\nPt\u2212K\n> 1\n\u001b\n\u2208{0, 1}.\nThe blend combines continuous slope strength and discrete direction:\npbull(t) \u2261\u03c9 ptrend(t) + (1 \u2212\u03c9) mt,\n\u03c9 = 0.6,\nand pbear(t) \u22611 \u2212pbull(t). The weight \u03c9 is optimized on the training window and then held fixed\nduring the OOS slice.",
    "chunk_index": 4,
    "start_char": 10641,
    "end_char": 13717,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "confirmation and blended regime probability\nTo prevent slope noise from triggering trades, we add a direction check via a K-day momentum\nindicator. With K = 50,\nmt \u2261\u22ae\n\u001a Pt\nPt\u2212K\n> 1\n\u001b\n\u2208{0, 1}.\nThe blend combines continuous slope strength and discrete direction:\npbull(t) \u2261\u03c9 ptrend(t) + (1 \u2212\u03c9) mt,\n\u03c9 = 0.6,\nand pbear(t) \u22611 \u2212pbull(t). The weight \u03c9 is optimized on the training window and then held fixed\nduring the OOS slice. A value above 0.5 indicates bullish conditions; below 0.5 indicates bearish.\n3.5\nActivation, entries, and exits\nA long is eligible when the regime is sufficiently bullish and the local slope is positive:\nactivate long at t if\npbull(t) \u22650.52\nand\n\u2206\u02dcyt > 0.\nWe use the Average True Range (ATR) for volatility-aware exits. Let Ht, Lt, Ct be high, low, and\nclose. The True Range is\nTRt \u2261max\n\b\nHt \u2212Lt, |Ht \u2212Ct\u22121|, |Lt \u2212Ct\u22121|\n \n,\n5\n\nand the n-day ATR is ATRn(t) \u22611\nn\nPn\u22121\nj=0 TRt\u2212j with n=14.\nGiven an entry at price Pent, we manage the position with:\nhard stop at Pent \u22122 ATR14(t),\ntrailing stop at (peak price) \u22121.5 ATR14(t),\nand a maximum age of 30 trading days. If pbear(t) rises above 0.50, we de-risk by halving or closing\nthe position. These rules bound losses while allowing winners to run.\n4\nSizing, risk targeting, costs, and capacity\nThis section converts regime intent into an executable portfolio weight that respects risk, costs,\nand market impact. All quantities that depend on data are estimated strictly within each training\nwindow and then frozen before the corresponding out-of-sample (OOS) slice, so every test decision\nuses only information that would have been available in real time.\n4.1\nVolatility targeting\nThe goal of volatility targeting is to keep risk roughly constant through time so that performance is\nnot dominated by a few high-volatility episodes. Let rt \u2261Pt/Pt\u22121 \u22121 be daily simple returns and\nlet Ft denote the information available at the close of day t. We forecast next-day variance via an\nexponentially weighted moving average (EWMA), the stationary solution to a local-level Kalman\nfilter and the RiskMetrics standard [18]:\nb\u03c32\nt+1 = \u03b8 b\u03c32\nt + (1 \u2212\u03b8) r2\nt ,\nb\u03c32\n0 = Vartrain(r),\n\u03b8 \u2208(0, 1).\nHere \u03b8 controls memory length (larger \u03b8 puts more weight on older data). We target annualized\nvolatility \u03c3\u22c6\nann = 15%, a common allocator risk budget for liquid macro sleeves [15]. Converting to\ndaily using D = 252 trading days,\n\u03c3\u22c6\u2261\u03c3\u22c6\nann\n\u221a\nD\n.\nWe then scale intended exposure inversely to the forecasted volatility and cap it at a leverage limit\nWmax:\nw(vol)\nt\n\u2261min\n\u0012\nWmax,\n\u03c3\u22c6\nb\u03c3t+1\n\u0013\n,\nWmax = 2.0.\nThis mapping increases weights in calm periods and trims them in turbulent periods, stabilizing\nex-ante risk and making the strategy\u2019s P&L more comparable across regimes. The cap prevents\nrunaway leverage if b\u03c3t+1 becomes very small due to a quiet spell.\n4.2\nConfidence shaping\nVolatility targeting allocates a risk budget;",
    "chunk_index": 5,
    "start_char": 13295,
    "end_char": 16166,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "intended exposure inversely to the forecasted volatility and cap it at a leverage limit\nWmax:\nw(vol)\nt\n\u2261min\n\u0012\nWmax,\n\u03c3\u22c6\nb\u03c3t+1\n\u0013\n,\nWmax = 2.0.\nThis mapping increases weights in calm periods and trims them in turbulent periods, stabilizing\nex-ante risk and making the strategy\u2019s P&L more comparable across regimes. The cap prevents\nrunaway leverage if b\u03c3t+1 becomes very small due to a quiet spell.\n4.2\nConfidence shaping\nVolatility targeting allocates a risk budget; the regime signal decides how much of that budget to\nuse. Let pbull(t) \u2208[0, 1] be the frozen regime probability from the previous section. We map it to a\n[0, 1] budget share via a centered, monotone transform:\npbull(t) \u22120.5\n0.5\n\u2208[0, 1],\nwhich equals 0 at neutrality (pbull = 0.5) and 1 at full conviction (pbull = 1). The confidence-shaped\nweight is\nw(conf)\nt\n\u2261w(vol)\nt\n\u00b7 pbull(t) \u22120.5\n0.5\n\u2208[0, w(vol)\nt\n].\n6\n\nThis ensures the system stands down in ambiguous regimes and only scales up when the blended\ntrend\u2013momentum evidence is strong. Because this mapping is linear and bounded, it preserves\ninterpretability and prevents sudden jumps.\n4.3\nFriction-adjusted Kelly growth and optimal fraction\nWe next connect expected edge to size using a Kelly-style growth objective, explicitly deducting\ntrading frictions so the solution is implementable. Let Rt denote the unit-notional strategy return\nper day that would result from following the signal with weight 1. Its conditional mean and variance\non the training window are\n\u00b5 \u2261E[Rt | Ft] ,\n\u03c32 \u2261Var[Rt | Ft] ,\nestimated only on training data to avoid leakage. Let f \u22650 be the leverage fraction applied to this\nunit-notional sleeve before risk targeting. We include two standard cost components:\n1. Linear (spread/fee) cost: k per round trip (RT), applied to absolute position change. If at most\nn \u22641 RT/day occurs, linear drag is nkf.\n2. Temporary market impact: large trades push prices against the trader. Empirically, impact\nscales close to the square-root of participation [5]. We parameterize temporary impact by \u03b3 > 0,\nyielding a daily growth penalty proportional to (nf)3/2 once one integrates execution paths [1].\nUnder a small-return log-utility expansion (standard in Kelly derivations [11, 19]), expected\ndaily log-growth as a function of f is approximated by\ng(f) \u2248\u00b5f \u22121\n2\u03c32f2 \u2212nkf \u2212\u03b3(nf)3/2.\nTo maximize g, it is convenient to set x \u2261\u221af (so f = x2 and x \u22650), giving\ng(f) = \u00b5x2 \u2212\n1\n2\u03c32x4 \u2212nkx2 \u2212\u03b3n3/2x3.\nDifferentiating with respect to x,\ndg\ndx = 2\u00b5x \u22122\u03c32x3 \u22122nkx \u22123\u03b3n3/2x2.\nFor g to be maximum, dg/dx = 0. Dividing by 2 yields a quadratic in x:\n\u03c32x2 +\n3\n2\u03b3n3/2x \u2212(\u00b5 \u2212nk) = 0.\nMultiplying by 2 for cleaner coefficients,\n2\u03c32x2 + 3\u03b3n3/2x \u22122(\u00b5 \u2212nk) = 0.\nThe economically relevant nonnegative root is\nx\u22c6= \u22123\u03b3n3/2 +\np\n9\u03b32n3 + 16\u03c32(\u00b5 \u2212nk)\n4\u03c32\n,\nf\u22c6= (x\u22c6)2\nif \u00b5 > nk,\nand f\u22c6= 0 otherwise (no net edge after linear costs). When \u03b3=k=0 and n=1, this reduces to the\nclassic Kelly fraction f\u22c6= \u00b5/\u03c32.\n7",
    "chunk_index": 6,
    "start_char": 15702,
    "end_char": 18612,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "maximum, dg/dx = 0. Dividing by 2 yields a quadratic in x:\n\u03c32x2 +\n3\n2\u03b3n3/2x \u2212(\u00b5 \u2212nk) = 0.\nMultiplying by 2 for cleaner coefficients,\n2\u03c32x2 + 3\u03b3n3/2x \u22122(\u00b5 \u2212nk) = 0.\nThe economically relevant nonnegative root is\nx\u22c6= \u22123\u03b3n3/2 +\np\n9\u03b32n3 + 16\u03c32(\u00b5 \u2212nk)\n4\u03c32\n,\nf\u22c6= (x\u22c6)2\nif \u00b5 > nk,\nand f\u22c6= 0 otherwise (no net edge after linear costs). When \u03b3=k=0 and n=1, this reduces to the\nclassic Kelly fraction f\u22c6= \u00b5/\u03c32.\n7\n\nIn live use we temper the solution with a fractional-Kelly multiplier to reduce estimation error\nsensitivity and drawdown amplitude [13]:\n\u02dcf \u2261\u03bbKelly f\u22c6,\n\u03bbKelly = 0.40.\nWhen the training-window estimate \u00b5 \u2212nk is close to zero such that f\u22c6is numerically tiny, we\nstill allocate a small baseline equal to 25% of the regime-scaled volatility budget. This avoids\npathological \u201czeroing out\u201d in slightly positive but uncertain conditions and reflects the fact that\nposition discovery itself can be informative.\n4.4\nFrom fraction to executable weight\nLet wt be the final portfolio weight (notional exposure per $1 of equity). We combine the three\nlayers\u2014risk budget, regime share, and friction-aware size\u2014into a single bound:\nwt \u2261\u02dcf \u00d7 w(conf)\nt\n,\ncapped at Wmax,\nsubject to the previously defined entry and exit rules (activation threshold, ATR-based stops, and\ntimeouts). Execution timing uses either T+0 (same-day close-to-close) or stressed T+1/T+2 fills in\nrobustness checks; in all cases, the same linear and impact costs are applied deterministically to the\nrealized position changes so that performance reflects implementable P&L.\n4.5\nCapacity curve and AUM scaling\nCapacity measures how far the strategy can scale before costs and impact overwhelm edge. Replacing\nthe decision fraction f with an average gross participation level L \u22650 gives a friction-aware growth\ncurve for unit-notional moments (\u00b5u, \u03c3u) estimated on the training window:\ng(L) \u2248\u00b5uL \u2212\n1\n2(\u03c3uL)2 \u2212nkL \u2212\u03b3(nL)3/2.\nThe first term grows linearly with size, but the variance term (quadratic in L) and impact term (L3/2)\nbend the curve downward, creating a concave frontier with a well-defined maximum. Practical AUM\nis the range of L for which g(L) > 0 after all frictions and operational limits. In our experiments,\nthe realized mean absolute weight |wt| \u22480.033 sits comfortably on the positive-growth branch, and\nstress tests on (k, \u03b3) indicate headroom into the sub-billion-dollar scale for gold liquidity assumptions\nconsistent with square-root impact estimates in large futures markets [5, 1, 14]. The point is not\nto claim an exact dollar ceiling\u2014capacity depends on venue, execution style, and crowding\u2014but\nto show that under conservative, citable frictions the growth curve remains positive at realized\nparticipation.\n5\nEstimation, freezing, and reproducibility\nEvery parameter that depends on historical data is estimated only within its corresponding training\nwindow and then permanently fixed before the start of the six-month out-of-sample (OOS) period.\nThis rule is non-negotiable: it guarantees that no information from the future influences model\nbehavior. Each OOS slice therefore represents a fully independent live-trading experiment rather\nthan a statistical backtest.",
    "chunk_index": 7,
    "start_char": 18211,
    "end_char": 21362,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "frictions the growth curve remains positive at realized\nparticipation.\n5\nEstimation, freezing, and reproducibility\nEvery parameter that depends on historical data is estimated only within its corresponding training\nwindow and then permanently fixed before the start of the six-month out-of-sample (OOS) period.\nThis rule is non-negotiable: it guarantees that no information from the future influences model\nbehavior. Each OOS slice therefore represents a fully independent live-trading experiment rather\nthan a statistical backtest.\nFormally, for each rolling window we estimate and then freeze the full set of parameters\n\b\n\u03bb, \u00b5train, \u03c3train, \u03c9, pbull threshold, ATR multipliers, timeout, \u03bbKelly, (\u00b5, \u03c3), (\u00b5u, \u03c3u)\n \ntrain.\nEach element in this set serves a distinct role:\n8\n\n\u2022 \u03bb is the exponential smoothing parameter controlling the signal\u2019s memory.\n\u2022 (\u00b5train, \u03c3train) are the mean and standard deviation used to standardize slopes for the z-score.\n\u2022 \u03c9 is the blend weight combining slope and momentum information into the regime probability.\n\u2022 The pbull threshold determines activation, set so that only meaningful signals (above historical\nnoise) trigger trades.\n\u2022 ATR multipliers and timeout define exit rules that bound losses and avoid overlong exposure.\n\u2022 \u03bbKelly sets the fraction of optimal Kelly leverage used for stability.\n\u2022 (\u00b5, \u03c3) and (\u00b5u, \u03c3u) are, respectively, the mean\u2013variance estimates for unit-notional and unscaled\nreturns that feed into friction-adjusted Kelly sizing and capacity analysis.\nOnce these are fixed, the model runs on the next six-month test window without modification.\nDuring testing, the only quantities that evolve are those that naturally depend on incoming prices,\nsuch as the exponential moving averages or volatility forecasts. Forecasts of variance, b\u03c3t+1, are\ncomputed recursively using information available up to day t:\nb\u03c32\nt+1 = \u03b8 b\u03c32\nt + (1 \u2212\u03b8) r2\nt ,\nwhere \u03b8 is the same decay constant estimated on the training slice. This ensures that at any given\npoint, the model has access only to data it could have realistically observed at that time.\nThis strict separation between training and testing is the foundation of reproducible finance\nresearch. Without it, even subtle look-ahead bias\u2014like recalculating a mean or threshold using\nfuture prices\u2014can dramatically overstate performance [2]. By freezing parameters and computing\nall forecasts recursively, each segment of our backtest mimics the behavior of an actual deployed\ntrading system operating under real-world constraints.\nThe result is a framework that satisfies empirical standards for live-like evaluation in quantitative\nfinance: every signal, forecast, and decision in the test window depends solely on Ft, the information\nknown up to that point [7, 14].\nThis not only ensures honest performance measurement but\nalso makes the entire pipeline reproducible by independent researchers or regulators\u2014an essential\ncondition for credible quantitative claims.\n6\nTrend Extraction, Regime Probability, and Trade Execution\nLogic\nThis section explains how raw gold prices are transformed into a stable, interpretable signal that\nguides trading decisions. The goal is to separate meaningful directional movement\u2014the \u201cdrift\u201d of\nthe market\u2014from random noise. Each mathematical step below is both operationally justified and\neconomically interpretable.",
    "chunk_index": 8,
    "start_char": 20830,
    "end_char": 24177,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "only ensures honest performance measurement but\nalso makes the entire pipeline reproducible by independent researchers or regulators\u2014an essential\ncondition for credible quantitative claims.\n6\nTrend Extraction, Regime Probability, and Trade Execution\nLogic\nThis section explains how raw gold prices are transformed into a stable, interpretable signal that\nguides trading decisions. The goal is to separate meaningful directional movement\u2014the \u201cdrift\u201d of\nthe market\u2014from random noise. Each mathematical step below is both operationally justified and\neconomically interpretable.\n6.1\nSmoothed trend\nWe begin with daily settlement prices Pt > 0 and work with log-prices yt = log Pt. Taking logarithms\nmakes percentage changes additive and ensures that equal proportional moves (e.g., +1% or \u22121%)\nare treated symmetrically.\n9\n\nRaw daily returns rt = yt \u2212yt\u22121 are extremely noisy: gold prices fluctuate even when there is\nno true trend. To reveal the underlying direction, we apply an exponential moving average (EMA)\nto smooth the series:\n\u02dcyt = \u03bb \u02dcyt\u22121 + (1 \u2212\u03bb) yt,\n\u03bb \u2208(0, 1),\nwhere \u03bb controls memory length. A larger \u03bb gives more weight to past values (slower reaction), while\na smaller \u03bb reacts faster to new prices. The EMA is mathematically equivalent to the steady-state\nKalman filter for a local-level model [6], which provides an optimal way to filter noise when shocks\nare approximately Gaussian.\nThe short-term slope of the smoothed log-price, representing the current trend, is\n\u2206\u02dcyt = \u02dcyt \u2212\u02dcyt\u22121.\nTo make this slope comparable across time and regimes, we standardize it using statistics from the\ntraining window only:\nzt = \u2206\u02dcyt \u2212\u00b5train\n\u03c3train\n,\nwhere \u00b5train and \u03c3train are the mean and standard deviation of \u2206\u02dcyt computed over the preceding\n10-year training period. This ensures that zt measures how unusual today\u2019s slope is relative to\npast conditions. For example, zt = 2 means the slope is two standard deviations stronger than the\nhistorical average, indicating a meaningful trend.\nSmoothing reduces variance and extracts a monotone, low-noise estimate of price drift that can\nbe safely translated into a risk allocation. It balances responsiveness with stability: fast enough to\ndetect new moves, but slow enough to ignore random reversals.\n6.2\nRegime probability\nA raw standardized slope zt can be large or small, positive or negative.\nTo interpret it as a\nprobability-like confidence score, we map it onto the bounded interval [0, 1]. We first clip extreme\nz-scores to the range [\u22123, 3] (avoiding unbounded tails) and then linearly rescale:\nptrend(zt) = min(max(zt, \u22123), 3) + 3\n6\n,\nptrend \u2208[0, 1].\nHere ptrend \u22481 means a strong positive trend, ptrend \u22480 means a strong negative trend, and\nptrend = 0.5 corresponds to neutrality.\nHowever, slope alone measures strength but not direction confirmation. For additional robustness,\nwe blend this continuous score with a simple momentum indicator that checks whether the current\nprice is higher than it was 50 days ago:\nmt = \u22ae{Pt/Pt\u221250 > 1},\nmt \u2208{0, 1}.\nThis term equals 1 if gold has appreciated over the past 50 days and 0 otherwise.",
    "chunk_index": 9,
    "start_char": 23603,
    "end_char": 26692,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "\u22480 means a strong negative trend, and\nptrend = 0.5 corresponds to neutrality.\nHowever, slope alone measures strength but not direction confirmation. For additional robustness,\nwe blend this continuous score with a simple momentum indicator that checks whether the current\nprice is higher than it was 50 days ago:\nmt = \u22ae{Pt/Pt\u221250 > 1},\nmt \u2208{0, 1}.\nThis term equals 1 if gold has appreciated over the past 50 days and 0 otherwise. Combining both\npieces yields a blended regime probability:\npbull,t = 0.6 ptrend(zt) + 0.4 mt,\npbear,t = 1 \u2212pbull,t.\nThe weights (0.6 for slope, 0.4 for momentum) are calibrated on the training window to maximize\nhit-rate stability and then frozen for all OOS tests. The slope provides continuous intensity (\u201chow\nstrong\u201d), while momentum provides binary confirmation (\u201cwhich direction\u201d). The blend remains\n10\n\nmonotonic and easily interpretable: a single number between 0 and 1 that summarizes the probability\nthat the market is in a bullish regime.\nThis blended regime variable is robust to noise and can be understood as a smoothed probability\nof directional persistence, consistent with the idea that short-term momentum tends to cluster in\ntime [10, 16].\n6.3\nEntry and exit logic\nThe signal above determines when to enter, adjust, or exit trades. A position is opened only when\nthe evidence of an upward trend is strong and consistent with positive slope:\nEnter long at day t if pbull,t \u22650.52 and \u2206\u02dcyt > 0.\nThe threshold 0.52 was empirically chosen during training to balance false positives (entering too\nearly) and false negatives (missing real trends). A symmetric rule could be applied for short positions,\nbut because gold often exhibits asymmetric drift and storage costs, we focus on the long side for\nclarity.\nOnce a trade is active, we protect against losses and lock in gains using volatility-adjusted exits\nbased on the Average True Range (ATR). Let Ht, Lt, and Ct denote the high, low, and close on\nday t. The true range is defined as\nTRt = max{Ht \u2212Lt, |Ht \u2212Ct\u22121|, |Lt \u2212Ct\u22121|} ,\nand the 14-day average true range (ATR) is\nATR14(t) = 1\n14\n13\nX\nj=0\nTRt\u2212j.\nATR measures how volatile prices have been recently; using it to size stops makes exits adaptive to\nmarket conditions.\nA position exits under any of the following conditions:\n1. Hard stop: the price falls below the entry by more than 2 \u00d7 ATR14, limiting drawdown.\n2. Trailing stop: the price drops by more than 1.5 \u00d7 ATR14 from its running peak, securing\ngains as the market reverses.\n3. Timeout: the trade has been open for 30 trading days, after which the signal\u2019s predictive\npower statistically decays.\n4. Regime de-risking: if pbear,t > 0.5, the model halves or closes the position, acknowledging a\nregime flip.\nThese rules have clear economic intent. Hard and trailing stops cap risk and enforce discipline;\nthe timeout prevents capital from being tied up in flat markets; and the regime de-risk rule\nensures consistency with the signal\u2019s current state.",
    "chunk_index": 10,
    "start_char": 26264,
    "end_char": 29220,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "reverses.\n3. Timeout: the trade has been open for 30 trading days, after which the signal\u2019s predictive\npower statistically decays.\n4. Regime de-risking: if pbear,t > 0.5, the model halves or closes the position, acknowledging a\nregime flip.\nThese rules have clear economic intent. Hard and trailing stops cap risk and enforce discipline;\nthe timeout prevents capital from being tied up in flat markets; and the regime de-risk rule\nensures consistency with the signal\u2019s current state. Together, they create an asymmetric payoff\nprofile\u2014bounded losses and open-ended winners\u2014that empirically underpins positive skew in\ntrend-following strategies [8].\nIn summary, we translate raw gold prices into a continuous regime probability that governs\ntrade activation and risk control. Each mathematical step\u2014from smoothing to standardization\nto probabilistic mapping\u2014serves to reduce noise, preserve interpretability, and ensure that every\ntrading decision can be justified both statistically and economically.\n11\n\n7\nSizing with friction-adjusted Kelly (derivation and use)\nPosition sizing determines how much capital to allocate to a trade once we know the direction and\nconfidence of the signal. The goal is to grow wealth efficiently while accounting for both statistical\nuncertainty and real trading frictions. We derive this from first principles using a friction-adjusted\nversion of the Kelly criterion.\n7.1\nFrom fraction to implementable weight\nWe connect the theoretical fraction \u02dcf to the actual position size wt used in the daily backtest. We\nfirst adjust for the market\u2019s current volatility level. Let b\u03c3t+1 be the forecast of next-day volatility\ncomputed recursively from historical data, and \u03c3\u22c6be the target daily volatility equivalent to 15%\nannualized:\n\u03c3\u22c6= 0.15\n\u221a\n252.\nTo prevent excessive leverage in quiet markets, we cap exposure at a maximum of Wmax = 2.0:\nw(vol)\nt\n= min\n\u0012\nWmax, \u03c3\u22c6\nb\u03c3t+1\n\u0013\n.\nNext, we scale this volatility budget by the signal\u2019s confidence, so that positions grow when the\nmodel is more certain and shrink when uncertain:\nw(conf)\nt\n= w(vol)\nt\n\u00b7 pbull,t \u22120.5\n0.5\n,\nw(conf)\nt\n\u2208[0, w(vol)\nt\n].\nFinally, the actual trade weight is obtained by combining all layers:\nwt = \u02dcf \u00d7 w(conf)\nt\nsubject to entry and exit rules defined earlier. When the friction-adjusted Kelly estimate \u02dcf is\nclose to zero, we use a baseline exposure equal to 0.25 \u00d7 w(vol)\nt\nto maintain participation without\novercommitting.\nIn operational terms, this formula translates a probabilistic signal into a position that adapts\nto both conviction and market conditions. High volatility automatically reduces exposure, high\nconfidence increases it, and estimated costs prevent over-scaling. Together, these elements create a\nsizing rule that is both theoretically optimal and practically implementable under realistic liquidity\nconstraints.\n8\nOut-of-sample results (FAST configuration)\nWe now evaluate the strategy\u2019s performance under the FAST configuration. This setup uses no\nadditional safety filters\u2014no long-term volatility caps or turnover throttles\u2014so that we can measure\nthe raw predictive power of the signal in a fair, live-like test. Every number reported here is strictly\nout-of-sample (OOS): the model never had access to this data during training. The test covers 2,793\ntrading days, or about 11 years.\n12",
    "chunk_index": 11,
    "start_char": 28737,
    "end_char": 32045,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "that is both theoretically optimal and practically implementable under realistic liquidity\nconstraints.\n8\nOut-of-sample results (FAST configuration)\nWe now evaluate the strategy\u2019s performance under the FAST configuration. This setup uses no\nadditional safety filters\u2014no long-term volatility caps or turnover throttles\u2014so that we can measure\nthe raw predictive power of the signal in a fair, live-like test. Every number reported here is strictly\nout-of-sample (OOS): the model never had access to this data during training. The test covers 2,793\ntrading days, or about 11 years.\n12\n\n2016\n2018\n2020\n2022\n2024\n2026\nDate\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\nHit-Rate (%)\nRolling Hit-Rate (252-Day EMA-Smoothed)\n2016\n2018\n2020\n2022\n2024\n2026\nDate\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nPay-Off Ratio\nRolling Pay-Off Ratio (252-Day EMA-Smoothed)\nFigure 1: Rolling Hit-Rate and Pay-Off Ratio (Forecast-to-Fill, 2015\u20132025).Each panel\nsummarizes the mechanical structure of the Forecast-to-Fill edge over time. The left panel shows the\nrolling six-month hit-rate\u2014the percentage of profitable trading days\u2014which remains consistently\nabove 50%, confirming that the strategy wins more often than it loses. The right panel shows the\ncorresponding rolling pay-off ratio (mean gain divided by mean loss), typically ranging between\n1.3\u00d7 and 1.6\u00d7, indicating that the average win is larger than the average loss. Together these\nmeasures demonstrate that profitability arises from a stable asymmetry in trade outcomes\u2014frequent\nsmall wins and controlled losses\u2014rather than from rare, high-impact events. Both series are lightly\nsmoothed (63-day EMA) for readability.\n8.1\nCore performance (net)\nWe measure net performance after applying both linear transaction costs (0.7 bps per round trip)\nand square-root impact costs with \u03b3 = 0.02. The key results are:\n\u2022 Days: 2,793\nAnnual return: 2.62%\nAnnual volatility: 0.91%\nSharpe: 2.88\n\u2022 CAGR: 2.65% (at a realized volatility of 0.91%, at 15% volatility, the CAGR is 43%)\nMax\ndrawdown: 0.52%\nCalmar: 5.11\n\u2022 Hit rate (calendar days): 26.67%\nUp months: 79.1%\n\u2022 Entries: 1,282\nNon-zero exposure days: 1,132\nMean absolute weight: |wt| =\n0.0326\n\u2022 Mean f\u22c6on train (non-zero): 0.0029\nShare of days with pbull \u22650.52: 59.5%\n\u2022 Bootstrap 95% CI for Sharpe: [2.49, 3.27] using 1,000 block bootstraps (block length = 20\ndays).\nWe find that the strategy achieves a Sharpe ratio of 2.88 with only 0.91% annualized\nvolatility. These results indicate that our volatility targeting and stop rules maintain very low\nrisk while preserving most of the signal\u2019s return potential. The Calmar ratio of 5.11 means\n13\n\nthe strategy\u2019s annual return exceeds its worst historical drawdown by a factor of five, reflecting\nconsistent compounding.\nThe bootstrap confidence interval confirms that the Sharpe ratio remains above 2.0 even\nunder resampling uncertainty, so the result is statistically significant. These findings show that our\nout-of-sample performance is both economically meaningful and statistically robust.\n8.2\nDistribution and skew\nWe examine the shape of daily returns to understand how the strategy earns its edge. Median daily\nreturn is approximately zero, but the distribution is highly right-skewed:\nSkewness = 3.90,\nKurtosis = 41.34.",
    "chunk_index": 12,
    "start_char": 31464,
    "end_char": 34699,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "bootstrap confidence interval confirms that the Sharpe ratio remains above 2.0 even\nunder resampling uncertainty, so the result is statistically significant. These findings show that our\nout-of-sample performance is both economically meaningful and statistically robust.\n8.2\nDistribution and skew\nWe examine the shape of daily returns to understand how the strategy earns its edge. Median daily\nreturn is approximately zero, but the distribution is highly right-skewed:\nSkewness = 3.90,\nKurtosis = 41.34.\nRoughly 58% of days show zero or near-zero returns because the model often sits flat. When active,\nit captures infrequent but large positive moves.\nThis profile is exactly what we design for. By cutting losses quickly and letting profits run,\nwe create an asymmetric return distribution with bounded downside and open-ended upside.\nHigh kurtosis arises naturally from a process that spends most of its time inactive but occasionally\ncompounds large winners. We confirm that this skew pattern is consistent with time-series momentum\nbehavior documented in (author?) [16] and is a key property of robust trend-following systems [8].\n8.3\nActive-day expectancy\nWe analyze profitability on the days when the strategy is actually active, defined as |wt| > 10\u22123.\nOut of 2,793 total trading days, 1,132 days meet this condition\u2014about 40% of the sample.\nAcross those active days, we find:\n\u2022 Hit rate: 65.81% (see Fig. 1)\n\u2022 Average gain: +6.00 basis points\n\u2022 Average loss: \u22124.01 basis points\n\u2022 Payoff ratio: 1.49\u00d7\n\u2022 Expected value per active day: +2.58 bps\nWe then annualize this daily expectancy by the proportion of active days:\n2.58 bps \u00d7 1,132\n2,793 \u00d7 252 \u22482.63%.\nThe resulting 2.63% annualized return matches the realized CAGR (2.65%, at a realized volatility\nof 0.91%) to two decimal places, confirming internal consistency. This means that the performance\nis not the product of a few extreme events but of many small, repeatable edges that compound over\ntime. At 15% target volatility, the CAGR is 43%.\nWe note that a 65.8% hit rate combined with a 1.49\u00d7 payoff ratio implies a strong positive\nexpectancy process: the strategy wins more often than it loses, and its wins are larger. These\nratios are well above what is typical for mechanical technical systems (often near 50% hit rate and\n1.1\u00d7 payoff) [12, 3]. The result shows that our blend of signal logic, volatility targeting, and stop\nstructure converts small predictive signals into durable, asymmetric payoffs.\n14\n\n8.4\nInterpretation\nOverall, we observe a system that trades infrequently, risks little, and compounds steadily. Even\nunder the FAST configuration\u2014which removes all long-term throttles and safety bands\u2014the strategy\nproduces stable returns with shallow drawdowns.\nThe consistency between expectancy, CAGR, and risk metrics tells us the process is statistically\nand economically coherent. In other words, the strategy behaves like a genuine forecasting-and-\nexecution system, not like an overfit backtest. It earns its edge by controlling losses, scaling risk to\nvolatility, and holding exposure only when the underlying signal is strong.\nWe conclude that the out-of-sample evidence supports our central claim: careful \u201cforecast-to-fill\u201d\nengineering\u2014smoothing, confidence mapping, volatility targeting, and",
    "chunk_index": 13,
    "start_char": 34195,
    "end_char": 37473,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "strategy\nproduces stable returns with shallow drawdowns.\nThe consistency between expectancy, CAGR, and risk metrics tells us the process is statistically\nand economically coherent. In other words, the strategy behaves like a genuine forecasting-and-\nexecution system, not like an overfit backtest. It earns its edge by controlling losses, scaling risk to\nvolatility, and holding exposure only when the underlying signal is strong.\nWe conclude that the out-of-sample evidence supports our central claim: careful \u201cforecast-to-fill\u201d\nengineering\u2014smoothing, confidence mapping, volatility targeting, and friction-aware sizing\u2014can\nconvert small state-dependent predictability in gold returns into reliable, allocator-grade performance.\n2024-09\n2024-11\n2025-01\n2025-03\n2025-05\n2025-07\n2025-09\nDate\n0.1\n0.0\n0.1\n0.2\n (252-day rolling)\nRolling 1-Year Beta\nFigure 2: Rolling 1-Year Beta vs Gold (2024\u20132025). The figure shows the evolution of the\n252-day rolling beta of the Forecast-to-Fill (FTF) strategy with respect to gold returns. Each point\nrepresents the estimated market sensitivity over the preceding trading year. A positive \u03b2 indicates\nthat FTF moves in the same direction as gold on average, while a negative \u03b2 reflects an inverse\nrelationship.\n9\nAlpha and benchmark neutrality\nTo verify that the strategy\u2019s performance is truly idiosyncratic and not just a leveraged exposure to\nthe gold market itself, we regress its daily returns against spot gold. This allows us to separate\ngenuine alpha (returns independent of the benchmark) from simple beta exposure.\n9.1\nCAPM-style regression\nWe estimate a standard one-factor regression of the form\nrstrat\nt\n= \u03b1d + \u03b2 rgold\nt\n+ \u03b5t,\n(1)\nwhere rstrat\nt\nis the daily return of our strategy, rgold\nt\nis the daily return of spot gold, \u03b1d is the daily\nalpha (intercept), \u03b2 measures sensitivity to gold, and \u03b5t is the residual noise term. We annualize \u03b1d\n15\n\nby multiplying by 252 trading days.\nWe find that the regression yields an annualized \u03b1 = 2.25% (t = 9.53, p < 0.001) and a \u03b2 = 0.03\n(t = 31.01) (see Fig. 2). The R2 of the regression is only 0.001, meaning that almost none of the\nstrategy\u2019s returns are explained by gold\u2019s daily price moves. These numbers confirm that the sleeve\nis essentially benchmark-neutral.\n9.2\nInterpreting \u03b1 and \u03b2\nWe interpret the intercept \u03b1 as the average excess return generated independent of gold\u2019s market\ndirection. A 2.25% annualized alpha, significant at p < 0.001, means that even if gold itself went\nnowhere, the system would have still compounded at roughly that rate after all frictions. The\nsmall \u03b2 = 0.03 indicates that our positions are not mechanically long or short gold\u2014only slightly\ncorrelated due to shared volatility structure.\nA \u03b2 near zero is essential for allocator relevance. It shows that our alpha is not the product\nof carrying gold exposure (which could be replicated cheaply with futures), but of identifying\nconditional persistence and exploiting it efficiently. This distinction matters because many \u201ctrend\u201d\nsystems appear profitable but are actually just long the underlying risk premium. Our regression\nconfirms that this is not the case here: our alpha is structural, not directional.",
    "chunk_index": 14,
    "start_char": 36875,
    "end_char": 40064,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "correlated due to shared volatility structure.\nA \u03b2 near zero is essential for allocator relevance. It shows that our alpha is not the product\nof carrying gold exposure (which could be replicated cheaply with futures), but of identifying\nconditional persistence and exploiting it efficiently. This distinction matters because many \u201ctrend\u201d\nsystems appear profitable but are actually just long the underlying risk premium. Our regression\nconfirms that this is not the case here: our alpha is structural, not directional.\n9.3\nInformation ratio and independence\nWe compute the volatility-matched information ratio (IR) as\nIR =\n\u03b1\ntracking error,\nwhere tracking error is the residual volatility from the regression. We obtain IR = 2.09, which\nconfirms that the alpha is not only statistically significant but also economically large relative to\nresidual risk. In practical terms, a volatility-matched IR above 2.0 implies the strategy\u2019s returns are\nhighly consistent and independent of the benchmark, even after adjusting for variance.\nThis behavior is consistent with an engineered risk-premia harvesting process rather than a\npassive beta exposure. The strategy generates excess returns by supplying liquidity in short-lived\ndirectional regimes\u2014earning small, uncorrelated payoffs from transient price persistence rather than\nfrom long-term gold appreciation. The low \u03b2 further ensures that combining this sleeve with other\nmacro or commodity exposures would improve portfolio diversification.\n9.4\nEconomic takeaway\nIn summary, we find that:\n\u2022 The strategy produces a statistically strong and economically meaningful alpha (2.25% annual-\nized, t = 9.5).\n\u2022 Benchmark sensitivity is negligible (\u03b2 = 0.03), confirming market neutrality.\n\u2022 The volatility-matched IR of 2.09 indicates high-quality, repeatable excess returns.\nThese findings confirm that our system\u2019s profitability does not depend on the gold market\u2019s\noverall drift. Instead, it extracts independent state-dependent risk premia that can coexist with\nconventional long or carry exposures. In allocator terms, the sleeve offers additive diversification\nvalue: its Sharpe ratio remains high even after neutralizing benchmark effects, and its alpha survives\nevery standard test for statistical independence.\n16\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nDate\n10\n0\n10\n1\n10\n2\nPortfolio Value (Million USD, log)\n$3.12M\n$100.18M\nFTF (net): \n 42.56%/yr, \n 15.00%/yr, Sharpe 2.84\n0.030, (CAPM, linear) 42.22%/yr, cost 1.76%/yr\nGrowth of $1M: Gold market vs Forecast-to-Fill (FTF)\nGold (Buy & Hold)\nForecast-to-Fill (15% vol, net 1.0\u00d70.7 bps/day)\nFigure 3: Gold vs. Forecast-to-Fill Performance (2015\u20132025).Growth of a $1 million invest-\nment in spot gold (buy-and-hold) versus the Forecast-to-Fill strategy operated at a 15% annual\nvolatility target. The engineered sleeve compounds at approximately 43% per year (Sharpe \u22482.9,\n\u03b2 \u22480.03, \u03b1 \u224843% yr\u22121), while gold remains largely flat over the same horizon. Both series are\nplotted on a logarithmic scale to highlight relative compounding. The figure illustrates how forecast-\nto-fill engineering converts weak predictability in gold returns into scalable, benchmark-neutral\nalpha.\n10\nIf we ran at 15% volatility: implied return, alpha, and scalability\nWe next ask a simple but practical question: if the same signal were operated at its full 15%\nannualized volatility budget, what performance would it imply, and could",
    "chunk_index": 15,
    "start_char": 39547,
    "end_char": 42972,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "\u224843% yr\u22121), while gold remains largely flat over the same horizon. Both series are\nplotted on a logarithmic scale to highlight relative compounding. The figure illustrates how forecast-\nto-fill engineering converts weak predictability in gold returns into scalable, benchmark-neutral\nalpha.\n10\nIf we ran at 15% volatility: implied return, alpha, and scalability\nWe next ask a simple but practical question: if the same signal were operated at its full 15%\nannualized volatility budget, what performance would it imply, and could the market absorb that\nscale? This exercise provides a volatility-normalized measure of efficiency and an explicit translation\nof model quality into deployable capital.\nScaling logic. The out-of-sample strategy exhibits a Sharpe ratio S = 2.88, realized annual\nvolatility \u03c3ann = 0.91%, annual alpha \u03b1ann = 2.25%, and information ratio IR = 2.09. We assume\nlinear scalability, meaning that multiplying all daily positions by a constant c multiplies all returns\nby the same c\u2014a standard property of volatility targeting. Sharpe and IR therefore remain invariant\nunder scaling, while mean return and alpha scale proportionally. The factor required to reach the\n15% target volatility is\nc = 15\n0.91 \u224816.5.\nAt this level, the implied annualized return is\nReturn15% = S \u00d7 15% = 2.88 \u00d7 0.15 = 43.2% per year,\n17\n\nand the implied alpha is\n\u03b115% = c \u00d7 \u03b1ann = 16.5 \u00d7 2.25% = 37.1% per year.\nBoth estimates are consistent when cross-checked through the information ratio (\u03b1 = IR \u00d7\ntracking error), confirming internal coherence. Sharpe and IR remain unchanged, reflecting constant\nefficiency.\nInterpretation. These numbers do not imply reckless leverage but rather express the system\u2019s\nefficiency on a common risk scale. In essence, the strategy delivers the same quality of edge\u2014Sharpe\n= 2.9, IR = 2.1\u2014whether run at 1% or 15% volatility. The difference is simply the amount of risk\nbudget consumed. Because the beta to spot gold is only 0.03, nearly all the scaled return represents\nbenchmark-neutral alpha. Figure 3 illustrates this scaling by comparing the growth of a $1 million\ninvestment in spot gold with the Forecast-to-Fill strategy at the 15% volatility budget.\nCapacity and realizability. To verify that such scaling is economically feasible, we evaluate the\nfriction-adjusted growth curve,\ng(L) = \u00b5uL \u22121\n2(\u03c3uL)2 \u2212nkL \u2212\u03b3(nL)3/2,\nwhere L is participation (fraction of daily market volume traded). Using measured parameters\n\u00b5u = 1.0 \u00d7 10\u22124, \u03c3u = 5.7 \u00d7 10\u22124, k = 0.7 bps, \u03b3 = 0.02, and n = 1, we obtain a zero-growth point\nat\nLmax \u22482.9 \u00d7 10\u22126.\nMapping this to CME gold futures liquidity (ADV \u223c50 billion/day, mean absolute turnover\n|\u2206wt| \u22480.066) yields\nAmax = Lmax ADV$\n|\u2206wt|\n\u22487.6 \u00d7 108 USD.\nHence the system\u2019s expected growth remains positive up to roughly 0.8\u20131.0 billion USD of deployable\ncapital. At this scale, the strategy\u2019s average daily volume share is only \u223c0.07%, far below any level\nthat would induce self-impact or liquidity stress.\nClarifying alpha conventions.",
    "chunk_index": 16,
    "start_char": 42444,
    "end_char": 45444,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "gold futures liquidity (ADV \u223c50 billion/day, mean absolute turnover\n|\u2206wt| \u22480.066) yields\nAmax = Lmax ADV$\n|\u2206wt|\n\u22487.6 \u00d7 108 USD.\nHence the system\u2019s expected growth remains positive up to roughly 0.8\u20131.0 billion USD of deployable\ncapital. At this scale, the strategy\u2019s average daily volume share is only \u223c0.07%, far below any level\nthat would induce self-impact or liquidity stress.\nClarifying alpha conventions. Two annualized alpha figures appear in this section: (i) the\nregression-based CAPM alpha of approximately 43% per year, obtained as the linear annualization\nof the daily intercept in rFTF = \u03b1 + \u03b2rgold + \u03b5 with \u03b2 \u22480.03, and (ii) the information-ratio\u2013based\nalpha of about 37% per year, reported in the abstract for comparability with allocator conventions\n(\u03b1IR = IR \u00d7 TE). The former represents the raw benchmark-neutral return of the Forecast-to-Fill\nsleeve at the 15% volatility budget, while the latter expresses the same efficiency in IR units. Both\nrefer to the same underlying performance and differ only by definition.\nSummary. At the 15% risk budget, the same process implies \u223c43% annualized return and \u223c37%\nbenchmark-neutral alpha, both statistically robust and economically feasible. The friction-adjusted\ncapacity frontier confirms that performance remains positive up to about 1 billion of capital, beyond\nwhich impact dominates. This places the strategy comfortably within the institutional scale regime\nwhile preserving allocator-grade stability and diversification value.\n11\nSub-period stability and regime attribution\nWe next test whether the strategy\u2019s performance is stable across time and whether returns concentrate\nin specific market environments. A system that performs only in one short period or under one\ntype of regime is not economically robust. To address this, we segment the out-of-sample period\ninto multiple sub-windows and classify days into ex-ante market regimes based on our own signal.\n18\n\n11.1\nSub-period stability\nWe divide the 2015\u20132025 test period into rolling multi-year slices and recompute key statistics\nwithin each slice. This allows us to see whether alpha persists as market structure, volatility, and\nliquidity evolve. Table 1 summarizes annualized returns, volatility, Sharpe ratio, and maximum\ndrawdown by sub-period.\nSpan\nAnn. Ret.%\nAnn. Vol.%\nSharpe\nMaxDD %\nCalmar\n2015\u20132025\n2.62\n0.91\n2.88\n0.52\n5.11\n2019+\n2.77\n0.95\n2.93\n0.47\n5.95\n2022+\n3.10\n1.07\n2.91\n0.47\n6.66\nTable 1: Sub-period stability of the strategy\u2019s out-of-sample performance.\nWe observe that the Sharpe ratio remains remarkably consistent across all periods\u2014hovering\nnear 2.9\u2014while annualized volatility increases only slightly as gold markets became more active\nafter 2021. The persistence of Sharpe implies that our signal adapts naturally to changing market\nnoise without any re-optimization. Even when the absolute volatility of gold rises (for instance,\naround 2022 when macro uncertainty spiked), our volatility targeting mechanism keeps realized risk\nstable, preventing blowups or loss of efficiency.\nWe also find that drawdowns remain small across all sub-periods, never exceeding 0.6%. This\nstability confirms that the risk-control logic\u2014volatility scaling, ATR exits, and fractional Kelly\nsizing\u2014remains effective regardless of external volatility. The gradual rise in annualized returns\nover time reflects the strategy\u2019s improved responsiveness during more volatile phases rather than\noverfitting or structural change.",
    "chunk_index": 17,
    "start_char": 45034,
    "end_char": 48487,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "when the absolute volatility of gold rises (for instance,\naround 2022 when macro uncertainty spiked), our volatility targeting mechanism keeps realized risk\nstable, preventing blowups or loss of efficiency.\nWe also find that drawdowns remain small across all sub-periods, never exceeding 0.6%. This\nstability confirms that the risk-control logic\u2014volatility scaling, ATR exits, and fractional Kelly\nsizing\u2014remains effective regardless of external volatility. The gradual rise in annualized returns\nover time reflects the strategy\u2019s improved responsiveness during more volatile phases rather than\noverfitting or structural change.\n11.2\nRegime attribution\nTo understand where returns come from economically, we classify each trading day ex-ante into one\nof three regimes\u2014bull, chop, or bear\u2014based on the signal\u2019s blended regime probability pbull(t).\nDays with pbull > 0.55 are labeled bull; days with pbull < 0.45 are bear; and the rest are chop\n(neutral). We then compute annualized mean returns and Sharpe ratios within each regime.\nRegime\nAnn. Return %\nSharpe\nObs. Days\nBull\n4.49\n3.82\n1,628\nChop\n0.02\n2.03\n49\nBear\n\u22120.00\n\u22120.02\n1,116\nTable 2: Regime-level attribution of returns by ex-ante signal classification.\nWe find that almost all realized profits occur during self-identified bull regimes, with a Sharpe of\n3.82 and annualized return of 4.49%. This is expected: the system is designed to take long exposure\nonly when trend and momentum jointly confirm persistent upside drift. The fact that performance\nconcentrates in these periods validates the model\u2019s intended behavior.\nDuring chop regimes\u2014periods of weak or conflicting signal\u2014the system trades lightly, keeps\nexposure small, and mostly breaks even.\nThat near-zero return is a feature, not a flaw: it\ndemonstrates that our confidence gating and volatility scaling successfully reduce activity when the\nenvironment is noisy.\n19\n\nIn bear regimes, performance hovers near zero because the strategy avoids fighting strong\ndowntrends in gold. While we could extend the framework symmetrically to short-side trading, the\ncurrent long-only configuration focuses on verifying robustness rather than maximizing directional\ncoverage.\n11.3\nInterpretation\nThese results confirm that our system\u2019s alpha is stable across time and aligned with the logic of\nits own signal. The model behaves like an adaptive risk-premia engine: it scales risk in favorable\nregimes, steps back in uncertain ones, and avoids unprofitable trades in adverse conditions.\nBy maintaining similar Sharpe ratios across different macro environments, the system demon-\nstrates that its performance does not rely on any one epoch of gold behavior (for instance, post-\npandemic volatility or low-rate conditions).\nInstead, it generalizes across multiple structural\nregimes\u2014an essential property for real-world deployability.\nIn allocator terms, this section shows that our sleeve behaves like a stable, low-drift alpha source:\nit compounds steadily in the presence of trend, loses little in noise, and protects capital during\nreversals. That persistence across regimes and time frames is what separates a genuine signal from\nan optimized backtest.\n12\nCosts, impact, and capacity\nWe now test how sensitive the strategy is to trading frictions and how much capital it can realistically\ndeploy before costs and market impact erode expected returns.",
    "chunk_index": 18,
    "start_char": 47859,
    "end_char": 51219,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "behaves like a stable, low-drift alpha source:\nit compounds steadily in the presence of trend, loses little in noise, and protects capital during\nreversals. That persistence across regimes and time frames is what separates a genuine signal from\nan optimized backtest.\n12\nCosts, impact, and capacity\nWe now test how sensitive the strategy is to trading frictions and how much capital it can realistically\ndeploy before costs and market impact erode expected returns. In practice, every live system faces\nexecution drag, so understanding its capacity frontier is as important as its raw alpha. We quantify\nboth through explicit stress tests on cost parameters and by deriving a friction-adjusted growth\ncurve.\n12.1\nStress testing execution costs\nWe re-run the entire out-of-sample (OOS) simulation under a grid of cost and impact multipliers to\nsee how the strategy behaves under harsher execution assumptions. Specifically, we scale both the\nlinear transaction cost (k) and the temporary market impact parameter (\u03b3) by factors ranging from\n0.5\u00d7 to 2.0\u00d7. This range covers the realistic spectrum of slippage and liquidity conditions for gold\nfutures, from highly liquid to stressed markets.\nCost\u00d7\nImpact\u00d7\nSharpe (estimated)\n0.5\n0.5\u20132.0\n1.907\n1.0\n0.5\u20132.0\n0.937\n1.5\n0.5\u20132.0\n\u22120.033\n2.0\n0.5\u20132.0\n\u22121.004\nTable 3: Sharpe ratio under cost and impact stress scenarios.\nWe find that at baseline frictions (1.0\u00d7), the Sharpe ratio remains well above 2.5, as reported\nearlier. Doubling both costs (2.0\u00d7) drives the Sharpe close to zero, which defines the boundary of\npractical capacity. The roughly linear deterioration between 1.0\u00d7 and 1.5\u00d7 indicates that the edge\ndecays predictably with execution drag, not catastrophically. In other words, we can scale trading\nactivity moderately or operate in slightly higher-cost environments without destroying profitability.\n20\n\nThis behavior is consistent with theoretical microstructure studies such as (author?) [1] and\n(author?) [5], which show that liquidity cost grows sub-linearly with trade size until impact\ndominates. Our empirical stress grid therefore functions as a live proxy for that curve: it confirms\nthat the system operates comfortably within the region of linear co\n13\nRobustness and falsification\nWe now stress-test the signal to confirm that its performance survives under realistic implementation\ndelays, structural inversions, and formal statistical falsification tests. Our goal is to show that the\nsystem\u2019s returns arise from genuine predictive structure rather than look-ahead bias, parameter\nluck, or favorable noise.\n13.1\nLatency robustness\nIn real markets, trades are not always filled at the theoretical close. To account for this, we shift\nthe execution horizon by one and two days\u2014testing whether the signal still works when applied\nwith realistic trading latency. We rerun the entire out-of-sample backtest under three scenarios:\nimmediate execution (T + 0), one-day delay (T + 1), and two-day delay (T + 2).\nDelay\nSharpe\nAnn. Vol.%\nMaxDD %\nT + 0\n2.88\n0.91\n0.52\nT + 1\n2.28\n0.79\n0.63\nT + 2\n2.24\n0.77\n0.56\nTable 4: Performance under trading latency stress.\nWe find that the Sharpe ratio declines moderately from 2.88 at T +0 to about 2.25 at T +2.",
    "chunk_index": 19,
    "start_char": 50754,
    "end_char": 53961,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "with realistic trading latency. We rerun the entire out-of-sample backtest under three scenarios:\nimmediate execution (T + 0), one-day delay (T + 1), and two-day delay (T + 2).\nDelay\nSharpe\nAnn. Vol.%\nMaxDD %\nT + 0\n2.88\n0.91\n0.52\nT + 1\n2.28\n0.79\n0.63\nT + 2\n2.24\n0.77\n0.56\nTable 4: Performance under trading latency stress.\nWe find that the Sharpe ratio declines moderately from 2.88 at T +0 to about 2.25 at T +2. The\nedge remains strongly positive even when positions are delayed by two trading days. This confirms\nthat our system\u2019s performance is not dependent on intraday timing or idealized fills; it is robust to\nrealistic operational lag. The mild Sharpe decay is expected because delay smooths entry response\nto trend inflections. Importantly, the performance drop is proportional\u2014not catastrophic\u2014implying\nthat the underlying signal is persistent enough to remain profitable under live execution conditions.\n13.2\nReversal and ablation tests\nWe next conduct falsification tests by deliberately breaking the signal. We first invert it\u2014flipping all\nlong conditions to short\u2014to confirm that the logic truly captures directional persistence rather than\nrandom drift. The reversed signal produces a Sharpe ratio of \u22122.95, as expected. This outcome\nvalidates that the original signal and its directionality are essential to the edge; if performance had\nremained positive, it would imply overfitting or hidden correlations.\nWe then perform ablation tests by removing one component at a time: first the trend slope,\nthen the momentum filter. Dropping either feature reduces Sharpe substantially, confirming that\nboth are required. The slope term provides continuous regime intensity, while the momentum\nterm provides discrete direction confirmation. Together they yield the highest information ratio;\nindependently, each contributes partial predictive power but lacks stability. This decomposition\nconfirms that performance arises from genuine interaction between complementary signal features\nrather than from any one arbitrary metric.\n21\n\n13.3\nStatistical significance and falsification\nTo formally test whether our observed alpha could arise by chance, we apply the Superior Predictive\nAbility (SPA) and Reality-Check tests [20, 9]. These tests correct for data-snooping bias across\nmultiple candidate configurations. We evaluate 64 grid combinations of smoothing parameters,\nmomentum windows, and activation thresholds using 800 block bootstraps (block length = 20 days).\nThe SPA test rejects the null hypothesis of \u201cno superior model\u201d with p = 0.000, confirming that\nour configuration significantly outperforms a broad universe of alternatives even after accounting\nfor multiple testing. This is the most conservative possible validation; a p-value that low effectively\nrules out the possibility of random chance.\nWe also bootstrap the Sharpe ratio itself to assess its sampling uncertainty. The 95% confidence\ninterval, [2.49, 3.27], excludes zero by a wide margin, consistent with a statistically stable process.\n13.4\nTail behavior\nFinally, we examine the tails of the return distribution to ensure that high Sharpe is not hiding\nunrecognized downside risk. The worst monthly return is only \u22120.20%, daily VaR95 is 0.04%, and\nCVaR95 is 0.09%. These tail metrics confirm that the system\u2019s payoff profile is well-behaved: fat\nright tails (large winners) but thin left tails (small, controlled losses).",
    "chunk_index": 20,
    "start_char": 53548,
    "end_char": 56952,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "bootstrap the Sharpe ratio itself to assess its sampling uncertainty. The 95% confidence\ninterval, [2.49, 3.27], excludes zero by a wide margin, consistent with a statistically stable process.\n13.4\nTail behavior\nFinally, we examine the tails of the return distribution to ensure that high Sharpe is not hiding\nunrecognized downside risk. The worst monthly return is only \u22120.20%, daily VaR95 is 0.04%, and\nCVaR95 is 0.09%. These tail metrics confirm that the system\u2019s payoff profile is well-behaved: fat\nright tails (large winners) but thin left tails (small, controlled losses).\nThis tail pattern reflects the asymmetric risk structure built into the signal: fixed stop-losses\nand open-ended winners.\nThe lack of deep left tails means that drawdown risk is genuinely\nlimited, not just averaged away by smoothing. In practice, this property is critical for allocator-\ngrade robustness\u2014high Sharpe with thin downside tails implies true scalability and psychological\ntradability.\n13.5\nInterpretation\nAcross all robustness tests, we observe consistent and economically interpretable behavior:\n\u2022 Delayed execution still produces Sharpe above 2.2, confirming time persistence.\n\u2022 Signal reversal produces negative performance, confirming directionality is causal.\n\u2022 Feature ablations degrade results, confirming both slope and momentum components are\nessential.\n\u2022 SPA and bootstrap tests confirm statistical significance beyond random chance.\n\u2022 Tail metrics show small drawdowns and bounded downside.\nThese falsification tests collectively demonstrate that our system\u2019s alpha is not a statistical\nillusion or artifact of look-ahead bias.\nThe edge survives realistic trading conditions, retains\nsignificance after correction for data snooping, and exhibits a risk profile consistent with disciplined\ntrend-following mechanics.\nIn short, we design the tests to try to break the model\u2014and it holds up.\n14\nDiscussion\nWe now interpret what the results imply about the underlying economics of the edge. Our objective\nis not only to show that the system works empirically but also to explain why it should exist in a\nliquid, competitive market like gold. We connect the statistical structure of the signal to market\nmicrostructure, risk transfer, and behavioral underreaction.\n22\n\n14.1\nState-dependent risk premia\nWe find that the system consistently profits in regimes where directional volatility persists\u2014periods\nwhen prices drift gradually rather than mean-revert violently. This behavior indicates that the\nalpha originates from state-dependent risk premia: transient returns available to agents willing\nto provide liquidity or inventory in trending environments.\nIn practice, large institutional hedgers (such as producers, refiners, and ETF issuers) often\nadjust positions slowly in response to information or macro shocks. This lag creates predictable\ncontinuation in prices [16, 14]. Our signal detects those periods of adjustment and selectively\nprovides liquidity in their direction. The alpha therefore compensates us for absorbing short-term\ninventory risk when the market\u2019s natural liquidity providers are constrained.\n14.2\nEngineering, not forecasting\nThe performance we observe does not depend on predicting price levels with precision. Instead, it\nemerges from a sequence of engineering choices that control risk and costs while allowing the system\nto stay exposed to conditional drift. We build a process that consistently transforms small, noisy\nsignals into statistically stable profits through structure:\n1.",
    "chunk_index": 21,
    "start_char": 56374,
    "end_char": 59888,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "selectively\nprovides liquidity in their direction. The alpha therefore compensates us for absorbing short-term\ninventory risk when the market\u2019s natural liquidity providers are constrained.\n14.2\nEngineering, not forecasting\nThe performance we observe does not depend on predicting price levels with precision. Instead, it\nemerges from a sequence of engineering choices that control risk and costs while allowing the system\nto stay exposed to conditional drift. We build a process that consistently transforms small, noisy\nsignals into statistically stable profits through structure:\n1. Smoothing extracts persistent direction from noisy returns without overreacting to random\nfluctuations.\n2. Confidence mapping converts that slope into a bounded, interpretable probability that scales\nexposure naturally.\n3. Volatility targeting keeps realized risk constant over time, preventing uncontrolled leverage\nin quiet markets.\n4. Friction-adjusted sizing ensures that every position has positive expected log-growth after\ntrading costs.\n5. Disciplined exits enforce an asymmetric payoff: small losses, large and infrequent gains.\nWe therefore do not rely on high forecasting accuracy. Instead, we exploit the statistical fact\nthat asymmetrically managing losses and volatility can produce consistent positive expectancy even\nwhen hit rates hover near random [12, 13]. This is the essence of risk engineering: building a system\nthat converts noise-filtered persistence into controlled compounding.\n14.3\nRelation to known market behavior\nOur findings are consistent with established evidence that commodity and currency markets exhibit\nshort-term momentum linked to behavioral and inventory effects. When traders face funding\nconstraints or margin pressure, they cannot immediately offset shocks, leading to continuation at\ndaily-to-weekly horizons. Gold in particular shows such behavior during macro uncertainty, when\nhedgers rebalance gradually rather than instantaneously.\nWe design our signal to identify these conditions and participate only when the risk\u2013reward ratio\nis favorable. The combination of slope and momentum acts as a probabilistic filter that detects\nperiods of delayed adjustment. The consistent positive skew of returns supports this interpretation:\nwe earn many small gains and a few large windfalls during sustained trends, while losses remain\ncapped.\n23\n\n14.4\nEconomic interpretation of robustness\nOur robustness tests provide further economic insight. The strategy\u2019s profitability survives T + 1\nand T + 2 delays because institutional order flow and information diffusion in gold are slow relative\nto our holding periods. The negative performance of the reversed signal confirms that the edge\nis not random; it depends on directionally persistent order imbalance. The SPA test\u2019s p = 0.000\nsignificance indicates that the effect is stronger than what can be produced by random noise or\narbitrary rule choice.\nTogether, these findings imply that our system does not exploit a statistical anomaly; it captures\na structural feature of market behavior. It earns compensation for taking short-term directional\nexposure when others demand immediacy and pay to offload it.\n14.5\nPortfolio implications\nFrom a portfolio perspective, this sleeve behaves like a self-hedging alpha source. Its Sharpe ratio\nabove 2.5, low volatility, and near-zero correlation with gold prices make it a diversifying addition\nto both macro and multi-asset portfolios.",
    "chunk_index": 22,
    "start_char": 59304,
    "end_char": 62758,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "not exploit a statistical anomaly; it captures\na structural feature of market behavior. It earns compensation for taking short-term directional\nexposure when others demand immediacy and pay to offload it.\n14.5\nPortfolio implications\nFrom a portfolio perspective, this sleeve behaves like a self-hedging alpha source. Its Sharpe ratio\nabove 2.5, low volatility, and near-zero correlation with gold prices make it a diversifying addition\nto both macro and multi-asset portfolios. Because the returns are driven by microstructural timing\nrather than broad economic cycles, the signal can coexist with carry, value, or long-term momentum\nexposures without crowding.\nMoreover, its risk profile is psychologically tradable: small drawdowns, positive skew, and\nconsistent compounding make it feasible for real capital deployment. The concave capacity frontier\nwe derived earlier ensures scalability up to institutional AUM levels before impact becomes binding.\n14.6\nKey takeaway\nWe interpret the overall system as an example of forecast-to-fill engineering\u2014the process of connecting\na transparent signal to executable trades through disciplined control of volatility, frictions, and tail\nrisk. By engineering rather than predicting, we transform small, persistent biases in order flow into\nallocator-grade alpha. The signal does not claim omniscience about the future; it simply behaves\nlike a rational liquidity provider, stepping in when the odds and costs align.\n15\nLimitations and next steps\nWe design the system to be simple, interpretable, and fully out-of-sample. That discipline comes\nwith trade-offs. In this section, we identify the main limitations of our current setup and outline\nhow future work can extend it toward broader markets, richer modeling, and live implementation.\n15.1\nSimplified friction model\nWe currently assume fixed linear and square-root impact parameters: k = 0.7 basis points per round\ntrip and \u03b3 = 0.02 for temporary impact. These values are realistic for liquid gold futures but do not\nadapt dynamically to changing depth or volatility. In real markets, costs vary with spread, volume,\nand volatility clustering [1, 5].\nFuture work can replace these static coefficients with an adaptive cost surface kt, \u03b3t estimated\ndirectly from order book or transaction data. This would allow the system to self-correct for liquidity\nstress, widening spreads, or volatility spikes. Although our stress tests already span 0.5\u00d7\u20132\u00d7 of\nbaseline costs, dynamic estimation would improve realism and capital efficiency for live deployment.\n24\n\n15.2\nSingle-asset scope\nWe test our architecture on gold because it provides a deep, transparent market with continuous\npricing. However, we expect the framework to generalize naturally to other liquid assets\u2014FX, energy,\nand equity index futures\u2014where similar persistence effects arise from slow order-flow adjustment\nand volatility targeting.\nExtending the pipeline cross-sectionally would allow us to study whether the state-dependent risk\npremia we observe in gold represent a global behavioral feature or a market-specific microstructure\nartifact.\nWe plan to run a multi-asset replication to measure correlation decay, cross-market\nconsistency, and portfolio diversification potential.\n15.3\nVolatility forecasting and signal precision\nOur volatility forecast uses a simple 20-day EWMA, which is sufficient for stable risk control but not\nnecessarily optimal.",
    "chunk_index": 23,
    "start_char": 62281,
    "end_char": 65695,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "targeting.\nExtending the pipeline cross-sectionally would allow us to study whether the state-dependent risk\npremia we observe in gold represent a global behavioral feature or a market-specific microstructure\nartifact.\nWe plan to run a multi-asset replication to measure correlation decay, cross-market\nconsistency, and portfolio diversification potential.\n15.3\nVolatility forecasting and signal precision\nOur volatility forecast uses a simple 20-day EWMA, which is sufficient for stable risk control but not\nnecessarily optimal. In practice, heteroskedastic models such as HAR-RV [4] or GARCH variants\ncould provide smoother volatility forecasts, reducing lag and noise.\nWe intentionally keep the forecasting model minimal to isolate the effect of the signal itself.\nHowever, integrating adaptive volatility models could further stabilize sizing and reduce unnecessary\nde-risking during short-lived volatility spikes.\n15.4\nUnmodeled execution frictions\nWe abstract away from micro-latency, partial fills, and overnight funding.\nIn live execution,\nthese effects can introduce small but systematic drags, particularly for intraday or leveraged\nimplementations. We assume at most one round trip per day (n \u22641), which approximates daily\nroll-level trading but does not model high-frequency feedback loops.\nIn future work, we can embed a microstructural simulator using empirical distributions of fill\ntimes, queue positions, and slippage. This would close the loop between forecast generation and\nexecution quality, aligning with the \u201cforecast-to-fill\u201d philosophy at the millisecond scale.\n15.5\nModel interpretability and causal testing\nAlthough we have verified directionality through reversal tests, further causal testing\u2014such as\nGranger causality or feature importance under permutation\u2014could deepen our understanding of\nwhich inputs drive the signal most strongly. We plan to augment the signal with interpretable\nfeatures (e.g., realized skew, option-implied volatility slope) to test whether the underlying driver is\nbehavioral inertia or structural inventory flow. By linking the signal to observable fundamentals, we\ncan clarify the causal channel behind its persistence.\n15.6\nToward live deployment\nFinally, while our backtest enforces strict out-of-sample discipline, we have not yet implemented con-\ntinuous live validation with streaming data. In practice, live systems require monitoring modules for\ndrift detection, regime tagging, and automatic parameter freezing. We plan to transition the current\nframework into a live sandbox that updates forecasts daily but re-trains only monthly\u2014mimicking\na real execution pipeline with automated logs, latency capture, and fill reconciliation.\nThis step will convert the system from a research prototype into a deployable alpha engine\nsuitable for institutional-scale operation.\n25\n\n15.7\nSummary of limitations\nIn summary, our key simplifications\u2014static frictions, single-asset scope, and basic volatility fore-\ncasting\u2014are intentional design choices that maximize interpretability and test purity. None of\nthem undermine the main result: a robust, out-of-sample alpha that survives costs, latency, and\nfalsification. Future extensions will focus on scaling the architecture horizontally (multi-asset),\nvertically (microstructure resolution), and adaptively (cost-aware volatility control). These directions\nwill turn a transparent academic model into a fully operational, self-calibrating trading system.\n16\nConclusion\nWe set out to test whether transparent, interpretable state variables\u2014trend and momentum\u2014can\ngenerate durable out-of-sample alpha in one of the world\u2019s most liquid assets once realistic risk,\ncost, and impact are included.",
    "chunk_index": 24,
    "start_char": 65166,
    "end_char": 68861,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "a robust, out-of-sample alpha that survives costs, latency, and\nfalsification. Future extensions will focus on scaling the architecture horizontally (multi-asset),\nvertically (microstructure resolution), and adaptively (cost-aware volatility control). These directions\nwill turn a transparent academic model into a fully operational, self-calibrating trading system.\n16\nConclusion\nWe set out to test whether transparent, interpretable state variables\u2014trend and momentum\u2014can\ngenerate durable out-of-sample alpha in one of the world\u2019s most liquid assets once realistic risk,\ncost, and impact are included. We built a full forecast-to-fill architecture to answer that question\nempirically, connecting a simple predictive signal to real, executable trades through a reproducible,\nlive-like pipeline.\nRather than forecasting prices precisely, the framework converts weak, state-dependent pre-\ndictability into consistent performance through structure: smoothing to suppress noise, confidence\nmapping to scale conviction, volatility targeting to stabilize risk, friction-adjusted Kelly sizing to\nkeep growth realistic, and disciplined exits to bound losses. Every parameter is trained on a rolling\n10-year window, frozen before testing, and evaluated strictly out-of-sample.\nAcross 2,793 out-of-sample trading days (2015\u20132025), the strategy delivers a Sharpe ratio of\n2.88, a CAGR of 2.65%, and a maximum drawdown of 0.52%, all net of realistic frictions. The\nresults are statistically significant (bootstrap CI [2.49, 3.27]; SPA p = 0.000) and economically\nrobust (latency-insensitive, reversal-falsified, and cost-stable). Regression tests show \u03b1 = 2.25% per\nyear with \u03b2 = 0.03, confirming near-perfect benchmark neutrality and validating the signal as a\ngenuine alpha source. These are at a realized volatility of 0.91%.\nNormalizing performance to the 15% annual volatility budget\u2014standard for liquid macro\nsleeves\u2014yields an implied return of 43.2% per year and a benchmark-neutral alpha of 37.1% per\nyear, with Sharpe = 2.88 and IR = 2.09 unchanged. This expresses the strategy\u2019s efficiency on an\nallocator-comparable risk scale rather than implying leverage. A friction-adjusted capacity analysis\nbased on the growth function g(L) = \u00b5uL \u22121\n2(\u03c3uL)2 \u2212nkL \u2212\u03b3(nL)3/2 shows positive expected\nlog-growth up to approximately $0.8\u20131.0 billion USD of deployable capital\u2014corresponding to\nonly \u223c0.07% of CME gold futures daily volume\u2014well inside the low-impact regime. Beyond that\npoint, costs and market impact concavely flatten expected growth, defining a realistic institutional\ncapacity frontier.\nEconomically, these results support viewing short-horizon alpha as a state-dependent risk\npremium. The strategy earns returns for assuming controlled directional exposure when other\nmarket participants face frictions, funding limits, or behavioral inertia. It does not exploit mispricing\nbut rather provides liquidity during trending states, transforming transient order-flow persistence\ninto repeatable profit. Its stability across delays, regimes, and cost conditions indicates that the\nedge arises from a persistent microstructural inefficiency rather than transient noise.\nConceptually, this work illustrates that disciplined engineering can be as powerful as complex\nprediction. By aligning each layer of the process\u2014signal, sizing, risk, and cost\u2014small probabilistic\nadvantages compound into allocator-grade performance. The framework generalizes naturally to\nother liquid assets and offers a reproducible blueprint for transparent, data-driven alpha generation\nat institutional scale.\n26\n\nIn summary, a strategy designed from forecast to fill can transform modest, interpretable signals\ninto statistically significant, economically meaningful, and billion-dollar-scalable performance.",
    "chunk_index": 25,
    "start_char": 68258,
    "end_char": 72038,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "microstructural inefficiency rather than transient noise.\nConceptually, this work illustrates that disciplined engineering can be as powerful as complex\nprediction. By aligning each layer of the process\u2014signal, sizing, risk, and cost\u2014small probabilistic\nadvantages compound into allocator-grade performance. The framework generalizes naturally to\nother liquid assets and offers a reproducible blueprint for transparent, data-driven alpha generation\nat institutional scale.\n26\n\nIn summary, a strategy designed from forecast to fill can transform modest, interpretable signals\ninto statistically significant, economically meaningful, and billion-dollar-scalable performance.\nThe edge lies not in guessing the future, but in structuring risk, friction, and execution so that even\nlimited predictability compounds reliably over time.\nMethodological Clarifications (Added, Non-Intrusive)\nTraded instrument and return construction.\nWe implement the strategy on CME Gold\n(GC) futures, continuous front-month, rolled two business days before first notice. Returns\nare close-to-close futures total returns (including roll P&L). LBMA PM fix is used only as a\nbenchmark regressor, not for trading decisions.\nTimestamp alignment and fills.\nSignals are computed at day t using only Ft and filled at t+1\nclose (T+1 baseline). Robustness includes T+0 and T+2. Exit logic (ATR, regime de-risking)\nuses information available at the decision time; no same-bar look-ahead is used for fill prices.\nFrom portfolio weight to participation and impact.\nGiven AUM A, contract price P cntr\nt\n,\nmultiplier M, and GC ADV (contracts/day) ADVt, the participation of a trade is\nqt \u2261|\u2206contractst|\nADVt\n=\n|\u2206wt| \u00b7 A\nP cntr\nt\n\u00b7 M \u00b7 ADVt\n.\nTemporary impact follows the square-root form impactt \u2248Y \u03c31d\u221aqt. The reduced-form penalty\n\u03b3(nf)3/2 used in the growth objective aggregates Y and volatility. We report (in code/data release)\nmedian qt and its distribution to audit capacity claims.\nOverlapping windows and dependence.\nMonthly-advanced 6-month OOS slices overlap; we\ntherefore use stationary/block bootstraps with block length \u224820 trading days (close to signal\nhalf-life) and additionally provide non-overlapping yearly summaries in the companion materials.\nSPA / Reality-Check disclosure.\nModel grid: 64 configs (EMA decay, momentum K, activation\nthreshold). Bootstrap: stationary, B = 800, block length l = 20. Loss: differential Sharpe vs\nzero-cost benchmark. Studentized test statistic. Result: p < 0.001.\nRegression diagnostics.\nCAPM-style regression uses Newey\u2013West (HAC) standard errors.\nA multifactor check (spot gold, DXY changes, rate level/changes) preserves alpha magnitude and\nnear-zero betas.\nRisk target vs realized volatility.\nThe 15% annual target defines a budget cap. Confidence\ngating and frequent flat periods make realized exposure small, so realized annualized volatility\n\u22480.91% is far below the cap. For transparency, we also track the unit-notional sleeve and a pure\nvol-targeted series.\nReproducibility checklist.\n(i) Freeze all train-time parameters before each OOS slice. (ii)\nCompute recursive filters forward-only. (iii) Use T+1 fills in baseline. (iv) Apply costs to realized\n|\u2206wt|. (v) Log full decisions, fills, and costs.\n27\n\nA\nUnit-notional vs policy series\nWe report (i) unit-notional sleeve (no volatility targeting), (ii) volatility-targeted series, (iii) realized\npolicy series with regime gating and ATR exits. This clarifies why realized vol (\u22480.91%) is far\nbelow the 15% target: the system is frequently flat or partially allocated by design.",
    "chunk_index": 26,
    "start_char": 71366,
    "end_char": 74918,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "series.\nReproducibility checklist.\n(i) Freeze all train-time parameters before each OOS slice. (ii)\nCompute recursive filters forward-only. (iii) Use T+1 fills in baseline. (iv) Apply costs to realized\n|\u2206wt|. (v) Log full decisions, fills, and costs.\n27\n\nA\nUnit-notional vs policy series\nWe report (i) unit-notional sleeve (no volatility targeting), (ii) volatility-targeted series, (iii) realized\npolicy series with regime gating and ATR exits. This clarifies why realized vol (\u22480.91%) is far\nbelow the 15% target: the system is frequently flat or partially allocated by design.\nB\nNon-overlapping summaries\nYearly, non-overlapping OOS tables (returns, vol, Sharpe, MaxDD, turnover) are provided to\ncomplement overlapping monthly-advanced slices and to avoid dependence-induced optimism.\nC\nCapacity and AUM mapping\nWe define participation L as average absolute weight scaled to ADV via Section ??. The growth\ncurve\ng(L) \u2248\u00b5uL \u22121\n2(\u03c3uL)2 \u2212nkL \u2212\u03b3(nL)3/2\nis plotted with an AUM axis A through L \u221dA. The realized L lies on the positive-growth branch;\nthe zero-growth point defines practical capacity. We report the distribution of qt and median qt to\ndocument where the system operates in liquidity space.\nD\nFactor checks\nWe run regressions of strategy returns on spot gold, DXY changes, and rate level/changes with Newey\u2013\nWest standard errors. Alpha persists with similar magnitude; betas are small and economically\nnegligible.\nE\nFriction-adjusted Kelly derivation (details)\nStarting from g(f) = E\n\u0002\nlog(1 + fR)\n\u0003\n, use a second-order expansion and incorporate linear and\nsquare-root impact terms to obtain g(f) \u2248\u00b5f \u22121\n2\u03c32f2 \u2212nkf \u2212\u03b3(nf)3/2. Set f = x2, differentiate\ng(x) = \u00b5x2 \u22121\n2\u03c32x4 \u2212nkx2 \u2212\u03b3n3/2x3, and solve 2\u03c32x2 + 3\u03b3n3/2x \u22122(\u00b5 \u2212nk) = 0 for x\u22c6. Fractional\nKelly uses \u02dcf = \u03bbKellyf\u22c6to limit estimation sensitivity and drawdowns.\nReferences\n[1] Almgren, R. and N. Chriss (2001). Optimal execution of portfolio transactions. Journal of Risk,\n3(2), 5\u201340.\n[2] Bailey, D. H., J. M. Borwein, M. L\u00b4opez de Prado, and Q. J. Zhu (2017). The probability of\nbacktest overfitting. Journal of Computational Finance, 20(4), 39\u201369.\n[3] Brogaard, J., T. Hendershott, and R. Riordan (2014). High-frequency trading and price\ndiscovery. Review of Financial Studies, 27(8), 2267\u20132306.\n[4] Corsi, F. (2009). A simple approximate long-memory model of realized volatility. Journal of\nFinancial Econometrics, 7(2), 174\u2013196.\n[5] Gatheral, J. (2010). No-dynamic-arbitrage and market impact. Quantitative Finance, 10(7),\n749\u2013759.\n28\n\n[6] Harvey, A. C. (1989). Forecasting, Structural Time Series Models and the Kalman Filter.\nCambridge University Press.\n[7] Harvey, C. R., Y. Liu, and H. Zhu (2016). . . . and the cross-section of expected returns. Review\nof Financial Studies, 29(1), 5\u201368.\n[8] Harris, L. (2002). Trading and Exchanges: Market Microstructure for Practitioners. Oxford\nUniversity Press.\n[9] Hansen, P. R. (2005). A test for superior predictive ability. Journal of Business & Economic\nStatistics, 23(4), 365\u2013380.\n[10] Jegadeesh, N. and S. Titman (1993). Returns to buying winners and selling losers: Implications\nfor stock market efficiency. Journal of Finance, 48(1), 65\u201391.\n[11] Kelly, J. L. (1956). A new interpretation of information rate. Bell System Technical Journal,\n35(4), 917\u2013926.\n[12] Lo, A. W. (2000).",
    "chunk_index": 27,
    "start_char": 74339,
    "end_char": 77640,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "Harris, L. (2002). Trading and Exchanges: Market Microstructure for Practitioners. Oxford\nUniversity Press.\n[9] Hansen, P. R. (2005). A test for superior predictive ability. Journal of Business & Economic\nStatistics, 23(4), 365\u2013380.\n[10] Jegadeesh, N. and S. Titman (1993). Returns to buying winners and selling losers: Implications\nfor stock market efficiency. Journal of Finance, 48(1), 65\u201391.\n[11] Kelly, J. L. (1956). A new interpretation of information rate. Bell System Technical Journal,\n35(4), 917\u2013926.\n[12] Lo, A. W. (2000). Finance: A selective survey. Journal of the American Statistical Association,\n95(450), 629\u2013635.\n[13] MacLean, L. C., E. O. Thorp, and W. T. Ziemba (2011). The Kelly Capital Growth Investment\nCriterion. World Scientific.\n[14] Menkveld, A. J. (2022). High-frequency trading and market quality. Annual Review of Financial\nEconomics, 14, 139\u2013159.\n[15] Moreira, A. and T. Muir (2017). Volatility-managed portfolios. Journal of Finance, 72(4),\n1611\u20131644.\n[16] Moskowitz, T. J., Y. H. Ooi, and L. H. Pedersen (2012). Time series momentum. Journal of\nFinancial Economics, 104(2), 228\u2013250.\n[17] Newey, W. K. and K. D. West (1987). A simple, positive semi-definite, heteroskedasticity and\nautocorrelation consistent covariance matrix. Econometrica, 55(3), 703\u2013708.\n[18] J. P. Morgan/Reuters (1996). RiskMetrics Technical Document. New York: J.P. Morgan.\n[19] Thorp, E. O. (2006). The Kelly criterion in blackjack, sports betting, and the stock market. In\nS. A. Zenios and W. T. Ziemba (Eds.), Handbook of Asset and Liability Management, Vol. 1,\n385\u2013428. Elsevier.\n[20] White, H. (2000). A reality check for data snooping. Econometrica, 68(5), 1097\u20131126.\n29",
    "chunk_index": 28,
    "start_char": 77107,
    "end_char": 78787,
    "paper_title": "Forecast-to-Fill Benchmark-Neutral Alpha and Billi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Forecast-to-Fill_Benchmark-Neutral_Alpha_and_Billi.pdf"
  },
  {
    "text": "arXiv:1205.2521v2 [q-fin.TR] 16 May 2013\nFrom Minority Game to Black&Scholes pricing\nMatteo Ortisi\u2217, Valerio Zuccolo\u2020\nAbstract\nIn this paper we study the continuum time dynamics of a stock in a market where\nagents behavior is modeled by a Minority Game and a Grand Canonical Minority\nGame. The dynamics derived is a generalized geometric Brownian motion; from the\nBlack&Scholes formula the calibration of both the Minority Game and the Grand\nCanonical Minority Game, by means of their characteristic parameters, is performed.\nWe conclude that for both games the asymmetric phase with characteristic parameters\nclose to critical ones is coherent with options implied volatility market.\nKey words: Minority Game, Grand Canonical Minority Game, Agent-based mod-\nels, Option pricing, Market calibration.\n1\nIntroduction\nIn recent years researchers endeavored to build models that reproduce some empirical\nstatistical regularities of the real \ufb01nancial markets, such as volatility clusters, fat tails,\nscaling, occurrence of crashes, etc., known as \u201cstylized facts\u201d (see [11, 13, 15]).\nOne\npossible way to address this problem is the \u201cblack-box\u201d stochastic approach. A complex\nstochastic process which possesses the relevant characteristics of the desired empirical facts\nis constructed. Processes used in this kind of models are nonlinear di\ufb00usion models, L\u00b4evy\nprocesses, jump processes and stochastic volatility models. Another possibility is to follow\nthe agent-based framework, which models the market and derives from the interaction\nbetween players the stock price dynamics. Herd behavior and Minority Game are examples\nof these models. Choosing between these two models families relies on the purpose of the\nmodeling itself. \u201cBlack-box\u201d stochastic modeling is used in \ufb01nancial mathematics, where\nthe main goal is practical use, i.e. the pricing of derivatives or portfolio allocation. Agent-\nbased models are mainly used in economics where the theoretical aspects, the explanation\nand understanding of \ufb01nancial markets, are the main intent.\nThere is a complementarity between these two approaches; indeed \u201cblack-box\u201d stochas-\ntic models lack of explanatory power by construction and agent-based models can be hardly\nused for pricing purposes given their complexity. Moreover one of most attractive features\nof the usual Black&Scholes type models is the possibility of obtaining closed, exact for-\nmulas for the pricing of \ufb01nancial derivative securities.\nSuch qualities are fundamental\nfrom the point of view of \ufb01nance practitioners, like \ufb01nancial institutions, that require\nfast calculations, closed pricing formula for calibration purposes and tools to hedge risky\nassets.\nA very promising way to combine these main issues, is to focus on highly simpli\ufb01ed\ntoy models of \ufb01nancial markets relying on the Minority Game (MG) [4]. Variants of this\n\u2217Pioneer\nInvestments,\nGalleria\nSan\nCarlo\n6,\n20122\nMilano,\nItaly\n(mat-\nteo.ortisi@pioneerinvestments.com)\n\u2020Intesa Sanpaolo, Piazza Paolo Ferrari 10, 20100 Milano, Italy (valerio.zuccolo@polimi.it)\n1\n\nmodel, like the GCMG, have been shown to reproduce quite accurately such stylized facts\nof \ufb01nancial markets (see [1, 3, 6, 12, 20]) and moreover the continuum time limit of the\nMG and the GCMG provides a bridge between the adherence to empirically observed\nfeatures of real markets and the practical usability of \u201cblack-box\u201d stochastic models, since\nits evolution follows a system of stochastic di\ufb00erential equations (see [16], [9]).",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3477,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "Investments,\nGalleria\nSan\nCarlo\n6,\n20122\nMilano,\nItaly\n(mat-\nteo.ortisi@pioneerinvestments.com)\n\u2020Intesa Sanpaolo, Piazza Paolo Ferrari 10, 20100 Milano, Italy (valerio.zuccolo@polimi.it)\n1\n\nmodel, like the GCMG, have been shown to reproduce quite accurately such stylized facts\nof \ufb01nancial markets (see [1, 3, 6, 12, 20]) and moreover the continuum time limit of the\nMG and the GCMG provides a bridge between the adherence to empirically observed\nfeatures of real markets and the practical usability of \u201cblack-box\u201d stochastic models, since\nits evolution follows a system of stochastic di\ufb00erential equations (see [16], [9]).\nIn this framework, the stock price process is driven by the excess demand or overall\nmarket bid. The excess demand evolution process for real and \u201cfake\u201d market histories\nhas been explicitly obtained in [9]. While a game with real market histories can \ufb01t stock\nprices better than a game with \u201cfake\u201d ones, the latter is more mathematically tractable,\nespecially if the purpose is to obtain closed formulas and not run the game through sim-\nulations. Models with \u201cfake\u201d market histories, where at each point in time all agents are\ngiven random rather than real market data upon which to base their decisions, have the\nadvantage of being Markovian and hence suitable to the application of classic results, like\nGirsanov theorem, that are at the hearth of option pricing techniques in mathematical\n\ufb01nance. On the other side, models with real market histories are strongly non-Markovian\nand a dynamics simple like (3) is no more available. The dependence of the process on the\nhistory makes meaningless all the averages present in equation (3); the evolution process\nbecomes a system of stochastic di\ufb00erential equations whose parameters are to be obtained\nas a solution of a dynamic system de\ufb01ning the bid evolution process (see [9] for details).\nSince our aim is to calibrate the MG and the GCMG on the real market of European call\noptions, as a \ufb01rst attempt to obtain a closed formula for pricing an European call option\non a single stock it is easier, and maybe compulsory, to use the simplest version of these\ngames with \u201cfake\u201d market histories and to recover the excess demand directly from the\nscores di\ufb00erence stochastic di\ufb00erential equations obtained in [5, 16]. Using this approach\nit is possible to apply the usual \u201cchange of measure technique\u201d, that guarantee the absence\nof arbitrage in the model and the existence of the hedging portfolio, and hence to obtain\nclosed pricing formulas allowing the model calibration to the market. Undoubtedly the\nMG with real market history must be investigated and the approach used in [9] must be\nkept in mind for further development.\nThe work is organized as follow. In Section 2 the classical MG and GCMG models and\ntheir continuum time limit versions are described. In Section 3 we derive the stock price\ndynamics and in Section 4, by using the classical risk-neutral pricing techniques (see [19]),\nwe apply the Black&Scholes formula to price European options.",
    "chunk_index": 1,
    "start_char": 2854,
    "end_char": 5883,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "investigated and the approach used in [9] must be\nkept in mind for further development.\nThe work is organized as follow. In Section 2 the classical MG and GCMG models and\ntheir continuum time limit versions are described. In Section 3 we derive the stock price\ndynamics and in Section 4, by using the classical risk-neutral pricing techniques (see [19]),\nwe apply the Black&Scholes formula to price European options. Section 5 contains the\nprocess to calibrate the MG and the GCMG on the European options market for several\nsets of game and option parameters.\n2\nThe Models\nIn this section the basic concept and the main results of MG and GCMG useful for our\npurposes will be exposed (for a comprehensive introduction to MG and GCMG refer to\n[7, 10]).\n2.1\nMinority Game\nHere we are considering a slightly modi\ufb01ed version of the MG, where there are N agents\nand its dynamics is de\ufb01ned in terms of dynamical variables Us,i(t), scores corresponding\nto each of the possible agents strategy choices s = +w, \u2212w. This version of the game with\nnon-unitary weights has already been studied in [3].\n2\n\nEach agent takes a decision (strategy choice) si(t) with\nProb{si(t) = s} =\ne\u0393iUs,i(t)\nP\ns\u2032 e\u0393iUs\u2032,i(t)\nwhere \u0393i > 0, and s\u2032 \u2208{\u2212w, +w}. The original MG corresponds to \u0393i = \u221eand was\ngeneralized to \u0393i = \u0393 < \u221e[2]; here we consider the latter case.\nThe public information variable \u00b5(t), that represent the common knowledge of the past\nrecord, is given to all agents; it belongs to the set of integers {1, . . . , P} and can either\nbe the binary encoding of last M winning choices or drawn at random from a uniform\ndistribution:\nProb{\u00b5(t) = \u00b5} = 1\nP ,\n\u00b5 = 1, . . . , P.\nHere we consider the latter case (see [2]).\nThe strategies a\u00b5\ns,i are uniform random variables taking values \u00b1w (Prob{a\u00b5\ns,i = \u00b1w} =\n1/2) independent on i, s and \u00b5. Here we consider S = 2, i.e. 2 strategy for each agent\nthat are randomly drawn at the beginning of the game and kept \ufb01xed.\nOn the basis of the outcome B(t) = PN\ni=1 a\u00b5(t)\nsi(t),i each agent update his scores according\nto\nUs,i(t + \u03b4t) = Us,i(t) \u2212a\u00b5(t)\ns,i\nB(t)\nP\n,\n(1)\nwhere \u03b4t \u21920 is the time increment.\nLet us introduce the following random variables (to ease the notation the choices +w\nand \u2212w are shorted with + and \u2212)\n\u03be\u00b5\ni =\na\u00b5\n+,i \u2212a\u00b5\n\u2212,i\n2\n,\n\u0398\u00b5 =\nN\nX\ni=1\na\u00b5\n+,i + a\u00b5\n\u2212,i\n2\nand their averages\n\u03bei\u0398 = 1\nP\nP\nX\n\u00b5=1\n\u03be\u00b5\ni \u0398\u00b5,\n\u03bei\u03bej = 1\nP\nP\nX\n\u00b5=1\n\u03be\u00b5\ni \u03be\u00b5\nj .\nThe only relevant quantity in the dynamics is the di\ufb00erence between the scores of the two\nstrategies:\nyi(t) = \u0393U+,i(\u03c4) \u2212U\u2212,i(\u03c4)\n2\n,\n(2)\nwhere \u03c4 = t\n\u0393.",
    "chunk_index": 2,
    "start_char": 5467,
    "end_char": 7992,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "=\na\u00b5\n+,i \u2212a\u00b5\n\u2212,i\n2\n,\n\u0398\u00b5 =\nN\nX\ni=1\na\u00b5\n+,i + a\u00b5\n\u2212,i\n2\nand their averages\n\u03bei\u0398 = 1\nP\nP\nX\n\u00b5=1\n\u03be\u00b5\ni \u0398\u00b5,\n\u03bei\u03bej = 1\nP\nP\nX\n\u00b5=1\n\u03be\u00b5\ni \u03be\u00b5\nj .\nThe only relevant quantity in the dynamics is the di\ufb00erence between the scores of the two\nstrategies:\nyi(t) = \u0393U+,i(\u03c4) \u2212U\u2212,i(\u03c4)\n2\n,\n(2)\nwhere \u03c4 = t\n\u0393.\nLet (\u2126, F, P) be the probability space respect to which all our random variables are\nde\ufb01ned, y = (yi)1\u2264i\u2264N, \u0398 = (\u0398\u00b5)1\u2264\u00b5\u2264P and \u03be = (\u03bei)1\u2264i\u2264N.\nAs shown in [16], if P/N = \u03b1 \u2208R+, S = 2 and \u0393i = \u0393 > 0 for all i, the dynamics of\nthe continuum time limit of the MG is given by the following N-dimensional stochastic\ndi\ufb00erential equation\ndyi(t) =\n\uf8eb\n\uf8ed\u2212\u03bei\u0398 \u2212\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(t))\n\uf8f6\n\uf8f8dt + Ai(\u03b1, N, \u0393, \u03be)dW(t),\ni = 1, . . . , N\n(3)\nwhere\nW(t) is an N-dimensional Wiener process,\n3\n\nAi is the i-th row of the N \u00d7 N matrix A = (Aij) such that\n(AA\u2032)ij(\u03b1, N, \u0393, \u03be) = \u0393\u03c32\nNw2\n\u03b1N\n\u03bei\u03bej,\n\u03c32\nN =\n1\nw2\u27e8B2\u27e9=\n1\nP w2\nPP\n\u00b5=1\n\nB2|\u00b5\n\u000b\n.\nThe relevant feature for our purposes is its limit behavior, as N grows to in\ufb01nity, with\nrespect to \u03b1: for \u03b1 \u2265\u03b1c, limN\u2192\u221e\n\u03c32\nN(y)\nN\n\u22641, \u2200y, while for \u03b1 < \u03b1c, limN\u2192\u221e\n\u03c32\nN\nN2 \u2264k, with\nk constant (see \ufb01gure 1, left). For more details see [10].\n2.2\nGrand Canonical Minority Game\nThe second market model we are going to analyze in this work is the GCMG (for more\ndetails see [5]). In this game each agent has only one trading strategy a\u00b5(t)\ni\n= \u00b1w randomly\nchosen from the set of 2P possible strategies.\nEach agent may decide not to play if\nthe strategy is not good enough. More precisely, following the same formalism used to\nintroduce the MG, each agent takes a bid bi(t) = \u03c6i(t)a\u00b5(t)\ni\nwhere \u03c6i(t) = 1 or 0 according\nto whether agent i trades or not.\nOn the basis of the outcome B(t) = PN\ni=1 bi(t) = PN\ni=1 \u03c6i(t)a\u00b5(t)\ni\neach agent update\nhis scores according to\nUi(t + \u03b4t) = Ui(t) \u2212a\u00b5(t)\ni\nB(t)\nP\n\u2212\u01ebi\nP ,\n(4)\nwhere \u03b4t \u21920 is the time increment.\nIf \u2212a\u00b5(t)\ni\nB(t) is larger than \u01ebi, the score Ui increases. The larger Ui the more likely it\nis that the agent trades (\u03c6i(t) = 1). Here we assume\nProb{\u03c6i(t) = 1} =\n1\n1 + e\u0393Ui(t) ,\nwith \u0393 > 0.\nThe threshold \u01ebi models the incentives of agents for trading in the market; investors\ninterested in trading because they need the market for exchanging assets have \u01ebi < 0\nwhilst speculators trading for pro\ufb01ting of price \ufb02uctuations have \u01ebi > 0. We will focus,\nfor simplicity, on the case\n\u01ebi\n=\n\u01eb > 0\nfor\ni \u2264Ns\n\u01ebi\n=\n\u2212\u221e\nfor\nNs < i \u2264N.",
    "chunk_index": 3,
    "start_char": 7713,
    "end_char": 10071,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "the agent trades (\u03c6i(t) = 1). Here we assume\nProb{\u03c6i(t) = 1} =\n1\n1 + e\u0393Ui(t) ,\nwith \u0393 > 0.\nThe threshold \u01ebi models the incentives of agents for trading in the market; investors\ninterested in trading because they need the market for exchanging assets have \u01ebi < 0\nwhilst speculators trading for pro\ufb01ting of price \ufb02uctuations have \u01ebi > 0. We will focus,\nfor simplicity, on the case\n\u01ebi\n=\n\u01eb > 0\nfor\ni \u2264Ns\n\u01ebi\n=\n\u2212\u221e\nfor\nNs < i \u2264N.\nThe Np = N \u2212Ns agents with \u01ebi = \u2212\u221eare the producers trading no matter what, whereas\nNs are the speculators trades only if the cumulated performance of their active strategies\nincreases more rapidly than \u01ebt,\nFrom equation (4) and (5) in [5] the dynamics of the continuum time limit of the\nGCMG is given by the following N-dimensional stochastic di\ufb00erential equation\ndyi(t) =\n\uf8eb\n\uf8ed\u2212\nN\nX\nj=1\naibjH(yj(t)) \u2212\u01eb\n\uf8f6\n\uf8f8dt + Ai(\u03b1, N, \u0393, a, b)dW(t),\ni = 1, . . . , N\n(5)\nwhere\n4\n\nyi(t) = \u0393Ui(\u03c4), \u03c4 = t\n\u0393,\nW(t) is an N-dimensional Wiener process,\nAi is the i-th row of the N \u00d7 N matrix A = (Aij) such that\n(AA\u2032)ij(\u03b1, N, \u0393, a, b) = \u0393\u03c32\nN\nP\naiaj = \u0393\u03c32\nNw2(ns + np)\nN\naiaj,\n\u03c32\nN =\n1\nw2\u27e8B2\u27e9=\n1\nP w2\nPP\n\u00b5=1\n\nB2|\u00b5\n\u000b\n,\nH is the generalized Heaviside function (Prob{\u03c6i(t) = 0, 1} = H(yi(t))),\n(. . .) stands for average over \u00b5(t).\nThe relevant features of \u03c32\nN for our purposes are in the thermodynamic limit, which\nis de\ufb01ned as the limit Ns, Np, P \u2192\u221e, keeping constant the fraction of speculators and\nproducers ns = Ns\nP and np = Np\nP . In this case the rule of key parameter \u03b1 in the MG is\nplayed by \u03b1ns :=\n1\nns+np. The transition phase is marked by a critical value of speculators\n\u03b1c\nns(P): for \u03b1ns \u2265\u03b1c\nns(P), limN\u2192\u221e\n\u03c32\nn\nN (y) \u22641 \u2200y, for \u03b1ns < \u03b1c\nns(P), limN\u2192\u221e\n\u03c32\nn\nN2 (y) \u2264\nk \u2200y (see \ufb01gure 1, right).\nA key aspect that characterize the GCMG is that as soon as \u01eb > 0 it is no more possible\nto \ufb01nd a solution for H\u01eb = 0, where H\u01eb is the predictability function. Like it happens\nin the \ufb01nancial markets where \u01eb, sum of interest rate and transaction costs, represents\nincompressible costs, in the GCMG the onset of unpredictability corresponds to minima\nof H\u01eb: as the fraction of speculators ns increases (and hence the system size increases) the\nminimum of volatility \u03c32\nN decreases and the market becomes more and more unpredictable\n(see [5]).\n3\nStock price dynamics\nIn this section, using the scores di\ufb00erence stochastic di\ufb00erential equations, the overall\nmarket bid and hence the stock price dynamics is derived.\nLet us consider a single stock in a market modeled by the continuum time MG or\nthe continuum time GCMG;",
    "chunk_index": 4,
    "start_char": 9649,
    "end_char": 12169,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "of H\u01eb: as the fraction of speculators ns increases (and hence the system size increases) the\nminimum of volatility \u03c32\nN decreases and the market becomes more and more unpredictable\n(see [5]).\n3\nStock price dynamics\nIn this section, using the scores di\ufb00erence stochastic di\ufb00erential equations, the overall\nmarket bid and hence the stock price dynamics is derived.\nLet us consider a single stock in a market modeled by the continuum time MG or\nthe continuum time GCMG; following [6], we de\ufb01ne the stock price dynamics in terms of\nexcess demand, as\nlog p(t + \u03b4t) = log p(t) + B(t)\nNw\n(6)\n(Nw is the volume of trades and w can be seen as the inverse liquidity parameter).\nIn the GCMG we are allowed to divide B(t) not by the number of active traders Nact,\nbut by the total number of agents N because we are interested in results as N \u2192\u221ein\nthe thermodynamic limit, where Np \u2192\u221eand Ns \u2192\u221e; hence as N grows to in\ufb01nity also\nthe number of active traders grows to in\ufb01nity of the same degree.\nWe address the analysis of the stock price dynamics for the MG and the GCMG\nseparately.\n5\n\n3.1\nMG case\nOur aim is to obtain the continuum time dynamics of the stock price p(t) from the con-\ntinuum time dynamics of y(t) de\ufb01ned by (2). The key point is the following identity, that\ncan be obtained directly from (1)\nU+,i(t + \u03b4t) \u2212U\u2212,i(t + \u03b4t)\n2\n= U+,i(t) \u2212U\u2212,i(t)\n2\n\u2212\na\u00b5(t)\n+,i \u2212a\u00b5(t)\n\u2212,i\n2\nB(t)\nP\n.\nTaking into account relation (2) and that \u0393 is \ufb01nite, we obtain\nyi(\u0393t)\n\u0393\n\u2212yi(\u0393(t + \u03b4t))\n\u0393\n= \u2212\u03be\u00b5(t)\ni\nB(t)\n\u03b1N .\nIn the continuum time limit \u03b4t \u21920,\ndyi(\u0393t) = \u2212\u0393\u03be\u00b5(t)\ni\nB(t)\n\u03b1N ,\n\u2200i = 1, 2, . . . , N.\n(7)\nDynamics (7) re\ufb02ects the stock price dynamics (6), where B(t)\nN\nis the generator of log p(t);\nindeed, like the right hand side of (6), also the right hand side of (7) does not contain a\ndt term and \u2212\u0393\u03be\u00b5(t)\ni\nB(t)\n\u03b1N is the generator of yi(\u0393t).\nMultiplying both sides of (7) by \u03be\u00b5(t)\ni\n, averaging over all the agents i, and taking the\nlimit as N goes to in\ufb01nity we have\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u03be\u00b5(t)\ni\ndyi(\u0393t) = \u2212\u0393\n\u03b1 lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\n\u00112 B(t)\nN .\nSince the \u03be\u00b5(t)\ni\nare independent from B(t) (the di\ufb00erence between two actions given the\npast history \u00b5(t) is \ufb01xed at the beginning of the game and does not depend on the action\nchosen by the agent on the basis of the score function Us,i(t)),\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\n\u00112 B(t)\nN\n= E\n\u0014\u0010\n\u03be\u00b5(t)\ni\n\u00112\u0015\nlim\nN\u2192\u221e\nB(t)\nN\n= w2\n2 wd log p(t);\nit follows that\nd log p(t) = \u22122\u03b1\nw3\u0393 lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u03be\u00b5(t)\ni\ndyi(\u0393t).",
    "chunk_index": 5,
    "start_char": 11703,
    "end_char": 14149,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "actions given the\npast history \u00b5(t) is \ufb01xed at the beginning of the game and does not depend on the action\nchosen by the agent on the basis of the score function Us,i(t)),\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\n\u00112 B(t)\nN\n= E\n\u0014\u0010\n\u03be\u00b5(t)\ni\n\u00112\u0015\nlim\nN\u2192\u221e\nB(t)\nN\n= w2\n2 wd log p(t);\nit follows that\nd log p(t) = \u22122\u03b1\nw3\u0393 lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u03be\u00b5(t)\ni\ndyi(\u0393t).\n(8)\nIntegrating over [0, t] both sides of (8) and taking into account equation (3), it follows that\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3lim\nN\u2192\u221e\nZ t\n0\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbds\n\u2212\nlim\nN\u2192\u221e\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n.\n(9)\nThe following proposition on the drift and di\ufb00usion term of equation (9) holds:\nProposition 3.1 Let t = O(N), that is limN\u2192+\u221et\nN = k. The drift term of (9) is at\nmost O(N); the di\ufb00usion term is di\ufb00erent from zero \u2200\u03b1, \ufb01nite for \u03b1 \u2265\u03b1c and at most\nO(N) for \u03b1 < \u03b1c.\n6\n\nProof\nIt is easy to see that, by applying the Law of Large Numbers, \u2200i limN\u2192\u221e\u03be2\ni = w2\n2 ,\nlimN\u2192\u221e\u03bei\u03bej = 0 and limN\u2192\u221e\u03bei\u0398 = 0 (for detailed computations see [18]); it follows that\nlim\nN\u2192\u221e\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb= \u03b1\nw3 lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u03be\u00b5(s)\ni\ntanh(yi(\u0393s)) < k\u03b1,\nwith k \ufb01nite constant. Hence the drift term is at most k\u03b1N and, due to di\ufb00erent initial\nconditions and the stochastic evolution of yi, in general di\ufb00erent from zero.\nConsider the zero mean random variable\nY =\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s);\nsince\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s) =\nN\nX\nj=1\n \n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u03be\u00b5(s)\ni\nAij\n!\ndWj(\u0393s),\nY has variance\n\u03bd\n=\nZ t\n0\nN\nX\nj=1\n \n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u03be\u00b5(s)\ni\nAij\n!2\nd(\u0393s)\n=\nZ t\n0\n4\u03b12\nw6N 2\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\n\u00112\n(AA\u2032)iids +\nZ t\n0\n4\u03b12\nw6N 2\u0393\nN\nX\ni,j=1,i\u0338=j\n\u03be\u00b5(s)\ni\n\u03be\u00b5(s)\nj\n(AA\u2032)ijds.(10)\nAs N grows to in\ufb01nity, by applying the Law of Large Numbers, limN\u2192\u221eAA\u2032 = limN\u2192\u221e\n\u0393\u03c32\nNw2\n2\u03b1N I,\nwhere I is the identity N \u00d7 N matrix (see [18] for details), and the \ufb01rst term of the right\nhand side of (10) is equal to limN\u2192\u221e\n\u03b1\u03c32\nNt\nw2N2 , while the second one is equal to 0. Since for\n\u03b1 \u2265\u03b1c, 0 < \u03c32\nN\nN < 1 and for \u03b1 < \u03b1c, 0 < \u03c32\nN\nN < kN, the thesis follows.",
    "chunk_index": 6,
    "start_char": 13810,
    "end_char": 15885,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "\u0393\u03c32\nNw2\n2\u03b1N I,\nwhere I is the identity N \u00d7 N matrix (see [18] for details), and the \ufb01rst term of the right\nhand side of (10) is equal to limN\u2192\u221e\n\u03b1\u03c32\nNt\nw2N2 , while the second one is equal to 0. Since for\n\u03b1 \u2265\u03b1c, 0 < \u03c32\nN\nN < 1 and for \u03b1 < \u03b1c, 0 < \u03c32\nN\nN < kN, the thesis follows.\n\u25a1\nIt is of worth to note that the hypothesis t = O(N) is necessary to have the di\ufb00usion\nterm di\ufb00erent from zero and that it is the proportionality factor of the time needed to\nreach the stationary state of the MG. In numerical terms it means that, given a maturity\nt, a higher number of player needs a higher number of time steps, i.e. the player must\ninteract more times to reach an equilibrium.\nConsider the dynamics\nq(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbds\n\u2212\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n;\nby de\ufb01nition q(t) converges pointwise towards p(t) (limN\u2192\u221eq(t) = p(t), \u2200\u03c9 \u2208\u2126) and\nhence almost surely; it follows that a.s. \u2200\u01eb0 > 0 there exists N > 0 such that, \u2200N > N,\n7\n\n|p(t) \u2212q(t)| < \u01eb0. By assuming t = kN and N \ufb01nite but su\ufb03ciently large, we have that\na.s. q(t) \u223cp(t) and proposition (3.1) holds for q(t).\nFrom now on we assume t = kN and N \ufb01nite but su\ufb03ciently large to have\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbds\n\u2212\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n.\n(11)\nLet us de\ufb01ne Gt = \u03c3(W(\u0393s), \u03be\u00b5(s); s \u2264t) the \ufb01ltration generated by both the processes\nW and \u03be. Since the drift and the di\ufb00usion terms are just the exponential of the drift and\nthe di\ufb00usion of equation (3) multiplied by \u03be\u00b5(s)\ni\n, they are adapted to the \ufb01ltration Gt.\nThe di\ufb00erential of the stock price process is therefore, by Ito\u2019s formula,\ndp(t)\np(t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\u03be\u00b5(t)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393t))\n\uf8f6\n\uf8f8\n+\n2\u03b12\n\u0393w6N 2\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAi\n! N\nX\ni=1\n\u03be\u00b5(t)\ni\nAi\n!\u2032\uf8f9\n\uf8fbdt \u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\u03be\u00b5(t)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393t))\n\uf8f6\n\uf8f8+\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\uf8f9\n\uf8fbdt\n\u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).",
    "chunk_index": 7,
    "start_char": 15607,
    "end_char": 17668,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "Ai\n!\u2032\uf8f9\n\uf8fbdt \u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\u03be\u00b5(t)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393t))\n\uf8f6\n\uf8f8+\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\uf8f9\n\uf8fbdt\n\u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).\n3.2\nGCMG case\nFor the case on the GCMG, since yi(t) = \u0393Ui(\u03c4), it is immediate to see that for \u03b4t \u21920\ndyi(\u0393t) = \u2212\u0393\nP\n\u0010\na\u00b5(t)\ni\nB(t) \u2212\u01ebi\n\u0011\n= \u2212\u0393(ns + np)\nN\n\u0010\na\u00b5(t)\ni\nB(t) \u2212\u01ebi\n\u0011\n,\n\u2200i = 1, 2, . . . , N. (12)\nMultiplying both sides of (12) by a\u00b5(t)\ni\n, averaging over all the agents i, and taking the\nlimit as N goes to in\ufb01nity we have\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\na\u00b5(t)\ni\ndyi(\u0393t) = \u2212\u0393(ns+np) lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u0010\na\u00b5(t)\ni\n\u00112 B(t)\nN +\u0393(ns+np) lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\nai(t) \u01ebi\nN .\nSince a\u00b5(t)\ni\nhave zero mean\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\nai(t) \u01ebi\nN =\nlim\nNs\u2192\u221e\n1\nNs\nNs\nX\ni=1\nai(t) \u01eb\nN +\nlim\nNp\u2192\u221e\n1\nNp\nN\nX\ni=Ns+1\nai(t)(\u2212\u221e)\nN\n= 0\nand are independent from B(t),\nlim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\n\u0010\na\u00b5(t)\ni\n\u00112 B(t)\nN\n= E\n\u0014\u0010\na\u00b5(t)\ni\n\u00112\u0015\nlim\nN\u2192\u221e\nB(t)\nN\n= w3d log p(t);\n8\n\nit follows that\nd log p(t) = \u2212\n1\nw3\u0393(ns + np) lim\nN\u2192\u221e\n1\nN\nN\nX\ni=1\na\u00b5(t)\ni\ndyi(\u0393t).\n(13)\nSince ns + np = N\nP = 1\n\u03b1, we can put\n1\nns+np = \u03b1ns (np is itself a function of ns). \u03b1ns as\nthe same role of \u03b1 in the MG, but whilst in the MG np = 0 and hence \u03b1 depends only on\nspeculators, in the GCMG ns is the fundamental quantity driving the behavior of \u03c32\nN.\nIntegrating over [0, t] both sides of (13) and taking into account equation (5), it follows\nthat\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3lim\nN\u2192\u221e\nZ t\n0\n\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(s)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393s))\n\uf8f6\n\uf8f8ds\n\u2212\nlim\nN\u2192\u221e\nZ t\n0\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n.\n(14)\nProposition 3.1 holds also for (14); the drift term is essentially the same of (9) and since\nin the GCMG limN\u2192\u221eAA\u2032 = limN\u2192\u221e\n\u0393\u03c32\nNw2\n\u03b1nsN I proceeding in the same way we did for\nproving proposition 3.1 we conclude that the di\ufb00usion term is di\ufb00erent from 0, \ufb01nite for\nns \u2264nc\ns(P) and at most O(N) for ns > nc\ns(P).\nAlso in this case we have that assuming t = kN and N \ufb01nite but su\ufb03ciently large\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(s)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393s))\n\uf8f6\n\uf8f8ds\n\u2212\nZ t\n0\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n.",
    "chunk_index": 8,
    "start_char": 17461,
    "end_char": 19522,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "limN\u2192\u221e\n\u0393\u03c32\nNw2\n\u03b1nsN I proceeding in the same way we did for\nproving proposition 3.1 we conclude that the di\ufb00usion term is di\ufb00erent from 0, \ufb01nite for\nns \u2264nc\ns(P) and at most O(N) for ns > nc\ns(P).\nAlso in this case we have that assuming t = kN and N \ufb01nite but su\ufb03ciently large\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(s)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393s))\n\uf8f6\n\uf8f8ds\n\u2212\nZ t\n0\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n.\n(15)\nLet us de\ufb01ne Gt = \u03c3(W(\u0393s), a\u00b5(s); s \u2264t) the \ufb01ltration generated by both the processes\nW and a. Since the drift and the di\ufb00usion terms are just the exponential of the drift and\nthe di\ufb00usion of equation (5) multiplied by a\u00b5(s)\ni\n, they are adapted to the \ufb01ltration Gt.\nThe di\ufb00erential of the stock price process is therefore, by Ito\u2019s formula,\ndp(t)\np(t)\n=\n\uf8ee\n\uf8f0\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(t)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393t))\n\uf8f6\n\uf8f8\n+\n\u03b12\nns\n\u0393w6N 2\n N\nX\ni=1\na\u00b5(t)\ni\nAi\n! N\nX\ni=1\na\u00b5(t)\ni\nAi\n!\u2032\uf8f9\n\uf8fbdt \u2212\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t)\n=\n\uf8ee\n\uf8f0\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(t)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393t))\n\uf8f6\n\uf8f8+\n\u03b12\nns\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\na\u00b5(t)\ni\nAij\n!2\uf8f9\n\uf8fbdt\n\u2212\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).\n9\n\n4\nDerivative security pricing\nIn this section we develop the risk-neutral pricing of a derivative security on a single stock\nwhose price dynamics is given by (11) and (15). We follow the usual scheme: construction\nof the risk-neutral measure, relying on Girsanov Theorem, for the discounted process and,\nby using the Martingale Representation Theorem, of the replication portfolio that allows\nto hedge a short position in the derivative security (see [19], p. 209-220). At the end of\nthe section we apply Black&Scholes formula to price a European call option.\n4.1\nDiscounted stock dynamics under risk-neutral: MG case\nConsider the interest rate process R(s) adapted to the \ufb01ltration Gt, 0 \u2264t \u2264T. The dis-\ncount process D(t) = e\u2212R t\n0 R(s)ds has, by Ito\u2019s formula, di\ufb00erential dD(t) = \u2212R(t)D(t)dt.\nThe discounted stock process is\nD(t)p(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n\u2212R(s) +\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbds\n\u2212\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n,\nand its di\ufb00erential\nd(D(t)p(t))\nD(t)p(t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n+\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\n\u2212R(t)\n\uf8f9\n\uf8fbdt \u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).",
    "chunk_index": 9,
    "start_char": 19112,
    "end_char": 21401,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "\uf8f6\n\uf8f8\n\uf8f9\n\uf8fbds\n\u2212\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n,\nand its di\ufb00erential\nd(D(t)p(t))\nD(t)p(t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n+\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\n\u2212R(t)\n\uf8f9\n\uf8fbdt \u2212\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).\nProposition 4.1 If T = O(N) there exists a probability measure eP (risk neutral measure)\nunder which the discounted stock price D(t)p(t) is a eP-martingale.\nProof\nLet us de\ufb01ne the market price of risk equation to be\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\n\u03b3(\u0393t)\n=\n\uf8ee\n\uf8f02\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb+\n+\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\n\u2212R(t)\n\uf8f9\n\uf8fb,\nwhere \u03b3(\u0393t) = (\u03b31(\u0393t), . . . , \u03b3N(\u0393t)) is an unknown process; the di\ufb00erential of the dis-\ncounted process becomes\nd(D(t)p(t)) = D(t)p(t)\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\n[\u03b3(\u0393t)dt \u2212dW(\u0393t)].\n10\n\nLet ||\u00b7|| denote the usual Euclidean norm; we show that for T = O(N),\nR T\n0 ||\u03b3(\u0393u)||2d(\u0393u) <\n\u221e. Consider N \ufb01nite and su\ufb03ciently large (we remind that proposition (3.1) for N \ufb01nite\nand su\ufb03ciently large); since, by proposition (3.1)\nZ T\n0\n\f\f\f\f\f\n\f\f\f\f\f\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(u)\ni\nAi\n\u0011\f\f\f\f\f\n\f\f\f\f\f\n2\nd(\u0393u) =\nZ T\n0\nN\nX\nj=1\n \n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u03be\u00b5(u)\ni\nAij\n!2\nd(\u0393u) > 0 and \ufb01nite,\nZ T\n0\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n2\u03b1\nw3N\nN\nX\ni=1\n\uf8ee\n\uf8f0\u03be\u00b5(s)\ni\n\uf8eb\n\uf8ed\u03bei\u0398 +\nN\nX\nj=1\n\u03bei\u03bej tanh(yj(\u0393s))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n2\nd(\u0393u) < \u221e,\nand\nZ T\n0\n\f\f\f\f\f\f\n\f\f\f\f\f\f\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\f\f\f\f\f\f\n\f\f\f\f\f\f\n2\nd(\u0393u) \u223c0,\nwe have\n0 <\nZ T\n0\n||\u03b3(\u0393u)||2 d(\u0393u) < \u221e.\nIt follows that \u03b3(\u0393t) satis\ufb01es the Novikov condition (see [17])\nE\n\u00141\n2\nZ T\n0\n||\u03b3(\u0393u)||2du\n\u0015\n< \u221e.\nAs a consequence we can apply Girsanov\u2019s Theorem (see [14], p.198); there exists a prob-\nability measure eP (risk neutral measure) under which f\nW(\u0393t) =\nR t\n0 \u03b3(\u0393u)du \u2212W(\u0393t) is an\nN-dimensional Brownian motion. It follows that\nd(D(t)p(t)) = D(t)p(t)\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(t)\ni\nAi\n\u0011\ndf\nW(\u0393t),\nand hence the discounted stock price is a eP-martingale.\n\u25a1\nBy making the replacement dW(\u0393t) = \u03b3(\u0393t)dt \u2212df\nW(\u0393t), dynamics (11) of the undis-\ncounted stock price p(t) becomes\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\nR(s) \u2212\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\nds+\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndf\nW(\u0393s)\n)\n.\n(16)\n4.2\nDiscounted stock dynamics under risk-neutral: GCMG case\nConsider the interest rate process R(s) adapted to the \ufb01ltration Gt, 0 \u2264t \u2264T.",
    "chunk_index": 10,
    "start_char": 21129,
    "end_char": 23431,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "By making the replacement dW(\u0393t) = \u03b3(\u0393t)dt \u2212df\nW(\u0393t), dynamics (11) of the undis-\ncounted stock price p(t) becomes\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\nR(s) \u2212\n2\u03b12\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\n\u03be\u00b5(t)\ni\nAij\n!2\nds+\nZ t\n0\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndf\nW(\u0393s)\n)\n.\n(16)\n4.2\nDiscounted stock dynamics under risk-neutral: GCMG case\nConsider the interest rate process R(s) adapted to the \ufb01ltration Gt, 0 \u2264t \u2264T. The dis-\ncount process D(t) = e\u2212\nR t\n0 R(s)ds has, by Ito\u2019s formula, di\ufb00erential dD(t) = \u2212R(t)D(t)dt.\n11\n\nThe discounted stock process is\nD(t)p(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\n\u2212R(s) + \u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(s)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393s))\n\uf8f6\n\uf8f8ds\n\u2212\nZ t\n0\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndW(\u0393s)\n)\n,\nand its di\ufb00erential\nd(D(t)p(t))\nD(t)p(t)\n=\n\uf8ee\n\uf8f0\u03b1ns\nw3N\nN\nX\ni=1\na\u00b5(s)\ni\n\uf8eb\n\uf8ed\nN\nX\nj=1\naibjH(yj(\u0393s))\n\uf8f6\n\uf8f8\n+\n\u03b12\nns\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\na\u00b5(t)\ni\nAij\n!2\n\u2212R(t)\n\uf8f9\n\uf8fbdt \u2212\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(t)\ni\nAi\n\u0011\ndW(\u0393t).\nWith the natural modi\ufb01cations to the market price of risk equation in proposition 4.1,\nthe same proposition holds also for the GCMG and dynamics (15) of the undiscounted\nstock price p(t) becomes\np(t)\n=\np(0) exp\n\uf8f1\n\uf8f2\n\uf8f3\nZ t\n0\nR(s) \u2212\n\u03b12\nns\nw6\u0393N 2\nN\nX\nj=1\n N\nX\ni=1\na\u00b5(s)\ni\nAij\n!2\nds+\nZ t\n0\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndf\nW(\u0393s)\n)\n.\n(17)\n4.3\nEuropean call option pricing\nSince, for T \u2212t = O(N), there exists a risk neutral measure eP for the discounted process\nD(t)p(t), we can apply the usual scheme, relying on the martingale representation theorem,\nto show the existence of a replication portfolio for a derivative security pricing.\nConsider T \u2212t = O(N); let V (T) be an GT -measurable random variable representing\nthe payo\ufb00at time T of a derivative security. The process E(t) = EeP[D(T)V (T)|Gt] is\na eP-martingale (it follows from iterated conditioning); by the Martingale Representation\nTheorem there exists an initial capital X(0) and a portfolio strategy \u2206(t) such that X(T) =\nV (T) almost surely and an adapted process \u03c6(t) which constructs E(t) out of D(t)p(t). It\nfollows that D(t)X(t) is a eP-martingale\nD(t)X(t) = EeP[D(T)X(T)|Gt] = EeP[D(T)V (T)|Gt].\n(18)\nThe value X(t) of the hedging portfolio in (18) is the capital needed at time t in order to\nsuccessfully hedge the position in the derivative security with payo\ufb00V (T). Hence V (t) is\nthe price of the derivative security at time t and we obtain the usual risk neutral pricing\nformula (see [19], p.218-222)\nD(t)V (t) = EeP[D(T)V (T)|Gt],\n0 \u2264t \u2264T;\nby recalling the de\ufb01nition of D(t),\nV (t) = EeP[e\u2212\nR T\nt R(s)dsV (T)|Gt],\n0 \u2264t \u2264T.\n12",
    "chunk_index": 11,
    "start_char": 23042,
    "end_char": 25515,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "of the hedging portfolio in (18) is the capital needed at time t in order to\nsuccessfully hedge the position in the derivative security with payo\ufb00V (T). Hence V (t) is\nthe price of the derivative security at time t and we obtain the usual risk neutral pricing\nformula (see [19], p.218-222)\nD(t)V (t) = EeP[D(T)V (T)|Gt],\n0 \u2264t \u2264T;\nby recalling the de\ufb01nition of D(t),\nV (t) = EeP[e\u2212\nR T\nt R(s)dsV (T)|Gt],\n0 \u2264t \u2264T.\n12\n\nProposition 4.2 Let us assume constant interest rate r; the price of a European call\noption with underlying p(t) and strike K is\nc(t, p(t); K, r, \u03bd) = p(t)\u03c8(d(\u03b8, p(t))) \u2212e\u2212r\u03b8K\u03c8\n\u0010\nd (\u03b8, p(t)) \u2212\u221a\u03bd\n\u221a\n\u03b8\n\u0011\n,\nwhere \u03b8 = T \u2212t, \u03bd = \u03bdMG =\n\u03b1\u03c32\nN\nw2N2 for the MG, \u03bd = \u03bdGCMG = \u03b1ns\u03c32\nN\nw2N2 for the GCMG,\nd(\u03b8, p(t)) =\nh\nlog p(t)\nK +(r+ 1\n2\u03bd)\u03b8\ni\n\u221a\n\u03bd\u03b8\nand \u03c8 the erf function.\nProof\nConsider the random variable\nY =\nZ T\nt\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAi\n\u0011\ndf\nW(\u0393s) =\nN\nX\nj=1\nZ T\nt\n2\u03b1\nw3N\u0393\nN\nX\ni=1\n\u0010\n\u03be\u00b5(s)\ni\nAij\n\u0011\ndWj(\u0393s)\nfor the MG case and\nY =\nZ T\nt\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAi\n\u0011\ndf\nW(\u0393s) =\nN\nX\nj=1\nZ T\nt\n\u03b1ns\nw3N\u0393\nN\nX\ni=1\n\u0010\na\u00b5(s)\ni\nAij\n\u0011\ndWj(\u0393s)\nfor the GCMG case.\nY has zero mean and, in the limit as N grows to in\ufb01nity, \ufb01nite variance \u03bd (see prop.\n3.1); since we consider N su\ufb03ciently large, we can assume Var[Y ] = \u03bd. For N su\ufb03ciently\nlarge we can apply the results obtained at the end of proof of proposition (3.1); it follows\nthat Y has a normal distribution with 0 mean and variance \u03bdMG = \u03b1\u03c32\nN\u03b8\nw2N2 , \u03bdGCMG = \u03b1ns\u03c32\nN\u03b8\nw2N2\nand equations (16) and (17) become\np(T) = p(t) exp\n\u001a\u0012\nr \u22121\n2\n\u03bd\n\u03b8\n\u0013\n\u03b8 \u2212\nr\u03bd\n\u03b8\n\u221a\n\u03b8Z\n\u001b\n,\nZ = \u2212Y\n\u221a\u03bd is a standard normal random variable.\nWe can apply Black&Scholes formula (see [19], pp.218-220) and obtain the price of a\ncall option with underlying p and strike K:\nc(t, p(t); K, r, \u03bd) = p(t)\u03c8(d(\u03b8, p(t))) \u2212e\u2212r\u03b8K\u03c8\n\u0000d (\u03b8, p(t)) \u2212\u221a\u03bd\n\u0001\n,\nwhere d(\u03b8, p(t)) =\nh\nlog p(t)\nK +(r+ 1\n2\n\u03bd\n\u03b8)\u03b8\ni\n\u221a\u03bd\nand \u03c8 the erf function.\n\u25a1\n5\nModel calibration\nCalibrating the model means to \ufb01nd values w, \u03b1,\u03b1ns, \u03c32\nN such that the option implied\nvolatility available on the market\n\u221a\n\u03bdimpl equals \u221a\u03bd, where \u221a\u03bd is the implied volatility\ncoming from the European call option pricing formula (19). Whilst parameters \u03b1, \u03b1ns\nand \u03c32\nN are characteristic of the model, parameter w is related to the market liquidity of\nthe stock. As a consequence it is meaningful to divide the calibration in two steps. The\n\ufb01rst step consists in \ufb01nding a value w to link the stock price return volatility with the\n13",
    "chunk_index": 12,
    "start_char": 25100,
    "end_char": 27491,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "means to \ufb01nd values w, \u03b1,\u03b1ns, \u03c32\nN such that the option implied\nvolatility available on the market\n\u221a\n\u03bdimpl equals \u221a\u03bd, where \u221a\u03bd is the implied volatility\ncoming from the European call option pricing formula (19). Whilst parameters \u03b1, \u03b1ns\nand \u03c32\nN are characteristic of the model, parameter w is related to the market liquidity of\nthe stock. As a consequence it is meaningful to divide the calibration in two steps. The\n\ufb01rst step consists in \ufb01nding a value w to link the stock price return volatility with the\n13\n\nmarket volatility\n\u221a\n\u03bdmkt; the second step consists in \ufb01nding, given the value of w already\ncalibrated, values \u03b1, \u03b1ns and \u03c32\nN such that \u03bdimpl = \u03bd.\nAt this point it worths to de\ufb01ne the market volatility\n\u221a\n\u03bdmkt. The market volatility\nis an index representing the volatility present in the market.\nIt could be the historic\nvolatility of the returns of an equity index, like the S&P500 or the DAX; in this case we\nshould choose the time window to compute the historic volatility (1 month, 1 year,...).\nThe other option is to choose an index like the VIX or the VDAX; these indices, widely\nused by \ufb01nance practitioners as a measure of the risk present in the market, are built\nstarting from listed options on S&P500 or DAX and have the advantage of being uniquely\nde\ufb01ned (no need to de\ufb01ne a time window), more responsive to change in market regime\nthan the historic volatility and of having almost the same behavior of the historic ones\n(maxima and minima of the short term historic volatility correspond with high degree to\nthe maxima and minima of the implied one). It follows that for our calibration purpose\nwe can use VDAX index as market volatility: what we need is just to \ufb01x values for (\u03b1, \u03c32\nN)\nand (\u03b1ns, \u03c32\nN) respectively and \ufb01nd a value for \u03c9 such that \u221a\u03bd =\n\u221a\n\u03bdmkt. Since the stock\nprice return variance is Var\nh\nlog p(t+\u03b4t)\np(t)\ni\n= \u03c32\nN\nN\n1\nw2N , it reaches the minimum value when\n\u03c32\nN = \u03c32\nc; it follows that also the minimum of the market volatility\n\u221a\n\u03bdmkt corresponds to\nthe critical value \u03c3c. This requires the market to be non-stationary. From the market point\nof view this assumption is natural: since stationarity corresponds to the transition point\nbetween e\ufb03ciency and ine\ufb03ciency it is fair to consider the market e\ufb03cient or ine\ufb03cient,\nbut not at the critical point; from the game point of view we have to require the game\nparameters not to be at the critical point \u03b1c.\nTaking into account the expression of the implied volatility \u03bd, w is such that\n\u221a\n\u00af\u03bdmkt = \u221a\u03b1c\n\u03c3c\n\u00afwN ,\n(19)\nwhere\n\u221a\n\u00af\u03bdmkt is the minimum market volatility.\nThe second step consists in calibrating the MG and the GCMG on the options, that\nis to \ufb01nd couples (\u03b1, \u03c32\nN) and (\u03b1ns, \u03c32\nN) respectively such that:\n\u221a",
    "chunk_index": 13,
    "start_char": 26981,
    "end_char": 29686,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "the game point of view we have to require the game\nparameters not to be at the critical point \u03b1c.\nTaking into account the expression of the implied volatility \u03bd, w is such that\n\u221a\n\u00af\u03bdmkt = \u221a\u03b1c\n\u03c3c\n\u00afwN ,\n(19)\nwhere\n\u221a\n\u00af\u03bdmkt is the minimum market volatility.\nThe second step consists in calibrating the MG and the GCMG on the options, that\nis to \ufb01nd couples (\u03b1, \u03c32\nN) and (\u03b1ns, \u03c32\nN) respectively such that:\n\u221a\n\u03bdimpl =\n\u221a\n\u03b1 \u03c3N\n\u00afwN\nfor the MG and\n\u221a\n\u03bdimpl =\np\n\u03b1ns\n\u03c3N\n\u00afwN\nfor the GCMG, where\n\u221a\n\u03bdimpl is the option implied volatility available on the market.\nIn order to perform the calibration on the real market we have \ufb01rst of all to select\na stock. We choose the DAX Index, a total return index reinvesting dividends and rep-\nresenting the most relevant stocks traded at the German stocks exchange, hence a very\nliquid instrument with a long time series. The calibration of \u00afw is performed over VDAX\nIndex, which is the index of implied volatility for DAX options. The minimum volatility,\ni.e. the minimum value of VDAX, since 03/01/2000 is reached the 11/02/2005 with a\nvalue of 10.98%:\n\u221a\n\u03bdmkt = 10.98%. For both the MG and the GCMG w can be obtained\nfrom the market volatility in correspondence with the minumun value \u03c3c reached by \u03c3N;\nof course the \u03c3c of the MG is di\ufb00erent from the one of the GCMG. The next calibration,\nperformed for the MG as M goes from 5 to 9 (it is not possible to calibrate for M too\nsmall), and for the GCMG for di\ufb00erent values of \u01eb and L := PNs, is ran for all options\nonce.\n14\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n0.5\n1\n1.5\n2\n2.5\n\u03b1\n\u03c32/N\n \n \nRandom history length 25\nRandom history length 26\nRandom history length 27\nRandom history length 28\nRandom history length 29\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n\u03b1\n\u03c32/N\n \n \n\u03b5 = 0.1\n\u03b5 = 0.01\n\u03b5 = \u22120.01\nFigure 1: Left:\n\u03c32\nN\nN\nas a function of \u03b1 for the MG obtained numerically for di\ufb00erent\nrandom histories. Right: \u03c32\nN\nN as a function of \u03b1ns for the GCMG obtained numerically for\nL := PNs = 8000, np = 1 and di\ufb00erent values of \u01eb.\n5\n6\n7\n8\n9\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n1000\n5000\n10000\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\nL\nFigure 2: Left: Comparison between average calibrated \u03b1 (continuos line) and average \u03b1c\n(dashed line) for di\ufb00erent values of M. Right: Comparison between average calibrated\n\u03b1ns (continuos line) and average \u03b1c\nns(dashed line) for di\ufb00erent values of L and \u01eb = 0.01\n(diamonds), \u01eb = \u22120.01 (squares), \u01eb = 0.1 (triangles).",
    "chunk_index": 14,
    "start_char": 29283,
    "end_char": 31734,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "0.6\n0.7\n0.8\n1000\n5000\n10000\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\nL\nFigure 2: Left: Comparison between average calibrated \u03b1 (continuos line) and average \u03b1c\n(dashed line) for di\ufb00erent values of M. Right: Comparison between average calibrated\n\u03b1ns (continuos line) and average \u03b1c\nns(dashed line) for di\ufb00erent values of L and \u01eb = 0.01\n(diamonds), \u01eb = \u22120.01 (squares), \u01eb = 0.1 (triangles).\nWe choose 18 call options, with di\ufb00erent maturities and di\ufb00erent moneyness, and\ncalibrate the games over these options minimizing the sum of squared di\ufb00erence of market\nimplied volatility and game implied volatility. Results of the calibration are displayed in\n\ufb01gures 3 - 8.\nFrom the analysis of the calibration results (see \ufb01gure 2) we can see that for both the\nMG and the GCMG, parameters \u03b1 and \u03b1ns are greater than the critical points and close\nto them for all the game parameters M, L and \u01eb used for the simulations: this means that\nthe option market operates in the asymmetric phase of both the MG and the GCMG in\nvicinity of the critical points.\nAbout the quality of the calibration it is also useful to look at the ability of the game\nto \ufb01t the sample of listed options: \ufb01gures 3 - 8 show a good \ufb01tting at di\ufb00erent game size\nand an improvement of the quality of the calibration as M and L increase. As one would\nexpect a game of bigger size is able to replicate more accurately the real option market\nthan a smaller size game.\nLast observation involves parameter w. A common feature of both games is that as\nM and L increase the inverse liquidity parameter \u00afw decreases: when the market e\ufb03ciency\nimproves, and this is what happens as M and L increase since in both games the minimum\n15\n\n0.2\n0.4\n0.6\n0.8\n1\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\nFigure 3: Comparison between market option implied volatility (continuous line) and MG\nimplied volatility (dashed line). Horizontal axis displays option time to maturity, vertical\naxis volatility level for options with moneyness equal to 1.06 (diamonds), 1 (squares), 0.94\n(triangles). Left: M = 6, right: M = 9.\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\nFigure 4: Comparison between market option implied volatility (continuous line) and MG\nimplied volatility (dashed line). Horizontal axis displays option time to maturity, vertical\naxis volatility level for options with moneyness equal to 1.1 (diamonds), 0.98 (squares),\n0.88 (triangles). Left: M = 6, right: M = 9.\nof \u03c32\nN decreases, each agent decreases the weight of its buy or sell decision: players acting\nin less e\ufb03cient markets characterized by scarce liquidity are required to increase the weight\nof the bets, like it happens in real market.",
    "chunk_index": 15,
    "start_char": 31344,
    "end_char": 34166,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "market option implied volatility (continuous line) and MG\nimplied volatility (dashed line). Horizontal axis displays option time to maturity, vertical\naxis volatility level for options with moneyness equal to 1.1 (diamonds), 0.98 (squares),\n0.88 (triangles). Left: M = 6, right: M = 9.\nof \u03c32\nN decreases, each agent decreases the weight of its buy or sell decision: players acting\nin less e\ufb03cient markets characterized by scarce liquidity are required to increase the weight\nof the bets, like it happens in real market. Whilst the behavior of w as inverse liquidity\nparameter is coherent with what happens in real markets, the very low values of the\ncalibrated w deserve further investigations. Looking at the expression of w coming from\nthe implied volatility formula (19) and keeping in mind that the di\ufb00usion term of equations\n(3) and (5) is linked to the characteristic time of the game dynamics (in our case the\nrescaled continuous time is \u03c4 = t\nP ), we can conclude that a di\ufb00erent time rescaling has the\ne\ufb00ect of increasing the level of w obtained from calibration. Using a rescaled time \u03c4 =\nt\nN ,\nw would increase of a factor\n1\n\u221a\u03b1c . A game without time rescaling (\u03c4 = t) would increase\nthe value of calibrated w of a factor\n\u221a\nN, obtaining values of w greater than 1 and more\nstable as M and L increase (see \ufb01gure 9).\n16\n\n0.1\n0.2\n0.4\n0.6\n0.8\n1\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\nFigure 5: Comparison between market option implied volatility (continuous line) and\nGCMG implied volatility (dashed line). Horizontal axis displays option time to matu-\nrity, vertical axis volatility level for options with moneyness equal to 1.06 (diamonds), 1\n(squares), 0.94 (triangles). Left: L = 1000, \u01eb = 0.01, right: L = 10000, \u01eb = 0.01.\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\nFigure 6: Comparison between market option implied volatility (continuous line) and\nGCMG implied volatility (dashed line). Horizontal axis displays option time to maturity,\nvertical axis volatility level for options with moneyness equal to 1.1 (diamonds), 0.98\n(squares), 0.88 (triangles). Left: L = 1000, \u01eb = 0.01, right: L = 10000, \u01eb = 0.01.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\nFigure 7: Comparison between market option implied volatility (continuous line) and\nGCMG implied volatility (dashed line). Horizontal axis displays option time to matu-\nrity, vertical axis volatility level for options with moneyness equal to 1.06 (diamonds), 1\n(squares), 0.94 (triangles). Left: L = 1000, \u01eb = \u22120.01, right: L = 10000, \u01eb = \u22120.01.\n17",
    "chunk_index": 16,
    "start_char": 33647,
    "end_char": 36415,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "0.19\n0.2\n0.21\n0.22\n0.23\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\nFigure 7: Comparison between market option implied volatility (continuous line) and\nGCMG implied volatility (dashed line). Horizontal axis displays option time to matu-\nrity, vertical axis volatility level for options with moneyness equal to 1.06 (diamonds), 1\n(squares), 0.94 (triangles). Left: L = 1000, \u01eb = \u22120.01, right: L = 10000, \u01eb = \u22120.01.\n17\n\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\nFigure 8: Comparison between market option implied volatility (continuous line) and\nGCMG implied volatility (dashed line). Horizontal axis displays option time to maturity,\nvertical axis volatility level for options with moneyness equal to 1.1 (diamonds), 0.98\n(squares), 0.88 (triangles). Left: L = 1000, \u01eb = \u22120.01, right: L = 10000, \u01eb = \u22120.01.\n4\n5\n6\n7\n8\n9\n10\n0\n0.2\n0.4\n4\n5\n6\n7\n8\n9\n10\n5.25\n5.3\n5.35\n1000\n5000\n10000\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nL\n1000\n5000\n10000\n3\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n4\n4.1\nFigure 9: Left: on left axis calibrated w for MG (continuous line), on the right axis\ncalibrated w without time rescaling for MG (dotted line). Right: on left axis calibrated\nw for GCMG (continuous line), on the right axis calibrated w without time rescaling for\nMG (dotted line); from top to bottom \u01eb = 0.01, \u01eb = \u22120.01, \u01eb = 0.1.\n18\n\nReferences\n[1] Bouchaud J.P. and Potters M. (2000) Theory of \ufb01nancial risks: from statistical physics\nto risk management. Cambridge University Press, Cambridge.\n[2] Cavagna A., Garrahan J P., Giardina I. and Sherrington D. (1999) A thermal model\nfor adaptive competition in a market. Phys. Rev. Lett., 83, 4429.\n[3] Challet D., Chessa A., Marsili M. and Zhang Y. C. (2001) From Minority Games to\nreal markets. Quantitative Finance, 1, 168\u2013176.\n[4] Challet D. and Zhang Y. C. (1997) Emergence of cooperation and organization in an\nevolutionary game. Physica A, 246, 407.\n[5] Challet D., Marsili M. (2003) Criticality and market e\ufb03ciency in a simple realistic\nmodel of the stock market. Phys. Rev. E, 68, 036132, 4 pages.\n[6] Challet D., Marsili M. and Zhang Y. C. (2001) Stylized facts of \ufb01nancial markets and\nmarket crashes in Minoriy Games Physica A, 294, 514\u2013524.\n[7] Coolen A.C.C. (2005) The Mathematical Theory of Minority Games - Statistical Me-\nchanics of Interacting Agents. Oxford University Press.\n[8] Coolen A.C.C. and J.A.F. Heimel (2001) Dynamical solution of the on-line minority\ngame. J. Phys. A, 34, 10783\u201310804.\n[9] Coolen A.C.C. (2005) Generating functional analysis of minority games with real\nmarket histories. J. Phys. A, 38, 2311\u20132347.\n[10] Challet D., Marsili M. and Zhang Y. C. (2004) Minority Games: Interacting agents\nin \ufb01nancial markets. Oxford Finance Series.\n[11] Giardina I., Bouchaud J.P. and Mezard M.",
    "chunk_index": 17,
    "start_char": 35979,
    "end_char": 38837,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "of Minority Games - Statistical Me-\nchanics of Interacting Agents. Oxford University Press.\n[8] Coolen A.C.C. and J.A.F. Heimel (2001) Dynamical solution of the on-line minority\ngame. J. Phys. A, 34, 10783\u201310804.\n[9] Coolen A.C.C. (2005) Generating functional analysis of minority games with real\nmarket histories. J. Phys. A, 38, 2311\u20132347.\n[10] Challet D., Marsili M. and Zhang Y. C. (2004) Minority Games: Interacting agents\nin \ufb01nancial markets. Oxford Finance Series.\n[11] Giardina I., Bouchaud J.P. and Mezard M. (2001) Microscopic models for long ranged\nvolatility correlations. Physica A, 299, 28-39.\n[12] Je\ufb00eries P., Hart M.L., Hui P.M. and Johnson N.F. (2001) From market games to\nreal-world markets. Eur.Phys. J. B 20, 493-501.\n[13] Johnson N.F., Hart M., Hui P.M. and Zheng D (2000) Trader dynamics in a model\nmarket. J. Theo. App. Fin., 3, 443.\n[14] Karatzas I. and Shreve S.E. (1991) Brownian Motion and Stochastic Calculus.\nSpringer-Verlag, New York.\n[15] Lux T. and Marchesi M. (1999) Scaling and criticality in a stochastic multi-agent\nmodel of \ufb01nancial market. Nature, 397, 498-500.\n[16] Marsili M. and Challet D. (2001) Continuum time limit and stationary states of the\nminority game. Phys. Rev. E, 64, 056138, 12 pages.\n[17] Novikov A.A. (1971) On moment inequalities for stochastic integrals. Theory Prob.\nAppl., 17, 717-720.\n19\n\n[18] Ortisi M. (2008) Polynomial-rate convergence to the stationary state for the\ncontinuum-time limit of the Minority Game. Journal of Applied Probability, 45, 376-\n387.\n[19] Shreve S.E. (2004) Stochastic Calculus for Finance, V. II. Springer Finance, NY.\n[20] Stanley H.S. and Mantegna R.N. (2000) An introduction to econophysics: correlations\nand complexity in \ufb01nance. Cambridge University Press, Cambridge.\n20",
    "chunk_index": 18,
    "start_char": 38320,
    "end_char": 40084,
    "paper_title": "From Minority Game to Black  Scholes pricing",
    "paper_category": "q-fin.TR",
    "paper_filename": "From_Minority_Game_to_Black__Scholes_pricing.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/From_Minority_Game_to_Black__Scholes_pricing.pdf"
  },
  {
    "text": "Heavy tailed distributions in closing auctions\nM. Derksen1,2, B. Kleijn2 and R. de Vilder1,2\n1 Deep Blue Capital N.V., Amsterdam\n2 Korteweg-de Vries Institute for Mathematics, University of Amsterdam\nDecember 21, 2020\nAbstract\nWe study the tails of closing auction return distributions for a sample of liquid\nEuropean stocks. We use the stochastic call auction model of Derksen et al. (2020a),\nto derive a relation between tail exponents of limit order placement distributions and\ntail exponents of the resulting closing auction return distribution and we verify this\nrelation empirically. Counter-intuitively, large closing price \ufb02uctuations are typically\nnot caused by large market orders, instead tails become heavier when market orders\nare removed.\nThe model explains this by the observation that limit orders are\nsubmitted so as to counter existing market order imbalance.\nKey Words: Closing auction; Closing prices; Stochastic models; Price formation;\nHeavy tails;\n1\nIntroduction\nDuring the trading day, most securities change hands in continuous double auctions, in\nwhich buy and sell orders are immediately matched if possible. However, to determine\nopening and closing prices, call auctions are often conducted. In a call auction, orders\nare aggregated for an interval of time, after which all possible transactions are conducted\nagainst a single clearing price that maximizes trading volume. In this paper we study\nthe tails of closing auction return distributions.\nNowadays it is widely recognized that distributions of (stock) price changes exhibit heavy\ntails: extreme price changes (of e.g. more than three standard deviations) are much more\nlikely than in a Gaussian model or other models with exponentially decaying tails. This\nissue was \ufb01rst adressed by Mandelbrot (1963) in his analysis of cotton prices, where he\nproposed L\u00b4evy stable distributions to model price \ufb02uctuations. It is generally assumed\nthat the tails follow a power law asymptotically. That is, the distribution of a return X\n1\narXiv:2012.10145v1 [q-fin.TR] 18 Dec 2020\n\nover some time interval satis\ufb01es1,\nP(X > x) \u223cCx\u2212a, as x \u2192\u221e,\n(1)\nwhere C > 0 is a constant (sometimes also replaced by a slowly varying factor L(x))\nand a > 0 is the tail exponent, determining how heavy the tail is . In early work (Fama,\n1965), the exponent a was believed to be below 2 for stock prices (in line with the stable\ndistributions of Mandelbrot (1963)). However, subsequent analyses have shown that the\nexponent is more likely to be around 3 on intraday time scales (see e.g. Gopikrishnan et\nal. (1998, 1999); Gu et al. (2008); Pagan (1996); Plerou and Stanley (2008), among many\nothers). Although it is generally accepted to model the tails as power laws, the exact\nfunctional form is also subject of debate. For example, Malevergne et al. (2005) conclude\nthat the tails decay slower than stretched exponential distributions, but somewhat faster\nthan power laws. In this paper, we do not aim to answer this question, but use power\nlaws because they describe the tails in enough detail for our analysis. Theoretically,\nthe functional form in equation (1) is justi\ufb01ed by extreme value theory, in the Fr\u00b4echet\n(heavy tailed) case (see e.g.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3203,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "it is generally accepted to model the tails as power laws, the exact\nfunctional form is also subject of debate. For example, Malevergne et al. (2005) conclude\nthat the tails decay slower than stretched exponential distributions, but somewhat faster\nthan power laws. In this paper, we do not aim to answer this question, but use power\nlaws because they describe the tails in enough detail for our analysis. Theoretically,\nthe functional form in equation (1) is justi\ufb01ed by extreme value theory, in the Fr\u00b4echet\n(heavy tailed) case (see e.g. Embrechts et al. (2003)).\nAlthough most part of the relevant literature focuses on description of the tails of stock\nprice return distributions, some e\ufb00ort has gone towards explanations of this tail be-\nhaviour. Gabaix et al. (2003, 2006) argue that large price \ufb02uctuations are due to large\norders submitted by large market participants. However, Farmer et al. (2004) and Weber\nand Rosenow (2006) study the issue on the microscopic level and \ufb01nd that large returns\nare not due to large transactions, but instead are caused by big gaps in the order book,\ni.e. \ufb02uctuations in liquidity. Mike and Farmer (2008) propose a simulation based model\nfor continuous trading, which suggests heavy tails in return distributions are caused by\nmarket microstructure e\ufb00ects, such as heavy tails in limit order placement and long\nmemory in order \ufb02ow. More theoretically, Bak et al. (1997) and Cont and Bouchaud\n(2000) propose models linking heavy tails to herd behaviour.\n1.1\nMain results\nIn this paper, we use the model of Derksen et al. (2020a) to study the distribution of\nreturns in the closing auction. In the model, limit orders are submitted to the auction\nrandomly, with a limit price that is sampled from an order placement distribution FA\n(for sell orders) or FB (for buy orders). We study the closing auctions of liquid Euro-\npean stocks listed on Euronext exchanges and \ufb01nd that both return distributions and\norder placement distributions exhibit heavy tails, with di\ufb00erent tail exponents. Zovko\nand Farmer (2002) conclude \u2018It seems that the power law for price \ufb02uctuations should be\nrelated to that of relative limit prices, but the precise nature and the cause of this rela-\ntionship is not clear.\u2019 Here, we solve this problem in the context of the closing auction:\n1Here, \u223cdenotes asymptotic equivalence, de\ufb01ned as f \u223cg \u21d4limx\u2192\u221e\nf(x)\ng(x) = 1.\n2\n\nwe provide analytical relations between the tails of order placement distributions and\nthe tails of the closing price return distribution. In a version of the model without mar-\nket orders, the tails of the closing price distribution behave as the product of the tails\nof the order placement distributions FA and FB. When we incorporate market orders,\nthis relation changes, depending on a proportionality relation between market order and\nlimit order imbalances. We empirically verify the relations between tail exponents of\norder placement and auction return distributions predicted by the model.\nIn theory, large market orders are a possible cause of large price \ufb02uctuations.",
    "chunk_index": 1,
    "start_char": 2664,
    "end_char": 5727,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "without mar-\nket orders, the tails of the closing price distribution behave as the product of the tails\nof the order placement distributions FA and FB. When we incorporate market orders,\nthis relation changes, depending on a proportionality relation between market order and\nlimit order imbalances. We empirically verify the relations between tail exponents of\norder placement and auction return distributions predicted by the model.\nIn theory, large market orders are a possible cause of large price \ufb02uctuations. We show\nhowever that this is typically not the case in closing auctions, which is our second im-\nportant result. Somewhat counter-intuitively, the empirical study shows that closing\nauction return distributions would have heavier tails if market orders are removed, sug-\ngesting that market orders have a stabilizing e\ufb00ect on price formation in closing auctions.\nTheoretically, we show (for the right tail) that this (initially perhaps somewhat puzzling)\nempirical fact can only arise whenever\n0 < MB \u2212MA\nNA \u2212NB\n\u2264aA\naB\n,\n(2)\nunder the assumption that FB and FA have heavy right tails with tail exponents aB\nand aA satisfying aB > aA > 0. Here, NA is the sell limit order volume, NB the buy\nlimit order volume and MA and MB denote the sell and buy market order volume. This\nequation poses two conditions that should be ful\ufb01lled to make it theoretically possible\nthat tails of closing auction return distributions are heavier without market orders. First,\nlimit order imbalance and market order imbalance should be of opposite signs (when\nMB > MA, it should hold that NA > NB and vice versa) and limit order imbalance\nNA \u2212NB should be larger in absolute value than market order imbalance MB \u2212MA,\nmeaning that limit orders overcompensate for market order imbalance.\nSecond, aB\nshould not be too large, i.e. the right tail of the buy limit order placement distribution\nneeds to be su\ufb03ciently heavy. We show that equation (2) is indeed satis\ufb01ed on average\nempirically, which is explained by the chronology of the closing auction: most of the\nmarket orders are submitted in the \ufb01rst seconds, revealing early in the auction the\nmarket order imbalance. This leads to strategic behaviour in which limit orders are\nplaced against the direction of the market order imbalance: when there are more buy\nthan sell market orders, one can submit a (possibly large) sell order without adversely\nimpacting the price. Our results suggest that large closing price \ufb02uctuations are not\ncaused by large market orders (at least, not directly), but by placement of limit orders,\nin accordance with the intraday results of Farmer et al. (2004) and Weber and Rosenow\n(2006). Also, our results suggest that heavy tails are market microstructure e\ufb00ects and\nthat the tail exponents vary between di\ufb00erent stocks and di\ufb00erent market mechanisms,\nin line with the view of Mike and Farmer (2008).\nThe remainder of this paper is structured as follows. In section 2 the model is described\n3",
    "chunk_index": 2,
    "start_char": 5214,
    "end_char": 8180,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "orders,\nin accordance with the intraday results of Farmer et al. (2004) and Weber and Rosenow\n(2006). Also, our results suggest that heavy tails are market microstructure e\ufb00ects and\nthat the tail exponents vary between di\ufb00erent stocks and di\ufb00erent market mechanisms,\nin line with the view of Mike and Farmer (2008).\nThe remainder of this paper is structured as follows. In section 2 the model is described\n3\n\nand theoretical results are derived. Then in section 3 the empirical results are presented\nand the relations that are predicted by the model are veri\ufb01ed. Concluding remarks are\nmade in section 4 and proofs of the mathematical theory are collected in the appendix.\n2\nTheoretical results\nIn this section we recall the auction model of Derksen et al. (2020a) and derive analytical\nexpressions for the tail behaviour of the return distribution, given the tails of order\nplacement distributions.\n2.1\nA stochastic model of the call auction\nIn the standard call auction, orders are aggregated over an interval of time and then\nmatched to transact at a clearing price that maximizes the total transacted volume.\nSuppose NA sell limit orders and NB buy limit orders are submitted to the auction\n(all orders have unit size). We assume market participants on both sides of the market\nformulate their orders independently, according to certain order placement distributions\nFA and FB. Here, FA denotes the distribution of sell orders and FB the distribution of\nbuy orders. That is, we model the sell order prices (A1, . . . , ANA) as an i.i.d. sample\nfrom FA and the buy order prices (B1, . . . , BNB) as an i.i.d. sample from FB2.\nFor convenience we consider the log return axis instead of the real price axis. We assume\nthere is some reference price x0 (for example the last traded price before the auction\nstarts or a volume weighted averaged version thereof) and all prices are expressed as\nlog returns relative to this reference price. So FA and FB are distributions on (\u2212\u221e, \u221e)\nand FA(x) or FB(x) denotes the probability that a sell or buy order price is below\nx0ex. Given (NA, NB), we denote by FA and FB the empirical distribution functions\ncorresponding to the samples (A1, . . . ANA) and (B1, . . . , BNB), meaning\nFA(x) =\n1\nNA\nNA\nX\ni=1\n1{Ai\u2264x},\nFB(x) =\n1\nNB\nNB\nX\ni=1\n1{Bi\u2264x}\nFurthermore, we de\ufb01ne the (monotone increasing) supply curve,\nDA(x) = NAFA(x)\nand the (monotone decreasing) demand curve,\nDB(x) = NB(1 \u2212FB(x)).\n2Of course these assumptions are not all realistic. In reality, orders have di\ufb00erent sizes and market\nparticipants may react to each other\u2019s orders. Despite these simplifying assumptions, the model provides\na reliable stochastic description of auction price formation (see Derksen et al. (2020a)).\n4\n\nThe supply curve denotes for every x \u2208R the number of sell orders below x0ex, the\ndemand curve gives for every x \u2208R the number of buy orders above x0ex.",
    "chunk_index": 3,
    "start_char": 7773,
    "end_char": 10652,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "NB(1 \u2212FB(x)).\n2Of course these assumptions are not all realistic. In reality, orders have di\ufb00erent sizes and market\nparticipants may react to each other\u2019s orders. Despite these simplifying assumptions, the model provides\na reliable stochastic description of auction price formation (see Derksen et al. (2020a)).\n4\n\nThe supply curve denotes for every x \u2208R the number of sell orders below x0ex, the\ndemand curve gives for every x \u2208R the number of buy orders above x0ex. Given all buy\nand sell orders, the clearing price is the price that maximizes the transactable volume\nin the auction, which is the price where supply and demand curves cross. That is, the\nclearing price X is de\ufb01ned as the solution to the market clearing equation,\nDA(X) = DB(X).\n(3)\nThis de\ufb01nition of X may give rise to problems with uniqueness and existence of solutions\nto equation (3), as illustrated in \ufb01gure 1. To solve these issues, consider the following\nde\ufb01nition.\nDefinition 2.1 For given supply curve DA and demand curve DB, the lower clearing\nprice is de\ufb01ned by\nX = inf{x \u2208R : DA(x) \u2265DB(x))}\n(4)\nand the upper clearing price is de\ufb01ned by\nX = sup{x \u2208R : DA(x) \u2264DB(x)}\n= inf{x \u2208R : DA(x) > DB(x))}.\n(5)\nThe interval [X, X) is the interval of all possible clearing prices.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n \n \nDA(\u00b7)\nDB(\u00b7)\nX\nX\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n \n \nDA(\u00b7)\nDB(\u00b7)\nX = X\nFigure 1: Two examples of the supply curve DA(\u00b7) (the increasing (red) step function)\nand the demand curve DB(\u00b7) (the decreasing (blue) step function). Left panel: a situation\nin which there is no unique point of intersection, but an interval [X, X) of possible\nclearing prices. Right panel: a situation in which there is a unique intersection point\nX = X.\nRemark 2.2 Euronext\u2019s closing auction rules say that when there are more possible\nclearing prices, the price closest to the last traded price is taken (Euronext, 2019, Rule\n4401/3). This means that when there is a large positive return, the closing price is equal\n5\n\nto the lower clearing price X. So in order to study the right tail of the closing price\nreturn distribution, we should study X. The same reasoning implies that for the left tail\nwe should consider X. Note that the model is symmetric when the roles of X and X\nand the sides of the market are interchanged. That is, the left tail of the distribution of\nX behaves the same as the right tail of the distribution of X, when FA and FB and NA\nand NB are interchanged. So without loss of generality we focus on the right tail of X.\nThe distribution of the lower clearing price, conditional on (NA, NB), has an analytically\ntractable distribution function, given in the following theorem (see Derksen et al.",
    "chunk_index": 4,
    "start_char": 10185,
    "end_char": 12950,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "Note that the model is symmetric when the roles of X and X\nand the sides of the market are interchanged. That is, the left tail of the distribution of\nX behaves the same as the right tail of the distribution of X, when FA and FB and NA\nand NB are interchanged. So without loss of generality we focus on the right tail of X.\nThe distribution of the lower clearing price, conditional on (NA, NB), has an analytically\ntractable distribution function, given in the following theorem (see Derksen et al. (2020a),\ntheorem 2.3).\nTheorem 2.3 (Lower clearing price distribution) The distribution of the lower clearing\nprice X, conditional on (NA, NB), is given by its survival function,\nP(X > M|NA, NB)\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013\n(1 \u2212FA(M))NA\u2212kFA(M)k(1 \u2212FB(M))lFB(M)NB\u2212l.\n(6)\nIn the situation described above, only limit orders are submitted to the auction. However,\nmarket participants also have the possibility to submit market orders. We de\ufb01ne the\n(possibly stochastic) market order imbalance by \u2206= MB\u2212MA, where MB is the number\nof buy market orders and MA is the number of sell market orders. Note that market\norders only play a role through \u2206, as matching market orders are executed against each\nother without a\ufb00ecting the price formation process. When market order imbalance \u2206is\ntaken into account, the market clearing equation (3) becomes\nDA(X) = DB(X) + \u2206\nand the de\ufb01nitions of X and X change accordingly. A positive (negative) value of \u2206\nmeans there is more buy (sell) market order volume than sell (buy) market order volume,\npossibly pushing the price up (down). The market order imbalance alters the clearing\nprice distribution as in the following proposition (a special case of proposition 2.8 in\nDerksen et al. (2020a)).\nProposition 2.4 (Lower clearing price distribution in case of market order imbalance)\nWhen market order imbalance \u2206plays a role, the lower clearing price distribution as\ncomputed in theorem 2.3 modi\ufb01es into\nP(X > M|NA, NB, \u2206)\n=\nNA\nX\nk=0\nNB\nX\nl=max(k\u2212\u2206+1,0)\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013\n(1 \u2212FA(M))NA\u2212kFA(M)k(1 \u2212FB(M))lFB(M)NB\u2212l.\n6\n\n2.2\nLimit order auctions\nNext we concentrate on the right tail of the lower clearing price return distribution, as\na function of the tails of the order placement distributions FA and FB, initially without\nmarket orders. We make the following assumption on the tails of FA and FB.\nAssumption 1 Assume FA has a heavier right tail than FB.\nThat is, there exists\nfunctions TA, TB such that\n1 \u2212FA(M) \u223cTA(M),\n1 \u2212FB(M) \u223cTB(M),\nas M \u2192\u221e\nand\nlim\nM\u2192\u221e\nTB(M)\nTA(M) = 0.\nThis assumption is intuitively reasonable and empirically veri\ufb01ed in section 3.1. Fur-\nthermore, we will assume that (NA, NB) follows a distribution PNA,NB on\nN = {1, . . . , N} \u00d7 {1, . . . , N},\nfor some N \u2208N, with probability mass function pNA,NB assigning positive probability\nto any point in N (we exclude the possibilities that NA = 0 or NB = 0, which describe\nfailing auctions in which clearing prices do not exist).",
    "chunk_index": 5,
    "start_char": 12452,
    "end_char": 15383,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "1 \u2212FB(M) \u223cTB(M),\nas M \u2192\u221e\nand\nlim\nM\u2192\u221e\nTB(M)\nTA(M) = 0.\nThis assumption is intuitively reasonable and empirically veri\ufb01ed in section 3.1. Fur-\nthermore, we will assume that (NA, NB) follows a distribution PNA,NB on\nN = {1, . . . , N} \u00d7 {1, . . . , N},\nfor some N \u2208N, with probability mass function pNA,NB assigning positive probability\nto any point in N (we exclude the possibilities that NA = 0 or NB = 0, which describe\nfailing auctions in which clearing prices do not exist).\nIn the following proposition we \ufb01rst derive an expression for the right tail of the lower\nclearing price distribution, conditional on (NA, NB). Finding an expression for the tail\nof the clearing price distribution amounts to \ufb01nding the slowest decaying term in the\ndouble sum of theorem 2.3. This is made formal in the following proposition, the proof\nof which is found in the appendix.\nProposition 2.5 Under assumption 1, we have\nP(X > M|NA, NB) \u223cNBTB(M)TA(M)NA, as M \u2192\u221e.\n(7)\nRemark 2.6 The proof of proposition 2.5 reveals the event that corresponds to the slow-\nest decaying term in the double sum of theorem 2.3, namely l = 1, k = 0, corresponding\nto the event that DA(M) = 0, DB(M) = 1, meaning all sell orders, but only one buy\norder, are above M. This is interpreted as an auction in which there is little consensus\nbetween both sides of the market (buy and sell orders do not overlap), but there is a\nvery aggressive buyer willing to pay a high price.\nWhen the conditional result of proposition 2.5 is summed with respect to the distribution\nof (NA, NB), the unconditional tail of X is discovered again by selecting the slowest\ndecaying term.\nThis leads to the main result of this subsection, a relation between\n7\n\nthe tail of the closing price return distribution and the tail of the order placement\ndistributions in a setting without market orders (its proof is again postponed to the\nappendix).\nTheorem 2.7 (Right tail of the lower clearing price distribution) Under assumption 1\nwe have\nP(X > M) \u223cCTA(M)TB(M), as M \u2192\u221e,\nwhere C = PN\nn=1 npNA,NB(1, n) = E[NB1{NA=1}] > 0.\nThe constant C indicates that the slowest decaying term in the sum corresponds to the\nevent that NA = 1: large positive returns are possible if there are only few sell orders.\n2.3\nMarket orders\nIn this subsection we incorporate market orders in the derivation of subsection 2.2. First\nconsider the following assumption for the market order imbalance \u2206.\nAssumption 2 We assume that \u2206\u2208(\u2212NB, NA) with probability one.\nThis assumption is necessary, because otherwise the clearing prices attain the values \u00b1\u221e\nwith non-zero probability. Under this assumption, the right tail of the conditional lower\nclearing price distribution is given by the next proposition (the proof is again postponed\nto the appendix and x+ = max(x, 0) and x\u2212= max(\u2212x, 0) denote the positive and\nnegative part of x \u2208R).",
    "chunk_index": 6,
    "start_char": 14907,
    "end_char": 17752,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "2.2. First\nconsider the following assumption for the market order imbalance \u2206.\nAssumption 2 We assume that \u2206\u2208(\u2212NB, NA) with probability one.\nThis assumption is necessary, because otherwise the clearing prices attain the values \u00b1\u221e\nwith non-zero probability. Under this assumption, the right tail of the conditional lower\nclearing price distribution is given by the next proposition (the proof is again postponed\nto the appendix and x+ = max(x, 0) and x\u2212= max(\u2212x, 0) denote the positive and\nnegative part of x \u2208R).\nProposition 2.8 Under assumptions 1 and 2, we have\nP(X > M|NA, NB, \u2206) \u223cK(NA, NB, \u2206\u22121)TB(M)(\u2206\u22121)\u2212TA(M)NA\u2212(\u2206\u22121)+,\n(8)\nas M \u2192\u221e, where\nK(NA, NB, \u2206) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u0000NA\n\u2206\n\u0001\nif \u2206> 0\n\u0000NB\n\u2212\u2206\n\u0001\nif \u2206\u22640\n.\nThis proposition shows that market orders potentially in\ufb02uence the tails heavily: if \u2206\nis positive and large (close to NA) the in\ufb02uence of the faster decaying term TB(M) is\nerased and only the slower decaying term TA(M) is left, possibly leading to very heavy\ntails. On the other hand, if \u2206is negative, the in\ufb02uence of the faster decaying term TB\ngrows, leading to less heavy tails. However, which combinations are possible depends on\nthe joint distribution of (NA, NB, \u2206). Until now, the tails TA and TB were unspeci\ufb01ed\nand few assumptions were made on the distribution of (NA, NB). To work towards an\nempirically testable theory, we will make the following assumptions on the distribution\nof (NA, NB, \u2206) and the tails of FA, FB. Empirically, these assumptions are veri\ufb01ed in\nsection 3.\n8\n\nAssumption 3 Assume (NA, NB, \u2206) follows a distribution P on {1, . . . , N}\u00d7{1, . . . , N}\u00d7\n{\u2212N, . . . , N}, with probability mass function denoted by p, for some N \u2208N. Further-\nmore, assume that market order imbalance MB \u2212MA is proportional to limit order\nimbalance NA \u2212NB (in the opposed direction), that is,\n\u2206= MB \u2212MA = c(NA \u2212NB),\n(9)\nalmost surely for some c \u2208(0, 1) and P(\u2206= 0) = 0 (as the case \u2206= 0 is already con-\nsidered in subsection 2.2). Finally, assume that all possible combinations have positive\nprobability, i.e.\np(n, m, d) > 0, for all n, m \u2208{1, . . . , N}, d \u2208\u00b1{1, . . . , N} such that d = c(n \u2212m).\nEquation (9) states that limit order imbalance points in the opposed direction of market\norder imbalance, which resembles that limit order submitters adjust their orders to the\nmarket order imbalance. This relation ensures assumption 2 holds and is empirically\nveri\ufb01ed in section 3.3.\nAssumption 4 Assume FA, FB both have power law tails, that is,\n1 \u2212FA(M) \u223cTA = M\u2212aA, 1 \u2212FB(M) \u223cTB(M) = M\u2212aB, as M \u2192\u221e,\nfor tail exponents aB > aA > 0.\nUnder assumptions 3 and 4, the following theorem (which is proved in the appendix)\ndescribes the tail behaviour of the clearing price distribution in terms of the parameters\nc (controlling the relation between market",
    "chunk_index": 7,
    "start_char": 17240,
    "end_char": 19995,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "This relation ensures assumption 2 holds and is empirically\nveri\ufb01ed in section 3.3.\nAssumption 4 Assume FA, FB both have power law tails, that is,\n1 \u2212FA(M) \u223cTA = M\u2212aA, 1 \u2212FB(M) \u223cTB(M) = M\u2212aB, as M \u2192\u221e,\nfor tail exponents aB > aA > 0.\nUnder assumptions 3 and 4, the following theorem (which is proved in the appendix)\ndescribes the tail behaviour of the clearing price distribution in terms of the parameters\nc (controlling the relation between market and limit order imbalance) and aA and aB\n(controlling the heaviness of the tails of the buy and sell limit order placement distribu-\ntion).\nTheorem 2.9 (Right tail of the lower clearing price distribution with market orders)\nUnder assumptions 3 and 4, there exists a constant C > 0, such that\nP(X > M) \u223cCM\u2212a, as M \u2192\u221e,\nwhere\na = min\n\u0012(c + 1)aA\nc\n, aA + 2aB\n\u0013\n.\n(10)\nNote that without market order imbalance \u2206we have by theorem 2.7 a = aA +aB. This\ntheorem makes testable predictions about the relation between the tails of the closing\nprice return distribution, the tails of the limit order placement distributions and the\nlimit and market order imbalance. In the next section we will investigate this relation\nempirically.\n9\n\n3\nEmpirical results\nIn this section we investigate empirically the relation between the tails of the closing\nauction return distributions and the tails of the limit order placement distributions.\nIn order to do so, we obtain detailed order-by-order data over 2018 and 2019, for 100\nliquid European stocks (with market capitalization above EUR 1 bn) listed on Euronext\nexchanges in Amsterdam, Paris, Brussels or Lisbon.\nEstimating the tails of a distribution comes with a couple of problems. First, the power\nlaw of equation (1) is not assumed to hold for all values of x, but only for the tail.\nThis necessarily involves a starting point xmin such that the power law holds for all\nx > xmin (see Newman (2005) for a discussion). Unfortunately, the eventual estimate\nfor the tail exponent will depend on this cut-o\ufb00point: if xmin is taken too small, the bulk\ninstead of the tail will determine the estimates. The cut-o\ufb00is often made through visual\ninspection of a double logarithmic plot. Then the second problem arises, because the\ncut-o\ufb00eliminates most of the available data, leaving only a small fraction of the data\navailable for estimation. Finally, models are often designed to describe only \u2018generic\u2019\nsituations well and are not intended to explain extreme events.\nIt is a noteworthy\nadvantage of the call auction model of section 2 that it is suitable to model both the\nbulk of the data (as in Derksen et al. (2020a)) and extreme events, as in the current\npaper.\nConcerning the amount of data relevant for the tails, in every closing auction a large\namount of orders is submitted, so the tails of order placement distributions can be studied\nper stock.",
    "chunk_index": 8,
    "start_char": 19546,
    "end_char": 22380,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "a noteworthy\nadvantage of the call auction model of section 2 that it is suitable to model both the\nbulk of the data (as in Derksen et al. (2020a)) and extreme events, as in the current\npaper.\nConcerning the amount of data relevant for the tails, in every closing auction a large\namount of orders is submitted, so the tails of order placement distributions can be studied\nper stock. Unfortunately, this is not possible for the closing auction return distribution:\nper stock, we have only around 500 trading days (two years of around 250 trading days\nper stock) and thus only that many closing auction returns, which is far insu\ufb03cient to\nexamine the tails. For example, if we take the 0.05-quantile for the cut-o\ufb00point xmin,\nonly about 25 data points reside in the tail, which is too few for meaningful statistical\nanalysis. So to investigate the tails of the closing auction return distribution, we merge\ntogether the closing auction returns of all stocks in the sample.\nIn the entire section, the reference price x0 will be the volume weighted average price over\nthe last \ufb01ve minutes of continuous trading. Closing auction returns will be measured in\nlog returns with respect to x0. Following Bouchaud et al. (2002) and Zovko and Farmer\n(2002), limit order prices are measured in the number of ticks a limit order is placed\naway from the reference price x0.\n3.1\nTails of order placement distributions\nThe mechanism of the call auction makes it possible to study both tails of both order\nplacement distributions. In \ufb01gure 2, both tails of the sell limit order distribution FA\n10\n\nand the buy limit order distribution FB are shown in log-log plots, for four stocks that\nare representative for the sample.\n101\n102\n103\n104\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n \n \n \n \nx\nCDF\nASML right tail FA\nSAINT GOBAIN\nSIGNIFY\nUBISOFT\n(a) Right tail of FA.\n102\n103\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nASML right tail FB\nSAINT GOBAIN\nSIGNIFY\nUBISOFT\n(b) Right tail of FB.\n102\n103\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nASML left tail FA\nSAINT GOBAIN\nSIGNIFY\nUBISOFT\n(c) Left tail of FA.\n101\n102\n103\n104\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\n \n \n \n \nx\nCDF\nASML left tail FB\nSAINT GOBAIN\nSIGNIFY\nUBISOFT\n(d) Left tail of FB.\nFigure 2:\nLog-log plots of the tails of the order placement distributions for 4 selected\nstocks (ASML Holding NV, Compagnie de Saint Gobain SA, Signify NV, Ubisoft Enter-\ntainment SA ). The x-axes show the number of ticks above (for the right tail) or below\n(for the left tail) the reference price x0.\nLet us \ufb01rst focus on the right tails, i.e. the upper panels (a) and (b) of \ufb01gure 2.",
    "chunk_index": 9,
    "start_char": 21998,
    "end_char": 24615,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "FB.\nFigure 2:\nLog-log plots of the tails of the order placement distributions for 4 selected\nstocks (ASML Holding NV, Compagnie de Saint Gobain SA, Signify NV, Ubisoft Enter-\ntainment SA ). The x-axes show the number of ticks above (for the right tail) or below\n(for the left tail) the reference price x0.\nLet us \ufb01rst focus on the right tails, i.e. the upper panels (a) and (b) of \ufb01gure 2. The plots\nof the right tails of FA show apparent power law behaviour in the range between 10 and\n1000 ticks above the reference price. After circa 1000 ticks the tails decay faster for a\nwhile, but starting around 5000 ticks a new part of the distribution seems to start. The\nplot is cut-o\ufb00at 10 000 ticks, but some even reach until 100 000 ticks. These extremes\ndo not contribute to price formation in the auction at all. We focus on the interval of\nthe price axis where price formation occurs: the intersection of the supports of FA and\nFB. For the right tail that means FB provides the e\ufb00ective upper bound (note that the\nclosing price can never take a value above the highest buy order). The support of FB\nranges until around 1000-2000 ticks above the reference price so that is the region we use\nin our analysis, roughly in line with the intraday results from Zovko and Farmer (2002)3.\n3The sell orders (far) above this region can be thought of as coming from another distribution de-\nscribing patient sellers not relevant to the auction result. To sketch how irrelevant those orders are: the\ntick size of a stock is normally between 1 and 5 basis points. Assuming a tick size of 2.5 basis points,\n2000 ticks correspond to a return of 50%, while a closing auction return in the order of 1% is already\nhigh.\n11\n\nPower law behaviour is less clear for FB, but in the range of 100 until 1000 ticks power\nlaw behaviour can be recognized for the liquid stocks ASML and Saint Gobain. For the\nless liquid stocks Signify and Ubisoft it stops earlier around 500 ticks, but this can also\nbe due to smaller volumes of available data. The lower panels (c) and (d) of \ufb01gure 2\nshow the left tails of the order placement distributions. These are very similar to the\nright tails, when the roles of FA and FB are switched. Also, on the left side there is\na real cut-o\ufb00point, corresponding to price 0, which is found somewhere between 2000\nand 10 000 ticks. In \ufb01gure 3 we zoom in on the right tails of FB and FA until around\n102\n103\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nASML right tail FA\nLinear \ufb01t, slope =-1.07\nASML right tail FB\nLinear \ufb01t, slope =-2.37\n102\n103\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nGOBAIN right tail FA\nLinear \ufb01t, slope =-0.40\nGOBAIN right tail FB\nLinear",
    "chunk_index": 10,
    "start_char": 24226,
    "end_char": 26879,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "somewhere between 2000\nand 10 000 ticks. In \ufb01gure 3 we zoom in on the right tails of FB and FA until around\n102\n103\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nASML right tail FA\nLinear \ufb01t, slope =-1.07\nASML right tail FB\nLinear \ufb01t, slope =-2.37\n102\n103\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nGOBAIN right tail FA\nLinear \ufb01t, slope =-0.40\nGOBAIN right tail FB\nLinear \ufb01t, slope =-2.01\n102\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nUBISOFT right tail FA\nLinear \ufb01t, slope =-0.58\nUBISOFT right tail FB\nLinear \ufb01t, slope =-3.63\n102\n6 \u00d7 101\n2 \u00d7 102\n3 \u00d7 102\n4 \u00d7 102\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nSIGNIFY right tail FA\nLinear \ufb01t, slope =-0.89\nSIGNIFY right tail FB\nLinear \ufb01t, slope =-3.69\nFigure 3: Log-log plots of the right tails of the order placement distributions for 4 stocks\n(ASML Holding NV, Compagnie de Saint Gobain SA, Signify NV, Ubisoft Entertainment\nSA). The x-axes show the number of tick sizes above the reference price x0. Linear \ufb01ts\nare also plotted, \ufb01tted on the 0.05-quantile of FB until the 0.001-quantile of FB, to\nestimate aB and aA\n1000 tick sizes above the reference price and provide linear \ufb01ts as estimators for the\nvalues of aA and aB (the tail exponents of FA and FB as in assumption 4). We perform\nlinear least square \ufb01ts on the log-log plots of the tails of FB, starting at its 0.05-quantile.\nVisual inspection shows that in the extreme tails, available data points are too sparse\nto form a coherent picture. So we stop the \ufb01t at the 0.001-quantile of FB, which seems\nreasonable when inspecting the plots and we make \ufb01ts for FA on the same interval4. For\nexample for ASML, we obtain aA \u22481.07, aB \u22482.37, \ufb01tted on the interval of 168 until\n862 tick sizes. For all four stocks, FA shows a straight, slowly decaying line, resembling\n4These choices are somewhat arbitrary, but cut-o\ufb00choices need to be made in any practical tail\nanalysis (see Newman (2005)) and moreover, results do not change substantially when we extend the \ufb01t\nto e.g. the 0.0001-quantile, or e.g. start the \ufb01t at the 0.01-quantile.\n12\n\na power law with exponents around or even below 1. Furthermore, the tails of FB decay\nfaster than the tails of FA, with exponents between 2 and 4 (more results are discussed\nin section 3.4).\n3.2\nTails of closing auction return distributions\nFor every stock i and day 1 \u2264d \u2264n we have a closing auction return Xi,d, de\ufb01ned as\nXi,d = log(Ci,d) \u2212log(xi,d\n0 ),\nwhere Ci,d is the closing price of stock i on day d and xi,d\n0\nis the reference price of stock\ni on day d. Following e.g. Gopikrishnan et al. (1998) we standardize the returns per\nstock.",
    "chunk_index": 11,
    "start_char": 26519,
    "end_char": 29084,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "in section 3.4).\n3.2\nTails of closing auction return distributions\nFor every stock i and day 1 \u2264d \u2264n we have a closing auction return Xi,d, de\ufb01ned as\nXi,d = log(Ci,d) \u2212log(xi,d\n0 ),\nwhere Ci,d is the closing price of stock i on day d and xi,d\n0\nis the reference price of stock\ni on day d. Following e.g. Gopikrishnan et al. (1998) we standardize the returns per\nstock. That is, we divide for every stock i the return sample {Xi,d : 1 \u2264d \u2264n} by its\nstandard deviation and obtain a sample of standardized returns of size n \u2248500. These\nsamples are all merged together into one large sample to study the tails of the closing\nauction return distributions. In \ufb01gure 4 the right and left tails of the return distribution\nare shown in log-log plots, showing clear power law behaviour from 2 up to 10 standard\ndeviations for both tails. Linear least square \ufb01ts are also shown (starting the \ufb01t at 2\nstandard deviations), giving tail exponents a = 5.28 for the left tail and a = 4.74 for the\nright tail.\nThis suggests closing auction returns are less heavy tailed than intraday returns over\nshort time intervals, for which a tail exponent a \u22483 is widely supported in the literature\n(see e.g. Gopikrishnan et al. (1998)). This di\ufb00erence might be explained in qualitative\nterms by the large transacted volumes in the closing auctions. It is known that tails of\nreturn distributions become thinner when longer time intervals are considered, an e\ufb00ect\nthat is known as aggregational Gaussianity (the empirical fact that return distributions\nconverge to normal distributions when the interval length increases, see e.g. Cont (2001)).\nThis is theoretically supported by the call auction model: the clearing price distribution\napproaches a normal distribution, when the number of orders tends to in\ufb01nity (see\nDerksen et al. (2020a), theorem 3.1). Moreover, the empirical e\ufb00ect is known to be\nstronger if time intervals are measured in trade time (Chakraborti et al., 2011).\nIn\nEurope nowadays around 30% of the daily volume is transacted in the closing auction,\nwhich makes the duration of the closing auction in trade time similar to approximately\nhalf a day of continuous trading5.\n5The fraction of daily transacted volume that is transacted in closing auctions has increased greatly\nover the past years, especially since the introduction of MiFID II, see Derksen et al. (2020b).\n13\n\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nCDF\nRight tail\nLinear \ufb01t, slope= \u22124.74\nLeft tail\nLinear \ufb01t, slope= \u22125.28\nFigure 4:\nLog-log plot of the tails of the closing auction return distribution for all 100\nstocks in our sample. Blue dots show the right tail, that is P(X > x), red squares the\nleft tail, that is P(X < \u2212x), the x-axis is in standardized returns. Linear \ufb01ts are also\nplotted, giving a tail exponent of a = 4.74 for the right tail and a = 5.28 for the left tail.",
    "chunk_index": 12,
    "start_char": 28716,
    "end_char": 31583,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "10\n2\n \n \n \n \nx\nCDF\nRight tail\nLinear \ufb01t, slope= \u22124.74\nLeft tail\nLinear \ufb01t, slope= \u22125.28\nFigure 4:\nLog-log plot of the tails of the closing auction return distribution for all 100\nstocks in our sample. Blue dots show the right tail, that is P(X > x), red squares the\nleft tail, that is P(X < \u2212x), the x-axis is in standardized returns. Linear \ufb01ts are also\nplotted, giving a tail exponent of a = 4.74 for the right tail and a = 5.28 for the left tail.\n3.3\nThe e\ufb00ect of market orders\nBefore we study the in\ufb02uence of market orders on the tail behaviour of closing auction\nreturn distributions, we \ufb01rst investigate the relation between the market order imbalance\nand the limit order imbalance. In \ufb01gure 5 the market order imbalance \u2206= MB \u2212MA\nin every closing auction is plotted against the limit order imbalance NA \u2212NB in that\nclosing auction, for the four stocks that were also studied in section 3.1. The \ufb01gure shows\nthat the proportionality relation between \u2206and NA \u2212NB introduced in equation (9)\nholds approximately, with values of c in the range 0.2-0.4, estimated using linear least\nsquare regression. This means that limit order imbalance is generally in the opposite\ndirection of market order imbalance.\nAn explanation for this lies in the chronology\nof the closing auction. We observe in auction data that the vast majority of market\norders is submitted in the \ufb01rst seconds of the closing auction, revealing the market\norder imbalance early in the auction (during the accumulation phase of the auction,\ninformation on the imbalance and an indicative price is released, so it is possible to act\non this information). Subsequently, limit orders are placed against the direction of the\nmarket order imbalance, re\ufb02ecting strategic behaviour: when there is a large positive\nmarket order imbalance (more buy market orders than sell market orders), one can\n14\n\nsubmit a (possibly large) sell order without adversely a\ufb00ecting the price.\n400000\n300000\n200000\n100000\n0\n100000\n200000\n200000\n150000\n100000\n50000\n0\n50000\n100000\n150000\n \n\u2206\nNA \u2212NB\nASML, c=0.329\n1000000750000 500000 250000\n0\n250000 500000 7500001000000\n300000\n200000\n100000\n0\n100000\n200000\n300000\n400000\n \n\u2206\nNA \u2212NB\nGOBAIN, c=0.236\n300000\n200000\n100000\n0\n100000\n200000\n300000\n100000\n50000\n0\n50000\n100000\n150000\n \n\u2206\nNA \u2212NB\nUBISOFT, c=0.204\n200000 100000\n0\n100000 200000 300000 400000 500000\n150000\n100000\n50000\n0\n50000\n100000\n150000\n \n\u2206\nNA \u2212NB\nSIGNIFY, c=0.252\nFigure 5:\nThe di\ufb00erence NA \u2212NB plotted against the market order imbalance \u2206,\nshowing limit order imbalance goes against the direction of market order imbalance.\nThe dashed red line is the result of linear least square regression, to estimate the value\nof c in equation (9), which is the slope of the dashed red line (outliers of more than four\nstandard deviations away from the mean are removed).\nNext, we will investigate the e\ufb00ect of market orders on the tail exponents. Consider \ufb01gure\n6, where two auction results are shown.",
    "chunk_index": 13,
    "start_char": 31134,
    "end_char": 34076,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "order imbalance goes against the direction of market order imbalance.\nThe dashed red line is the result of linear least square regression, to estimate the value\nof c in equation (9), which is the slope of the dashed red line (outliers of more than four\nstandard deviations away from the mean are removed).\nNext, we will investigate the e\ufb00ect of market orders on the tail exponents. Consider \ufb01gure\n6, where two auction results are shown. Supply and demand curves are represented by\nthe solid lines and the point of intersection is the closing price, indicated by the black\nstar. When market orders are removed, translated supply and demand curves (plotted\nby the dashed lines) lead to an alternative closing price, represented by the black square.\nThe upper panel shows a situation in which a large positive closing auction return is\ncaused by a high market order imbalance. When the market order imbalance would be\nremoved, the closing price would be much lower (black square). The lower panel shows a\nvery di\ufb00erent situation: a small positive closing auction return, but a strongly negative\nmarket order imbalance. If in this case the market order imbalance would be removed,\nthe closing auction return would get much higher (black square).\nThe two scenarios presented in \ufb01gure 6 raise the question which is more common: are\nlarge closing auction returns caused by large market order imbalances or is this potential\ne\ufb00ect cancelled by limit order imbalance and are limit orders usually the driver of large\n15\n\n155\n160\n165\n170\n175\n180\n185\n190\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n7000000\n \n \n \n \n \n \n \nPrice\nVolume\nDA(x) + MA\nDB(x) + MB\nDA(x)\nDB(x)\nReference price x0\nClosing price\nAlternative closing price\n(a) Closing auction ASML, 2018-03-16.\n2.55\n2.60\n2.65\n2.70\n2.75\n2.80\n2.85\n2.90\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n \n \n \n \n \n \n \nPrice\nVolume\nDA(x) + MA\nDB(x) + MB\nDA(x)\nDB(x)\nReference price x0\nClosing price\nAlternative closing price\n(b) Closing auction KPN, 2018-02-07.\nFigure 6:\nTwo closing auction results. Solid lines are the supply (red) and demand\n(blue) curves of the particular closing auction, including market orders (for convenience\nsell (buy) market orders are placed just below (above) the lowest sell (highest buy) limit\norder). Dashed lines show the supply and demand curves without market orders. The\nblack dot denotes the reference price x0, the black star denotes the closing price and the\nblack square denotes the alternative price when only limit orders are considered.\n16\n\nreturns? To answer this question, we also investigate the tails of the return distribution of\nthe alternative closing price, de\ufb01ned as the intersection point of the supply and demand\ncurves when the market orders are removed (black squares in \ufb01gure 6). So, for every\nstock i and day d we have an alternative closing auction return \u02dcXi,d, de\ufb01ned as\n\u02dcXi,d = log( \u02dcCi,d) \u2212log(xi,d\n0 ),\nwhere \u02dcCi,d is the alternative closing price of stock i on day d.",
    "chunk_index": 14,
    "start_char": 33640,
    "end_char": 36595,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "tails of the return distribution of\nthe alternative closing price, de\ufb01ned as the intersection point of the supply and demand\ncurves when the market orders are removed (black squares in \ufb01gure 6). So, for every\nstock i and day d we have an alternative closing auction return \u02dcXi,d, de\ufb01ned as\n\u02dcXi,d = log( \u02dcCi,d) \u2212log(xi,d\n0 ),\nwhere \u02dcCi,d is the alternative closing price of stock i on day d. We again standardize these\nreturns per stock, giving for every stock around 500 alternative closing auction returns,\nwhich are merged to study the tails. In \ufb01gure 7 the tails of the alternative closing price\nreturn distribution are shown, together with the tails of the real closing price return\ndistribution from \ufb01gure 4. The \ufb01gure shows that the tails become heavier when market\norders are removed. For the right tail we document a tail exponent a = 3.75 without\nmarket orders, compared to a = 4.74 with market orders. For the left tail, the tail\nexponent becomes a = 3.9 when market orders are removed, compared to the value\na = 5.28 when market orders are included.\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nWithout market orders\nLinear \ufb01t, slope= \u22123.75\nWith market orders\nLinear \ufb01t, slope= \u22124.74\n(a) Right tail return distribution, P(X > x)\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nWithout market orders\nLinear \ufb01t, slope= \u22123.9\nWith market orders\nLinear \ufb01t, slope= \u22125.28\n(b) Left tail return distribution, P(X < \u2212x)\nFigure 7:\nLog-log plot of the tails of the closing auction return distribution for all\n100 stocks in the sample. Blue dots show the tails for the real closing auction return\ndistribution, red dots the tails for the alternative closing auction returns that emerge\nwhen market orders are removed.\nIt is thus concluded that large closing price \ufb02uctuations are in general not caused by a\nlarge market order imbalance (at least, not directly). The explanation for this counter-\nintuitive result lies in the chronology of the auction and the placement of limit orders:\nwhen the market order imbalance is positive (negative), there are more sell (buy) limit\norders submitted (cf. \ufb01gure 5). Theorems 2.7 and 2.9 give the model\u2019s view on the matter\n17\n\nand state that without market orders the tail exponent is equal to aA + aB and with\nmarket orders it is equal to min((c+1)aA\nc\n, aA + 2aB). This means that tails get heavier\nwithout market orders, whenever\nc \u2264aA\naB\n.\n(11)\nThis equation in fact resembles two conditions that should be ful\ufb01lled to make it possi-\nble that tails are heavier without market orders (see also equation (2)). First, c should\nbe small and positive, re\ufb02ecting that the abovementioned strategic behaviour is strong:\nwhen there is a large market order imbalance, in general the limit order di\ufb00erence over-\ncompensates for this.",
    "chunk_index": 15,
    "start_char": 36205,
    "end_char": 39008,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "to min((c+1)aA\nc\n, aA + 2aB). This means that tails get heavier\nwithout market orders, whenever\nc \u2264aA\naB\n.\n(11)\nThis equation in fact resembles two conditions that should be ful\ufb01lled to make it possi-\nble that tails are heavier without market orders (see also equation (2)). First, c should\nbe small and positive, re\ufb02ecting that the abovementioned strategic behaviour is strong:\nwhen there is a large market order imbalance, in general the limit order di\ufb00erence over-\ncompensates for this. Second, aB should not be too large compared to aA. This is a\ncondition on the right tail of the buy limit order distribution. Without market orders,\nthe highest buy limit order serves as an upper bound for the closing price. So to obtain\nheavier tails without market orders, the right tail of FB should be su\ufb03ciently heavy\n(small aB). It turns out that condition (11) is indeed satis\ufb01ed for most of the stocks: for\nexample, for ASML we obtained estimators aB \u22482.37, aA \u22481.07, c \u22480.329 (cf. \ufb01gures\n3 and 5), satisfying the condition in equation (11). Indeed, theorems 2.7 and 2.9 imply\nthat the tail exponent for closing auction returns of ASML is aA + aB = 3.44 without\nmarket orders and (c+1)aA\nc\n= 4.32 with market orders. In the next subsection we will\nverify the theoretical results on the whole sample consisting of 100 stocks.\n3.4\nModel-predicted and realized tail exponents compared\nIn this subsection the relations predicted by the model are tested over the whole sample\nof 100 stocks. For every stock we estimate the tail exponents of the order placement\ndistributions (aA and aB) and the value of the parameter c (as in equation (9)). The\nresults are shown in table 2 (for 50 stocks with the lower market capitalizations) and\ntable 3 (for 50 stocks with the higher market capitalizations). To estimate the parameter\nc, we use linear least squares regression and to estimate the values of aA and aB we use\nthe method described in section 3.1: for every stock, we make linear least square \ufb01ts\non double logarithmic plots as in \ufb01gure 3, on the interval between the 0.05- and 0.001-\nquantiles of FB. The absolute values of the resulting slopes are the estimators for aA and\naB. For example, for ASML we obtain in this way estimators aB \u22482.37, aA \u22481.07 and\nfor Ubisoft we \ufb01nd aB \u22483.63, aA \u22480.58, cf. \ufb01gure 3. In tables 2 and 3 the results are\nshown for all stocks in the sample, the columns aB(r) and aA(r) give the estimated tail\nexponents for the right tails of FB and FA. For the left tails, the same method applies\nwhen the roles of FB and FA are interchanged. On the left side, FB has a heavier tail\nand FA provides the e\ufb00ective lower bound.\nIn \ufb01gure 8 the left tails of the order placement distributions are shown for ASML and\nUbisoft, as well as the linear least square \ufb01ts, showing that for the left tails aA \u2248\n2.50, aB \u22481.17 for ASML and aA \u22482.81, aB \u22480.87 for Ubisoft. In tables 2 and 3 the\n18",
    "chunk_index": 16,
    "start_char": 38519,
    "end_char": 41417,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "the columns aB(r) and aA(r) give the estimated tail\nexponents for the right tails of FB and FA. For the left tails, the same method applies\nwhen the roles of FB and FA are interchanged. On the left side, FB has a heavier tail\nand FA provides the e\ufb00ective lower bound.\nIn \ufb01gure 8 the left tails of the order placement distributions are shown for ASML and\nUbisoft, as well as the linear least square \ufb01ts, showing that for the left tails aA \u2248\n2.50, aB \u22481.17 for ASML and aA \u22482.81, aB \u22480.87 for Ubisoft. In tables 2 and 3 the\n18\n\n102\n103\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nASML left tail FA\nLinear \ufb01t, slope =-2.50\nASML left tail FB\nLinear \ufb01t, slope =-1.17\n102\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n \n \n \n \nx\nCDF\nUBISOFT left tail FA\nLinear \ufb01t, slope =-2.81\nUBISOFT left tail FB\nLinear \ufb01t, slope =-0.87\nFigure 8:\nLog-log plots of the left tails of the order placement distributions for ASML\nHolding NV and Ubisoft Entertainment SA. The x-axis shows the number of tick sizes\nabove the reference price x0. Linear \ufb01ts are also plotted, \ufb01tted on the 0.05-quantile of\nFA until the 0.001-quantile of FA, to estimate aB and aA\ncolumns aB(l) and aA(l) give the estimated tail exponents for the left tails of FB and FA.\nEstimates for aA, aB and c give rise to an estimate for the tail exponent a for the return\ndistribution of that particular stock. With market orders a = min((c+1)aA\nc\n, aA + 2aB)\n(cf. theorem 2.9) and without market orders a = aA + aB (cf. theorem 2.7)6. Ideally, we\nwould test these predictions against the realized tail exponents of the return distribution\nfor every stock. However, as noted in the beginning of this section, this is not possible,\nbecause we only have around 500 closing auction returns per stock. Instead, we can\nverify the predictions over groups of stocks, by comparing estimated tail coe\ufb03cients\nwith the model\u2019s average predicted values.\nFirst, consider the whole sample of 100 stocks. In \ufb01gure 7 it was shown that the right\ntail of the closing price return distribution has an estimated tail exponent of a = 4.74,\nwhich changes to a = 3.75 if market orders are removed. If we take the average of the\nmodel\u2019s predictions over all 100 stocks, we \ufb01nd an average predicted tail exponent of\n4.89 with market orders (column \u2018a(r) MO\u2019 in tables 2 and 3) and 3.89 without market\norders (column \u2018a(r) no MO\u2019 in tables 2 and 3). Furthermore, \ufb01gure 7 shows that the left\ntail of the closing price return distribution has an estimated tail exponent of a = 5.28,\nwhich changes to a = 3.90 if the market orders are removed. For the left tail, the average\npredicted tail exponent over all 100 stocks equals 5.01 with market orders (column \u2018a(l)\nMO\u2019 in tables 2 and 3) and 3.72 without market orders (column \u2018a(l) no MO\u2019 in tables 2\nand 3).",
    "chunk_index": 17,
    "start_char": 40893,
    "end_char": 43639,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "tables 2 and 3) and 3.89 without market\norders (column \u2018a(r) no MO\u2019 in tables 2 and 3). Furthermore, \ufb01gure 7 shows that the left\ntail of the closing price return distribution has an estimated tail exponent of a = 5.28,\nwhich changes to a = 3.90 if the market orders are removed. For the left tail, the average\npredicted tail exponent over all 100 stocks equals 5.01 with market orders (column \u2018a(l)\nMO\u2019 in tables 2 and 3) and 3.72 without market orders (column \u2018a(l) no MO\u2019 in tables 2\nand 3). The predicted tail exponents vary a lot between the di\ufb00erent stocks, suggesting\nthat the heaviness of the tails depends on the stock. To additionally test if these per\nstock predictions give information about the real tail exponents, we split our sample into\n50 stocks with the lower market caps (those in table 2) and 50 stocks with the higher\nmarket caps (table 3). In that way, the groups are kept large enough to examine the\ntails of the closing auction return distributions.\n6Note that for the left tails the roles of aA and aB need to be interchanged (see also remark 2.2).\n19\n\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nCDF\nWithout market orders\nLinear \ufb01t, slope= \u22124.10\nWith market orders\nLinear \ufb01t, slope= \u22124.95\n(a) Small caps, right tail return distribution.\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nCDF\nWithout market orders\nLinear \ufb01t, slope= \u22124.11\nWith market orders\nLinear \ufb01t, slope= \u22125.26\n(b) Small caps, left tail return distribution.\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nCDF\nWithout market orders\nLinear \ufb01t, slope= \u22123.48\nWith market orders\nLinear \ufb01t, slope= \u22124.60\n(c) Large caps, right tail return distribution.\n101\n2 \u00d7 100\n3 \u00d7 100\n4 \u00d7 100\n6 \u00d7 100\n10\n5\n10\n4\n10\n3\n10\n2\n \n \n \n \nx\nCDF\nWithout market orders\nLinear \ufb01t, slope= \u22123.75\nWith market orders\nLinear \ufb01t, slope= \u22125.39\n(d) Large caps, left tail return distribution.\nFigure 9:\nLog-log plots of the tails of the closing auction return distributions for the\n50 small cap stocks of table 2 (upper panel) and 50 large cap stocks of table 3 (lower\npanel). Blue dots show the tails for the real closing auction return distribution, red dots\nthe tails for the alternative closing auction returns that emerge when market orders are\nremoved.\n20\n\nIn \ufb01gure 9 the tails of the closing auction return distribution for the 50 small caps\nand the 50 large caps are shown in double logarithmic plots, again with and without\nmarket orders (similar to \ufb01gure 7). The linear \ufb01ts to the double logarithmic plots are the\nrealized tail exponents for the both groups, which can again be compared to the average\npredicted values in tables 2 and 3.",
    "chunk_index": 18,
    "start_char": 43146,
    "end_char": 45799,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "for the alternative closing auction returns that emerge when market orders are\nremoved.\n20\n\nIn \ufb01gure 9 the tails of the closing auction return distribution for the 50 small caps\nand the 50 large caps are shown in double logarithmic plots, again with and without\nmarket orders (similar to \ufb01gure 7). The linear \ufb01ts to the double logarithmic plots are the\nrealized tail exponents for the both groups, which can again be compared to the average\npredicted values in tables 2 and 3. The results are summarized in table 1, showing \ufb01rst\nof all that the model\u2019s predicted exponents are quite close to the realized exponents.\nGiven that estimation of tails (and tail exponents in particular) is generally thought of\nas a di\ufb03cult statistical problem, the congruence is quite remarkable. Second, based on\nthe modelling assumption in equation (9), the model predicts correctly that the tails get\nheavier if market orders are removed, and by how much. The theoretical predictions are\nespecially accurate for the case without market orders, which is not surprising: theorem\n2.7 holds very generally and follows directly from the mechanics of the closing auction.\nFor the case with market orders, more assumptions were made (see assumption 3). Most\nimportantly, we assumed equation (9) holds true, which of course in reality holds only\napproximately (see also \ufb01gure 5). When looking at tables 2 and 3, the predictions for\nthe case with market orders vary strongly between the stocks. We do not claim that the\nmost extreme values that are predicted are close to reality, but we have shown that, on\naverage, model predicted and realized tail exponents match remarkably well.\nLeft tails\nRight tails\nMO\nNo MO\nMO\nNo MO\nPredicted\nRealized\nPredicted\nRealized\nPredicted\nRealized\nPredicted\nRealized\nAll stocks\n5.01\n5.28\n3.72\n3.90\n4.89\n4.74\n3.89\n3.75\nSmall caps\n5.76\n5.26\n4.14\n4.11\n5.19\n4.95\n4.17\n4.10\nLarge caps\n4.25\n5.39\n3.29\n3.75\n4.59\n4.60\n3.61\n3.48\nTable 1:\nAverage predicted tail exponents compared to realized tail exponents. Pre-\ndicted exponents are averages over tables 2 and 3, realized exponents are the results of\nthe linear \ufb01ts in \ufb01gures 7 (all stocks) and 9 (small and large caps), for the cases with\n(MO) and without (No MO) market orders.\n21\n\nStock\nExch.\nMcap\naA(l)\naB(l)\naA(r)\naB(r)\nc\na(l)\nno MO\na(l)\nMO\na(r)\nno MO\na(r)\nMO\nASM INTL\nAMS\n6.7\n3.312\n1.154\n0.717\n3.377\n0.127\n4.466\n7.778\n4.094\n6.357\nAALBERTS\nAMS\n3.8\n3.178\n0.763\n1.207\n3.301\n0.174\n3.941\n5.135\n4.508\n7.808\nWDP REIT\nBRU\n5.2\n3.142\n3.279\n1.739\n3.032\n0.113\n6.421\n9.562\n4.771\n7.804\nREXEL\nPAR\n3.2\n2.105\n1.782\n1.465\n2.626\n0.184\n3.887\n5.992\n4.091\n6.718\nEURONEXT\nPAR\n6.8\n3.319\n1.943\n1.918\n3.229\n0.164\n5.262\n8.582\n5.146\n8.375\nIMCD GROUP\nAMS\n6.0\n3.328\n0.972\n1.859\n3.016\n0.158\n4.300\n7.118\n4.875\n7.892\nSIGNIFY\nAMS\n4.6\n3.485\n1.181\n0.888\n3.690\n0.252\n4.666\n5.866\n4.579\n4.413\nALTEN\nPAR\n2.8\n3.443\n1.476\n1.017\n2.420\n0.137\n4.919\n8.362\n3.437\n5.857\nBIC\nPAR\n1.9\n3.025\n1.644\n0.916\n3.820\n0.324\n4.669",
    "chunk_index": 19,
    "start_char": 45323,
    "end_char": 48247,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "5.2\n3.142\n3.279\n1.739\n3.032\n0.113\n6.421\n9.562\n4.771\n7.804\nREXEL\nPAR\n3.2\n2.105\n1.782\n1.465\n2.626\n0.184\n3.887\n5.992\n4.091\n6.718\nEURONEXT\nPAR\n6.8\n3.319\n1.943\n1.918\n3.229\n0.164\n5.262\n8.582\n5.146\n8.375\nIMCD GROUP\nAMS\n6.0\n3.328\n0.972\n1.859\n3.016\n0.158\n4.300\n7.118\n4.875\n7.892\nSIGNIFY\nAMS\n4.6\n3.485\n1.181\n0.888\n3.690\n0.252\n4.666\n5.866\n4.579\n4.413\nALTEN\nPAR\n2.8\n3.443\n1.476\n1.017\n2.420\n0.137\n4.919\n8.362\n3.437\n5.857\nBIC\nPAR\n1.9\n3.025\n1.644\n0.916\n3.820\n0.324\n4.669\n6.720\n4.736\n3.746\nEUTELSAT COM\nPAR\n1.9\n2.969\n0.993\n0.677\n3.752\n0.201\n3.962\n5.972\n4.429\n4.068\nINGENICO GROUP\nPAR\n8.5\n1.534\n0.970\n0.740\n2.996\n0.129\n2.504\n4.038\n3.735\n6.461\nEURAZEO\nPAR\n3.3\n4.512\n1.953\n1.536\n3.928\n0.134\n6.464\n10.976\n5.464\n9.393\nAEGON\nAMS\n5.0\n1.552\n0.801\n0.421\n2.561\n0.15\n2.353\n3.905\n2.982\n3.220\nKPN KON\nAMS\n10.0\n2.363\n1.011\n0.584\n3.942\n0.207\n3.373\n5.736\n4.526\n3.413\nRANDSTAD\nAMS\n8.4\n3.071\n1.130\n0.939\n3.603\n0.291\n4.201\n5.016\n4.541\n4.166\nKLEPIERRE REIT\nPAR\n3.5\n3.242\n1.016\n0.503\n3.317\n0.371\n4.259\n3.754\n3.820\n1.858\nSUEZ\nPAR\n9.9\n1.416\n0.866\n0.724\n2.588\n0.250\n2.281\n3.697\n3.312\n3.613\nGALP ENERGIA\nLIS\n6.8\n4.467\n2.173\n1.438\n3.977\n0.484\n6.640\n6.663\n5.415\n4.410\nARKEMA\nPAR\n7.2\n4.292\n1.231\n1.354\n4.441\n0.278\n5.522\n5.659\n5.795\n6.225\nCOVIVIO\nPAR\n5.2\n3.400\n3.485\n2.230\n3.279\n0.304\n6.884\n10.284\n5.509\n8.788\nICADE REIT\nPAR\n3.4\n4.179\n1.649\n1.371\n3.499\n0.231\n5.828\n8.798\n4.870\n7.314\nIPSEN\nPAR\n6.5\n2.515\n1.463\n0.844\n2.594\n0.200\n3.978\n6.493\n3.439\n5.072\nORPEA\nPAR\n5.9\n1.605\n0.786\n0.987\n1.683\n0.126\n2.391\n3.997\n2.671\n4.354\nSCOR\nPAR\n4.5\n3.363\n1.475\n0.941\n3.944\n0.378\n4.837\n5.372\n4.885\n3.427\nGETLINK\nPAR\n6.4\n1.910\n1.341\n1.166\n3.294\n0.148\n3.251\n5.161\n4.460\n7.753\nJ.MARTINS SGPS\nLIS\n9.2\n4.043\n1.022\n1.027\n3.621\n0.276\n5.065\n4.730\n4.648\n4.752\nDASSAULT AVIAT\nPAR\n6.3\n4.021\n1.155\n0.764\n4.088\n0.282\n5.176\n5.247\n4.852\n3.471\nEDENRED\nPAR\n10.1\n3.832\n2.070\n2.352\n3.763\n0.282\n5.902\n9.419\n6.115\n9.879\nPUBLICIS GROUPE\nPAR\n7.6\n2.600\n1.251\n0.704\n3.221\n0.390\n3.851\n4.462\n3.925\n2.509\nATOS\nPAR\n7.5\n2.323\n0.834\n0.499\n2.449\n0.267\n3.157\n3.959\n2.948\n2.368\nJCDECAUX\nPAR\n2.9\n3.682\n1.578\n1.177\n3.845\n0.229\n5.260\n8.482\n5.022\n6.323\nEIFFAGE\nPAR\n6.9\n4.086\n1.324\n1.785\n4.083\n0.248\n5.410\n6.656\n5.868\n8.973\nGECINA\nPAR\n7.8\n2.988\n2.394\n2.004\n2.928\n0.321\n5.382\n8.370\n4.932\n7.860\nNATIXIS\nPAR\n6.5\n0.763\n0.513\n0.524\n1.672\n0.155\n1.276\n2.040\n2.196\n3.868\nSES FDR\nPAR\n3.0\n3.138\n0.649\n0.905\n3.218\n0.186\n3.786\n4.132\n4.123\n5.768\nSEB\nPAR\n7.6\n3.998\n1.284\n0.998\n3.807\n0.215\n5.282\n7.246\n4.805\n5.629\nUBISOFT\nPAR\n10.2\n2.814\n0.870\n0.578\n3.635\n0.204\n3.684\n5.136\n4.213\n3.409\nALSTOM\nPAR\n9.4\n1.988\n0.715\n0.866\n2.955\n0.279\n2.703\n3.280\n3.821\n3.975\nTECHNIPFMC\nPAR\n2.6\n1.113\n0.729\n0.552\n2.478\n0.217\n1.842\n2.955\n3.029\n3.097\nACCOR\nPAR\n5.9\n2.384\n0.677\n0.645\n3.079\n0.251\n3.061\n3.374\n3.724\n3.215\nVEOLIA\nPAR\n9.8\n1.476\n0.838\n0.484\n2.598\n0.267\n2.314\n3.791\n3.083\n2.296\nCOLRUYT\nBRU\n7.3\n3.340\n1.373\n1.245\n3.390\n0.318\n4.713\n5.689\n4.635\n5.159\nAGEAS\nBRU\n6.7\n2.408\n1.115\n1.254\n2.735\n0.321\n3.524\n4.585\n3.989",
    "chunk_index": 20,
    "start_char": 47792,
    "end_char": 50670,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "2.814\n0.870\n0.578\n3.635\n0.204\n3.684\n5.136\n4.213\n3.409\nALSTOM\nPAR\n9.4\n1.988\n0.715\n0.866\n2.955\n0.279\n2.703\n3.280\n3.821\n3.975\nTECHNIPFMC\nPAR\n2.6\n1.113\n0.729\n0.552\n2.478\n0.217\n1.842\n2.955\n3.029\n3.097\nACCOR\nPAR\n5.9\n2.384\n0.677\n0.645\n3.079\n0.251\n3.061\n3.374\n3.724\n3.215\nVEOLIA\nPAR\n9.8\n1.476\n0.838\n0.484\n2.598\n0.267\n2.314\n3.791\n3.083\n2.296\nCOLRUYT\nBRU\n7.3\n3.340\n1.373\n1.245\n3.390\n0.318\n4.713\n5.689\n4.635\n5.159\nAGEAS\nBRU\n6.7\n2.408\n1.115\n1.254\n2.735\n0.321\n3.524\n4.585\n3.989\n5.155\nSOLVAY\nBRU\n8.0\n1.866\n0.856\n0.440\n1.241\n0.193\n2.722\n4.589\n1.681\n2.725\nUMICORE\nBRU\n8.9\n2.428\n0.834\n0.486\n2.003\n0.314\n3.262\n3.490\n2.489\n2.033\nPROXIMUS\nBRU\n5.2\n2.618\n0.845\n0.666\n2.820\n0.271\n3.463\n3.964\n3.486\n3.126\nABN AMRO BANK\nAMS\n6.9\n1.989\n1.216\n0.627\n3.004\n0.195\n3.205\n5.194\n3.631\n3.838\nCNP ASSURANCES\nPAR\n7.3\n3.899\n1.615\n2.046\n2.756\n0.381\n5.514\n5.855\n4.802\n7.418\nUNIBAIL RODAMCO\nAMS\n5.7\n1.983\n1.368\n1.052\n1.947\n0.390\n3.351\n4.876\n2.999\n3.750\nSODEXO\nPAR\n8.8\n3.559\n1.852\n1.696\n3.668\n0.245\n5.411\n8.970\n5.364\n8.629\nAverage\n-\n6.4\n2.84\n1.30\n1.06\n3.12\n0.24\n4.14\n5.76\n4.17\n5.19\nTable 2: Table of results, for the 50 stocks in our sample with the lower market cap. The\ncolumn Exch. displays the exchange the stock is traded on (Amsterdam, Paris, Brussels\nor Lisbon) and the column Mcap shows the market capitalization of the stock in billions\nof euros (in October 2020). Then, aA and aB are the estimated tail exponents of sell\nand buy limit order distributions, for the left (l) and right (r) tail. c is the estimator\nfor the constant in equation (9) and a = aA + aB without market orders (no MO), and\na = min((c+1)aA\nc\n, aA + 2aB) with market orders (MO), both displayed for left (l) and\nright (r) tails.\n22\n\nStock\nExch.\nMcap\naA(l)\naB(l)\naA(r)\naB(r)\nc\na(l)\nno MO\na(l)\nMO\na(r)\nno MO\na(r)\nMO\nAMUNDI\nPAR\n12.3\n2.066\n1.544\n0.816\n2.496\n0.110\n3.610\n5.676\n3.311\n5.807\nBIOMERIEUX ORD\nPAR\n16.4\n2.815\n1.595\n2.294\n3.312\n0.247\n4.410\n7.225\n5.606\n8.917\nNN GROUP\nAMS\n10.9\n3.187\n1.051\n1.010\n3.797\n0.376\n4.238\n3.847\n4.807\n3.698\nSARTORIUS\nPAR\n28.9\n2.928\n1.069\n1.449\n2.474\n0.280\n3.997\n4.892\n3.923\n6.397\nWORLDLINE\nPAR\n13.2\n2.386\n1.001\n1.131\n2.902\n0.241\n3.386\n5.146\n4.034\n5.819\nEDP\nLIS\n18.0\n3.236\n0.710\n1.043\n3.683\n0.341\n3.946\n2.793\n4.727\n4.106\nTELEPERFORMANCE\nPAR\n16.1\n3.869\n1.862\n1.256\n4.010\n0.134\n5.731\n9.601\n5.266\n9.276\nBOUYGUES\nPAR\n11.7\n2.302\n1.124\n0.953\n2.452\n0.331\n3.425\n4.518\n3.405\n3.833\nAHOLD DEL\nAMS\n26.6\n1.334\n0.727\n0.875\n2.234\n0.326\n2.061\n2.960\n3.108\n3.562\nAKZO NOBEL\nAMS\n17.7\n2.678\n1.921\n1.190\n2.669\n0.292\n4.599\n7.276\n3.859\n5.268\nASML HOLDING\nAMS\n138.3\n2.497\n1.165\n2.369\n1.073\n0.329\n3.662\n4.704\n3.442\n4.334\nDSM KON\nAMS\n24.9\n3.283\n1.333\n1.158\n3.381\n0.312\n4.616\n5.601\n4.539\n4.866\nHEINEKEN\nAMS\n45.4\n3.917\n1.101\n1.146\n3.833\n0.270\n5.018\n5.177\n4.979\n5.391\nING GROEP\nAMS\n24.7\n1.344\n1.067\n0.419\n2.333\n0.139\n2.411\n3.755\n2.752\n3.428\nPHILIPS KON\nAMS\n37.5\n3.129\n0.498\n0.884",
    "chunk_index": 21,
    "start_char": 50206,
    "end_char": 53021,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "1.334\n0.727\n0.875\n2.234\n0.326\n2.061\n2.960\n3.108\n3.562\nAKZO NOBEL\nAMS\n17.7\n2.678\n1.921\n1.190\n2.669\n0.292\n4.599\n7.276\n3.859\n5.268\nASML HOLDING\nAMS\n138.3\n2.497\n1.165\n2.369\n1.073\n0.329\n3.662\n4.704\n3.442\n4.334\nDSM KON\nAMS\n24.9\n3.283\n1.333\n1.158\n3.381\n0.312\n4.616\n5.601\n4.539\n4.866\nHEINEKEN\nAMS\n45.4\n3.917\n1.101\n1.146\n3.833\n0.270\n5.018\n5.177\n4.979\n5.391\nING GROEP\nAMS\n24.7\n1.344\n1.067\n0.419\n2.333\n0.139\n2.411\n3.755\n2.752\n3.428\nPHILIPS KON\nAMS\n37.5\n3.129\n0.498\n0.884\n3.566\n0.318\n3.626\n2.063\n4.450\n3.666\nUNILEVER\nAMS\n138.7\n2.929\n0.619\n1.417\n2.879\n0.322\n3.548\n2.543\n4.297\n5.818\nWOLTERS KLUWER\nAMS\n19.3\n3.473\n1.417\n1.823\n2.997\n0.244\n4.891\n7.235\n4.820\n7.817\nDANONE\nPAR\n34.6\n2.195\n1.112\n0.761\n2.312\n0.404\n3.307\n3.862\n3.073\n2.642\nBNP PARIBAS ACT.A\nPAR\n40.2\n1.326\n0.771\n0.636\n2.133\n0.223\n2.098\n3.424\n2.769\n3.496\nAXA\nPAR\n36.1\n0.740\n0.698\n0.548\n1.878\n0.321\n1.438\n2.178\n2.426\n2.253\nSOCIETE GENERALE\nPAR\n10.2\n0.815\n0.923\n0.383\n2.051\n0.160\n1.738\n2.553\n2.434\n2.777\nL\u2019OREAL\nPAR\n163.0\n3.201\n0.685\n1.209\n2.552\n0.270\n3.886\n3.222\n3.760\n5.683\nSANOFI\nPAR\n108.1\n1.245\n0.741\n0.790\n2.080\n0.411\n1.986\n2.544\n2.869\n2.712\nSAINT GOBAIN\nPAR\n20.0\n1.242\n0.936\n0.402\n2.014\n0.237\n2.178\n3.420\n2.416\n2.099\nLEGRAND\nPAR\n18.6\n4.179\n1.304\n1.790\n3.851\n0.328\n5.482\n5.283\n5.641\n7.254\nTOTAL\nPAR\n74.8\n0.592\n0.632\n0.549\n0.534\n0.308\n1.224\n1.816\n1.082\n1.616\nHEINEKEN HOLDING\nAMS\n20.2\n3.198\n1.244\n2.004\n3.506\n0.333\n4.442\n4.982\n5.509\n8.025\nESSILORLUXOTTICA\nPAR\n50.8\n2.433\n0.555\n1.040\n2.691\n0.347\n2.988\n2.155\n3.731\n4.037\nAB INBEV\nBRU\n93.5\n1.800\n0.809\n0.499\n2.441\n0.210\n2.610\n4.410\n2.940\n2.879\nDASSAULT SYSTEM\nPAR\n41.4\n3.381\n1.439\n1.144\n2.779\n0.341\n4.819\n5.653\n3.923\n4.495\nCHRISTIAN DIOR SE\nPAR\n74.0\n3.482\n1.585\n1.860\n4.092\n0.095\n5.068\n8.550\n5.952\n10.044\nARCELORMITTAL\nAMS\n13.4\n0.450\n0.610\n0.566\n2.015\n0.138\n1.060\n1.511\n2.580\n4.595\nSAFRAN\nPAR\n38.7\n2.944\n1.467\n1.154\n2.380\n0.386\n4.411\n5.267\n3.534\n4.141\nENGIE\nPAR\n28.3\n0.485\n0.652\n0.269\n1.846\n0.138\n1.136\n1.621\n2.114\n2.220\nEDF\nPAR\n31.9\n0.789\n0.872\n0.906\n2.429\n0.234\n1.662\n2.451\n3.335\n4.780\nCREDIT AGRICOLE\nPAR\n21.2\n0.642\n0.780\n0.670\n1.729\n0.193\n1.422\n2.065\n2.399\n4.128\nCAPGEMINI\nPAR\n18.5\n2.622\n0.754\n0.747\n2.987\n0.256\n3.377\n3.701\n3.735\n3.667\nAIRBUS\nPAR\n50.4\n2.921\n1.232\n1.320\n3.268\n0.116\n4.153\n7.075\n4.588\n7.856\nORANGE\nPAR\n25.3\n0.480\n0.925\n0.515\n2.162\n0.250\n1.405\n1.885\n2.676\n2.577\nTHALES\nPAR\n13.9\n2.656\n1.241\n0.865\n2.351\n0.317\n3.897\n5.161\n3.216\n3.598\nMICHELIN\nPAR\n16.6\n2.360\n0.740\n0.416\n2.724\n0.320\n3.100\n3.050\n3.139\n1.713\nKERING\nPAR\n73.7\n2.572\n0.864\n0.684\n2.463\n0.332\n3.436\n3.466\n3.146\n2.742\nPERNOD RICARD\nPAR\n37.1\n2.740\n1.090\n1.153\n2.256\n0.297\n3.829\n4.761\n3.409\n5.036\nSCHNEIDER ELECTRIC\nPAR\n58.2\n2.644\n1.534\n1.058\n2.594\n0.326\n4.178\n6.240\n3.652\n4.304\nPEUGEOT\nPAR\n14.2\n1.725\n0.888\n0.805\n2.248\n0.128\n2.613\n4.339\n3.053\n5.301\nROYAL DUTCH SHELL\nAMS\n83.0\n0.677\n0.529\n0.350\n0.753\n0.137\n1.206\n1.883\n1.103\n1.856\nGBL\nBRU\n11.9\n2.952\n1.323\n1.095\n2.457\n0.194\n4.275\n7.227\n3.552\n6.009\nKBC\nBRU\n18.5\n2.124\n0.702\n0.716\n2.881\n0.217\n2.826\n3.944\n3.597\n4.018\nUCB\nBRU\n17.9",
    "chunk_index": 22,
    "start_char": 52562,
    "end_char": 55503,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "2.742\nPERNOD RICARD\nPAR\n37.1\n2.740\n1.090\n1.153\n2.256\n0.297\n3.829\n4.761\n3.409\n5.036\nSCHNEIDER ELECTRIC\nPAR\n58.2\n2.644\n1.534\n1.058\n2.594\n0.326\n4.178\n6.240\n3.652\n4.304\nPEUGEOT\nPAR\n14.2\n1.725\n0.888\n0.805\n2.248\n0.128\n2.613\n4.339\n3.053\n5.301\nROYAL DUTCH SHELL\nAMS\n83.0\n0.677\n0.529\n0.350\n0.753\n0.137\n1.206\n1.883\n1.103\n1.856\nGBL\nBRU\n11.9\n2.952\n1.323\n1.095\n2.457\n0.194\n4.275\n7.227\n3.552\n6.009\nKBC\nBRU\n18.5\n2.124\n0.702\n0.716\n2.881\n0.217\n2.826\n3.944\n3.597\n4.018\nUCB\nBRU\n17.9\n2.607\n0.760\n1.136\n3.077\n0.228\n3.367\n4.101\n4.213\n6.126\nVIVENDI\nPAR\n27.9\n1.968\n0.936\n0.733\n2.806\n0.318\n2.904\n3.874\n3.539\n3.033\nAverage\n-\n39.7\n2.27\n1.02\n1.00\n2.61\n0.26\n3.29\n4.25\n3.61\n4.59\nTable 3:\nTable of results, for the 50 stocks in our sample with the higher market cap.\nThe column Exch. displays the exchange the stock is traded on (Amsterdam, Paris,\nBrussels or Lisbon) and the column Mcap shows the market capitalization of the stock\nin billions of euros (in October 2020). Then, aA and aB are the estimated tail exponents\nof sell and buy limit order distributions, for the left (l) and right (r) tail.\nc is the\nestimator for the constant in equation (9) and a = aA + aB without market orders (no\nMO), and a = min((c+1)aA\nc\n, aA + 2aB) with market orders (MO), both displayed for left\n(l) and right (r) tails.\n23\n\n4\nConclusions\nIn this paper we study the tails of closing auction return distributions, both from a\ntheoretical and empirical point of view, focusing on large closing price \ufb02uctuations. Using\nthe stochastic call auction model of Derksen et al. (2020a), we relate tail exponents of\norder placement distributions and tail exponents of the return distribution. Empirical\nanalysis supports the model\u2019s predictions. In theory, large market orders could be a\ncause of large closing price \ufb02uctuations, but this potential e\ufb00ect is cancelled by limit\norders that are submitted against the direction of the market order imbalance. Instead,\nlimit order placement appears to be the primary cause of observed heavy tails in closing\nauction return distributions.\nReferences\nBak, P., Paczuski. M. and Shubik, M., Price variations in a stock market with many agents.\nPhysica A: Statistical Mechanics and its Applications 246(3-4): 430-453, 1997.\nBouchaud, J. P., M\u00b4ezard, M. and Potters, M., Statistical properties of stock order books: em-\npirical results and models. Quantitative Finance, 2(4):251-256, 2002.\nChakraborti, A., Muni Toke, I., Patriarca, M. and Abergel, F., Econophysics review: I. Empirical\nfacts. Quantitative Finance 11(7): 991-1012, 2011.\nCont, R., Empirical properties of asset returns: Stylized facts and statistical issues. Quantitative\nFinance 1: 223-236, 2001.\nCont, R. and Bouchaud, J. P., Herd behavior and aggregate \ufb02uctuations in \ufb01nancial markets,\nMacroeconomic Dynamics 4: 170-196, 2000.\nDerksen, M., Kleijn, B. and De Vilder, R., Clearing price distributions in call auctions. Quanti-\ntative Finance 20(9): 1475-1493, 2020a.\nDerksen, M., Kleijn, B. and De Vilder, R., E\ufb00ects of MiFID II on stock price formation, 2020b.\nAvailable at SSRN: https://ssrn.com/abstract=3559500.\nEmbrechts, P., Kl\u00a8uppelberg, C.",
    "chunk_index": 23,
    "start_char": 55040,
    "end_char": 58142,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "of asset returns: Stylized facts and statistical issues. Quantitative\nFinance 1: 223-236, 2001.\nCont, R. and Bouchaud, J. P., Herd behavior and aggregate \ufb02uctuations in \ufb01nancial markets,\nMacroeconomic Dynamics 4: 170-196, 2000.\nDerksen, M., Kleijn, B. and De Vilder, R., Clearing price distributions in call auctions. Quanti-\ntative Finance 20(9): 1475-1493, 2020a.\nDerksen, M., Kleijn, B. and De Vilder, R., E\ufb00ects of MiFID II on stock price formation, 2020b.\nAvailable at SSRN: https://ssrn.com/abstract=3559500.\nEmbrechts, P., Kl\u00a8uppelberg, C. and Mikosch, T., Modelling Extremal Events, Springer, 2003.\nEuronext, Euronext harmonized rule book, 2019. Available online at https://www.euronext.\ncom/en/regulation/euronext-regulated-markets.\nFama, E. F., The behavior of stock-market prices. The Journal of Business 38(1): 34-105, 1965.\nFarmer, J. D., Gillemot, L., Lillo, F., Mike, S. and Sen, A., What really causes large price\nchanges? Quantitative Finance 4(4): 383-397, 2004\nGabaix, X., Gopikrishnan, P., Plerou, V. and Stanley, H. E., A theory of power-law distributions\nin \ufb01nancial market \ufb02uctuations. Nature 423: 267-270, 2003.\nGabaix, X., Gopikrishnan, P., Plerou, V. and Stanley, H. E., Institutional Investors and Stock\nMarket Volatility. Quarterly Journal of Economics 121(2): 461-504, 2006.\n24\n\nGopikrishnan, P., Meyer, M., Amaral, L. A. N. and Stanley, H. E., Inverse Cubic Law for the\nProbability Distribution of Stock Price Variations. The European Physical Journal B 3: 139-\n140, 1998.\nGopikrishnan, P., Plerou, V., Amaral, L. A. N., Meyer, M. and Stanley, H. E., Scaling of the\ndistribution of \ufb02uctuations of \ufb01nancial market indices, Physical Review E 60: 5305, 1999.\nGu, G. F., Chen, W. and Zhou, W. X., Empirical distributions of Chinese stock returns at\ndi\ufb00erent microscopic timescales. Physica A 387: 495-502, 2008.\nMalevergne, Y., Pisarenko, V. and Sornette, D., Empirical distributions of stock returns: between\nthe stretched exponential and the power law? Quantitative Finance 5(4):379-401, 2005.\nMandelbrot, B., The variation of certain speculative prices. The Journal of Business 36(4):\n394-419, 1963.\nMike, S. and J. D. Farmer, An empirical behavioral model of liquidity and volatility. Journal of\nEconomic Dynamics and Control 32: 200-234, 2008.\nNewman, M. E. J., Power laws, Pareto distributions and Zipf\u2019s law. Contemporary Physics 46:\n323-351, 2005.\nPagan, A., The econometrics of \ufb01nancial markets. Journal of Empirical Finance 3: 15-102, 1996.\nPlerou, V. and Stanley, H. E., Stock return distributions: Tests of scaling and universality from\nthree distinct stock markets. Physical Review E 77, 037101, 2008.\nWeber, P. and Rosenow, B., Large stock price changes: volume or liquidity? Quantitative Finance\n6 (1): 7-14, 2006.\nZovko, I. and Farmer, J. D., The power of patience: a behavioural regularity in limit-order\nplacement. Quantitative Finance 2(5): 387-392, 2002.\nA\nProofs\nProposition 2.5 Under assumption 1, we have\nP(X > M|NA, NB) \u223cNBTB(M)TA(M)NA, as M \u2192\u221e.\n(7)\n25\n\nProof The expression for the conditional distribution of X in equation (6), implies\nlim\nM\u2192\u221e\nP(X > M|NA, NB)\nTB(M)TA(M)NA\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u00121 \u2212FA(M)\nTA(M)\n\u0013NA\n(1 \u2212FA(M))\u2212k\n\u00d7\n\u00121 \u2212FB(M)\nTB(M)\n\u0013\n(1 \u2212FB(M))l\u22121\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\nlim\nM\u2192\u221e\n\u0012NA\nk",
    "chunk_index": 24,
    "start_char": 57596,
    "end_char": 60868,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "2(5): 387-392, 2002.\nA\nProofs\nProposition 2.5 Under assumption 1, we have\nP(X > M|NA, NB) \u223cNBTB(M)TA(M)NA, as M \u2192\u221e.\n(7)\n25\n\nProof The expression for the conditional distribution of X in equation (6), implies\nlim\nM\u2192\u221e\nP(X > M|NA, NB)\nTB(M)TA(M)NA\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u00121 \u2212FA(M)\nTA(M)\n\u0013NA\n(1 \u2212FA(M))\u2212k\n\u00d7\n\u00121 \u2212FB(M)\nTB(M)\n\u0013\n(1 \u2212FB(M))l\u22121\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u00121 \u2212FB(M)\n1 \u2212FA(M)\n\u0013k\n(1 \u2212FB(M))l\u22121\u2212k\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u0012TB(M)\nTA(M)\n\u0013k\n(1 \u2212FB(M))l\u22121\u2212k\n= NB,\nwhere we exchange limit and sum by dominated convergence, and the last line follows\nbecause all terms are 0, except when l = 1, k = 0.\n\u25a1\nTheorem 2.7 (Right tail of the lower clearing price distribution) Under assumption 1\nwe have\nP(X > M) \u223cCTA(M)TB(M), as M \u2192\u221e,\nwhere C = PN\nn=1 npNA,NB(1, n) = E[NB1{NA=1}] > 0.\nProof The result of proposition 2.5 implies\nlim\nM\u2192\u221e\nP(X > M)\nCTA(M)TB(M) = lim\nM\u2192\u221e\nENBTB(M)TA(M)NA\nCTA(M)TB(M)\n= lim\nM\u2192\u221e\nPN\ni=1\nPN\nj=1 pNA,NB(i, j)jTB(M)TA(M)i\nCTA(M)TB(M)\n=\nN\nX\ni=1\nN\nX\nj=1\njpNA,NB(i, j)\nC\nlim\nM\u2192\u221eTA(M)i\u22121\n=\nPN\nj=1 jpNA,NB(1, j)\nC\n= 1,\nwhere sum and limit are exchanged by dominated convergence and the last line follows\nby the fact that all terms in the sum are 0, except for i = 1.\n\u25a1\nProposition 2.8 Under assumptions 1 and 2, we have\nP(X > M|NA, NB, \u2206) \u223cK(NA, NB, \u2206\u22121)TB(M)(\u2206\u22121)\u2212TA(M)NA\u2212(\u2206\u22121)+,\n(8)\nas M \u2192\u221e, where\nK(NA, NB, \u2206) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u0000NA\n\u2206\n\u0001\nif \u2206> 0\n\u0000NB\n\u2212\u2206\n\u0001\nif \u2206\u22640\n.\n26\n\nProof Suppose \ufb01rst that \u2206\u22121 > 0. Then\nlim\nM\u2192\u221e\nP(X > M|NA, NB, \u2206)\nTA(M)NA\u2212(\u2206\u22121)\n=\nNA\nX\nk=0\nNB\nX\nl=max(k\u2212\u2206+1,0)\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u00121 \u2212FA(M)\nTA(M)\n\u0013NA\u2212(\u2206\u22121)\n\u00d7 (1 \u2212FA(M))\u2206\u22121\u2212k(1 \u2212FB(M))l\n=\nNA\nX\nk=0\nNB\nX\nl=max(k\u2212\u2206+1,0)\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013\nTB(M)l\nTA(M)k\u2212\u2206+1\n=\n\u0012 NA\n\u2206\u22121\n\u0013\n,\nwhere the last line follows because all terms are 0, except when k = \u2206\u22121, l = 0.\nNow let \u2206\u22121 < 0, then\nlim\nM\u2192\u221e\nP(X > M|NA, NB, \u2206)\nTA(M)NATB(M)1\u2212\u2206\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\u2212\u2206\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013 \u00121 \u2212FA(M)\nTA(M)\n\u0013NA\u2212k\n\u00d7 TA(M)\u2212k\n\u00121 \u2212FB(M)\nTB(M)\n\u0013l\nTB(M)l+\u2206\u22121\n=\nNA\nX\nk=0\nNB\nX\nl=k+1\u2212\u2206\nlim\nM\u2192\u221e\n\u0012NA\nk\n\u0013\u0012NB\nl\n\u0013TB(M)l+\u2206\u22121\nTA(M)k\n=\n\u0012 NB\n1 \u2212\u2206\n\u0013\n,\nwhere the last line follows because all terms are 0, except when k = 0, l = 1 \u2212\u2206.\n\u25a1\nTheorem 2.9 (Right tail of the lower clearing price distribution with market orders)\nUnder assumptions 3 and 4, there exists a constant C > 0, such that\nP(X > M) \u223cCM\u2212a, as M \u2192\u221e,\nwhere\na = min\n\u0012(c + 1)aA\nc\n, aA + 2aB\n\u0013\n.",
    "chunk_index": 25,
    "start_char": 60476,
    "end_char": 62817,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "k\n\u0013\u0012NB\nl\n\u0013TB(M)l+\u2206\u22121\nTA(M)k\n=\n\u0012 NB\n1 \u2212\u2206\n\u0013\n,\nwhere the last line follows because all terms are 0, except when k = 0, l = 1 \u2212\u2206.\n\u25a1\nTheorem 2.9 (Right tail of the lower clearing price distribution with market orders)\nUnder assumptions 3 and 4, there exists a constant C > 0, such that\nP(X > M) \u223cCM\u2212a, as M \u2192\u221e,\nwhere\na = min\n\u0012(c + 1)aA\nc\n, aA + 2aB\n\u0013\n.\n(10)\nProof Under assumptions 3 and 4, proposition 2.8 transforms into,\nP(X > M) \u223c\nN\nX\nn=1\nN\nX\nm=1\nN\nX\nd=\u2212N\nK(n, m, d \u22121)M\u2212(aA(n\u2212d+1)+(aB\u2212aA) max(\u2212d+1,0))p(n, m, d),\nas M \u2192\u221e. Here, we used that max(\u2212x, 0) \u2212max(x, 0) = \u2212x, for all x \u2208R. By noting\nthat K(n, m, d) is bounded from above and below (by\n\u0000 N\nN/2\n\u0001\nand 1), we see that the\n27\n\nstatement of the theorem holds true, for\na =\nmin\nn,d: p(n,d)>0(aA(n \u2212d + 1) \u2212(d \u22121)(aB \u2212aA)1{d<1}),\nwhere the minimum is taken over all n, d such that p(n, d) = P\nm p(n, m, d) > 0. Now\nnote that the function F(n, d) := aA(n \u2212d + 1) \u2212(d \u22121)(aB \u2212aA)1{d<1} is increasing\nin n, for every d. So the minimum is attained for the lowest n with positive probability.\nRecall that we assumed \u2206= c(NA \u2212NB) and NB \u2208{1, . . . , N}, so p(n, d) = 0 for\nn < d\nc + 1, so the lowest n with positive probability is \u02c6n(d) = max(d\nc + 1, 1), for given d.\nInserting into F leads to\nF(\u02c6n(d), d) =\n\uf8f1\n\uf8f2\n\uf8f3\naA \u2212(d \u22121)aB\nif d \u2264\u22121\naA((1\nc \u22121)d + 2)\nif d \u22651\n,\nwhich is minimal for d = \u00b11, proving that\na = min\n\u0012\naA\n\u00121\nc + 1\n\u0013\n, aA + 2aB\n\u0013\n= min\n\u0012(c + 1)aA\nc\n, aA + 2aB\n\u0013\n.\n\u25a1\n28",
    "chunk_index": 26,
    "start_char": 62470,
    "end_char": 63899,
    "paper_title": "Heavy tailed distributions in closing auctions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Heavy_tailed_distributions_in_closing_auctions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Heavy_tailed_distributions_in_closing_auctions.pdf"
  },
  {
    "text": "1\nModelling bid-ask spread conditional distributions\nusing hierarchical correlation reconstruction\nJaros\u0142aw Duda1 Robert Syrek2 Henryk Gurgul3\n1 Institute of Computer Science, Jagiellonian University, Krak\u00b4ow, Poland. Email: jaroslaw.duda@uj.edu.pl\n2 Institute of Economics, Finance and Management, Jagiellonian University, Krak\u00b4ow, Poland\n3 Department of Applications of Mathematics in Economics, Faculty of Management, AGH University of\nScience and Technology, ul. Gramatyka 10, 30-067 Krak\u00b4ow, Poland\nAbstract\u2014While we would like to predict exact values,\navailable incomplete information is rarely suf\ufb01cient - usually\nallowing only to predict conditional probability distributions.\nThis article discusses hierarchical correlation reconstruction\n(HCR) methodology for such prediction on example of usually\nunavailable bid-ask spreads, predicted from more accessible\ndata like closing price, volume, high/low price, returns. In\nHCR methodology we \ufb01rst normalize marginal distributions\nto nearly uniform like in copula theory. Then we model (joint)\ndensities as linear combinations of orthonormal polynomials,\ngetting its decomposition into (mixed) moments. Then here\nwe model each moment (separately) of predicted variable as a\nlinear combination of mixed moments of known variables using\nleast squares linear regression - getting accurate description\nwith interpretable coef\ufb01cients describing linear relations be-\ntween moments. Combining such predicted moments we get\npredicted density as a polynomial, for which we can e.g. calcu-\nlate expected value, but also variance to evaluate uncertainty\nof such prediction, or we can use the entire distribution e.g.\nfor more accurate further calculations or generating random\nvalues. There were performed 10-fold cross-validation log-\nlikelihood tests for 22 DAX companies, leading to very accurate\npredictions, especially when using individual models for each\ncompany as there were found large differences between their\nbehaviors. Additional advantage of the discussed methodology\nis being computationally inexpensive, \ufb01nding and evaluation\na model with hundreds of parameters and thousands of data\npoints takes a second on a laptop.\nKeywords: machine learning, conditional probability dis-\ntribution, econometrics, bid-ask spread, liquidity\nI. INTRODUCTION\nWhile it is more convenient to work on exact values, real\nlife predictions usually have some uncertainty, controlling\nof which could allow e.g. to distinguish nearly certain\npredictions from the practically worthless ones. Generally,\nwanting to predict Y variable from X = (X1, . . . , Xd)\nvariables, if there is no a strict relation, they often come from\nsome complicated joint probability distribution - knowing\nX = x, we can only predict Pr(Y |X = x) conditional\nprobability distribution. This article discusses such predic-\ntion of conditional probability distributions on example of\nbid-ask spreads, which is often publicly unavailable, from a\nFigure 1. General concept, some \ufb01rsts functions of the used 1 and 2 dimen-\nsional basis of orthornormal polynomials (fj1j2(x) = fj1(x1)fj2(x2)),\nand application example. For simplicity we assume working on variables\nnormalized to nearly uniform marginal densities on [0, 1] as e.g. in copula\ntheory. We would like to model distortion from this uniform distribution\nfor predicted variable Y based on the context X: as a linear combination\ne.g. of orthornormal polynomials here, for which coef\ufb01cients have similar\ninterpretation as moments/cumulants: a1 shifts right/left like expected\nvalue, a2 increases/decreases probability of extreme values as variance etc.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3613,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "1 and 2 dimen-\nsional basis of orthornormal polynomials (fj1j2(x) = fj1(x1)fj2(x2)),\nand application example. For simplicity we assume working on variables\nnormalized to nearly uniform marginal densities on [0, 1] as e.g. in copula\ntheory. We would like to model distortion from this uniform distribution\nfor predicted variable Y based on the context X: as a linear combination\ne.g. of orthornormal polynomials here, for which coef\ufb01cients have similar\ninterpretation as moments/cumulants: a1 shifts right/left like expected\nvalue, a2 increases/decreases probability of extreme values as variance etc.\nEach such coef\ufb01cient is separately modelled using analogous coef\ufb01cients\nof X variable: \u02dc\u03c1(y|x) = P\nj fj(y) P\nk \u03b2jkfk(x), e.g. using least-squares\nlinear regression here. Such predicted density as polynomial sometimes gets\nbelow zero, hence there is used e.g. \u03c1 = max(\u02dc\u03c1, 0.03)/N instead, with\nN normalization factor to integrate to 1. Example of application of such\npredicted density is just taking its expected value, getting a conservative\nprediction of value (avoiding extremes), also with estimated uncertainty if\nadditionally calculating variance of the predicted density.\nfew variables which are available. There is used hierarchical\ncorrelation reconstruction (HCR) [1] methodology as brie\ufb02y\npresented in Fig. 1: \ufb01rst normalize all variables to nearly\nuniform distribution like in copula theory [2], then model\ndensities as polynomials using basis of orthonormal poly-\nnomials - for which coef\ufb01cients are analogous to (mixed)\nmoments. Then predict such coef\ufb01cients of density of Y\narXiv:1911.02361v1 [q-fin.TR] 4 Nov 2019\n\n2\nFigure 2.\nTop: examples of predicted conditional densities - predicted\n\u02dc\u03c1(y|xi) = P\nj fj(y) P\nk \u03b2jkfk(xi) polynomial for i-th datapoint under-\ngoes \u03c1 = max(\u02dc\u03c1, 0.03)/N to remove negative densities, and normalization\nto integrate to 1 =\nR 1\n0 \u03c1(y|x)dy. Each diagram contains 10 example\npredictions, vertical lines show the actual values (yi, \u03c1(yi|xi)): the higher\nthe better prediction, without prediction all would have height 1. There\nwere chosen companies having best/worst prediction. The best ones predict\nmainly narrow unimodal distributions in line with the actual values, weaker\nones can rather only predict wide often multimodal distributions. We can\nsee rapid growths at the ends - they are likely artifacts of using polynomials,\ntheir additional removal might improve prediction. Bottom: their sorted\npredicted densities in the actual values {\u03c1(yi|xi)}i, with marked gray\n\u03c1 = 1 line of using no prediction and green exp(log-likelihood) line\ncorresponding to average improvement over no prediction. The points are\nof different colors denoting one of 10 rounds of 10-fold cross-validation.\nfrom coef\ufb01cients of x, for example using least-squares linear\nregression here, separately for each modelled moment of Y .\nThe evaluation is performed using 10-fold cross-validation\nfor log-likelihood of normalized variables: average natural\nlogarithm of predicted conditional density in the actual\nvalue, what can be interpreted as estimated minus condi-\ntional entropy EXY (ln(\u03c1(Y |X))) = \u2212H(Y |X). Figure\n2 contains examples of such predicted densities, Figure 3\ncontains main evaluation of the used variables and models.\nHaving predicted conditional probability distributions, a\nbasic application can be just taking expected values - getting\nconservative predictions of values, e.g.",
    "chunk_index": 1,
    "start_char": 3013,
    "end_char": 6429,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "for each modelled moment of Y .\nThe evaluation is performed using 10-fold cross-validation\nfor log-likelihood of normalized variables: average natural\nlogarithm of predicted conditional density in the actual\nvalue, what can be interpreted as estimated minus condi-\ntional entropy EXY (ln(\u03c1(Y |X))) = \u2212H(Y |X). Figure\n2 contains examples of such predicted densities, Figure 3\ncontains main evaluation of the used variables and models.\nHaving predicted conditional probability distributions, a\nbasic application can be just taking expected values - getting\nconservative predictions of values, e.g. avoiding predicting\nextremes, as presented in Figure 7. We can additionally\ncalculate variance of such predicted density to estimate\nuncertainty of value predicted as expected value. We can\nalso handle more sophisticated situations like bimodal distri-\nbution with two (or more) separate maxima: when pointing\nexpected value might not be a good choice (can have\nmuch lower density), a better prediction might be e.g. one\nof the maxima, or maybe both: providing prediction as\nalternative of two (or more) possibilities. Finally, we can\nalso work on complete predicted densities, e.g. to generate\nFigure 3. Top: Log-likelihoods from 10-fold cross-validation for individual\nmodels for companies using various types of information, e.g. \u2019123\u2019\ndenotes using all basic 1,2,3 variables, where \u20191\u2019 denotes closing price\n(P), \u20192\u2019 volume (V ), and \u20193\u2019 difference between high and low price\nnormalized by dividing by closing price: (H \u2212L)/P. The \u2019+\u2019 denotes\nusing also 3 additional variables: depth, midpoint changes intraday and\nmidpoint volatility. The last column presents averaged evaluation for using\ncommon model for all data. Bottom: examples of pairwise dependencies in\ndataset for the 3 variables (columns) for the least and the most dependent\ncompanies for a given variable (heights of corresponding dots). For example\nvolume (\u20192\u2019) does not help for RWEG (nearly uncorrelated - blue dot is near\nzero), but is useful for FREG. We can see that there are large differences\nbetween companies, hence we will mostly focus on building individual\nmodels for each company.\nits random values for Monte-Carlo methods, or processing\nsuch entire probability distributions through some further\nfunctions for their more accurate modelling, as e.g. generally\nE(f(X)) \u0338= f(E(X)) for nonlinear functions: expected\nvalue of function is not equal function of expected value.\nTo model such conditional distributions we will use\nHCR methodology, which combines advantages of classical\nstatistics and machine learning. While the former allows\nfor well controlled and interpretable but relatively small\n(rough) models/descriptions, machine learning allows for\nvery accurate descriptions using huge models, but usually\nlacks uniqueness of solution, control and interpretability\nof coef\ufb01cients, and often is computationally costly. HCR\nallows to inexpensively work on huge models obtained from\n\n3\n(unique) least-squares optimization, using well interpretable\ncoef\ufb01cients: as mixed moments of variables, starting e.g.\nwith moments of single variables and correlations of coef-\n\ufb01cients.\nMore speci\ufb01cally, HCR conveniently starts with normal-\nization of all marginal distributions to nearly uniform dis-\ntributions like in copula theory - so they can be interpreted\nas quantiles, compactifying tails problematic for linear re-\ngression.",
    "chunk_index": 2,
    "start_char": 5834,
    "end_char": 9236,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "and interpretability\nof coef\ufb01cients, and often is computationally costly. HCR\nallows to inexpensively work on huge models obtained from\n\n3\n(unique) least-squares optimization, using well interpretable\ncoef\ufb01cients: as mixed moments of variables, starting e.g.\nwith moments of single variables and correlations of coef-\n\ufb01cients.\nMore speci\ufb01cally, HCR conveniently starts with normal-\nization of all marginal distributions to nearly uniform dis-\ntributions like in copula theory - so they can be interpreted\nas quantiles, compactifying tails problematic for linear re-\ngression. Now we can model distortion from uniform distri-\nbution on this [0, 1]d hypercube with a linear combination,\ne.g. of orthonormal polynomials, for which coef\ufb01cients can\nbe interpreted analogously to (mixed) moments. E.g. for\n3 variables, \u2019000\u2019 coef\ufb01cient is always 1 - corresponding\nto normalization, \u2019100\u2019 is analogous to expected value of\n1st variable, \u2019020\u2019 to variance of 2nd variable, \u2019011\u2019 to\ncorrelation coef\ufb01cient between 2nd and 3rd variable, \u2019202\u2019 is\nlarge if with large variance of 1st variable there comes large\nvariance of 3rd variable (like heteroskedasticity in ARCH\nmodel), and so on also for higher moments and depen-\ndencies between 3 or more variables, getting hierarchical\ndecomposition of statistical dependencies (joint distribution)\ninto mixed moments.\nWhile we could directly extract and exploit (X, Y ) joint\ndistribution with HCR, experimental tests have shown that\nalternative approach from [3] gives slightly better evaluation\nby extracting additional dependencies, hence we will focus\nonly on it: use separate bases of (mixed) moments for Y, X\nand predict each considered coef\ufb01cient for Y with least\nsquares linear regression using coef\ufb01cients of X. While [3]\nhas used only moments of separate variables for X, here we\nexpand this methodology by using also their mixed moments\n- starting with \u201911\u2019 correlation-like coef\ufb01cient.\nIts advantage over modelling joint distribution is being\nable to notice and exploit e.g. that difference of two\nvariables has some useful relation with the predicted\nvariable. Looking e.g. at RWEG in Fig. 3, \u20192\u2019-nd variable is\npractically noise, its (blue) dot is nearly zero. However, the\ndifference between \u201913\u2019 and \u2019123\u2019 model (red and orange\ndot) is much larger: relations with other variables allowed\nto extract more information from this noise.\nIn the discussed example we would like to predict con-\nditional probability distributions for (nearly inaccessible)\nbid-ask spreads (relative quoted) from more available in-\nformation. The basic considered \u2019123\u2019 model began with\n5 classically used variables: closing price (P), high and\nlow value (H, L), volume (V ) and log-return (R). Sur-\nprisingly, it has turned out that using R and L alone did\nnot help improving evaluation (log-likelihood in 10-fold\ncross-validation), hence \ufb01nally there were used 3 variables\nP, V, (H \u2212L)/P. The second considered \u2019123+\u2019 model\ncomplements this information with other relatively available\nvariables, searching through which has lead to \ufb01nal use\nof 3 additional variables: (market) depth, midpoint changes\nintraday, and midpoint volatility.\nThe choice of basis of moments is a dif\ufb01cult question: too\nsmall leads to under\ufb01tting by not being able to express de-\npendencies of",
    "chunk_index": 3,
    "start_char": 8661,
    "end_char": 11955,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "and L alone did\nnot help improving evaluation (log-likelihood in 10-fold\ncross-validation), hence \ufb01nally there were used 3 variables\nP, V, (H \u2212L)/P. The second considered \u2019123+\u2019 model\ncomplements this information with other relatively available\nvariables, searching through which has lead to \ufb01nal use\nof 3 additional variables: (market) depth, midpoint changes\nintraday, and midpoint volatility.\nThe choice of basis of moments is a dif\ufb01cult question: too\nsmall leads to under\ufb01tting by not being able to express de-\npendencies of data behavior, too large leads to over\ufb01tting by\nrepresenting features of training set which do not generalize\nto test set. For \u2019123\u2019 and \u2019123+\u2019 models there was chosen\na compromise to optimize for all companies: for \u2019123\u2019\npredicting 8 \ufb01rst moments of Y using 53 mixed moments\nof 3 variables of X, hence using 8 \u00b7 53 = 424 coef\ufb01cient\nmodels. For \u2019123+\u2019 predicting 6 moments from 205 mixed\nmoments of 6 variables of X, getting 6 \u00b7 205 = 1230\ncoef\ufb01cient models.\nThe used data is for 22 DAX companies for which large\nenough dataset was available, arbitrarily chosen as contain-\ning at least 2000 daily datapoints. As there were observed\nlarge difference between models for different companies,\ncorresponding to different behaviors of traders of its stock,\nthere were mainly considered individual models for each\ncompany. There was also performed hierarchical search for\ncombinations of companies for which using common model\nleads to the smallest loss of evaluation, however, such loss\noften turns out signi\ufb01cant.\nII. DATASET AND BASIC CONCEPTS\nThis section discusses dataset and reminds standard con-\ncepts, to be used for building the used methodology in the\nnext Section.\nA. Dataset and variables\nThere was used daily data for DAX companies from\n1999-2013 period (source in Acknowledgment), selected as\nhaving available at least 2000 datapoints: Deutsche Telekom\n(DTEG), Daimler (DAIG), SAP (SAPG), Siemens (SIEG),\nDeutsche Post (DPWG), Allianz (ALVG), Bay Motoren\n(BMW), In\ufb01neon (IFXG), Volkswagen (VOWG), Fresenius\n(FREG), Henkel (HNKG), Continental (CONG), Merck\n(MRCG), Muench. Rueckvers (MUVG), Deutsche Boerse\n(DB1G), Lufthansa (LHAG), Fresen Med Care (FME),\nDeutsche Bank (DBKG), Heidelbergcement (HEIG), RWE\n(RWEG), Beiersdorf (BEIG), Theyssenkrupp (TKA).\nThe basic set of variables is P - closing price, V - volume,\nR - return, H, L - high/low price. However, it has turned\nout that trying to exploit dependence on R and L alone\ndid improve evaluation, hence \ufb01nally the basic considered\nmodel: \u2019123\u2019 uses only P as \u20191\u2019-st variable, V as \u20192\u2019nd-\nvariable and normalized (H \u2212L)/P as \u20193\u2019-rd variable.\nThere were also performed trials to improve the prediction\nby using information from some additional relatively avail-\nable variables - 3 were found helpful in predictions: (market)\ndepth, midpoint changes intraday and midpoint volatility.\n\n4\nB. Bid-ask spread and some its standard predictors\nBid-ask spread is the difference between the lowest asking\nprice (ask, offered by a seller) and the highest bid price (bid,\noffered by a buyer). While this value is important because it\nis a main measure of market quality [4], [5], this information\nis usually publicly unavailable.",
    "chunk_index": 4,
    "start_char": 11427,
    "end_char": 14636,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "the prediction\nby using information from some additional relatively avail-\nable variables - 3 were found helpful in predictions: (market)\ndepth, midpoint changes intraday and midpoint volatility.\n\n4\nB. Bid-ask spread and some its standard predictors\nBid-ask spread is the difference between the lowest asking\nprice (ask, offered by a seller) and the highest bid price (bid,\noffered by a buyer). While this value is important because it\nis a main measure of market quality [4], [5], this information\nis usually publicly unavailable. Therefore, there is an interest\nin being able to predict this value based on other, more\naccessible data.\nMore speci\ufb01cally, we work on relative quoted spread,\nwhich is normalized by dividing by midpoint (ask+bid)/2:\nS =\nask \u2212bid\n(ask + bid)/2\n(1)\nSimple examples of its predictor based on the 5 basic\nvariables are AMI [6], [7], HLR [8]:\nAMI = ln\n\u0012\n1 +\n|R|\nP \u00b7 V\n\u0013\n(2)\nHLR = 2H \u2212L\nH + L\n(3)\nThey are intended for a simpler task than discussed: to pre-\ndict values, while here we want to predict entire conditional\nprobability distributions. We can reduce predicted probabil-\nity distributions into predicted values e.g. as expected value,\nmedian, or positions of maxima (especially for multimodal\ndistributions). Figure 7 presents comparisons using such\npredictions reduced with expected value.\nHowever, in practice such prediction is often further\nprocessed through some functions, generally E(f(X)) \u0338=\nf(E(X)) for nonlinear, hence it is more accurate to process\nthe probability distribution (e.g. on a lattice) through the\nfunctions before e.g. taking expected value.\nC. Normalization to nearly uniform marginal distributions\nLike in copula theory, in HCR methodology it is conve-\nnient to initially normalize all variables to nearly uniform\nmarginal distributions in [0, 1], hence we further only work\non such normalized variables, what beside usually better\nprediction, also allows for better presentation of evaluation:\ne.g. density without prediction is 1, log-likelihood is 0.\nThis standard normalization requires estimations of cu-\nmulative distribution function (CDF), individually for each\nvariable, and applying this CDF function to the original\nvalues. Finally, having a prediction we can go back to the\noriginal variable using CDF\u22121, for example as in the original\n[3] article, however, for simplicity we omit this step here -\nworking only on normalized variables. Also AMI, HLR\npredictions underwent such normalization for the purpose\nof Fig. 7 visual performance comparison - making that an\nideal predictor would give diagonal.\nThere was used empirical distribution function (EDF) for\nsuch normalization here: for each variable its n observed\nvalues are sorted, then i-th value in such order obtains\n(i \u22120.5)/n normalized value. Hence values become their\nestimated quantiles this way, difference of two normalized\nvalues describes percentage of population between these two\nvalues.\nHaving predicted density for normalized variable, we can\ntransform it to the original variable e.g. by discretizing this\ndensity to probability distribution on {(i \u22120.5)/n}i=1..n\nlattice, and assigning probability of its i-th position to i-th\nordered original value. For simplicity it is omitted in this\narticle.",
    "chunk_index": 5,
    "start_char": 14105,
    "end_char": 17346,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "n observed\nvalues are sorted, then i-th value in such order obtains\n(i \u22120.5)/n normalized value. Hence values become their\nestimated quantiles this way, difference of two normalized\nvalues describes percentage of population between these two\nvalues.\nHaving predicted density for normalized variable, we can\ntransform it to the original variable e.g. by discretizing this\ndensity to probability distribution on {(i \u22120.5)/n}i=1..n\nlattice, and assigning probability of its i-th position to i-th\nordered original value. For simplicity it is omitted in this\narticle.\nD. Evaluation: log-likelihood with 10-fold cross-validation\nThe most standard evaluation of probability distributions\nis log-likelihood like in ML estimation: average (natural)\nlogarithm of (predicted) density in the actually observed\nvalue. Hence we will use this evaluation here.\nWorking on variables normalized to \u03c1 \u22481 marginal distri-\nbutions, without prediction they would have practically zero\nlog-likelihood. It allows to imagine gains from predictions\nas averaged improvement over this \u03c1 \u22481, as in Fig. 2. For\nexample the best observed log-likelihood \u22481.2 corresponds\nto \u2248exp(1.2) \u22483.3 times better density than without\nprediction, the same as if we could squeeze [0, 1] range 3.3\ntimes to a 0.3 wide range. Sorting predicted densities in the\nactually observed values, we can get additional information\nabout distribution of prediction, as presented in this Figure.\nWe predict here conditional density - denoted as \u03c1(Y =\ny|X = x) for density of Y predicted based on known value\nof X. Hence the used evaluation can be seen as estimation\nof EXY (ln(\u03c1(Y |X)), which is minus conditional entropy\n\u2212H(Y |X). While being unknown, random variables have\nsome concrete value of conditional entropy - we can hope-\nfully try to approach it with better and better models.\nWe are focusing here on large models using hundreds\nof coef\ufb01cients, hence we need to be careful not to over\ufb01t:\nrepresent only behavior which indeed generalizes, is not just\na statistical artifact of the training set. Machine learning\nalso builds large models, usually evaluating using cross-\nvalidation: randomly split dataset into training and test\nset, training set is used to build the model, then test (or\nvalidation) set is used to evaluate the built model.\nHowever, such evaluation depends on the random splitting\ninto training and test set. There is used standard 10-fold\ncross validation to weaken this random effect: dataset is\nrandomly split into 10 nearly equal size subsets, evaluation\nis average from 10 cross validations: using successive sub-\nsets as the test set and the remaining as the training set.\nHowever, there is still observed scale \u22480.01 randomness\nof such evaluation, hence only two digits after coma are\nbeing presented.\n\n5\nIII. USED HCR-BASED METHODOLOGY\nThis section discusses the used methodology, being ex-\npansion of the one used in [3]. We decompose X and Y\nvariables into mixed moments and model separately each\nmoments of Y using least-squares linear regression of mo-\nments of X, then combine them into predicted conditional\nprobability distribution of Y .\nA.",
    "chunk_index": 6,
    "start_char": 16784,
    "end_char": 19909,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "\u22480.01 randomness\nof such evaluation, hence only two digits after coma are\nbeing presented.\n\n5\nIII. USED HCR-BASED METHODOLOGY\nThis section discusses the used methodology, being ex-\npansion of the one used in [3]. We decompose X and Y\nvariables into mixed moments and model separately each\nmoments of Y using least-squares linear regression of mo-\nments of X, then combine them into predicted conditional\nprobability distribution of Y .\nA. Decomposing joint distribution into mixed moments\nAfter normalization of marginal distributions of all vari-\nables to nearly uniform on [0, 1], for d variables their joint\ndistribution on [0, 1]d would be also nearly uniform if they\nwere statistically independent.\nDistortion from uniform joint distribution corresponds to\nstatistical dependencies between these variables - we would\nlike to model and exploit it. In HCR we model it as just\na linear combination using an orthornormal basis e.g. of\npolynomials, which gives the coef\ufb01cients similar interpreta-\ntion as moments and mixed moments: dependencies between\nmoments for multiple variables.\nThe \ufb01rst orthonormal polynomials (rescaled Legendre) for\n[0, 1] are f0 = 1 and f1, f2, f3, f4 correspondingly (plotted\nin Fig. 1):\n\u221a\n3(2x\u22121),\n\u221a\n5(6x2 \u22126x+1),\n\u221a\n7(20x3 \u221230x2 +12x\u22121),\n3(70x4 \u2212140x3 + 90x2 \u221220x + 1)\nWe\ncould\nalternatively\nuse\ne.g.\n1,\n\u221a\n2 sin(2\u03c0xk),\n\u221a\n2 cos(2\u03c0xk) for k\n\u22651 orthonormal basis, however,\nexperimentally it usually leads to inferior evaluation.\nDecomposing density \u03c1(x) = P\nj ajfj(x), we need\na0 = 1 normalization to integrate to 1. Due to orthogo-\nnality,\nR 1\n0 fj(x)dx = 0 for j > 0, hence the following\ncoef\ufb01cients do not affect normalization. As we can see\nin their plots in Fig. 1, positive a1 shifts density toward\nright - acting analogously as expected value. Positive a2\nincreases probability of extreme values at cost of central\nvalues - analogously as variance. Skewness-like higher order\nasymmetry is brought by a3 and so on - we can intuitively\ninterpret these coef\ufb01cients as moments (cumulants). This\nis only an approximation, but useful for interpretation of\ndiscussed models.\nIn multiple dimensions we can use product basis:\nfj(x) = fj1(x1) \u00b7 . . . fjd(xd)\nfor j = (j1, . . . , jd) (4)\nleading to model of joint distribution:\n\u03c1(x) =\nX\nj\u2208B\nfj(x) =\nX\nj\u2208B\najfj1(x1) \u00b7 . . . \u00b7 fjd(xd)\n(5)\nwhere B is the basis of mixed moments we are using\nfor our modelling. It is required to contain (0, . . . , 0) for\nnormalization. Beside, there is a freedom of choosing this\nbasis, what allows to hierarchically decompose statistical\ndependencies of multiple variables into mixed moments.\nFigure 1 contains some \ufb01rst 5 functions of such product\nbasis for d = 2 variables: f00 corresponds to normalization\nand requires a00 = 1. Coef\ufb01cients of f10, f20 describe\nexpected value and variance of the \ufb01rst variable, f01 and\nf02 analogously of the second. Then we can start including\nmoment dependencies, starting with a11 which determines\ndecrease/increase of expected value of one variable with\ngrowth of expected value of the second variables - analo-\ngously to correlation coef\ufb01cient.",
    "chunk_index": 7,
    "start_char": 19471,
    "end_char": 22561,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "freedom of choosing this\nbasis, what allows to hierarchically decompose statistical\ndependencies of multiple variables into mixed moments.\nFigure 1 contains some \ufb01rst 5 functions of such product\nbasis for d = 2 variables: f00 corresponds to normalization\nand requires a00 = 1. Coef\ufb01cients of f10, f20 describe\nexpected value and variance of the \ufb01rst variable, f01 and\nf02 analogously of the second. Then we can start including\nmoment dependencies, starting with a11 which determines\ndecrease/increase of expected value of one variable with\ngrowth of expected value of the second variables - analo-\ngously to correlation coef\ufb01cient. We have also dependencies\nbetween higher moments, like asymmetric a12 relating ex-\npected value of the \ufb01rst variable and variance of the second.\nAnd analogously for more variables, e.g. a010010 de-\nscribes correlation between 2nd and 5th out of 6 variables.\nFinally we can hierarchical decompose statistical dependen-\ncies between multiple variables into their mixed moments.\nHowever, to fully describe general joint distribution, we\nwould need B = Nd in\ufb01nite number of mixed moments\nthis way - for practical applications we need to choose some\n\ufb01nite basis B of moments to focus on.\nB. Estimation with least squares linear regression\nHaving a data sample X, we would like to estimate\nsuch mixed moments as coef\ufb01cients for linear combination\nof some orthonomal basis of functions e.g. polynomials.\nSmoothing the sample with kernel density estimation, \ufb01nd-\ning linear combination minimizing square distance to such\nsmoothened sample, and performing limit to zero width of\nthe used kernel, we get convenient and inexpensive MSE\nestimation [1]: independently for each coef\ufb01cient j as just\naverage over dataset of value of the corresponding function:\naj =\n1\n|X|\nX\nx\u2208X\nfj(x)\n(6)\nWe could use such obtained model for predicting conditional\ndistribution: substitute the known variables to the modeled\njoint distribution, after normalization getting (conditional)\ndensity of the unknown variables.\nHowever, for the bid-ask spread prediction problem,\nslightly better evaluation was obtained by generalizing al-\nternative approach from [3], which allows to additionally\nexploit subtle variable dependencies, hence we will focus\non it.\nSpeci\ufb01cally, to model \u03c1(Y\n= y|X = x), let us use\nseparate bases of (mixed) moments: BX for X, BY for\nY , and model relations between them. While there could\nbe considered more sophisticated models for such relations\nincluding neural networks, for simplicity and interpretability\nwe focus on linear models here, treating fj(x) as inter-\n\n6\npretable features:\n\u03c1(y|x) =\nX\nj\u2208BY\nfj(y)aj(x)\nfor\naj(x) =\nX\nk\u2208BX\n\u03b2jkfk(x)\n(7)\nhence the model is de\ufb01ned by the \u03b2 matrix, which examples\nare visualized in Fig. 4 for |BX| = 53, |BY | = 1 + 8.\nIt allows for good interpretability: \u03b2jk coef\ufb01cient is linear\ncontribution of k-th mixed moment of X to j-th (mixed)\nmoment of Y . We focus on one-dimensional Y , but the\nformalism allows to analogously predict multidimensional.\nTo \ufb01nd the \u03b2 model we use least-squares optimization\nhere - it is very inexpensive, can be made independently\nfor each j",
    "chunk_index": 8,
    "start_char": 21930,
    "end_char": 25065,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "by the \u03b2 matrix, which examples\nare visualized in Fig. 4 for |BX| = 53, |BY | = 1 + 8.\nIt allows for good interpretability: \u03b2jk coef\ufb01cient is linear\ncontribution of k-th mixed moment of X to j-th (mixed)\nmoment of Y . We focus on one-dimensional Y , but the\nformalism allows to analogously predict multidimensional.\nTo \ufb01nd the \u03b2 model we use least-squares optimization\nhere - it is very inexpensive, can be made independently\nfor each j \u2208BY thanks to using orthonormal basis, and\nintuitively is a proper heuristic: least-squares optimization\nestimates the mean - exactly as we would like for coef\ufb01cient\nestimation (6). However, this is not necessarily the optimal\nchoice - it might be worth to explore also more sophisticated\nways.\nSuch least-squares optimization has to be performed sep-\narately for each j \u2208BY . Denoting \u03b2j\u00b7 = (\u03b2jk)k\u2208BX as\ncoef\ufb01cient vector for j-th moment and Z = {(yi, xi)}i=1..n\nas (e.g. training) dataset of (y, x) pairs:\n\u03b2j\u00b7 = argminv\nX\n(y,x)\u2208Z\n X\nk\u2208BX\nfk(x)vk \u2212fj(y)\n!2\n=\n= argminv\n\r\rMv \u2212bj\r\r2\nfor M = [fk(xi)]i=1..n,k\u2208BX\nbj = (fj(xi))i=1..n\nmatrix M and vector bj for j \u2208BY . Such least-squares\noptimization has unique solution:\n\u03b2j\u00b7 = (M T M)\u22121M T bj\n(8)\nSeparately calculated for each j \u2208BY , leading to the entire\nmodel as \u03b2 matrix with \u03b2j\u00b7 rows, like in Fig. 4.\nC. Applying the model, enforcing nonnegativity\nHaving such model \u03b2 we can apply it to (e.g. test)\ndatapoints as in 7, getting predicted conditional density for\ny on [0, 1] as a polynomial. However, sometimes it can\nget below 0, so let us refer to it as \u02dc\u03c1 and then enforce\nnonnegativity required for densities:\n\u02dc\u03c1(y|x) =\nX\nj\u2208BY\nfj(y)\nX\nk\u2208BX\n\u03b2jkfk(x)\n(9)\nSuch obtained polynomial always integrates to 1. How-\never, it occasionally can get below zero, what should be\ninterpreted as corresponding to some low positive density.\nSuch interpretation to nonnegative density \u03c1 is referred\nas calibration, and can be optimized based on dataset as\ndiscussed in [9]. For simplicity there was just used:\n\u03c1(y|x) = max (\u02dc\u03c1(y|x), 0.03) /N\n(10)\nFigure 4.\nVisualized coef\ufb01cients of \u2019123\u2019 models (9 \u00d7 53 matrix \u03b2\nfor \u03c1(y|x) = P\nj fj(y) P\nk \u03b2jkfk(x)), the numbers above names are\nlog-likelihoods. The \u2019common\u2019 is the model built for combined all data\n- presents general trends. Trying to split all companies into subsets of\nsimilar behavior, as visualized in tree Fig. 6, splitting into two subsets we\nget the presented comL and comR models correspondingly for left (DPWG,\nBEIG, HNKG, FME, SAPG, DB1G, RWEG, FREG, HEIG, DTEG, IFXG)\nand right (DAIG, SIEG, TKA, CONG, MRCG, LHAG, VOWG, MUVG,\nALVG, BMW, DBKG) subtree of this tree. Then there presented individual\nmodels for 5 selected companies. Rows correspond to predicted moments\nof Y , as a linear combinations of mixed moments of X corresponding\nto columns. The zeroth row has always only 000 nonzero coef\ufb01cient\nequal 1 for normalization.",
    "chunk_index": 9,
    "start_char": 24629,
    "end_char": 27492,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "comL and comR models correspondingly for left (DPWG,\nBEIG, HNKG, FME, SAPG, DB1G, RWEG, FREG, HEIG, DTEG, IFXG)\nand right (DAIG, SIEG, TKA, CONG, MRCG, LHAG, VOWG, MUVG,\nALVG, BMW, DBKG) subtree of this tree. Then there presented individual\nmodels for 5 selected companies. Rows correspond to predicted moments\nof Y , as a linear combinations of mixed moments of X corresponding\nto columns. The zeroth row has always only 000 nonzero coef\ufb01cient\nequal 1 for normalization. The next row describes prediction of expected\nvalue, the next one of variance and so on. In the top model, common\nfor all companies, we can e.g. see large positive 001 \u21921 coef\ufb01cient:\nspread increases with growth of H \u2212L, negative 010 \u21921: spread\ndecreases with growth of V , and negative 011 \u21922: variance of spread\ndecreases for correlated V and H \u2212L. Blue 100 \u21923 for FREG denotes\nreduction of skewness of spread with growth of price. Generally, we can see\nquite individual behavior for different companies, starting with 100 \u21921\nanalogous to price-spread correlation, which seems the main dividing factor\nbetween comL and comR companies.\nwhere N normalization factor is chosen to integrate to 1:\nN =\nR 1\n0 max (\u02dc\u03c1(y|x), 0.03) dy. The 0.03 threshold was\nexperimentally chosen as a compromise for the used dataset,\nits tuning can slightly improve evaluation.\nFigure 2 contains examples of such \u03c1(y|xi) predicted\ndensities on the test set with yi actual values marked as\nvertical lines. Flat near zero regions come from max(\u00b7, 0.03)\nthresholding. While they are relatively frequent in such\npredicted densities, in plots of sorted {\u03c1(yi|xi)}i below\n\n7\nwe can see that these close to zero densities are very rare\namong the actual values: prediction properly excludes these\n\u02dc\u03c1 < 0.03 regions as unlikely.\nIntegration is relatively costly to compute, especially\nin higher dimensions, hence for ef\ufb01cient calculation the\npredicted polynomial \u02dc\u03c1 was discretized here into 100 values\non ((i \u22120.5)/100)i=1..100 lattice, what corresponds to\napproximating density with piecewise constant function on\nlength 1/100 subranges. Then max(\u00b7, 0.03) was applied,\nand division by sum for normalization. Finally density in\ndiscretized \u2308100yi\u2309/100 position was used as \u03c1(yi|xi) in\nlog-likelihood evaluation.\nD. Basic basis selection\nThe optimal choice of basis is a dif\ufb01cult open question.\nAs the basic choice there was used combinatorial family:\nB((m1, . . . , md), s, r) :=\n(11)\n=\n(\nj \u2208Nd : \u2200iji \u2264mi,\nd\nX\ni=1\nji \u2264s,\nd\nX\ni=1\nsgn(ji) \u2264r\n)\nwhere mi chooses how many \ufb01rst moments to use for i-th\nvariable, s bounds the sum of used moments (and formally\ndegree of corresponding polynomial), r bounds the number\nof nonzero ji: to include dependencies of up to r variables.\nFor example the \u2019123\u2019 model infers 8 moments BY =\nB((8), 8, 1) from 3 variables using a compromise: BX =\nB((4, 4, 4), 5, 3) of size |BX| = 53 basis, directly written\ne.g. in Fig. 4.",
    "chunk_index": 10,
    "start_char": 27021,
    "end_char": 29917,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "for i-th\nvariable, s bounds the sum of used moments (and formally\ndegree of corresponding polynomial), r bounds the number\nof nonzero ji: to include dependencies of up to r variables.\nFor example the \u2019123\u2019 model infers 8 moments BY =\nB((8), 8, 1) from 3 variables using a compromise: BX =\nB((4, 4, 4), 5, 3) of size |BX| = 53 basis, directly written\ne.g. in Fig. 4. The \u2019123+\u2019 model infers 6 moments BY =\nB((6), 6, 1) from 6 variables: BX = B((4, 4, 3, 1, 2, 1), 5, 3)\nof size |BX| = 205.\nIV. BID-ASK SPREAD MODELLING\nThis section discusses application of the presented\nmethodology to model conditional distribution of (relative\nquoted) bid-ask spread.\nA. \u2019123\u2019 model using basic variables\nThe initial plan for this article was to improve predic-\ntion from standard models: AMI, HLR, trying to predict\nconditional distribution of spread from their values using\nthe discussed methodology. However, the results were dis-\nappointing, especially for AMI, as we can see in Fig. 7.\nTherefore, we have decided to use the original variables\n(P, V, L, H, R) instead, what has turned out to lead to es-\nsentially better predictions. There was manually performed\nsearch for parameters using B basic basis selection (III-D)\nto maximize averaged log-likelihood in 10-fold cross valida-\ntion. This search has \ufb01nally lead to BX = B((4, 4, 4), 5, 3)\nbasis for only 3 variables: P, V, (H \u2212L)/P to predict up to\n8-th moment of Y . Surprisingly, adding dependence on R\nFigure 5. Top: Optimizing basis and model size on example of FREG com-\npany and BX = B((4, 4, 4), 5, 3) size 53 basis of mixed moments from\n\u2019123\u2019 model. Log-likelihoods for predicting \ufb01rst 1 . . . 10 moments (denoted\nby colors) using some \ufb01rst of mixed moments (sorted lexicographically)\nof 3 X variables: P, V, (H \u2212L)/P. We can see that we should predict\n\u22488 moments, higher moments are necessary to represent more complex\ndistributions. Middle: selective removal of successive mixed moments\nto maximize log-likelihood - we can see that we can slightly improve\nevaluation this way, additionally reducing model size. However, it rather\nrequires individual optimization for each company. Bottom: analogously\nas top, but using size 181 larger BX = B((5, 5, 5), 10, 3), also trying\ndifferent order of mixed moments: accordingly to P\ni(ji)p. While using\nall such mixed moments clearly leads to over\ufb01tting, selectively using some\n\ufb01rst of them can lead to slightly improved evaluation.\nand L alone was worsening evaluation - their dependence\ndid not generalize from training to test sets.\nWhile the optimal choice of basis seems a dif\ufb01cult open\nproblem, exhaustive search over all subsets is rather imprac-\ntically costly, Fig. 5 presents some heuristic approaches. The\nB family seems generally a good start, e.g. successively\nmodifying some parameter by one as long as observing\nimprovement.",
    "chunk_index": 11,
    "start_char": 29552,
    "end_char": 32387,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "was worsening evaluation - their dependence\ndid not generalize from training to test sets.\nWhile the optimal choice of basis seems a dif\ufb01cult open\nproblem, exhaustive search over all subsets is rather imprac-\ntically costly, Fig. 5 presents some heuristic approaches. The\nB family seems generally a good start, e.g. successively\nmodifying some parameter by one as long as observing\nimprovement. In this Figure we can see large improvement\nwhile rising the number of predicted moments up to \u22487,\nwhat suggests that complexity of conditional distributions\nfor the considered problem requires this degree of polyno-\nmial for proper description. This Figure also contains trials\nof using some \ufb01rst of such mixed moments accordingly to\ndifferent orders. A heuristic optimization of a reasonable\n\n8\ncost is the presented selective removal: for each mixed\nmoment from BX calculate evaluation when it is removed,\n\ufb01nally remove the one leading to the best evaluation, and\nso on as long as evaluation improves.\nB. Individual vs common models, universality\nA natural question is how helpful for prediction is a given\nvariable - Figure 3 presents some answers by calculating\nlog-likelihood also for models using only some of the\nvariables. We can see different companies can have very\ndifferent behavior here, e.g. for some V is helpful, for some\nit is not, what we can also see in the presented points from\ndataset. Figure 4 shows that they can even have opposite\nbehavior: e.g. 100 \u21921 dependence on price.\nThis is a general lesson that while we would like pre-\ndictors as nice simple formulas, the reality might be much\nmore complicated - models found here are results of cultures\nof traders of stocks of individual companies, which can\nessentially vary between companies.\nTherefore, to get the most accurate predictions we should\nbuild individual models for each company. Even more, a\nspeci\ufb01c behavior of a given company can additionally evolve\nin time - what could be exploited e.g. by building separate\nmodels for shorter time periods, or using adaptive least-\nsquares linear regression [10], and is planned for future\ninvestigation.\nHowever, building such models requires training data,\nwhich in case of variables like bid-ask spread might be\ndif\ufb01cult to access. Hence it is also important to search\nfor universality - e.g. try to guess a model for a company\nfor which we lack such data, based on data available\nfor other companies. This generally seems a very dif\ufb01cult\nproblem, Fig. 6 shows that even having all the data, using\ncommon model for multiple companies we should expect\nlarge evaluation drop. For example we can see that behavior\nof DTEG completely disagrees with common model for all.\nAs we can see in this tree Figure, the one common\nmodel situation improves if we can cluster companies into\ngroups of similar behavior (orange dots) - there are also\npresented results for splitting companies into just two groups\nwith separate models (comL, comR in Fig. 4), also visually\nleading to slightly better prediction as we can see comparing\n3rd and 4th column in Fig.",
    "chunk_index": 12,
    "start_char": 31993,
    "end_char": 35060,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "expect\nlarge evaluation drop. For example we can see that behavior\nof DTEG completely disagrees with common model for all.\nAs we can see in this tree Figure, the one common\nmodel situation improves if we can cluster companies into\ngroups of similar behavior (orange dots) - there are also\npresented results for splitting companies into just two groups\nwith separate models (comL, comR in Fig. 4), also visually\nleading to slightly better prediction as we can see comparing\n3rd and 4th column in Fig. 7.\nC. \u2019123+\u2019 model with additional variables\nThe information from P, V, H, L, R basic variables can\noften be complemented with some additional - a natural\napproach is checking if we can improve log-likelihood in\nthe discussed methodology if adding information from some\nnew variables.\nThe size of basis can even grow exponentially with the\nnumber of variables here: there are (m+1)d mixed moments\nFigure 6.\nVisualization of optimized hierarchical grouping and loss\nof using common models for multiple companies, height denotes log-\nlikelihoods. Heights of names show evaluation of using individual model\nfor a given company, orange dots show successive reduction of log-\nlikelihood for a given company while using common models for subsets\ngrowing accordingly to the presented tree. The lowest dots correspond\nto using one common model for all (common in Fig. 4) - we can see\nthat only for DTEG it is worse than zero (using no prediction at all).\nSplitting companies into left and right subtree and using separate two\nmodels for them (comL and comR in Fig. 4), we get essentially better\nprediction (one dot up). The tree structure was calculated by combining\nsubsets to maximize (log-likelihood of common model / average log-\nlikelihood of individual models) - grouping companies into pairs and then\nfurther, up to a single common model for all. Positions of lines represent\nsuch grouped companies: light-gray line their averaged log-likelihoods of\nindividual models, dark-gray line their log-likelihood for a common model.\nThe difference between these two lines represent loss of using common\nmodel. The common models are \ufb01xed hence there is no cross validation\n(CV), what arti\ufb01cially improves performance, for example for the \ufb01rst dot\nof FME corresponding to common model with HNKG - making it above\nCV individual model, generally suggesting large time inhomogeneities - to\nbe included in future adaptive models.\nif using all up to m-th moment for all d variables. The\nr parameter: maximal number of interacting variables in\nPd\ni=1 sgn(ji) \u2264r allows to bound it by O((dm)r). The\nsum s limitation in Pd\ni=1 ji \u2264s seems also very useful,\nbounding degree of used polynomials.\nDue to quickly growing basis size for increasing number\nof variables, we could easily exceed the size of dataset -\nexperimentally seen as over\ufb01tting: decreasing performance.\nManual search for using additional variables has started with\nBX = B((4, 4, 4), 5, 3) basis for the standard variables,\nand carefully increasing m in BX = B((4, 4, 4, m), 5, 3)\nbasis with separate single additional variable to consider.",
    "chunk_index": 13,
    "start_char": 34561,
    "end_char": 37646,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "The\nsum s limitation in Pd\ni=1 ji \u2264s seems also very useful,\nbounding degree of used polynomials.\nDue to quickly growing basis size for increasing number\nof variables, we could easily exceed the size of dataset -\nexperimentally seen as over\ufb01tting: decreasing performance.\nManual search for using additional variables has started with\nBX = B((4, 4, 4), 5, 3) basis for the standard variables,\nand carefully increasing m in BX = B((4, 4, 4, m), 5, 3)\nbasis with separate single additional variable to consider.\n\n9\nThe most promising variables were later considered together,\nby modifying parameters by 1 as long as improvement was\nobserved (of averaged log-likelihood over individual models\nfor all companies).\nIt has \ufb01nally lead to \u2019123+\u2019 model: BY = B((6), 6, 1)\nand |BX| = 205 size BX = B((4, 4, 3, 1, 2, 1), 5, 3) using\n3 additional variables: depth, midpoint changes intraday\nand midpoint volatility. Such model has 6 \u00b7 205 = 1230\ncoef\ufb01cients.\nDue to rapid growth of the number of coef\ufb01cients, for\nadding further variables it is worth to consider e.g. building\nsome features from multiple variables to be directly used\nhere, or use some alternative way for choosing basis for\nX, e.g. directly optimized on the dataset like PCA or other\ndimensionality reduction.\nV. CONCLUSIONS AND FURTHER WORK\nThere was presented a general methodology for extracting\nand exploiting complex statistical dependencies between\nmultiple variables in inexpensive and interpretable way for\npredicting conditional probability distributions, on example\nof dif\ufb01cult problem of predicting bid-ask spread from more\navailable information. It expands approach form [3] by\ninferring from mixed moments, and searching for the basis\nin large spaces of possibilities.\nFigure 7 presents its comparison with standard methods\nwhen using only expected value from such predicted\nconditional density - perfect predictor would lead to\ndiagonal, standard methods give rather a noise instead,\nwhile the predictions from the discussed approaches indeed\noften resemble diagonal, especially when using individual\nmodels. Predicted conditional probability density provides\nmuch more information: e.g. allows to additionally estimate\nuncertainty of such prediction, or provide or-or prediction\nfor multimodal densities, or allows for generating its\nrandom values e.g. for Monte-Carlo simulations, or just\nprovide the entire density for accurate considerations if\ntransforming such random variables through some further\nfunctions.\nThere are many directions for further development of this\nrelatively new general methodology, for example:\n\u2022 Optimal choice of basis is a dif\ufb01cult problem, necessary\nto be automatized especially for a larger number of\nvariables - selecting from discussed basis of orthonor-\nmal polynomials, or maybe automatically optimizing a\ncompletely different basis based on dataset.\n\u2022 There are observed large differences between behaviors\nof individual companies - bringing dif\ufb01cult questions of\ntrying to optimize for common behavior, optimize mod-\nels based on incomplete information, etc. Additionally,\nsuch behavior has probably also time inhomogeneity -\nthe models should evolve in time, requiring adaptive\nmodels to improve performance, where the problem of\ndata availability becomes even more crucial.",
    "chunk_index": 14,
    "start_char": 37138,
    "end_char": 40421,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "orthonor-\nmal polynomials, or maybe automatically optimizing a\ncompletely different basis based on dataset.\n\u2022 There are observed large differences between behaviors\nof individual companies - bringing dif\ufb01cult questions of\ntrying to optimize for common behavior, optimize mod-\nels based on incomplete information, etc. Additionally,\nsuch behavior has probably also time inhomogeneity -\nthe models should evolve in time, requiring adaptive\nmodels to improve performance, where the problem of\ndata availability becomes even more crucial.\n\u2022 The discussed models rapidly grow with the number\nof variables, what requires some modi\ufb01cations for ex-\nploiting high dimensional information - like extracting\nfeatures from these variables, dimensionality reduction\nlike PCA, etc.\n\u2022 We have predicted conditional distribution for one-\ndimensional variable, but the methodology was intro-\nduced to be more general: predicting for multidimen-\nsional Y should be just a matter of using proper BY ,\nwhat is planned to be tested in future.\n\u2022 The predicted densities as polynomials have often rapid\ngrowths at the ends of [0, 1] - their removal might\nimprove performance.\n\u2022 There was assumed linear relation between moments\nwith least-squares optimization, what is inexpensive\nand has good interpretability, but is not necessarily\noptimal - there could be considered e.g. using neural\nnetworks instead, and optimizing criteria closer to log-\nlikelihood of \ufb01nal predictions.\nACKNOWLEDGEMENT\nHenryk Gurgul thanks Professor Roland Mestel for pro-\nviding the bid-ask data from data bank \u201dFinance Research\nGraz Data Services\u201d and Professor Erik Theissen and Ste-\nfan Scharnowski from Mannheim for providing data from\n\u201dMarket Microstructure Database\u201d.\nREFERENCES\n[1] J. Duda, \u201cHierarchical correlation reconstruction with missing\ndata, for example for biology-inspired neuron,\u201d arXiv preprint\narXiv:1804.06218, 2018.\n[2] F. Durante and C. Sempi, Principles of copula theory.\nChapman\nand Hall/CRC, 2015.\n[3] J.\nDuda\nand\nA.\nSzulc,\n\u201cCredibility\nevaluation\nof\nincome\ndata with hierarchical correlation reconstruction,\u201d arXiv preprint\narXiv:1812.08040, 2018.\n[4] R. Mestel, M. Murg, and E. Theissen, \u201cAlgorithmic trading and liq-\nuidity: Long term evidence from austria,\u201d Finance Research Letters,\nvol. 26, pp. 198\u2013203, 2018.\n[5] H. Gurgul and A. Machno, \u201cThe impact of asynchronous trading on\nepps effect on warsaw stock exchange,\u201d Central European Journal of\nOperations Research, vol. 25, no. 2, pp. 287\u2013301, 2017.\n[6] Y. Amihud, \u201cIlliquidity and stock returns: cross-section and time-\nseries effects,\u201d Journal of \ufb01nancial markets, vol. 5, no. 1, pp. 31\u201356,\n2002.\n[7] K. Y. Fong, C. W. Holden, and C. A. Trzcinka, \u201cWhat are the best\nliquidity proxies for global research?\u201d Review of Finance, vol. 21,\nno. 4, pp. 1355\u20131401, 2017.\n[8] B. Bedowska-S\u00b4ojka and K. Echaust, \u201cCommonality in liquidity\nindices: The emerging european stock markets,\u201d Systems, vol. 7, no. 2,\np. 24, 2019.\n[9] J. Duda, \u201cExploiting statistical dependencies of time series with hier-\narchical correlation reconstruction,\u201d arXiv preprint arXiv:1807.04119,\n2018.\n[10] \u2014\u2014, \u201cParametric context adaptive laplace distribution for multimedia\ncompression,\u201d arXiv preprint arXiv:1906.03238, 2019.\n\n10\nFigure 7.\nComparison of spread predictors on dataset for visual evaluation: perfect predictor would give a diagonal, a completely useless one would\ngive uniform distribution.",
    "chunk_index": 15,
    "start_char": 39887,
    "end_char": 43295,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "K. Echaust, \u201cCommonality in liquidity\nindices: The emerging european stock markets,\u201d Systems, vol. 7, no. 2,\np. 24, 2019.\n[9] J. Duda, \u201cExploiting statistical dependencies of time series with hier-\narchical correlation reconstruction,\u201d arXiv preprint arXiv:1807.04119,\n2018.\n[10] \u2014\u2014, \u201cParametric context adaptive laplace distribution for multimedia\ncompression,\u201d arXiv preprint arXiv:1906.03238, 2019.\n\n10\nFigure 7.\nComparison of spread predictors on dataset for visual evaluation: perfect predictor would give a diagonal, a completely useless one would\ngive uniform distribution. All variables are normalized to nearly uniform marginal distributions, including outcomes of standard methods: AMI, HLR.\nThe following 3 columns use expected values of predicted densities from discussed \u2019123\u2019 model (using P, V, (H \u2212L)/P variables, 8 \u00b7 53 = 424\ncoef\ufb01cients), the last one is for \u2019123+\u2019 model (using also depth, midpoint changes intraday and midpoint volatility, 6 \u00d7 205 = 1230 coef\ufb01cients). The\n\u201d1 common\u201d column uses 1 model for all, \u201d2 common\u201d groups companies into two subsets and uses one of two models (as in Fig. 6, using models\ncomL, comR from Fig. 4). The last two columns use models individually optimized for each company.",
    "chunk_index": 16,
    "start_char": 42715,
    "end_char": 43944,
    "paper_title": "Modelling bid-ask spread conditional distributions",
    "paper_category": "q-fin.TR",
    "paper_filename": "Modelling_bid-ask_spread_conditional_distributions.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Modelling_bid-ask_spread_conditional_distributions.pdf"
  },
  {
    "text": "arXiv:1808.00515v1 [q-fin.TR] 1 Aug 2018\nOptimal Trading with General Signals and\nLiquidation in Target Zone Models\nChristoph Belak\u2217\nJohannes Muhle-Karbe\u2020\nKevin Ou\u2021\nAugust 1, 2018\nAbstract\nWe study optimal trading in an Almgren-Chriss model with running and terminal inventory\ncosts and general predictive signals about price changes. As a special case, this allows to treat\noptimal liquidation in \u201ctarget zone models\u201d: asset prices with a re\ufb02ecting boundary enforced by\nregulatory interventions. In this case, the optimal liquidation rate is the \u201ctheta\u201d of a lookback\noption, leading to explicit formulas for Bachelier or Black-Scholes dynamics.\nMathematics Subject Classi\ufb01cation: (2010) 93E20, 91G80, 60H07.\nJEL Classi\ufb01cation: G11, C61\nKeywords: Optimal trading, inventory costs, market impact, liquidation, target zone models.\n1\nIntroduction\nIn classical models of optimal liquidation, the una\ufb00ected asset price is assumed to be a martingale\n[6, 3, 14, 1, 15]. This allows to focus on the liquidation program, while abstracting from signals about\nfuture price changes. The e\ufb00ect of such signals1 is studied using stochastic control techniques in\n[8, 12], leading to a PDE characterization for Markovian signals and explicit formulas in the special\ncase where the signal processes have Ornstein-Uhlenbeck dynamics.\nA rather di\ufb00erent signal about future price changes is studied by [13]. Motivated by caps on\nexchange rates enforced by central banks,2 they study the optimal liquidation of assets that re\ufb02ect\no\ufb00an upper threshold. Under the assumption that the liquidating agent only sells at this most\nfavorable execution price,3 they characterize the optimal trading strategy by a PDE that admits a\nprobabilistic representation in terms of catalytic superprocesses.\n\u2217University of Trier, Department IV \u2013 Mathematics, Universit\u00a8atsring 19, 54296 Trier, Germany, e-mail:\nbelak@uni-trier.de.\n\u2020Carnegie Mellon University, Department of Mathematical Sciences, 5000 Forbes Avenue, Pittsburgh, PA 15213,\nUSA, email\njohannesmk@cmu.edu. Parts of this research were completed while this author was visiting ETH Z\u00a8urich.\nHe thanks the Forschungsinstitut f\u00a8ur Mathematik and H. Mete Soner for their hospitality.\n\u2021Carnegie Mellon University, Department of Mathematical Sciences, 5000 Forbes Avenue, Pittsburgh, PA 15213,\nUSA, email\nyangxio@andrew.cmu.edu.\n1Typical examples include order book imbalances or forecasts of the future order \ufb02ow of other market participants.\n2A recent example is the upper bound on the CHF/EUR exchange rate guaranteed by the Swiss National Bank.\n3Unlike in the standard optimal execution models surveyed above, the resulting optimal trading rate is singular,\nsince it only acts on the local time of the re\ufb02ected price process. Accordingly, the liquidity costs in the model of [13]\nis imposed on the trading rate in local time.\n1\n\nSelling only at the highest possible price seems reasonable for agents with long liquidation\nhorizons and low inventory costs.\nYet, for shorter planning horizons or higher inventory costs,\nsubstantial immediate trading is necessary since it becomes too costly to wait for the asset price\nto approach its maximum.\nIn the present study, we solve the optimal liquidation problem with a price cap without con-\nstraining the selling price.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3284,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "price seems reasonable for agents with long liquidation\nhorizons and low inventory costs.\nYet, for shorter planning horizons or higher inventory costs,\nsubstantial immediate trading is necessary since it becomes too costly to wait for the asset price\nto approach its maximum.\nIn the present study, we solve the optimal liquidation problem with a price cap without con-\nstraining the selling price. To do so, we \ufb01rst extend the results of [12] on trading with running\nand terminal inventory costs to general, not necessarily Markovian trading signals by adapting the\ncalculus-of-variations argument developed for optimal tracking problems in [5, 7]. As a special case,\nthis allows to treat optimal liquidation for the case of re\ufb02ected price processes: these reduce to\ncomputing the \u201ctheta\u201d4 of a lookback call option. If the una\ufb00ected price process is modelled by\na Bachelier or Black-Scholes model, the optimal trading rate can in turn be computed in closed\nform up to the numerical evaluation of an integral with explicit integrand. These results con\ufb01rm\nthe intuition outlined above. Indeed, we \ufb01nd that all sales occur close to the barrier if inventory\ncosts are low. In contrast, for higher inventory costs, the in\ufb02uence of the barrier diminishes, as it\nbecomes prohibitively expensive to hold an asset position while the asset price is far from its upper\nbound.\nThe remainder of this paper is organized as follows. The general model and the special case of\na price cap are introduced in Section 2. The solution of the general trading problem with inventory\ncosts is subsequently derived in Section 3 and applied to optimal liquidation in models with a price\ncap in Section 4.\nNotation\nThroughout, we \ufb01x a \ufb01ltered probability space (\u2126, F, {Ft}t\u2208[0,T], P) satisfying the usual\nconditions and write Et[\u00b7] := E[\u00b7|Ft] for t \u2208[0, T]. The set H2 denotes the special semimartingales\nwhose canonical decomposition S = M + A into a (local) martingale M and a predictable \ufb01nite-\nvariation process A satis\ufb01es E[\u27e8M\u27e9T ]+E[(\nR T\n0 | dAt|)2] < \u221e. Finally, we write V for the progressively\nmeasurable processes u satisfying E[\nR T\n0 |ut|2 dt] < \u221e.\n2\nModel\nWe consider optimal trading in a risky asset with price process P \u2208H2. The asset position at\ntime t \u2208[0, T] is denoted by Xt, where the given initial position is X0 := x > 0. As in [3], the\nposition can only be adjusted gradually, since trades incur a cost \u03bb > 0 quadratic in the selling\nrate ut := \u2212dXt/dt. With a running inventory cost \u03b3 > 0 and a terminal inventory cost \u0393 > 0,\nthis leads to the following standard goal functional:5\nV (u) := E\n\" Z T\n0\nexecution price\nz\n}|\n{\n(Pt \u2212\u03bbut) ut dt\n|\n{z\n}\nterminal cash position\n+\nPT XT\n| {z }\nterminal\nasset position\n\u2212\nZ T\n0\n\u03b3X2\nt dt\n|\n{z\n}\nrunning\ninventory cost\n\u2212\n\u0393X2\nT\n|{z}\nterminal\ninventory cost\n#\n.",
    "chunk_index": 1,
    "start_char": 2887,
    "end_char": 5680,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "a running inventory cost \u03b3 > 0 and a terminal inventory cost \u0393 > 0,\nthis leads to the following standard goal functional:5\nV (u) := E\n\" Z T\n0\nexecution price\nz\n}|\n{\n(Pt \u2212\u03bbut) ut dt\n|\n{z\n}\nterminal cash position\n+\nPT XT\n| {z }\nterminal\nasset position\n\u2212\nZ T\n0\n\u03b3X2\nt dt\n|\n{z\n}\nrunning\ninventory cost\n\u2212\n\u0393X2\nT\n|{z}\nterminal\ninventory cost\n#\n.\n(2.1)\nFor asset prices with martingale dynamics, criteria of this type were \ufb01rst introduced by [2, 10] and\nsubsequently studied by, e.g., [16, 4, 11]. If the asset price is of the form dPt = It dt + dMt for a\none-dimensional di\ufb00usion I, then (2.1) corresponds to the setup of [12]. Here, we allow for more\ngeneral \u2013 potentially singular \u2013 asset dynamics. This allows to cover the \u201ctarget zone models\u201d\nstudied in [13], where the asset price is capped at some \ufb01nite level:\n4That is, the derivative with respect to the time variable.\n5Note that V (u) is well de\ufb01ned for all u \u2208V. As the terminal inventory penalty \u0393 grows, this criterion approaches\noptimal liquidation, where the position has to be closed out completely at maturity.\n2\n\nExample 2.1. Consider a martingale M \u2208H2 and a constant \u00afP \u2265M0. Then the price \u201ccapped\u201d\nat level \u00afP is de\ufb01ned as the solution of the Skorokhod map\nP := M \u2212(M\u2217\u2212\u00afP)+,\nwhere M\u2217\nt := sups\u2208[0,t] Ms. This corresponds to the minimal amount of intervention necessary to\nkeep the asset price below level \u00afP, akin to regulatory interventions to keep an exchange rate below\na certain threshold.\n3\nGeneral Solution\nIn our general - not necessarily Markovian - setup, the dynamic programming approach of [12] is\nno longer applicable. Instead, we adapt the calculus-of-variation argument of [5, 7] to the present\nsetting, where the asset has a general \u2013 possibly singular \u2013 drift.\nAs the goal functional u 7\u2192V (u) is strictly concave, it has a unique maximum \u02c6u characterized\nby the \ufb01rst-order condition that the G\u02c6ateaux derivative V \u2032(\u02c6u) vanishes at this critical point. This\nleads to a system of linear forward-backward stochastic di\ufb00erential equations (FBSDEs) for the\noptimal trading rate and the position, which can be solved explicitly:\nTheorem 3.1. Set\n\u03b2 :=\np\n\u03b3/\u03bb,\nG(t) := \u03b2 cosh(\u03b2t) + \u03bb\u22121\u0393 sinh(\u03b2t).\nLet P = P0 + M + A be the canonical decomposition of the (special) semimartingale P into a local\nmartingale M and a \ufb01nite-variation process A, and de\ufb01ne\nv2(t) := \u2212G\u2032(T \u2212t)\nG(T \u2212t) ,\nv1(t) := Et\n\u0014 1\n2\u03bb\nZ T\nt\nG(T \u2212s)\nG(T \u2212t) dAs\n\u0015\n,\nv0(t) := Et\n\u0014Z T\nt\nv1(s)2 ds\n\u0015\n.\nThe unique maximizer \u02c6u of u 7\u2192V (u) over V solves the (random) linear di\ufb00erential equation\n\u02c6ut = \u2212v2(t)X \u02c6u\nt \u2212v1(t),\n(3.1)\nso that the optimal liquidation trajectory is given by\nX \u02c6u\nt = G(T \u2212t)\nG(T)\nx +\nZ t\n0\nG(T \u2212t)\nG(T \u2212s)v1(s) ds.",
    "chunk_index": 2,
    "start_char": 5343,
    "end_char": 8011,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "de\ufb01ne\nv2(t) := \u2212G\u2032(T \u2212t)\nG(T \u2212t) ,\nv1(t) := Et\n\u0014 1\n2\u03bb\nZ T\nt\nG(T \u2212s)\nG(T \u2212t) dAs\n\u0015\n,\nv0(t) := Et\n\u0014Z T\nt\nv1(s)2 ds\n\u0015\n.\nThe unique maximizer \u02c6u of u 7\u2192V (u) over V solves the (random) linear di\ufb00erential equation\n\u02c6ut = \u2212v2(t)X \u02c6u\nt \u2212v1(t),\n(3.1)\nso that the optimal liquidation trajectory is given by\nX \u02c6u\nt = G(T \u2212t)\nG(T)\nx +\nZ t\n0\nG(T \u2212t)\nG(T \u2212s)v1(s) ds.\n(3.2)\nThe corresponding optimal value for (2.1) is\nV (\u02c6u) = P0x + \u03bb\n\u0002\nv0(0) + 2v1(0)x + v2(0)x2\u0003\n.\n(3.3)\nProof. We adapt the argument from [5, 7]. Recall that Xt = x \u2212\nR t\n0 us ds. Therefore, X is an a\ufb03ne\nfunction of u. As the goal functional (2.1) is a quadratic in (u, X) with strictly negative quadratic\ncoe\ufb03cients, it admits a unique maximizer characterized by the critical point; see, e.g., [9]. We now\nsolve for this critical point in feedback form.\nStep 1: Compute the G\u02c6ateaux derivative. We \ufb01x a direction of variation \u03b1 \u2208V and compute\n\u27e8V \u2032(u), \u03b1\u27e9= lim\n\u03b5\u21920\n1\n\u03b5(V (u + \u01eb\u03b1) \u2212V (u))\n= E\n\u0014Z T\n0\nPt\u03b1t dt \u22122\u03bb\nZ T\n0\nut\u03b1t dt \u2212PT\nZ T\n0\n\u03b1t dt + 2\u03b3\nZ T\n0\nXt\nZ t\n0\n\u03b1s ds dt + 2\u0393XT\nZ T\n0\n\u03b1t dt\n\u0015\n= E\n\u0014Z T\n0\n\u03b1tEt\n\u0014\nPt \u22122\u03bbut \u2212PT + 2\u03b3\nZ T\nt\nXs ds + 2\u0393XT\n\u0015\ndt\n\u0015\n.\n3\n\nThis derivative has to vanish for any variation at the critical point \u02c6u, which is equivalent to\nEt\n\u0014\n\u22122\u03bb\u02c6ut + Pt \u2212PT + 2\u03b3\nZ T\nt\nX \u02c6u\ns ds + 2\u0393X \u02c6u\nT\n\u0015\n= 0.\n(3.4)\nWe therefore obtain that the optimal trading rate \u02c6u and the corresponding optimal position X \u02c6u\nsolve the following system of linear forward-backward stochastic di\ufb00erential equations (FBSDE):\ndX \u02c6u\nt = \u2212\u02c6ut dt,\nX0 = x,\nd\u02c6ut = 1\n2\u03bb\n\u0010\ndPt \u22122\u03b3X \u02c6u\nt dt \u2212dNt\n\u0011\n,\n\u02c6uT = \u0393\n\u03bbX \u02c6u\nT .\n(3.5)\nHere, N is a square-integrable martingale that needs to be determined as part of the solution.\nStep 2: Solve the FBSDE for the critical point. Setting,\nY :=\n\u0012X\nu\n\u0013\n,\nZ :=\n\u0012\n0\nP \u2212N\n\u0013\n,\nB :=\n\u0012\n0\n\u22121\n\u2212\u03b3/\u03bb\n0\n\u0013\n,\nthe FBSDE (3.5) can be written in vector form as\ndYt = BYt dt + 1\n2\u03bb dZt,\nY 1\n0 = x,\n(\u0393/\u03bb, \u22121)YT = 0.\nIntegration by parts shows that d(e\u2212BtYt) =\n1\n2\u03bbe\u2212Bt dZt and in turn\nYT = eB(T\u2212t)Yt + 1\n2\u03bb\nZ T\nt\neB(T\u2212s) dZs.\nNow, multiply by (\u0393/\u03bb, \u22121), take into account the terminal condition (\u0393/\u03bb, \u22121)YT = 0, and use\n(\u0393/\u03bb, \u22121)eB(T\u2212t) =\n\u0012\n\u0393\n\u03bb cosh(\u03b2(T \u2212t)) + \u03b2 sinh(\u03b2(T \u2212t))\n\u2212cosh(\u03b2(T \u2212t)) \u2212\u0393\n\u03bb\u03b2\u22121 sinh(\u03b2(T \u2212t))\n\u0013\n= 1\n\u03b2\n\u0012 G\u2032(T \u2212t)\n\u2212G(T \u2212t)\n\u0013\n.",
    "chunk_index": 3,
    "start_char": 7658,
    "end_char": 9856,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "BYt dt + 1\n2\u03bb dZt,\nY 1\n0 = x,\n(\u0393/\u03bb, \u22121)YT = 0.\nIntegration by parts shows that d(e\u2212BtYt) =\n1\n2\u03bbe\u2212Bt dZt and in turn\nYT = eB(T\u2212t)Yt + 1\n2\u03bb\nZ T\nt\neB(T\u2212s) dZs.\nNow, multiply by (\u0393/\u03bb, \u22121), take into account the terminal condition (\u0393/\u03bb, \u22121)YT = 0, and use\n(\u0393/\u03bb, \u22121)eB(T\u2212t) =\n\u0012\n\u0393\n\u03bb cosh(\u03b2(T \u2212t)) + \u03b2 sinh(\u03b2(T \u2212t))\n\u2212cosh(\u03b2(T \u2212t)) \u2212\u0393\n\u03bb\u03b2\u22121 sinh(\u03b2(T \u2212t))\n\u0013\n= 1\n\u03b2\n\u0012 G\u2032(T \u2212t)\n\u2212G(T \u2212t)\n\u0013\n.\nAs a consequence,\n0 = G\u2032(T \u2212t)X \u02c6u\nt \u2212G(T \u2212t)\u02c6ut \u22121\n2\u03bb\nZ T\nt\nG\u2032(T \u2212s) d(Ps \u2212Ns).\nAfter solving for the trading rate and taking conditional expectations, this gives\n\u02c6ut = \u2212G\u2032(T \u2212t)\nG(T \u2212t) X \u02c6u\nt \u22121\n2\u03bbEt\n\u0014Z T\nt\nG(T \u2212s)\nG(T \u2212t) dPs\n\u0015\n.\nBy the Doob-Meyer decomposition and since P \u2208H2, we can replace dPs with dAs. The variation\nof constants formula now yields the explicit formula (3.2) for the corresponding optimal position\nX \u02c6u. Since both G and G\u2032 are bounded from above and below away from zero, we have E[|\u02c6ut|2] \u2264\nC1 + C2\nR t\n0 E[|\u02c6us|2] ds for some C1, C2 > 0. Gronwall\u2019s lemma in turn shows that E[|\u02c6ut|2] \u2264C1eC2T\nand hence \u02c6u \u2208V by Fubini\u2019s theorem.\nStep 3: Compute the value function. The \ufb01rst-order condition 0 = \u27e8V \u2032(\u02c6u), \u03b1\u27e9for \u03b1 = \u02c6u and its\nconsequence (3.4) for t = 0 imply\nV (\u02c6u) = 1\n2E\n\u0014Z T\n0\nPt\u02c6ut dt + PT X \u02c6u\nT\n\u0015\n+ 1\n2x(\u22122\u03bb\u02c6u0 + P0).\n4\n\nIntegration by parts as well as the Formulas (3.1) for \u02c6u0 and (3.2) for X \u02c6u\nt in turn show that\nV (\u02c6u) = 1\n2E\n\u0014\nxP0 +\nZ T\n0\nX \u02c6u\nt dPt\n\u0015\n+ 1\n2x(2\u03bbv2(0)x + 2\u03bbv1(0) + P0)\n= xP0 + \u03bb(v2(0)x2 + v1(0)x) + 1\n2E\n\u0014Z T\n0\n\u0012G(T \u2212t)\nG(T)\nx +\nZ t\n0\nG(T \u2212t)\nG(T \u2212s)v1(s) ds\n\u0013\ndPt\n\u0015\n.\nSince the price process P \u2208H2 can be replaced by its \ufb01nite-variation part A in the right-most\nexpectation, the asserted form of the optimal value now follows from Fubini\u2019s theorem and the\nde\ufb01nition of v1(t).\nRemark 3.2. The optimal trading rate (3.2) consists of two parts. The \ufb01rst prescribes to sell the\ninitial position at a deterministic rate to reduce inventory. The second exploits signals about future\nprice changes as summarized by a discounted conditional expectation of the asset\u2019s predictable drift.\nIf the asset price P is a martingale, the second term disappears and we obtain the classical optimal\nliquidation result of [3].\nRemark 3.3. The value function (3.3) consists of two parts: xP0 is the mark-to-market value\nof the position.\nThe second part, \u03bb(v0(0) + 2v1(0)x + v2(0)x2), is the expected risk-adjusted\nimplementation shortfall of the meta order under the optimal strategy.",
    "chunk_index": 4,
    "start_char": 9480,
    "end_char": 11879,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "the asset price P is a martingale, the second term disappears and we obtain the classical optimal\nliquidation result of [3].\nRemark 3.3. The value function (3.3) consists of two parts: xP0 is the mark-to-market value\nof the position.\nThe second part, \u03bb(v0(0) + 2v1(0)x + v2(0)x2), is the expected risk-adjusted\nimplementation shortfall of the meta order under the optimal strategy.\n4\nSolution for Target-Zone Models\nWe now apply Theorem 3.1 to the target-zone model of Example 2.1, where\nPt = Mt \u2212(M\u2217\nt \u2212\u00afP)+,\nt \u2208[0, T],\nfor a martingale M \u2208H2, its running maximum M\u2217\nt = maxs\u2208[0,t] Ms and a constant price cap\n\u00afP \u2265M0. The key to applying Theorem 3.1 is to compute the conditional expectation of future\nprice changes. For the target zone models, we have\nEt[Ps] = Et\n\u0002\nMs \u2212(M\u2217\ns \u2212\u00afP)+\u0003\n= Mt \u2212Et\n\u0002\n(M\u2217\ns \u2212\u00afP)+\u0003\n= Mt \u2212(M\u2217\nt \u2212\u00afP)+ \u2212Et\n\u0002\n(M\u2217\ns \u2212\u00afP)+ \u2212(M\u2217\nt \u2212\u00afP)+\u0003\n= Pt \u2212Et\n\u0002\n(M\u2217\ns \u2212\u00afP \u2228M\u2217\nt )+\u0003\n,\n0 \u2264t \u2264s \u2264T.\nThus, dAs = dsEt[Ps] = \u2212dsLs(t), where Ls(t) := Et[(M\u2217\ns \u2212\u00afP \u2228M\u2217\nt )+] is the price at time\nt \u2208[0, T] of a lookback call option on M with \ufb01xed strike \u00afP \u2228M\u2217\nt and maturity s \u2208[t, T] . We\nhave therefore reduced the computation of the optimal trading strategies from Theorem 3.1 to the\ncalculation of the \u201ctheta\u201d of a lookback call option written on the uncapped asset price. If the\nuncapped asset price follows arithmetic or geometric Brownian motions, the joint distribution of\nBrownian motion and its running maximum can in turn be used to compute the optimal trading\nstrategy explicitly.\n4.1\nBachelier Model\nSuppose that Mt := M0 + \u03c3Bt, where B is a standard one-dimensional Brownian motion, M0 \u2208R\nand \u03c3 > 0 are constants, so that Ls(t) is the price of a lookback call in the Bachelier model. A\nstraightforward calculation shows that\ndsLs(t) =\n\u03c3\n\u221as \u2212t\u03c6\n\u0012 \u00afP \u2212Pt\n\u03c3\u221as \u2212t\n\u0013\nds,\nwhere \u03c6(x) :=\n1\n\u221a\n2\u03c0\ne\u22121\n2x2.\n5\n\nThus, the optimal trading rate from Theorem 3.1 is \u02c6ut = \u00afu(t, X \u02c6u\nt , Pt), where\n\u00afu(t, x, p) := G\u2032(T \u2212t)\nG(T \u2212t) x + 1\n2\u03bb\nZ T\nt\n\u03c3\n\u221as \u2212t\nG(T \u2212s)\nG(T \u2212t) \u03c6\n\u0012\n\u00afP \u2212p\n\u03c3\u221as \u2212t\n\u0013\nds\n(4.1)\nand where the constant \u03b2 and the function G are de\ufb01ned as in Theorem 3.1. Setting\n\u00afuAC(t, x) := G\u2032(T \u2212t)\nG(T \u2212t) x\nand\n\u00afuBA(t, p) := \u00afu(t, 0, p) = \u00afu(t, x, p) \u2212\u00afuAC(t, x),\nwe observe that \u00afuAC is the optimal trading speed in the absence of a price cap, cf. [3], which does\nnot depend on the current asset price. In contrast, with a price cap, the optimal trading speed also\ndepends on the distance of the current asset price Pt from the cap \u00afP through \u00afuBA. Since \u00afuBA \u22650,\nthe position is liquidated at a higher rate if a price cap is present.",
    "chunk_index": 5,
    "start_char": 11498,
    "end_char": 14037,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "\u00afuBA(t, p) := \u00afu(t, 0, p) = \u00afu(t, x, p) \u2212\u00afuAC(t, x),\nwe observe that \u00afuAC is the optimal trading speed in the absence of a price cap, cf. [3], which does\nnot depend on the current asset price. In contrast, with a price cap, the optimal trading speed also\ndepends on the distance of the current asset price Pt from the cap \u00afP through \u00afuBA. Since \u00afuBA \u22650,\nthe position is liquidated at a higher rate if a price cap is present.\n0\n2\n0\n0.5\n1\n0\n1\n2\n\u00b710\u22122\n\u00afP \u2212p\nT \u2212t\nOptimal Rate \u00afu\nBachelier Model with \u03b3 = \u0393 = 10\u221205\n0\n2\n0\n0.5\n1\n0\n0.5\n1\n\u00b7104\n\u00afP \u2212p\nT \u2212t\nRelative Increase \u00afuBA/\u00afuAC\nBachelier Model with \u03b3 = \u0393 = 10\u221205\nFigure 1: Optimal trading rate (left) and relative increase of the trading rate over the Almgren-\nChriss rate (right) in the Bachelier model in the low inventory regime \u03b3 = \u0393 = 10\u22125. The other\nmodel parameters are T = 1, x = 1, \u03bb = 0.1, \u03c3 = 0.5.\nThe additional rate \u00afuBA(t, p) is decreasing in \u00afP \u2212p and maximized at p = \u00afP, i.e., most additional\ntrading happens when the current asset price is near the price cap. In [13], it is assumed that the\nasset is only sold when its price p coincides with the price cap \u00afP. Our unconstrained solution shows\nthat this assumption is justi\ufb01ed for small inventory costs \u03b3, \u0393. Indeed, suppose for simplicity that\n\u03b3 = \u0393, so that the function G depends on the model parameters only through \u03b2 =\np\n\u03b3/\u03bb:\nG(T \u2212t) = G(T \u2212t; \u03b2) = \u03b2 cosh(\u03b2(T \u2212t)) + \u03b22 sinh(\u03b2(T \u2212t)).\nOne then immediately veri\ufb01es that\nlim\n\u03b2\u21930 \u00afuAC(t, x; \u03b2) = 0\nand\nlim\n\u03b2\u21930 \u00afuBA(t, p; \u03b2) = 1\n2\u03bb\nZ T\nt\n\u03c3\n\u221as \u2212t\u03c6\n\u0012\n\u00afP \u2212p\n\u03c3\u221as \u2212t\n\u0013\nds.\nWhence, for small inventory costs \u03b3 = \u0393, the optimal trading rate is largely determined by \u00afuBA.\nIn particular, the trader sells at a high rate if the asset price p is close to the price cap \u00afP, whereas\nthe trading rate vanishes as the di\ufb00erence \u00afP \u2212p becomes large. For \u03b3 = \u0393 = 10\u22125 and\nT = 1,\nx = 1,\n\u03bb = 0.1,\n\u03c3 = 0.5,\n6\n\nthis is illustrated in Figure 1. There, we plot the optimal trading rate \u00afu and the relative increase\n(\u00afu\u2212\u00afuAC)/\u00afuAC = \u00afuBA/\u00afuAC of the optimal rate compared to the Almgren-Chriss solution as functions\nof length of the liquidation period T \u2212t and moneyness \u00afP \u2212p.\nWe observe that the optimal trading rate in this case is almost equal to zero if the asset price\np is away from the price cap \u00afP, i.e., the asset is only sold near the price cap. In particular, at\nthis critical level \u00afP, the trading rate with price cap is up to 104 times higher than the trading rate\nwithout price cap.",
    "chunk_index": 6,
    "start_char": 13613,
    "end_char": 16053,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "as functions\nof length of the liquidation period T \u2212t and moneyness \u00afP \u2212p.\nWe observe that the optimal trading rate in this case is almost equal to zero if the asset price\np is away from the price cap \u00afP, i.e., the asset is only sold near the price cap. In particular, at\nthis critical level \u00afP, the trading rate with price cap is up to 104 times higher than the trading rate\nwithout price cap. Therefore, in this low inventory cost regime, only allowing trades at p = \u00afP as\nin [13] is a reasonable approximation.\nThe corresponding results for higher inventory costs \u03b3 = \u0393 = 1 are reported in Figure 2.\n(All the other model parameters are the same as for the previous example.) We observe that the\noptimal rate is decreasing as a function of moneyness \u00afP \u2212p and increasing as a function of the\nliquidation period T \u2212t. The plot of the relative increase \u00afuBA/\u00afuAC shows that as the asset price p\napproaches the price cap \u00afP, there is an increase in the liquidation rate of initially more than 30%.\nThis additional e\ufb00ect is more pronounced if the trading horizon T \u2212t is large and vanishes for\nsmall liquidation periods. However, except for small values of moneyness, the qualitative shape of\nthe optimal rate is for the most part determined by the Almgren-Chriss rate \u00afuAC. In other words,\nunless the asset price is close to the price cap, the trader with higher inventory costs essentially\nneglects its presence.\n0\n2\n0\n0.5\n1\n0\n500\n\u00afP \u2212p\nT \u2212t\nOptimal Rate \u00afu\nBachelier Model with \u03b3 = \u0393 = 1\n0\n2\n0\n0.5\n1\n0\n0.2\n0.4\n\u00afP \u2212p\nT \u2212t\nRelative Increase \u00afuBA/\u00afuAC\nBachelier Model with \u03b3 = \u0393 = 1\nFigure 2: Optimal trading rate (left) and relative increase of the trading rate over the Almgren-\nChriss rate (right) in the Bachelier model in the moderate inventory regime \u03b3 = \u0393 = 1. The other\nmodel parameters are T = 1, x = 1, \u03bb = 0.1, \u03c3 = 0.5.\n4.2\nBlack-Scholes Model\nNow suppose that the uncapped asset price M follows a geometric Brownian motion, Mt :=\nM0 exp\n\u0000\u03c3Bt \u22121\n2\u03c32t\n\u0001\nfor a standard Brownian motion B and constants S0, \u03c3 > 0.\nThen, the\ncomputation of the optimal selling rate boils down to the computation of the theta of a lookback\ncall in the Black-Scholes model. A standard calculation shows\ndsLs(t) = Mt\n\u0014\n\u03c3\n\u221as \u2212t\u03c6\n\u0000f(s \u2212t, Mt, Pt)\n\u0001\n+ \u03c32\n2 \u03a6\n\u0000f(s \u2212t, Mt, Pt)\n\u0001\u0015\nds,\n7\n\nwhere \u03a6 denotes the cumulative distribution function of the standard normal law, and\nf(u, m, p) := \u03c3\u221au\n2\n\u2212\n1\n\u03c3\u221au log\n\u0012 \u00afP \u2212p\nm\n+ 1\n\u0013\n.\nTherefore, the optimal liquidation rate \u02c6u is\n\u02c6ut = \u00afuAC(t, X \u02c6u\nt ) + \u00afuBS(t, Mt, Pt),\nwhere \u00afuAC is de\ufb01ned as in the Bachelier model and\n\u00afuBS(t, m, p) := m\n2\u03bb\nZ T\nt\nG(T \u2212s)\nG(T \u2212t)\n\u0014\n\u03c3\n\u221as \u2212t\u03c6 (f(s \u2212t, m, p)) + 1\n2\u03c32\u03a6\n\u0000f(s \u2212t, m, p)\n\u0001\u0015\nds.",
    "chunk_index": 7,
    "start_char": 15659,
    "end_char": 18302,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "\u0000f(s \u2212t, Mt, Pt)\n\u0001\n+ \u03c32\n2 \u03a6\n\u0000f(s \u2212t, Mt, Pt)\n\u0001\u0015\nds,\n7\n\nwhere \u03a6 denotes the cumulative distribution function of the standard normal law, and\nf(u, m, p) := \u03c3\u221au\n2\n\u2212\n1\n\u03c3\u221au log\n\u0012 \u00afP \u2212p\nm\n+ 1\n\u0013\n.\nTherefore, the optimal liquidation rate \u02c6u is\n\u02c6ut = \u00afuAC(t, X \u02c6u\nt ) + \u00afuBS(t, Mt, Pt),\nwhere \u00afuAC is de\ufb01ned as in the Bachelier model and\n\u00afuBS(t, m, p) := m\n2\u03bb\nZ T\nt\nG(T \u2212s)\nG(T \u2212t)\n\u0014\n\u03c3\n\u221as \u2212t\u03c6 (f(s \u2212t, m, p)) + 1\n2\u03c32\u03a6\n\u0000f(s \u2212t, m, p)\n\u0001\u0015\nds.\nAs in the Bachelier model, the trader liquidates the position at a higher rate if a price cap is present\nsince \u00afuBS \u22650, and the e\ufb00ect is more pronounced for small trading costs \u03bb. Moreover, we have\nd\ndf\nh\n\u03c3\n\u221as \u2212t\u03c6 (f) + 1\n2\u03c32\u03a6\n\u0000f\n\u0001i\n=\n\u03c3\n\u221as \u2212t\u03c6(f)\nh\u03c3\u221as \u2212t\n2\n\u2212f\ni\n,\nfrom which we infer that \u00afuBS is decreasing in \u00afP \u2212p since f(s\u2212t, m, p) \u2264\u03c3\u221as \u2212t/2 and f is clearly\ndecreasing in \u00afP \u2212p. This is again in line with our \ufb01ndings in the Bachelier model. In contrast to\nthe Bachelier model, however, the additional rate \u00afuBS also depends on the uncapped asset price\nMt and the dependence is monotonically increasing.\nNevertheless, the numerical results in the\nBlack-Scholes model are qualitatively very similar to their counterparts for the Bachelier model,\ncompare [10]; we therefore do not report them here.\nReferences\n[1] A. Alfonsi, A. Fruth, and A. Schied. Optimal execution strategies in limit order books with general\nshape functions. Quant. Finance, 10(2):143\u2013157, 2010.\n[2] R.F. Almgren. Optimal trading with stochastic liquidity and volatility. SIAM J. Fin. Math., 3(1):\n163\u2013181, 2012.\n[3] R.F. Almgren and N. Chriss. Optimal execution of portfolio transactions. J. Risk, 3:5\u201340, 2001.\n[4] S. Ankirchner, M. Jeanblanc, and T. Kruse. BSDEs with singular terminal condition and a control\nproblem with constraints. SIAM J. Control Optim., 52(2):893\u2013913, 2014.\n[5] P. Bank, H.M. Soner, and M. Vo\u00df. Hedging with temporary price impact. Math. Fin. Econ., 11(2):\n215\u2013239, 2017.\n[6] D. Bertsimas and A. W. Lo. Optimal control of execution costs. J. Fin. Markets, 1(1):1\u201350, 1998.\n[7] B. Bouchard, M. Fukasawa, M. Herdegen, and J. Muhle-Karbe. Equilibrium returns with transaction\ncosts. Finance Stoh., 22(3):569\u2013601, 2018.\n[8] \u00b4Alvaro Cartea and Sebastian Jaimungal. Incorporating order-\ufb02ow into optimal execution. Math. Fin.\nEcon., 10(3):339\u2013364, 2016.\n[9] I. Ekeland and R. Temam. Convex analysis and variational problems. SIAM, Philadeplphia, PA, 1999.\n[10] P.A. Forsyth, J.S. Kennedy, S.T. Tse, and H. Windcli\ufb00. Optimal trade execution: a mean quadratic\nvariation approach. J. Econ. Dyn. Control, 36(12):1971\u20131991, 2012.\n[11] P. Graewe, U. Horst, and J. Qiu. A non-markovian liquidation problem and backward SPDEs with\nsingular terminal conditions. SIAM J. Control Optim., 53(2):690\u2013711, 2015.\n[12] C.-A. Lehalle and E. Neuman. Incorporating signals into optimal trading. Preprint, 2017.\n[13] E. Neuman and A. Schied. Optimal portfolio liquidation in target zone models and catalytic superpro-\ncesses.",
    "chunk_index": 8,
    "start_char": 17872,
    "end_char": 20802,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "1999.\n[10] P.A. Forsyth, J.S. Kennedy, S.T. Tse, and H. Windcli\ufb00. Optimal trade execution: a mean quadratic\nvariation approach. J. Econ. Dyn. Control, 36(12):1971\u20131991, 2012.\n[11] P. Graewe, U. Horst, and J. Qiu. A non-markovian liquidation problem and backward SPDEs with\nsingular terminal conditions. SIAM J. Control Optim., 53(2):690\u2013711, 2015.\n[12] C.-A. Lehalle and E. Neuman. Incorporating signals into optimal trading. Preprint, 2017.\n[13] E. Neuman and A. Schied. Optimal portfolio liquidation in target zone models and catalytic superpro-\ncesses. Finance Stoch., 20(2):495\u2013509, 2016.\n[14] A.A. Obizhaeva and J. Wang. Optimal trading strategy and supply/demand dynamics. J. Fin. Markets,\n16(1):1\u201332, 2013.\n[15] S. Predoiu, G. Shaikhet, and S.E. Shreve. Optimal execution in a general one-sided limit-order book.\nSIAM J. Fin. Math., 2(1):183\u2013212, 2011.\n[16] A. Schied. A control problem with fuel constraint and Dawson\u2013Watanabe superprocesses. Ann. Appl.\nProbab., 23(6):2472\u20132499, 2013.\n8",
    "chunk_index": 9,
    "start_char": 20247,
    "end_char": 21242,
    "paper_title": "Optimal Trading with General Signals and Liquidati",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_Trading_with_General_Signals_and_Liquidati.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_Trading_with_General_Signals_and_Liquidati.pdf"
  },
  {
    "text": "Optimal order placement in limit order markets\nRama Cont and Arseniy Kukanov\nImperial College London & AQR Capital Management\nTo execute a trade, participants in electronic equity markets may choose to submit limit orders or market\norders across various exchanges where a stock is traded. This decision is in\ufb02uenced by characteristics of\nthe order \ufb02ows and queue sizes in each limit order book, as well as the structure of transaction fees and\nrebates across exchanges. We propose a quantitative framework for studying this order placement problem\nby formulating it as a convex optimization problem. This formulation allows to study how the optimal\norder placement decision depends on the interplay between the state of order books, the fee structure, order\n\ufb02ow properties and the aversion to execution risk. In the case of a single exchange, we derive an explicit\nsolution for the optimal split between limit and market orders. For the general case of order placement across\nmultiple exchanges, we propose a stochastic algorithm that computes the optimal routing policy and study\nthe sensitivity of the solution to various parameters. Our solution exploits data on recent order \ufb01lls across\nexchanges in the numerical implementation of the algorithm.\nKey words : limit order markets, optimal order execution, execution risk, order routing, fragmented\nmarkets, transaction costs, \ufb01nancial engineering, stochastic approximation, Robbins-Monro algorithm\nFirst version: October 2012. Revised: October 2014.\n1\narXiv:1210.1625v4 [q-fin.TR] 23 Nov 2014\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n2\n1. Introduction\nThe trading process in today\u2019s automated \ufb01nancial markets can be divided into several stages,\neach taking place on a di\ufb00erent time horizon: portfolio allocation decisions are made over a time\nscales of weeks or days and translate into trades that are executed over time intervals of several\nminutes to several days through streams of orders placed at high frequency, sometimes thousands\nin a single minute (Cont 2011). Existing studies on optimal trade execution have investigated how\nthe execution cost of a large trade may be reduced by splitting it into multiple orders spread in\ntime. Once this order scheduling decision is made, one still needs to specify how each individual\norder should be placed: this order placement decision involves the choice of an order type (limit\norder or market order), order size and destination, when multiple trading venues are available. For\nexample, in the U.S. equity market there are more than ten active exchanges where a trader can\nbuy or sell the same securities. Order placement in a fragmented market is a non-trivial task and\nbrokers o\ufb00er their clients smart order routing systems in addition to (and often separately from)\ntheir suite of trade execution algorithms. We focus here on this order placement problem: given an\norder which has been scheduled, choosing an order type \u2013market or limit order\u2013 and which trading\nvenue(s) to submit it to.\nBrokers and other active market participants need to make order placement and order routing\ndecisions repeatedly, thousands of times a day, and their outcomes have a signi\ufb01cant impact on each\nparticipant\u2019s transaction cost.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3261,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "a non-trivial task and\nbrokers o\ufb00er their clients smart order routing systems in addition to (and often separately from)\ntheir suite of trade execution algorithms. We focus here on this order placement problem: given an\norder which has been scheduled, choosing an order type \u2013market or limit order\u2013 and which trading\nvenue(s) to submit it to.\nBrokers and other active market participants need to make order placement and order routing\ndecisions repeatedly, thousands of times a day, and their outcomes have a signi\ufb01cant impact on each\nparticipant\u2019s transaction cost. An empirical study of proprietary order data from a large execution\nbroker by Battalio et al. (2013) demonstrates that brokers use both limit and marketable orders\nto execute trades, and the distribution of their orders across trading venues points to strategic\nrouting behavior. Order execution quality is materially a\ufb00ected by order routing choices which\nrecently motivated a number of inquiries by regulators into brokers\u2019 ability to optimally place orders\non behalf of their clients (U.S. Senate 2014, Lynch and Flitter 2014, Phillips 2014). The choice\nbetween limit and market orders and their routing is important for most market participants, not\njust brokers. For instance, high-frequency traders can opportunistically provide liquidity with limit\norders or demand it with marketable orders and a large group of \u201cmixed\u201d high-frequency strategies\nindeed relies on both order types in various proportions (see Baron et al. (2014), Brogaard et al.\n(2014)). At the same time market-makers that simultaneously provide liquidity on multiple trading\nvenues (see Menkveld (2013)) need to take into account the fee and rebate structure and the\ncurrent state of limit order books at these venues. In aggregate, strategic order placement and\nrouting choices made by a variety of traders shape order \ufb02ow dynamics in a fragmented market.\nBoehmer and Jennings (2007) \ufb01nd evidence that marketable orders are sent to trading venues that\nprovide lower execution costs and Foucault and Menkveld (2008) show that trading fees and the\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n3\nnumber of available venues a\ufb00ect consolidated market depth. In Moallemi et al. (2012) market\norders gravitate towards exchanges with larger posted quote sizes and low fees, while limit orders\nare submitted to exchanges with high rebates and lower execution waiting times. The importance\nof order placement and routing decisions on trading performance of individual market participants\njusti\ufb01es a more detailed modeling of order placement and routing decisions.\n1.1. Literature review\nTheoretical studies of limit order markets have previously considered the choice of market/limit\norder type and execution venue using stylized models of trader behavior. In Foucault (1999),\nFoucault et al. (2005), Rosu (2009) traders can submit one limit or market order for one trading\nunit to a single exchange. There are no order queues and bid-ask spreads are determined in a\ncompetitive equilibrium. The choice between a limit and a market order by each trader depends\non his exogenous patience parameter and the spread that he observes upon arrival.",
    "chunk_index": 1,
    "start_char": 2695,
    "end_char": 5908,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "market/limit\norder type and execution venue using stylized models of trader behavior. In Foucault (1999),\nFoucault et al. (2005), Rosu (2009) traders can submit one limit or market order for one trading\nunit to a single exchange. There are no order queues and bid-ask spreads are determined in a\ncompetitive equilibrium. The choice between a limit and a market order by each trader depends\non his exogenous patience parameter and the spread that he observes upon arrival. In Parlour\n(1998) traders choose to place a single-unit limit or market order based on their patience and\na probability of limit order execution. There is a single exchange and traders form queues at\nexogenously given bid/ask prices. Foucault and Menkveld (2008) present a model where limit order\ntraders form queues at two exchanges based on a pro\ufb01t break-even condition and then a broker can\nchoose to send market orders to one or both venues. These models describe market dynamics and\ntrader behavior in equilibrium, but require strong simplifying assumptions regarding each trader\u2019s\norder placement and routing behavior. Our analysis complements existing literature by studying\nthe order placement decision itself and providing insights into its structure. For instance, most\ntheoretical models assume that market participants need to execute one trading lot, whereas in\npractice traders often need to place larger orders (e.g. the average limit order size in the U.S.\nequity market is close to 400 shares). We \ufb01nd that the size of an order to be placed is actually\none of the most interesting input variables. It largely determines the optimal mix of market and\nlimit orders as well as traders\u2019 sensitivity to exchange fees and rebates. Stylized models of trader\nbehavior that previously appeared in theoretical literature translate into simple, binary choices of\norder type and venue based on one or two variables (e.g. bid-ask spread and trader patience). Our\nanalysis shows that order placement and routing decisions are more complex and need to be based\non a variety of factors. Comparing the performance of our optimal order placement strategy to\nsimpler rules-of-thumb for a range of parameters we \ufb01nd that simple heuristics are too in\ufb02exible\nand often result in large transaction costs. This further motivates a need for a detailed analysis of\norder placement and routing decisions, but there are few studies dedicated to this topic.\nA reduced-form model for routing an in\ufb01nitesimal limit order to one destination is presented in\nMoallemi et al. (2012), Almgren and Harts (2008) propose a market order routing algorithm in\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n4\npresence of hidden liquidity, while Ganchev et al. (2010) and Laruelle et al. (2011) propose numer-\nical algorithms to optimize order executions across multiple dark pools, where supply/demand is\nunobserved. To the best of our knowledge this work is the \ufb01rst to provide a detailed treatment of\ntrader\u2019s order placement decision in a multi-exchange market.\nOur results also complement the literature on optimal trade execution.",
    "chunk_index": 2,
    "start_char": 5437,
    "end_char": 8554,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "Kukanov: Optimal order placement and routing in limit order markets\n4\npresence of hidden liquidity, while Ganchev et al. (2010) and Laruelle et al. (2011) propose numer-\nical algorithms to optimize order executions across multiple dark pools, where supply/demand is\nunobserved. To the best of our knowledge this work is the \ufb01rst to provide a detailed treatment of\ntrader\u2019s order placement decision in a multi-exchange market.\nOur results also complement the literature on optimal trade execution. Early work on this subject\n(Bertsimas and Lo 1998, Almgren and Chriss 2000) focused on the scheduling of orders in time\nbut did not explicitly model the process whereby each order is \ufb01lled. More recent formulations\nhave tried to incorporate some elements in this direction. In one stream of literature (Obizhaeva\nand Wang 2012, Alfonsi et al. 2010, Predoiu et al. 2011) traders are restricted to using only\nmarket orders whose execution costs are given by an idealized order book shape function. Another\napproach has been to model the process through which an order is \ufb01lled as a dynamic random\nprocess (Cont 2011, Cont and De Larrard 2013) leading to a formulation of the optimal execution\nproblem as a stochastic control problem. This formulation has been studied in various settings with\nlimit orders (Bayraktar and Ludkovski 2011, Gueant and Lehalle 2014) or limit and market orders\n(Guilbaud and Pham 2014, Huitema 2012, Guo et al. 2013, Li 2013) but its complexity makes\nit computationally intractable unless restrictive assumptions are made on price and order book\ndynamics. For example, these studies commonly assume that a trader places a single limit order\nfor one unit at a time and its execution probability is given by a simpli\ufb01ed function of distance\nto best quotes. In our formulation limit order \ufb01lls are based on order quantity, queue position and\norder \ufb02ows generated by other traders, as they are in actual limit order books. Existing approaches\nto optimal trade execution do not consider the option of simultaneously placing limit orders on\nmultiple exchanges, the queue position of individual orders and the possibility of receiving partial\n\ufb01lls, all of which play a central role in our model.\n1.2. Summary of contributions\nIn the present work, we adopt a more tractable approach which is closer to order routing methods\nused in practice, by separating the order placement decision from the scheduling decision: assuming\nthat the trade execution schedule has been speci\ufb01ed, we focus on the task of \ufb01lling the scheduled\nbatch of orders (a trade \u201cslice\u201d) by optimally distributing it across trading venues and order types.\nDecoupling the scheduling problem from the order placement problem is closer to market practice\n(see e.g. Almgren and Harts (2008)) and leads to a more tractable approach allowing us to incorpo-\nrate some realistic features which matter for order placement decisions, while conserving analytical\ntractability. Although simultaneous optimization of order timing, type and routing decisions is an\ninteresting problem, it also appears to be intractable and its solution would likely omit details that\nare important for either order placement or scheduling.",
    "chunk_index": 3,
    "start_char": 8058,
    "end_char": 11250,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "trading venues and order types.\nDecoupling the scheduling problem from the order placement problem is closer to market practice\n(see e.g. Almgren and Harts (2008)) and leads to a more tractable approach allowing us to incorpo-\nrate some realistic features which matter for order placement decisions, while conserving analytical\ntractability. Although simultaneous optimization of order timing, type and routing decisions is an\ninteresting problem, it also appears to be intractable and its solution would likely omit details that\nare important for either order placement or scheduling.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n5\nOur key contribution is a quantitative formulation of the order placement problem which illus-\ntrates how various factors - the size of an order to be placed, lengths of order queues across\nexchanges, statistical properties of order \ufb02ows in these exchanges, trader\u2019s execution preferences,\nand the structure of liquidity rebates across trading venues - blend into an optimal allocation of\nlimit orders and market orders across available trading venues. When only one exchange is avail-\nable for execution, this order placement problem reduces to the problem of choosing an optimal\nsplit between market orders and limit orders. We derive an explicit solution for this problem and\nanalyze its sensitivity to the order size, the trader\u2019s urgency for \ufb01lling the order and other factors.\nIn a case of multiple exchanges we also derive a characterization of the optimal order allocation\nacross trading venues. Finally, we propose a fast and \ufb02exible numerical method for solving the\norder placement problem in a general case and demonstrate its e\ufb03ciency through examples.\nAn important aspect of our framework is to account for execution risk, i.e. the risk of not \ufb01lling\nan order. Previous studies focus on the risk of price variations over the course of a trade execution\nAlmgren and Chriss (2000), Huberman and Stanzl (2005) but assume that orders are always \ufb01lled.\nHowever execution risk is a major concern for strategies that involve limit orders (Harris and\nHasbrouck 1996). When it is costly to catch up on the un\ufb01lled portion of the order, we \ufb01nd that\nthe optimal allocation shifts from limit to market orders. Although market orders are executed\nat a less favorable price, it becomes optimal to use them when the execution risk is a primary\nconcern - for example when execution is subject to a deadline or when traders have time-sensitive\ninformation about returns.\nOptimal limit order sizes are strongly in\ufb02uenced by queue position that they can achieve at each\nexchange and by distributions of order out\ufb02ows from these queues. For example, if the queue size\nat one of the exchanges is much smaller than the expected future order out\ufb02ow there, it is optimal\nto place a larger limit order on that exchange. Moallemi et al. (2012) argue that such favorable\nlimit order placement opportunities vanish in equilibrium due to competition and strategic order\nrouting of individual traders. However their empirical results also show that short-term deviations\nfrom the equilibrium are a norm, and can therefore be exploited in our optimization framework.",
    "chunk_index": 4,
    "start_char": 10665,
    "end_char": 13883,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "exchange and by distributions of order out\ufb02ows from these queues. For example, if the queue size\nat one of the exchanges is much smaller than the expected future order out\ufb02ow there, it is optimal\nto place a larger limit order on that exchange. Moallemi et al. (2012) argue that such favorable\nlimit order placement opportunities vanish in equilibrium due to competition and strategic order\nrouting of individual traders. However their empirical results also show that short-term deviations\nfrom the equilibrium are a norm, and can therefore be exploited in our optimization framework.\nOur order placement model brings new insights into the structure of order placement decisions.\nWe \ufb01nd that the targeted execution size plays an important role due to a bounded execution capacity\nof limit orders. Relatively small quantities can be executed with a high probability using limit orders\nplaced just at the cheapest exchange. Faced with progressively larger quantities a trader realizes\nthat \ufb01lling the entire amount with a single order at the cheapest exchange is unlikely and is forced\nto place orders on more expensive venues. Interestingly, for relatively large quantities the optimal\norder allocation becomes practically insensitive to rebates as the non-execution risk outweighs the\ncost of placing limit orders on more expensive exchanges. To \ufb01ll even larger quantities a trader\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n6\nneeds to start using market orders. After some point the optimal market order size increases linearly\nwith the targeted execution quantity while limit order sizes remain bounded at a certain value. As\na result of this feature, a larger fraction of the targeted quantity is executed with market orders\nwhen that quantity is relatively large.\nAnother important aspect that appears in our analysis is the tendency of an optimal order\nallocation to place more orders than it needs to \ufb01ll. This overbooking behavior is due to the\npossibility of receiving partial \ufb01lls and the availability of multiple exchanges in our model. Instead\nof placing one big limit order, a trader can place more orders to all available venues, collect\ntheir \ufb01lls and cancel the excess orders afterwards. This results in a reduction of non-execution\nrisk because limit order \ufb01lls are not perfectly correlated. Since each venue adds new limit order\nexecution opportunities, overbooking becomes more prominent as the number of available venues\nincreases. This may explain the so-called \u201cphantom liquidity\u201d observed in the U.S. equity market,\ni.e. a large amount of limit orders that are quickly canceled before market order traders can\nexecute against them. Assuming that market-makers and other limit order traders indeed post\nmultiple orders across exchanges and cancel all substitute orders once one of them is \ufb01lled, these\n\u201cphantom\u201d orders may represent rational attempts to reduce risk rather than a manipulative\npractice. Further extrapolating this overbooking behavior to all limit order traders, our model\nwould predict an increase in consolidated depth with each additional trading venue.",
    "chunk_index": 5,
    "start_char": 13299,
    "end_char": 16431,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "can\nexecute against them. Assuming that market-makers and other limit order traders indeed post\nmultiple orders across exchanges and cancel all substitute orders once one of them is \ufb01lled, these\n\u201cphantom\u201d orders may represent rational attempts to reduce risk rather than a manipulative\npractice. Further extrapolating this overbooking behavior to all limit order traders, our model\nwould predict an increase in consolidated depth with each additional trading venue. This intuition\nis similar to Foucault and Menkveld (2008), although in their model limit orders are placed only\nby pro\ufb01t-maximizing market-makers that would not post orders at an expensive venue. In our\nmodel, overbooking (and thus an increase in consolidated depth) can occur despite high exchange\nfees, which suggests that consolidated depth increases with the introduction of additional trading\nvenues, as long as these venues allow limit order traders to diversify their execution risk.\n1.3. Outline\nSection 2 describes our formulation of the order placement problem and presents conditions for the\nexistence of an optimal order placement. In Section 3.1 we derive an optimal split between market\nand limit orders for a single exchange. Section 3.2 analyzes the general case of order placement on\nmultiple trading venues. Section 4 presents a numerical algorithm for solving the order placement\nproblem in a general case and studies its convergencee properties. Section 5.1 analyses the structure\nof order placement decisions through comparative statics. Section 5.2 presents an application of\nour method to historical tick data and Section 6 concludes.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n7\n2. The order placement problem\n2.1. Decision variables\nConsider a trader who needs to buy S shares of a stock within a short time interval [0,T]. The\ndeadline T may be a \ufb01xed time horizon such as 1 minute, or a random stopping time, for instance\ntriggered by price changes or cumulative trading volume. The execution target S is assumed to\nbe relatively small - it is a slice of the daily trade that the trader expects to \ufb01ll during [0,T].\nNevertheless, this slice S is usually larger than a single trading lot because limit orders may have\nto queue for execution for several seconds or even minutes. Our objective is to de\ufb01ne a meaningful\nframework in which the trader can compare alternative approaches to executing this trade slice\nS, for example choose between sending a limit order to a single exchange, splitting it in some\nproportion across K exchanges or using a combination of market and limit orders. We assume the\nfollowing two-step execution strategy - at time 0 the trader may submit K limit orders for Lk \u22650\nshares to exchanges k = 1,...,K and also market orders for M \u22650 shares.\nAt time T if the total executed quantity is less than S the trader also submits a market order\nto execute the remaining amount. The trader\u2019s order placement decision is thus summarized by a\nvector X = (M,L1,...,LK) \u2208RK+1\n+\nof order sizes.",
    "chunk_index": 6,
    "start_char": 15966,
    "end_char": 19006,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "the\nfollowing two-step execution strategy - at time 0 the trader may submit K limit orders for Lk \u22650\nshares to exchanges k = 1,...,K and also market orders for M \u22650 shares.\nAt time T if the total executed quantity is less than S the trader also submits a market order\nto execute the remaining amount. The trader\u2019s order placement decision is thus summarized by a\nvector X = (M,L1,...,LK) \u2208RK+1\n+\nof order sizes. The components of X are non-negative (only\nbuy orders are allowed) and we assume that the trader has no other (e.g. pre-existing) orders in\nthe market.\n2.2. Order execution\nWe will assume for simplicity that a market order of any size up to S can be \ufb01lled immediately at\nany single exchange. Thus a trader chooses the cheapest venue for his market orders.\nLimit orders with quantities (L1,...,LK) join queues of (Q1,...,QK) orders in K limit order\nbooks, where Qk \u22650. As a simpli\ufb01cation we assume that all of these limit orders have the same\nprice - the highest bid price across venues, i.e. the National Best Bid. The case Qk = 0 is allowed in\nour model and corresponds to placing limit orders inside the bid-ask spread at one of the venues.\nDenote by (x)+ = max(x,0). Using the assumption that limit orders are not modi\ufb01ed before\ntime T, we can explicitly calculate their \ufb01lled amounts (full or partial) as a function of their initial\nqueue position and future order \ufb02ow:\nmin(max(\u03bek \u2212Qk,0),Lk) = (\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+,\nk = 1,...,K\nwhere \u03bek is a total out\ufb02ow from the front of the k-th queue. The order out\ufb02ow \u03bek consists of\norder cancelations that occurred before time T from queue positions in front of an order Lk, and\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n8\nFigure 1\nLimit order execution on exchange k depends on the order size Lk, the queue Qk in front of it, total\nsizes of order cancelations Ck and marketable orders Dk, speci\ufb01cally on \u03bek = Ck + Dk.\nmarketable orders that reach the k-th exchange before T. The mechanics of limit order \ufb01lls in a\nFIFO queue are illustrated in Figure 1.\nWe note that limit order \ufb01ll amounts are random because they depend on queue out\ufb02ows \u03be =\n(\u03be1,...,\u03beK) during [0,T], modeled as a random variable with a distribution F. Our formulation also\ndoes not require specifying the distribution of \u03be. The total amount of shares A(X,\u03be) bought by the\ntrader during [0,T) can be written as a function of the initial order allocation X and intermediate\nqueue out\ufb02ows \u03be:\nA(X,\u03be) = M +\nK\nX\nk=1\n((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+)\n(1)\nIf the executed amount A(X,\u03be) < S we assume that the trader submits a market order at time T\nensuring the execution of this trade slice S.\n2.3. Cost function\nThe trader\u2019s objective is to minimize the sum of explicit and implicit costs associated with order\nexecution.",
    "chunk_index": 7,
    "start_char": 18595,
    "end_char": 21372,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "the\ntrader during [0,T) can be written as a function of the initial order allocation X and intermediate\nqueue out\ufb02ows \u03be:\nA(X,\u03be) = M +\nK\nX\nk=1\n((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+)\n(1)\nIf the executed amount A(X,\u03be) < S we assume that the trader submits a market order at time T\nensuring the execution of this trade slice S.\n2.3. Cost function\nThe trader\u2019s objective is to minimize the sum of explicit and implicit costs associated with order\nexecution. There is a variety of explicit costs that vary by exchange. Most U.S. equity exchanges\ncharge fees to market order traders for consuming liquidity and pay rebates to limit order traders\nfor providing it. In contrast, some inverse exchanges pay traders for marketable orders and charge\nfor providing liquidity. For example, at the time of writing the U.S. equity exchange Direct Edge\nEDGA had a negative rebate \u2212$0.0006 per share for passive orders and a negative fee \u2212$0.0004\nper share for marketable orders1. These fees and rebates are economically signi\ufb01cant and similar\nin magnitude to bid-ask spread costs ($0.0050 per share for liquid U.S. stocks). Exchanges outside\n1 See Battalio et al. (2013) for a comprehensive summary of fees charged by U.S. equity exchanges.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n9\nof U.S. have adopted similar fee/rebate pricing structures (e.g. BATS Chi-X Europe), or special\nrebate programs for liquidity providers (e.g. Singapore Stock Exchange, BMF-BOVESPA).\nImplicit execution costs include adverse selection and market impact. Adverse selection is\nre\ufb02ected in the correlation between limit order executions and price changes (Glosten and Milgrom\n(1985)). For example, after a buy limit order is \ufb01lled prices tend to go down below its limit price\ncreating an immediate loss for a limit order trader. The magnitude of adverse selection losses varies\nby exchange, and venues with high rebates are typically exposed to more adverse selection (see\nBattalio et al. (2013)) which can be explained in a rational equilibrium model of Moallemi et al.\n(2012). Small but consistent losses on limit order \ufb01lls can accumulate to a signi\ufb01cant adverse selec-\ntion cost over time, which motivates us to use e\ufb00ective rebates rk = re\nk +ASk, where re\nk are rebates\nset by exchanges and ASk are exchange-speci\ufb01c penalties for adverse selection. In practice these\npenalties are often chosen empirically as average returns measured over a short interval following\na limit order execution.\nUsing the mid-quote price as a benchmark, we calculate execution costs relative to mid-quote\nfor an order allocation X = (M,L1,...,LK) as:\n(h + f)M \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+),\n(2)\nwhere h is one-half of the bid-ask spread at time 0, f is a fee for market orders and rk are e\ufb00ective\nrebates for limit orders on exchanges k = 1,...,K.\nIn addition to average adverse selection losses ASk incurred on \ufb01lled limit orders, a trader may\nexperience a shortfall due to un\ufb01lled limit orders.",
    "chunk_index": 8,
    "start_char": 20932,
    "end_char": 23919,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "for an order allocation X = (M,L1,...,LK) as:\n(h + f)M \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+),\n(2)\nwhere h is one-half of the bid-ask spread at time 0, f is a fee for market orders and rk are e\ufb00ective\nrebates for limit orders on exchanges k = 1,...,K.\nIn addition to average adverse selection losses ASk incurred on \ufb01lled limit orders, a trader may\nexperience a shortfall due to un\ufb01lled limit orders. In the event A(X,\u03be) < S the trader has to\npurchase the remaining S \u2212A(X,\u03be) shares at time T with a costly market order. Adverse selection\nimplies that conditionally on this event prices have likely increased and the cost of market orders\nat time T is higher than their cost at time 0, i.e. \u03bbu > h + f. Alternatively, in the event A(X,\u03be) >\nS the prices likely decreased even more than after an average limit order \ufb01ll, and \u03bbo measures\nthis additional adverse selection cost. To capture this execution risk we include, in the objective\nfunction, a penalty for violations of target quantity in both directions:\n\u03bbu (S \u2212A(X,\u03be))+ + \u03bbo (A(X,\u03be) \u2212S)+ ,\n(3)\nwhere \u03bbu \u22650,\u03bbo \u22650 are marginal penalties in dollars per share for, respectively falling behind or\nexceeding the execution target S. In addition to adverse selection, the penalties \u03bbu,\u03bbo may re\ufb02ect\ntrader\u2019s private execution preferences. Generally, a trader can tolerate some di\ufb00erences between\nA(X,\u03be) and S because S, T are fractions of the overall trade quantity and time horizon. The\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n10\npenalties need not be symmetric - a trader with a positive forecast of short-term returns within\nthe period T has a larger opportunity cost and may set \u03bbu > \u03bbo.\nWe also include market impact as a function of the volume of submitted orders. The target\nquantity S is assumed to be small so orders (M,L1,...,LK) may have little immediate impact\non prices in the interval [0,T]. However, this impact may accumulate over the course of trading.\nAccounting for average impact costs is important: it penalizes order placement strategies that\nsubmit too many orders or orders that are too large. Empirical studies show that both market and\nlimit orders a\ufb00ect prices, and the average impact of small orders can be well approximated by a\nlinear function (Cont et al. (2014)), as in Kyle (1985). We assume that the impact cost is paid on\nall orders placed at times 0 and T, irrespective of whether they are \ufb01lled, leading to the following\ntotal impact:\n\u03b8\n \nM +\nK\nX\nk=1\nLk + (S \u2212A(X,\u03be))+\n!\n,\n(4)\nwhere \u03b8 > 0 is the impact coe\ufb03cient.\nAdding these di\ufb00erent terms we obtain:\nDefinition 1 (Cost function). The cost function is de\ufb01ned as the sum of explicit and implicit\nexecution costs:\nv(X,\u03be) : = (h + f)M \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+)\n+ \u03b8\n \nM +\nK",
    "chunk_index": 9,
    "start_char": 23513,
    "end_char": 26285,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "T, irrespective of whether they are \ufb01lled, leading to the following\ntotal impact:\n\u03b8\n \nM +\nK\nX\nk=1\nLk + (S \u2212A(X,\u03be))+\n!\n,\n(4)\nwhere \u03b8 > 0 is the impact coe\ufb03cient.\nAdding these di\ufb00erent terms we obtain:\nDefinition 1 (Cost function). The cost function is de\ufb01ned as the sum of explicit and implicit\nexecution costs:\nv(X,\u03be) : = (h + f)M \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+)\n+ \u03b8\n \nM +\nK\nX\nk=1\nLk + (S \u2212A(X,\u03be))+\n!\n+ \u03bbu (S \u2212A(X,\u03be)))+ + \u03bbo (A(X,\u03be) \u2212S)+\n(5)\nIt involves the following ingredients:\n\u2022 Execution objectives: target quantity S, time horizon T\n\u2022 Trading costs: half of bid-ask spread h, market order fee f and e\ufb00ective limit order rebates\nrk, market impact coe\ufb03cient \u03b8, penalties for under- and over\ufb01lling the target \u03bbu,\u03bbo\n\u2022 Market con\ufb01guration: number of exchanges K, limit order queues Qk.\n2.4. Optimal order placement problem\nWe can now formulate the search for an optimal order placement as a cost minimization problem:\nProblem 1 (Optimal order placement problem) An optimal order placement is a vector\nX\u2217\u2208RK+1\n+\nsolution of\nmin\nX\u2208RK+1\n+\nV (X)\n(6)\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n11\nwhere\nV (X) = E[v(X,\u03be)] =\nZ\nRd F(dy)v(X,y)\n(7)\nis the expected execution cost for the allocation X and the expectation is taken with respect to the\ndistribution F of order out\ufb02ows (\u03be1,...,\u03beK) at horizon T.\nThe output is an order allocation X\u22c6= (M \u22c6,L\u22c6\n1,...,L\u22c6\nK) consisting of a market order quantity M \u22c6\nand limit order quantities L\u22c6\n1,...,L\u22c6\nK which minimizes the expected execution cost over [0,T].\n2.5. Discussion\nBefore proceeding to results we discuss some of the important assumptions in our model and their\nimplications.\nContinuous decision variables: In reality orders are integer multiples of a share; however batch\nsizes are often large and one can neglect in \ufb01rst instance the granularity of orders and optimize\nover X \u2208RK+1\n+\n, then round to number of shares in the last step. This procedure, which corresponds\nto the convex relaxation of the underlying integer optimization problem (Williamson and Shmoys\n2011), is indeed the standard approach used in the optimal execution literature Almgren and Chriss\n(2000), Alfonsi et al. (2010), Bayraktar and Ludkovski (2011), Guilbaud and Pham (2014).\nStatic optimization problem: In a dynamic setting, one would need to solve Problem 1 one-step\nahead, using the conditional distribution of \u03be if known:\nV (tk,X\u2217\nk) =\nmin\nX\u2208RK+1\n+\nE[v(X,\u03bet)|Ft]\n(8)\nso insights from Problem 1 are useful for understanding the dynamic version of the problem.\nProblem 1 may be seen as the stationary/ ergodic version of the problem, in which one considers\nthe average cost over many trades, rather than the one-step-ahead execution cost for a single\norder placement. An alternative approach to order placement based on a constrained optimization\nproblem is discussed in the Appendix.",
    "chunk_index": 10,
    "start_char": 25898,
    "end_char": 28762,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "X\u2208RK+1\n+\nE[v(X,\u03bet)|Ft]\n(8)\nso insights from Problem 1 are useful for understanding the dynamic version of the problem.\nProblem 1 may be seen as the stationary/ ergodic version of the problem, in which one considers\nthe average cost over many trades, rather than the one-step-ahead execution cost for a single\norder placement. An alternative approach to order placement based on a constrained optimization\nproblem is discussed in the Appendix.\nExecution certainty for market orders: This assumption appears to be valid as long as S is of the\nsame magnitude as the prevailing market depth (roughly 600 shares for an average US stock). Our\nresults are easily extended, at the expense of additional notation, to a case where S is large but\nstill can be \ufb01lled with multiple market orders with progressively higher prices or fees. For example,\nconsider a case when there are only D1 < S shares available at the cheapest exchange with a fee\nf1, but additional shares are available at a more expensive venue with a fee f2. The trader can\n\ufb01ll S shares by sending two market orders and their total explicit cost becomes a piece-wise linear\nfunction of total size: f1 min(S,D1)+f2 max(S \u2212D1,0). If S is even larger, one may add more terms\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n12\nto this function - additional exchanges or deeper levels in the order book - by suitably increasing\ntheir marginal costs. This generalization remains tractable as long as the cost function remains\nconvex (e.g. piece-wise linear). However, we note that market order routing is itself a non-trivial\nproblem. Practical solutions need to take into account hidden liquidity, market data speed and\ntrading venue geography which a\ufb00ects latency. These considerations are outside of the scope for\nour paper that focuses primarily on limit orders and their execution in multiple order queues.\nLimit order placement: We assume that limit orders L1,...,LK are all placed at the same price\n- the best prevailing quote. E\ufb00ectively the pricing decision is narrowed to two options - a limit\norder at the best quote or a marketable order. This allows us to study limit order execution in\nmore detail focusing on a queue of orders within a speci\ufb01c price level. The assumed choice appears\nto be the most interesting case for applications, since in practice brokers submit the majority of\nlimit orders at the best bid and ask prices - see Cao et al. (2008), Battalio et al. (2013) for recent\nstatistics.\nExogenous spread: The spread h is exogenously set in our model, which is related our limit\norder placement assumption - the trader joins an existing best quote but does not improve it.\nAlthough this assumption appears to be restrictive from a theoretical viewpoint, in practice many\nliquid assets consistently trade with a bid-ask spread equal to a single price increment, which is\nexogenously set by exchanges or regulators.",
    "chunk_index": 11,
    "start_char": 28320,
    "end_char": 31242,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "Exogenous spread: The spread h is exogenously set in our model, which is related our limit\norder placement assumption - the trader joins an existing best quote but does not improve it.\nAlthough this assumption appears to be restrictive from a theoretical viewpoint, in practice many\nliquid assets consistently trade with a bid-ask spread equal to a single price increment, which is\nexogenously set by exchanges or regulators. When the spread narrows to a single price increment,\ntraders cannot improve the existing quote and need to queue with their limit orders, as described\nin our model.\nMarket impact coe\ufb03cient: The coe\ufb03cient \u03b8 could theoretically be di\ufb00erent for market and limit\norders, as well as for orders sent to di\ufb00erent exchanges. However, empirical studies show that market\nimpact di\ufb00erences between limit and market orders are small (Eisler et al. (2012), Mastromatteo\net al. (2013)). We also note that market impact occurs over time horizons that are much longer\nthan those involved in order placement (days as opposed to minutes or seconds), suggesting that\n\u03b8 - the marginal impact of a single share - is much smaller in magnitude than bid-ask spread h\nand explicit costs f,rk. For example, consider a trade to buy 5% of daily volume in a liquid stock.\nRecent empirical studies (Almgren et al. 2005, Mastromatteo et al. 2013) \ufb01nd that the impact of\nsuch trade is in a range of 1-6% of daily volatility, corresponding to 2-12 basis points for a stock\nwith 40% annualized volatility. In other words, the average change in a stock price due to this trade\nis 2-12 basis points over the course of multiple hours when the trade is executed. Costs associated\nwith order placement decisions have similar magnitude but are incurred over much smaller time\nhorizons. For example, the di\ufb00erence between using a market order and a limit order for a U.S.\nstock priced at 30 dollars per share and a spread of 1 penny can be more than 5 basis points after\naccounting for liquidity fees and rebates.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n13\n2.6. Existence of solutions\nWe begin by stating certain economically reasonable restrictions on parameter values, that will be\nassumed for all of our results.\nA1: min\nk {rk} + h > 0. Interpretation: even if some e\ufb00ective rebates rk are negative, limit order\nexecutions let the trader earn a fraction of the bid-ask spread.\nA2: \u03bbo > h+max\nk {rk} and \u03bbo > \u2212(h+f). Interpretation: the trader has no incentive to exceed the\ntarget quantity S.\nA3: \u03bbu > h+f. Interpretation: market orders sent at time 0 are less expensive compared to market\norders that are sent at time T conditionally on not being able to \ufb01ll the target before T.\nAlthough negative rebate values are possible, in the U.S. they are smaller than the smallest possible\nvalue of h = $0.005, justifying our assumptions A1 and A2. Assumption A3 is motivated by adverse\nselection and explained above.",
    "chunk_index": 12,
    "start_char": 30817,
    "end_char": 33757,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "has no incentive to exceed the\ntarget quantity S.\nA3: \u03bbu > h+f. Interpretation: market orders sent at time 0 are less expensive compared to market\norders that are sent at time T conditionally on not being able to \ufb01ll the target before T.\nAlthough negative rebate values are possible, in the U.S. they are smaller than the smallest possible\nvalue of h = $0.005, justifying our assumptions A1 and A2. Assumption A3 is motivated by adverse\nselection and explained above.\nOur \ufb01rst result, whose proof is given in the Appendix, shows that it is not optimal to submit\nlimit or market orders that are a priori too large or too small (larger than the target size S or\nwhose sum is less than S):\nProposition 1 Under assumptions A1-A3, any optimal order allocation belongs to the set:\nC =\n(\nX \u2208RK+1\n+\n\f\f\f 0 \u2264M \u2264S,\n0 \u2264Lk \u2264S \u2212M,k = 1,...,K,\nM +\nK\nX\nk=1\nLk \u2265S\n)\n.\nProposition 1 shows that it is never optimal to over\ufb02ow the target size S with a single order, but\nit may be optimal to exceed the target S with the sum of order sizes M + PK\nk=1 Lk. The penalty\nfunction (3) e\ufb00ectively implements a soft constraint for order sizes and focuses the search for an\noptimal order allocation to the set C. Speci\ufb01c economic or operational considerations could also\nmotivate adding hard constraints to problem (6), e.g. M = 0 or PK\nk=1 Lk = S. Such constraints\ncan be easily included in our framework but absent the aforementioned considerations we do not\nimpose them here.\nThe existence of an optimal solution is then guaranteed:\nProposition 2 Under assumptions A1-A3, V : RK+1\n+\n7\u2192R is convex, bounded from below and has\na global minimizer X\u22c6\u2208C.\nWe note that the optimal solution may be non-unique, i.e. there could be an optimal \u201cplateau\u201d\ndepending on the distribution of \u03be.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n14\n3. Optimal order allocation\n3.1. Choice of order type: limit orders vs market orders\nTo highlight the tradeo\ufb00between limit and market order executions in our optimization setup, we\n\ufb01rst consider the case when the stock is traded on a single exchange, and the trader has to choose\nan optimal split between limit and market orders. This special case is also economically important\nbecause many \ufb01nancial assets (e.g. futures contracts, emerging market equities) in fact trade on a\nsingle exchange.\nProposition 3 (Single exchange: optimal split between limit and market orders)\nUnder assumptions A1-A3:\n(i) If \u03bbu \u2264\u03bbu = 2h + f + r\nF(Q + S) \u2212(h + r + \u03b8), (M \u22c6,L\u22c6) = (0,S) is an optimal allocation.\n(ii) If \u03bbu \u2265\u03bbu = 2h + f + r\nF(Q)\n\u2212(h + r + \u03b8), (M \u22c6,L\u22c6) = (S,0) is an optimal allocation.",
    "chunk_index": 13,
    "start_char": 33290,
    "end_char": 35910,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "optimal split between limit and market orders)\nUnder assumptions A1-A3:\n(i) If \u03bbu \u2264\u03bbu = 2h + f + r\nF(Q + S) \u2212(h + r + \u03b8), (M \u22c6,L\u22c6) = (0,S) is an optimal allocation.\n(ii) If \u03bbu \u2265\u03bbu = 2h + f + r\nF(Q)\n\u2212(h + r + \u03b8), (M \u22c6,L\u22c6) = (S,0) is an optimal allocation.\n(iii) If \u03bbu \u2208(\u03bbu,\u03bbu), an optimal allocation is a mix of limit and market orders, given by\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nM \u22c6= S \u2212F \u22121\n\u0012\n2h + f + r\n\u03bbu + h + r + \u03b8\n\u0013\n+ Q,\nL\u22c6= F \u22121\n\u0012\n2h + f + r\n\u03bbu + h + r + \u03b8\n\u0013\n\u2212Q,\n(9)\nwhere F(x) = P(\u03be \u2264x) is the distribution of the bid queue out\ufb02ows \u03be and F \u22121 its left-inverse.\nIn the case of a single exchange, Proposition 1 implies that M \u22c6+ L\u22c6= S, i.e. the trader does not\noversize orders. As a consequence there is no risk of exceeding the target size and \u03bbo does not\na\ufb00ect the optimal solution. The trader is only concerned with the risk of falling behind the target\nquantity, and balances this risk with a fee, rebate and other parameters. The parameter \u03bbu is an\nopportunity cost of not \ufb01lling the target size, and higher values of \u03bbu lead to smaller limit order\nsizes, as illustrated on Figure 2. Given that M \u22c6+ L\u22c6= S, the optimal market order size increases\nwith an increase in \u03bbu.\nThe optimal split between market and limit orders depends on the distribution of out\ufb02ow (execu-\ntion+cancellation) through its quantile at the level\n2h+f+r\n\u03bbu+h+r+\u03b8: this last formula expresses a tradeo\ufb00\nbetween marginal costs and savings from a market order. Increasing M by 1 share immediately\nincreases the cost by h + f. Since M + L = S, the trader also needs to reduce his limit order size.\nThis will increase the cost as the trader loses h + r in potential savings, if that limit order is\nassumed to \ufb01ll. However if that limit order does not \ufb01ll, the trader will need to catch up paying\nadditional \u03bbu +\u03b8 and not realizing any of the savings. The numerator 2h+f +r re\ufb02ects costs that\nthe trader accepts by increasing M by 1 share, assuming his limit order will \ufb01ll. The denominator\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n15\nre\ufb02ects market order bene\ufb01ts - if a limit order does not get a \ufb01ll, the trader does not need to pay\nan additional \u03bbu + \u03b8 and can forfeit h + r in unrealized savings.\nThe optimal limit order size decreases with \u03bbu as it becomes more expensive not to ful\ufb01ll the\ntarget, and increases with f as market orders become more expensive.\nAnother interesting feature is that within the range \u03bbu \u2208(\u03bbu,\u03bbu), L\u22c6is fully determined by Q,\nF and cost parameters, while M \u22c6increases with S.",
    "chunk_index": 14,
    "start_char": 35656,
    "end_char": 38162,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "order does not get a \ufb01ll, the trader does not need to pay\nan additional \u03bbu + \u03b8 and can forfeit h + r in unrealized savings.\nThe optimal limit order size decreases with \u03bbu as it becomes more expensive not to ful\ufb01ll the\ntarget, and increases with f as market orders become more expensive.\nAnother interesting feature is that within the range \u03bbu \u2208(\u03bbu,\u03bbu), L\u22c6is fully determined by Q,\nF and cost parameters, while M \u22c6increases with S. The consequences of this solution feature are\nimportant. As the target size S increases, a larger fraction M\u22c6\nS of that size is executed with a market\norder. To \ufb01ll a large limit order a trader would need an improbably large queue out\ufb02ow \u03be, so to\naccommodate larger quantities one needs to resort to market orders. This bounded capacity feature\nof limit orders also appears in our solutions for multiple exchanges. For example, as the number\nof available exchanges K increases, the overall prospects of \ufb01lling limit orders improve (i.e. their\ncapacity increases) and the share of market orders M\u22c6\nS decreases. This is related to the \u201ccrowding-\nout\u201d e\ufb00ect studied by Parlour (1998) and empirically con\ufb01rmed by Cao et al. (2008). Traders that\nobserve short queues take advantage of that by submitting limit orders, which increases queues\nand causes subsequent traders to use market orders.\nIt is important to note that the optimal solution (M \u22c6,L\u22c6) depends on the full distribution F(\u00b7)\nof \u03be and not just on its mean. Limit orders are \ufb01lled when \u03be \u2265Q+L, so the tail of F(\u00b7) a\ufb00ects order\nexecutions and is an important determinant of the optimal order allocation. Figure 2 compares\norder allocations for exponential and Pareto distributions with equal means.\nFigure 2\nOptimal limit order size L\u22c6for one exchange. The parameters for this \ufb01gure are: Q = 2000,S = 1000,h =\n0.02,r = 0.002,f = 0.003,\u03b8 = 0.0005. Colors correspond to di\ufb00erent order out\ufb02ow distributions - expo-\nnential with means 2200 and 2500 and Pareto with mean 2200 and a tail index 5.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n16\n3.2. Optimal routing of limit orders across multiple exchanges\nWhen multiple trading venues are available, dividing a target quantity among them reduces the\nrisk of not \ufb01lling an order and improves execution quality, and one may even consider sending S\nshares to each exchange. However, sending too many orders leads to excessive market impact and\nan undesirable possibility of over-trading. Proposition 4 gives optimality conditions for an order\nallocation X\u22c6= (M \u22c6,L\u22c6\n1,...,L\u22c6\nK):\nProposition 4 Assume A1-A3 hold, the distribution of \u03be is continuous, max\nk\n{Fk(Qk + S)} < 1\nand \u03bbu < max\nk\n\u001a2h + f + rk\nFk(Qk)\n\u2212(h + rk + \u03b8)\n\u001b\n. Then:\n1. Let p0 = P(\u03be1 \u2264Q1,...,\u03bek \u2264QK). If\n\u03bbu \u2265\n2h + f + max\nk {rk}\np0\n\u2212(h + max\nk {rk})\nthen optimal order placement strategy involves market orders: M \u22c6> 0.",
    "chunk_index": 15,
    "start_char": 37732,
    "end_char": 40583,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "conditions for an order\nallocation X\u22c6= (M \u22c6,L\u22c6\n1,...,L\u22c6\nK):\nProposition 4 Assume A1-A3 hold, the distribution of \u03be is continuous, max\nk\n{Fk(Qk + S)} < 1\nand \u03bbu < max\nk\n\u001a2h + f + rk\nFk(Qk)\n\u2212(h + rk + \u03b8)\n\u001b\n. Then:\n1. Let p0 = P(\u03be1 \u2264Q1,...,\u03bek \u2264QK). If\n\u03bbu \u2265\n2h + f + max\nk {rk}\np0\n\u2212(h + max\nk {rk})\nthen optimal order placement strategy involves market orders: M \u22c6> 0.\n2. If cost savings and \ufb01ll probability on exchange j overweigh market impact :\n(h + rj \u2212\u03bbo)P(\u03bej > Qj) > \u03b8\nthen any optimal order placement involves limit orders on exchange j: L\u22c6\nj > 0.\n3. If previous assumptions hold for all exchanges j = 1,...,K, X\u22c6\u2208C is an optimal allocation\nif and only if\nP\n \nM \u22c6+\nK\nX\nk=1\n((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212L\u22c6\nk)+) < S\n!\n= h + f + \u03bbo + \u03b8\n\u03bbu + \u03bbo + \u03b8\n(10)\nP\n \nM \u22c6+\nK\nX\nk=1\n((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212L\u22c6\nk)+) < S\n\f\f\f\f\u03bej > Qj + L\u22c6\nj\n!\n=\n\u03b8\nP(\u03bej>Qj+L\u2217\nj ) + \u03bbo \u2212(h + rj)\n\u03bbu + \u03bbo + \u03b8\n,\nfor\nj = 1,...,K.\n(11)\nThe optimality criterion (10)-(11) is simple - it depends on the probabilities of an execution\nshortfall A(X,\u03be) < S. Proposition 4 thus establishes that for X to be optimal it is necessary and\nsu\ufb03cient that X equates these execution shortfall probabilities to values computed with model\nparameters.\nWhen the number of exchanges K is large, shorfall probabilities in (10)-(11) are di\ufb03cult to\ncompute in closed-form. However, the case K = 2 is relatively tractable and will be analyzed as an\nillustration. The assumption of independence between \u03be1,\u03be2 is made only in this example and is\nnot required for the rest of our results. In Section 4 we present results for correlated order \ufb02ows.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n17\nCorollary In addition to assumptions of Proposition 4, assume that K = 2 and \u03be1,\u03be2 are indepen-\ndent. Also, assume that F1,2(Q1,2) < 1 \u2212h + r2,1\n\u03bbo\nand (h +r1,2)P(\u03be1,2 > Q1,2 + S) > \u03b8. Then there is\nan optimal order allocation X\u22c6= (M \u22c6,L\u22c6\n1,L\u22c6\n2) \u2208int{C} and it solves the following equations\nL\u22c6\n1 = Q2 + S \u2212M \u22c6\u2212F \u22121\n2\n\u0012\u2212\u03b8/ \u00afF1(Q1 + L\u22c6\n1) + \u03bbu + \u03b8 + h + r1\n\u03bbu + \u03bbo + \u03b8\n\u0013\n(12a)\nL\u22c6\n2 = Q1 + S \u2212M \u22c6\u2212F \u22121\n1\n\u0012\u2212\u03b8/ \u00afF2(Q2 + L\u22c6\n2) + \u03bbu + \u03b8 + h + r2\n\u03bbu + \u03bbo + \u03b8\n\u0013\n(12b)\n\u00afF1(Q1 + L\u22c6\n1) \u00afF2(Q2 + S \u2212M \u22c6\u2212L\u22c6\n1) +\nQ1+L\u22c6\n1\nZ\nQ1+S\u2212M\u22c6\u2212L\u22c6\n2\n\u00afF2(Q1 + Q2 + S \u2212M \u22c6\u2212x1)dF1(x1) = \u03bbu \u2212(h + f)\n\u03bbu + \u03bbo + \u03b8 , (12c)\nwhere F1(\u00b7),F2(\u00b7) are the cdf functions of \u03be1,\u03be2 respectively and \u00afFi = 1 \u2212Fi.",
    "chunk_index": 16,
    "start_char": 40219,
    "end_char": 42543,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "\u03b8\n\u0013\n(12a)\nL\u22c6\n2 = Q1 + S \u2212M \u22c6\u2212F \u22121\n1\n\u0012\u2212\u03b8/ \u00afF2(Q2 + L\u22c6\n2) + \u03bbu + \u03b8 + h + r2\n\u03bbu + \u03bbo + \u03b8\n\u0013\n(12b)\n\u00afF1(Q1 + L\u22c6\n1) \u00afF2(Q2 + S \u2212M \u22c6\u2212L\u22c6\n1) +\nQ1+L\u22c6\n1\nZ\nQ1+S\u2212M\u22c6\u2212L\u22c6\n2\n\u00afF2(Q1 + Q2 + S \u2212M \u22c6\u2212x1)dF1(x1) = \u03bbu \u2212(h + f)\n\u03bbu + \u03bbo + \u03b8 , (12c)\nwhere F1(\u00b7),F2(\u00b7) are the cdf functions of \u03be1,\u03be2 respectively and \u00afFi = 1 \u2212Fi.\nIn (12a)\u2013(12b) the optimal limit order quantities L\u22c6\n1,L\u22c6\n2 are linear functions of M \u22c6. When (12a)-\n(12b) are substituted into (12c) we obtain a non-linear equation for M \u22c6, which can be solved for\nM \u2217.\n4. An optimal order routing algorithm\n4.1. A stochastic algorithm based on order \ufb02ow sampling\nPractical applications require a fast and \ufb02exible method for optimizing order placement across\nmultiple trading venues. As the number of venues increases, it becomes progressively more di\ufb03cult\nto evaluate the objective function V (X) - a K-dimensional integral - and to obtain analytical\nsolutions for the order placement problem. In this section we propose a stochastic approximation\nmethod for e\ufb03ciently computing the optimal allocation even in high dimensions. The idea is to\nsample order out\ufb02ows \u03bek and approximate the gradient of V (X) along a random optimization path.\nApplying this approach for our problem formulation yields an intuitive iterative algorithm that\nupdates trader\u2019s order allocation in response to past order execution outcomes.\nOur numerical solution is based on the robust stochastic approximation algorithm of Nemirovski\net al. (2009). Denote by g(X,\u03be) = \u2207v(X,\u03be) the gradient of v(X,\u03be) with respect to X. The idea is\nto use a random samples \u03ben \u2208RK from \u03be to approximate, iteratively, V (X) and its gradient:\n1: Choose an initial X0 \u2208RK+1 and \ufb01x a step size \u03b3;\n2: for n = 1,...,N do\n3:\nXn = Xn\u22121 \u2212\u03b3g(Xn\u22121,\u03ben)\n4: end\n5: An estimator X\u2217is given by: \u02c6X\u22c6\nN = 1\nN\nPN\nn=1 Xn\nHere random variables \u03ben are assumed to be an ergodic sequence sampled from the distribution F,\nwhich may or may not be known. Under mild assumptions, satis\ufb01ed by our objective function, the\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n18\nestimator \u02c6X\u22c6\nN converges to an optimal solution X\u22c6and has a performance bound\nV ( \u02c6X\u22c6) \u2212V (X\u22c6) \u2264DM\n\u221a\nN , where D = max\nX,X\u2032\u2208C\u2225X \u2212X\u2032\u22252, M =\nq\nmax\nX\u2208C E[\u2225g(X,\u03be)\u22252\n2].\nThe optimal step size is \u03b3\u22c6=\nD\n\u221a\nNM and we use a step size\n\u03b3 = K1/2S\n \nN(h + f + \u03b8 + \u03bbu + \u03bbo)2 + N\nK\nX\nk=1\n(h + rk + \u03b8 + \u03bbu + \u03bbo)2\n!\u22121/2\nthat scales appropriately with problem parameters. For more details on stochastic approximation\nmethods we refer to Kushner and Yin (2003) and Nemirovski et al. (2009).\nIn general, this method requires a sample of size N of the random variable \u03be.",
    "chunk_index": 17,
    "start_char": 42243,
    "end_char": 44860,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "size is \u03b3\u22c6=\nD\n\u221a\nNM and we use a step size\n\u03b3 = K1/2S\n \nN(h + f + \u03b8 + \u03bbu + \u03bbo)2 + N\nK\nX\nk=1\n(h + rk + \u03b8 + \u03bbu + \u03bbo)2\n!\u22121/2\nthat scales appropriately with problem parameters. For more details on stochastic approximation\nmethods we refer to Kushner and Yin (2003) and Nemirovski et al. (2009).\nIn general, this method requires a sample of size N of the random variable \u03be. Since g(Xn,\u03be)\ndepends on random variables \u03be only through indicator functions:\ng(Xn,\u03be) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nh + f + \u03b8 \u2212(\u03bbu + \u03b8)1{A(Xn,\u03be)<S} + \u03bbo1{A(Xn,\u03be)>S}\n\u03b8 + 1{\u03be1>Q1+L1,n}\n\u0000\u2212(h + r1) \u2212(\u03bbu + \u03b8)1{A(Xn,\u03be)<S} + \u03bbo1{A(Xn,\u03be)>S}\n\u0001\n...\n\u03b8 + 1{\u03beK>QK+LK,n}\n\u0000\u2212(h + rK) \u2212(\u03bbu + \u03b8)1{A(Xn,\u03be)<S} + \u03bbo1{A(Xn,\u03be)>S}\n\u0001\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\na practical approach is to store the values of these indicator functions from past order executions\nat each exchange and use them to compute the sample values \u03ben.\nThese indicators have simple interpretations: 1{A(Xn,\u03be)<S} = 1 if on the n\u2212th order submission\nthe trader fell behind the target quantity, 1{A(Xn,\u03be)>S} = 1 if he was ahead of the target, and\n1{\u03bek>Qk+Lk,n} = 1 if a limit order on exchange k was fully executed. This leads to a non-parametric\nonline implementation of the algorithm, which updates order sizes in response to previous order\nexecution outcomes, and can be interpreted as a sequential learning procedure. For example, the\n\ufb01rst row of g(Xn,\u03be) describes updates of the market order quantity M:\n\u2022 on each iteration M is decreased by \u03b3(h + f + \u03b8) to reduce trading costs and market impact\n\u2022 if a trader fell behind his target quantity, M will increase by \u03b3(\u03bbu + \u03b8) to reduce the shortfall\non the next execution\n\u2022 since overtrading is also penalized, M is decreased by \u03b3\u03bbo whenever a target quantity is\nexceeded\nLimit order sizes are updated similarly. If a limit order is not \ufb01lled, its quantity will be reduced by\n\u03b3\u03b8 to reduce market impact, otherwise the update depends on a \ufb01ll outcome.\nThe algorithm is fast - it updates an allocation vector sequentially and each update involves\nat most 5(K + 1) arithmetic operations. On a retail laptop computer, an optimal order allocation\nacross 12 exchanges can be computed in less than 200 milliseconds using an approximation with\nN = 1000 iterations. The sequential nature of the algorithm allows to easily adapt it for low-latency\ntrading applications: each iteration takes a fraction of a millisecond. First, optimal allocations are\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n19\ncomputed o\ufb00-line and stored in a table. The table can be indexed by parameters such as Qk,S,T.\nThen, an allocation is retrieved from the table in real-time, routing decisions are made and orders\nare sent. Finally, order \ufb01lls are collected, the allocation is updated by one iteration as described by\ng, and stored in the table for future use.",
    "chunk_index": 18,
    "start_char": 44494,
    "end_char": 47308,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "low-latency\ntrading applications: each iteration takes a fraction of a millisecond. First, optimal allocations are\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n19\ncomputed o\ufb00-line and stored in a table. The table can be indexed by parameters such as Qk,S,T.\nThen, an allocation is retrieved from the table in real-time, routing decisions are made and orders\nare sent. Finally, order \ufb01lls are collected, the allocation is updated by one iteration as described by\ng, and stored in the table for future use. In Section 5.2 we present an illustration of this approach\nbased on historical tick data.\n4.2. Numerical convergence\nThe robust stochastic approximation algorithm has a theoretical convergence rate of\n\u221a\nN as shown\nin Nemirovski et al. (2009), i.e. V (X\u22c6\nN)\u2212V (X\u22c6) \u2264\nC\n\u221a\nN , where V (X\u22c6\nN) is the objective function at an\napproximate solution X\u22c6\nN computed with N iterations. When applied to our problem, this algorithm\ntypically converges to an optimal point after 1,000-10,000 iterations for a given set of parameter\nvalues. To illustrate this, we applied it to an example with K = 1 venue, and compared its results\nwith a closed-form solution (9). Using the same parameter values as on Figure 2 with \u03be \u223cPois(\u00b5T),\n\u00b5 = 2200 shares per minute and T = 1 minute the optimal solution is (M \u22c6,L\u22c6) = (730,270) shares.\nNumerical solutions \u02c6X\u22c6were then computed for \ufb01ve starting points X0 with a progressively larger\nnumber of iterations N. For each choice of X0 and N we also estimated an average cost per share\nW( \u02c6X\u22c6) using an additional L = 1000 samples of \u03be generated after \u02c6X\u22c6is estimated. Figure 3 shows\nthat numerical solutions converge to X\u22c6regardless of the initial point X0 and moreover starting\nfrom an optimal point with X0 = X\u22c6the iterates remained close to this point. Convergence is quite\nfast - for most choices of the initial point the algorithm is within 2% of the optimal objective value\nafter only 50 iterations.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n20\nFigure 3\nConvergence of numerical solutions and objective values to an optimal point from di\ufb00erent initial points.\n5. Numerical results\n5.1. Comparative statics\nTo gain insights into the structure of order placement and routing decisions we turn to comparative\nstatics analysis. Varying one model parameter at a time, we recompute an optimal allocation using\nthe numerical algorithm from Section 4 and plot optimal order sizes for di\ufb00erent parameter values.\nWe also compare them to analytical solutions for K = 1 obtained with same parameters.\nSeveral solution features come into view in this analysis. First, we notice that in most cases it\nis optimal to oversize the total quantity of limit orders placed on multiple exchanges. Since limit\norder executions at individual exchanges are random, placing larger orders on multiple venues\nreduces the overall execution risk. This tendency to \u201coverbook\u201d with multiple limit orders gradually\nvanishes as market impact increases.",
    "chunk_index": 19,
    "start_char": 46767,
    "end_char": 49793,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "parameter values.\nWe also compare them to analytical solutions for K = 1 obtained with same parameters.\nSeveral solution features come into view in this analysis. First, we notice that in most cases it\nis optimal to oversize the total quantity of limit orders placed on multiple exchanges. Since limit\norder executions at individual exchanges are random, placing larger orders on multiple venues\nreduces the overall execution risk. This tendency to \u201coverbook\u201d with multiple limit orders gradually\nvanishes as market impact increases. Similarly, it becomes less useful to oversize orders as order\n\ufb02ows become more correlated across exchanges, decreasing the diversi\ufb01cation advantage of having\nmultiple orders. The oversizing becomes more prominent as the number of trading venues grows\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n21\nleading to more opportunities to diversify limit order \ufb01lls. Larger target trade sizes also lead to\nmore order oversizing. This suggests that large traders need to seek liquidity opportunities to \ufb01ll\na large trade even at a cost of creating some market impact with their limit orders.\nSecond, we \ufb01nd that limit orders have a bounded capacity for executing large trades. The quantity\nthat is likely to be \ufb01lled with a limit order on exchange k is given by a queue size Qk and a order\nout\ufb02ow distribution, more precisely by its tail P(\u03bek > x),x > Qk. Filling a relatively small quantity\nS can usually be achieved by just placing several limit orders, but to \ufb01ll larger quantities a trader\nneeds to rely on market orders. A number of available venues plays a role in this tradeo\ufb00- the\nmore venues are available, the more likely a trader can \ufb01ll some of his limit orders, which makes\nmarket orders less necessary.\nThird, we show that the target quantity S itself determines which parameters drive its optimal\nallocation. For smaller S, our solutions quickly shift to limit orders on venues that provide the\nlargest e\ufb00ective rebate. For larger S we see that under- and over\ufb01ll penalties play a more signi\ufb01cant\nrole and solutions become insensitive to rebates. Queue sizes and order out\ufb02ow distributions appear\nto be important in all cases.\nThe baseline set of parameters in our analysis is representative of a typical stock in the U.S.\nequity market. We set f = 0.003,h = 0.02,\u03b8 = 0.0005,\u03bbu = \u03bbo = 0.05 and K = 2. Exchange param-\neters are set to Qk = 2000,rk = 0.002 for all venues. Order out\ufb02ows follow a simple single-factor\nmodel, capturing the fact that order \ufb02ows in a fragmented market are usually positively correlated.\nSpeci\ufb01cally we put \u03bek = \u03b1\u03be0 + (1 \u2212\u03b1)\u03f5k, where \u03be0,\u03f5k are i.i.d. Poisson random variables with\na common mean parameter \u00b5T, \u00b5 = 2200 shares/minute, T = 1 minute and \u03b1 = 0.6. To compute\neach numerical solution we used N = 1000 samples. This was enough for convergence, regardless\nof the number of exchanges K as the algorithm remains largely unchanged with di\ufb00erent K.",
    "chunk_index": 20,
    "start_char": 49260,
    "end_char": 52217,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "the fact that order \ufb02ows in a fragmented market are usually positively correlated.\nSpeci\ufb01cally we put \u03bek = \u03b1\u03be0 + (1 \u2212\u03b1)\u03f5k, where \u03be0,\u03f5k are i.i.d. Poisson random variables with\na common mean parameter \u00b5T, \u00b5 = 2200 shares/minute, T = 1 minute and \u03b1 = 0.6. To compute\neach numerical solution we used N = 1000 samples. This was enough for convergence, regardless\nof the number of exchanges K as the algorithm remains largely unchanged with di\ufb00erent K. With\nthe exception of a comparison across values of \u00b5, in all other solution comparisons we used the\nsame samples of \u03bek across a range of parameter values. For analytical solutions we assumed a sim-\nple Poisson distribution for order \ufb02ows with the same mean \u00b5T. Comparing solutions for K = 1\nand K = 2 we \ufb01nd that order allocations for multiple venues are in many cases similar to their\nsingle-venue counterparts.\nFirst, we compare solutions across a range of target quantities S. The left panel on Figure 4\nshows that market orders can be avoided to \ufb01ll small quantities (S \u2264400 in this example), but\nafter a certain point an optimal market order size increases linearly with S whereas limit order\nsizes stay relatively constant. Total order quantities M \u22c6+ L\u22c6\n1 + L\u22c6\n2 are larger than S, especially\nfor bigger S. The right panel shows how this excess size decreases due to larger market impact\nthat makes it more costly to oversize orders. For a single exchange there is no bene\ufb01t in oversizing\norders and according to Proposition 3 M a + La = S.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n22\nFigure 4\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La), left panel: across a\nrange of target quantities S, right panel: across a range of market impact coe\ufb03cients \u03b8\nThe second set of plots compares order allocations for di\ufb00erent queue sizes. We distinguish\nbetween two scenarios - a target quantity S = 200 which is small relative to our queue size and\norder \ufb02ow distribution parameters and a large target quantity S = 1000. Increasing the queue size\nat an exchange decreases the chance of \ufb01lling a limit order there and creates a neccessity to \ufb01nd a\nsubstitute for that order. The left panel on Figure 5 shows that when S is small the trader can keep\nup with his execution using just limit orders even when the queue size at exchange 1 increases.\nThe only di\ufb00erence it makes is that he places all orders on exchange 2 instead of splitting them\nbetween two venues. The right panel shows a di\ufb00erent scenario - when S is large the trader cannot\nsubstitute his limit orders as easily. All of the capacity for limit orders on exchange 2 is already\nused by his 200-share limit order, and he is forced to substitute his limit orders on exchange 1 with\nmore expensive market orders.",
    "chunk_index": 21,
    "start_char": 51770,
    "end_char": 54556,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "increases.\nThe only di\ufb00erence it makes is that he places all orders on exchange 2 instead of splitting them\nbetween two venues. The right panel shows a di\ufb00erent scenario - when S is large the trader cannot\nsubstitute his limit orders as easily. All of the capacity for limit orders on exchange 2 is already\nused by his 200-share limit order, and he is forced to substitute his limit orders on exchange 1 with\nmore expensive market orders.\nThe third set of plots compares solutions for di\ufb00erent order out\ufb02ow rates \u00b5. We again distinguish\nbetween a small and a large trade quantity S but \ufb01nd that both cases are similarly a\ufb00ected by\norder \ufb02ow rates. When \u00b5 is larger, i.e. there are more market orders, execution prospects improve\nfor limit orders on all exchanges at the same time. More limit orders are used and the share of\nmarket orders in the optimal solution decreases.\nWe also compare optimal solution across a range of pricing parameters - e\ufb00ective rebates r,\nexecution shortfall penalties \u03bbu,\u03bbo, fee f and bid-ask spread h. Figure 7 shows that traders with\nsmall execution quantities favor exchanges with larger rebates (net of adverse selection). Small\nquantities can be \ufb01lled with limit orders at any exchange. Traders that have this \ufb02exibility prefer\nto capture a larger rebate. In contrast, when the execution quantity is large traders need to use\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n23\nFigure 5\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La) across a range of queue\nsizes Q1, left panel: small trade S = 200 shares, right panel: larger trade S = 1000 shares\nFigure 6\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La) across a range of order\n\ufb02ow rates \u00b5, left panel: small trade S = 200 shares, right panel: larger trade S = 1000 shares\nlimit orders on all venues. In this case the optimal solution is insensitive to rebates because limit\norders at any exchange are less costly than market orders and the trader needs to use limit orders\nat all exchanges to the maximum extent to \ufb01ll a large quantity.\nOptimal solutions for small trade quantities are practically una\ufb00ected by changes in \u03bbu,\u03bbo,h\nand f. For small trades market orders can be avoided and there is little execution risk which\nrenders these parameters unimportant. For large target quantities these parameter play a larger\nrole. Figures 8 and 9 show that more market orders are used when under\ufb01lling an order is expensive,\nand less market orders are used when these orders themselves become more costly due to large\nspread or fee. The dependence of market order sizes on \u03bbo is also intuitive - as over\ufb01lls become\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n24\nFigure 7\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La) across a range of rebates\nr1, left panel:",
    "chunk_index": 22,
    "start_char": 54118,
    "end_char": 57013,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "orders are used when under\ufb01lling an order is expensive,\nand less market orders are used when these orders themselves become more costly due to large\nspread or fee. The dependence of market order sizes on \u03bbo is also intuitive - as over\ufb01lls become\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n24\nFigure 7\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La) across a range of rebates\nr1, left panel: small trade S = 200 shares, right panel: larger trade S = 1000 shares\nmore expensive the trader will use more limit orders instead of market orders because they have a\nsmaller risk of over\ufb01lling and compensate for this risk with a lower cost.\nFigure 8\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La), left panel: across a\nrange of \u03bbu, right panel: across a range of \u03bbo\nLastly, we compare optimal solutions for a di\ufb00erent number of exchanges and estimate trading\ncosts for our optimal solution. The optimal cost is compared to that of a few simpler strategies: a\npure market order allocation XM = (S,0,...,0), a single limit order allocation XL = (0,S,0,...,0)\nand an equal split allocation XE =\n\u0012\nS\nK + 1,\nS\nK + 1,...,\nS\nK + 1\n\u0013\n.\nOptimal order allocations presented in Table 1 suggest that the best strategy is to use limit\norders to the maximum possible extent, which depends on a trade quantity. As the number of\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n25\nFigure 9\nOptimal order sizes with two exchanges (M,L1,L2) and one exchange (M a,La), left panel: across a\nrange of spreads h, right panel: across a range of fees f\nexchanges increases, the optimal allocation shifts from market orders to limit orders for all trade\nsizes, and for smaller sizes it shifts entirely. Market orders do not need to be used at all if there are\nenough exchanges for the trader to place limit orders on. The tendency to place more limit orders\nthan needed is also pronounced in these examples, especially for larger trades which require the\ntrader to use all opportunities to \ufb01ll his limit orders.\nOur analysis of execution costs for di\ufb00erent order placement strategies shows a clear bene\ufb01t in\noptimizing the tradeo\ufb00between market orders and limit orders in a fragmented market. When\nmultiple exchanges are available the di\ufb00erence between a naive strategy and an optimal solution\nis 2-4 cents per share, which is economically signi\ufb01cant. More importantly our results show that\nsimple static strategies cannot accomodate di\ufb00erent execution sizes. For example, the pure market\norder strategy XM performs relatively well for large S but it is too expensive for small trades which\ncan be executed more e\ufb03ciently with limit orders. Similarly, a trader can achieve relatively low\ncosts by equally splitting his target quantity into multiple orders if the total size is small, but this\nrule performs even worse than XM for a large sizes.",
    "chunk_index": 23,
    "start_char": 56557,
    "end_char": 59496,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "cents per share, which is economically signi\ufb01cant. More importantly our results show that\nsimple static strategies cannot accomodate di\ufb00erent execution sizes. For example, the pure market\norder strategy XM performs relatively well for large S but it is too expensive for small trades which\ncan be executed more e\ufb03ciently with limit orders. Similarly, a trader can achieve relatively low\ncosts by equally splitting his target quantity into multiple orders if the total size is small, but this\nrule performs even worse than XM for a large sizes. The optimal proportion of market and limit\norders as well as their allocation across exchanges varies signi\ufb01cantly based on trade characteristics\nand market dynamics.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n26\nK\nOrder allocation in % of S\nAverage cost, in cents per share\n\u02c6\nM \u22c6\nL\u22c6\n1\nL\u22c6\n2\nL\u22c6\n3\nL\u22c6\n4\n\u02c6L\u22c6\n5\nTotal\nW(XM) W(XL) W(XE) W(X\u22c6)\nS = 500\n1\n82% 18%\n100%\n2.35\n2.19\n0.83\n1.54\n2\n15% 44% 44%\n103%\n2.35\n2.22\n-0.57\n-0.85\n3\n1% 34% 34% 34%\n102%\n2.35\n2.21\n-1.02\n-1.99\n4\n0% 26% 25% 26% 26%\n102%\n2.35\n2.20\n-1.25\n-2.06\n5\n0% 22% 21% 20% 20%\n20%\n103%\n2.35\n2.22\n-1.40\n-2.05\nS = 1000\n1\n94%\n6%\n100%\n2.35\n3.65\n2.27\n2.07\n2\n56% 27% 27%\n111%\n2.35\n3.64\n1.28\n0.77\n3\n35% 23% 23% 23%\n104%\n2.35\n3.65\n0.09\n-0.07\n4\n15% 23% 23% 22% 23%\n106%\n2.35\n3.64\n-0.88\n-0.90\n5\n1% 21% 21% 21% 21%\n21%\n106%\n2.35\n3.65\n-1.29\n-1.64\nS = 5000\n1\n97%\n3%\n100%\n2.35\n4.81\n3.44\n2.22\n2\n88%\n8%\n8%\n104%\n2.35\n4.81\n3.60\n2.10\n3\n83%\n9%\n9%\n9%\n110%\n2.35\n4.81\n3.55\n1.95\n4\n79% 11% 11% 11% 11%\n124%\n2.35\n4.81\n3.39\n1.79\n5\n75% 11% 11% 11% 11%\n11%\n129%\n2.35\n4.82\n3.22\n1.62\nTable 1\nLeft panel: optimal order allocations, right panel: costs for the optimal allocation and simpler\nbenchmarks\nK\nCosts, cents per share\nAverage shortfall, shares\nShortfall probabilities\nFees Impact Penalties Total\nUnder\ufb01ll\nOver\ufb01ll\nUnder\ufb01ll\nOver\ufb01ll\nS = 500\n1\n1.49\n0.05\n0.00\n1.54\n0\n0\n100%\n0%\n2\n-1.35\n0.06\n0.44\n-0.85\n40\n4\n71%\n29%\n3\n-2.18\n0.05\n0.14\n-1.99\n6\n9\n20%\n80%\n4\n-2.23\n0.05\n0.11\n-2.06\n0\n11\n3%\n97%\n5\n-2.24\n0.05\n0.14\n-2.05\n0\n14\n1%\n100%\nS = 1000\n1\n2.02\n0.05\n0.00\n2.07\n0\n0\n100%\n0%\n2\n0.41\n0.06\n0.30\n0.77\n51\n9\n74%\n26%\n3\n-0.48\n0.06\n0.36\n-0.07\n64\n7\n73%\n27%\n4\n-1.38\n0.06\n0.42\n-0.90\n71\n12\n67%\n34%\n5\n-2.07\n0.06\n0.38\n-1.64\n55\n21\n51%\n49%\nS = 5000\n1\n2.17\n0.05\n0.00\n2.22\n2\n0\n100%\n0%\n2\n1.85\n0.05\n0.19\n2.10\n192\n0\n100%\n0%\n3\n1.66\n0.06\n0.23\n1.95\n230\n0\n99%\n1%\n4\n1.46\n0.06\n0.26\n1.79\n256\n1\n98%\n2%\n5\n1.30\n0.07\n0.26\n1.62\n257\n2\n96%\n4%\nTable 2\nOrder placement costs and \ufb01ll statistics for the optimal allocation",
    "chunk_index": 24,
    "start_char": 58953,
    "end_char": 61437,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "3\n-0.48\n0.06\n0.36\n-0.07\n64\n7\n73%\n27%\n4\n-1.38\n0.06\n0.42\n-0.90\n71\n12\n67%\n34%\n5\n-2.07\n0.06\n0.38\n-1.64\n55\n21\n51%\n49%\nS = 5000\n1\n2.17\n0.05\n0.00\n2.22\n2\n0\n100%\n0%\n2\n1.85\n0.05\n0.19\n2.10\n192\n0\n100%\n0%\n3\n1.66\n0.06\n0.23\n1.95\n230\n0\n99%\n1%\n4\n1.46\n0.06\n0.26\n1.79\n256\n1\n98%\n2%\n5\n1.30\n0.07\n0.26\n1.62\n257\n2\n96%\n4%\nTable 2\nOrder placement costs and \ufb01ll statistics for the optimal allocation\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n27\n5.2. Application to tick data\nTo further illustrate our method, we applied it to historical tick data using a speci\ufb01c trade example.\nWe considered an execution of a moderate-sized order to buy S = 2000 shares of Microsoft Cor-\nporation (MSFT) stock with an execution deadline T = 1 minute, to be traded on two exchanges\n- NASDAQ and BATS Z. This liquid stock is traded on multiple exchanges, but for simplicity\nwe considered only these two. We assumed in this simulation that rebates are rNSDQ = 0.2 and\nrBAT S = 0.25 cents per share which is close to their historical averages. The fee was assigned to\nf = 0.29 cents per share and the half-spread was set to h = 0.50 cents per share which is typical\nfor this stock. To perform our numerical optimization we used trade and quote (TAQ) data from\nJanuary to March 2012, and then analyzed its performance on a dataset from April 2012.\nAverage queue sizes on NASDAQ and BATS Z in the calibration dataset were equal to 12,392\nand 8,179 shares respectively, and average 1-minute market sell order volumes for each exchange\nwere equal to 848 and 922 shares. These averages however do not re\ufb02ect the tails of a volume\ndistribution. Figure 10 shows that in about 15% of observations the volume of market sell orders\nduring an interval [0,T] was larger than the queue size at time 0, often by thousands of shares. In\nthese cases limit orders placed at the back of a queue at time 0 will be \ufb01lled and our theoretical\nresults show that the likelihood of these tail events drives optimal limit order placement decisions.\nFigure 10\nEmpirical cumulative distribution functions of market order \ufb02ows in excess of initial queue size\nOur numerical optimization does not require to estimate or specify order \ufb02ow distributions.\nInstead, as described in Section 4 it requires data on limit order \ufb01lls, which were simulated in\nthis example using tick data. To simulate a limit order \ufb01ll we assumed that a buy limit order L\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n28\nplaced behind a queue of Q shares at the best bid is \ufb01lled if the volume of sell market orders\nduring its placement horizon (T = 1 minute in our example) is larger than Q.",
    "chunk_index": 25,
    "start_char": 61065,
    "end_char": 63729,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "order \ufb01lls, which were simulated in\nthis example using tick data. To simulate a limit order \ufb01ll we assumed that a buy limit order L\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n28\nplaced behind a queue of Q shares at the best bid is \ufb01lled if the volume of sell market orders\nduring its placement horizon (T = 1 minute in our example) is larger than Q. The execution is\ncomplete if the volume of sell market orders exceeds Q by more than L, otherwise it is a partial\nexecution. One-minute sell market order volumes were estimated as a sum of trade sizes at a given\nexchange whose trade prices were equal to the prevailing bid price. In this simulation we made\nseveral simpli\ufb01cations that are related to TAQ data limitations. First, our limit order \ufb01ll estimates\nare conservative as they do not include possible order cancelations from the front of a bid queue.\nTAQ data does not have information on order cancelations, but this information is available in\nmore detailed \u201clevel-2\u201d datasets. Second, buy limit orders can be \ufb01lled when ask price ticks down,\nbut estimating these \ufb01lls accurately requires knowledge of order queue position which is unavailable\nin TAQ. To simplify our simulation we restricted it to one-minute samples where the best quotes\ndid not change up or down.\nTo estimate sensitivity to queue sizes QBAT S or QNSDQ we ranked observations by queue sizes\nQBAT S,QNSDQ and grouped them into three equal sized bins labeled \u201chigh\u201d, \u201cmedium\u201d and \u201clow\u201d.\nSimilarly we ranked, grouped and labeled observations by total trading volume in the previous\nminute V OLBAT S,V OLNSDQ. Each label was then used as a state variable leading to 34 = 81 dif-\nferent combinations of state variables. Each combination of states de\ufb01ned a subsample of historical\ndata, which we used to simulate limit order \ufb01lls. Based on simulated limit order \ufb01lls we computed\na numerical solution for each subsample following the algorithm from Section 4.\nThe result is a lookup table where one can \ufb01nd an approximate solution for each of 81\nstate variable combinations. Order allocations in this table take into account the magnitude of\nQBAT S,QNSDQ and V OLBAT S,V OLNSDQ relative to their historical values. We recomputed order\nallocations using this method for a number of parameter values and Table 1 presents a subset\nof these solutions for di\ufb00erent levels of V OLBAT S and QBAT S,QNSDQ,V OLNSDQ all set to their\ncorresponding \u201clow\u201d states.\nShortfall penalties \u03bbu,\u03bbo\nV OLBAT S\nOrder allocation in % of S\nM \u22c6L\u22c6\nBAT S\nL\u22c6\nNSDQ Total\n1.0 cent per share\nlow\n99%\n1%\n1% 100%\nmedium\n98%\n2%\n1% 101%\nhigh\n4%\n93%\n8% 105%\n0.9 cents per share\nlow\n13%\n82%\n82% 176%\nmedium\n3%\n95%\n59% 157%\nhigh\n3%\n96%\n4% 103%\n0.5 cents per share\nlow\n0%\n99%\n99% 199%\nmedium\n0%\n99%\n99% 199%\nhigh\n0%\n100%\n21% 121%\nTable 3\nOptimal order allocations conditioned on a state variable V OLBAT S",
    "chunk_index": 26,
    "start_char": 63341,
    "end_char": 66215,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "Shortfall penalties \u03bbu,\u03bbo\nV OLBAT S\nOrder allocation in % of S\nM \u22c6L\u22c6\nBAT S\nL\u22c6\nNSDQ Total\n1.0 cent per share\nlow\n99%\n1%\n1% 100%\nmedium\n98%\n2%\n1% 101%\nhigh\n4%\n93%\n8% 105%\n0.9 cents per share\nlow\n13%\n82%\n82% 176%\nmedium\n3%\n95%\n59% 157%\nhigh\n3%\n96%\n4% 103%\n0.5 cents per share\nlow\n0%\n99%\n99% 199%\nmedium\n0%\n99%\n99% 199%\nhigh\n0%\n100%\n21% 121%\nTable 3\nOptimal order allocations conditioned on a state variable V OLBAT S\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n29\nShortfall penalties \u03bbu,\u03bbo\nShares \ufb01lled\nCost\nX\u22c6\nXE\nX\u22c6\nXE\n1.0 cent per share 1469\n867 0.85 0.84\n0.9 cents per share\n564\n867 0.74 0.78\n0.5 cents per share\n526\n867 0.38 0.55\nTable 4\nAverage execution quantity and cost (in cents per share) based on a tick data simulation\nThis example demonstrates how an optimal order allocation shifts from limit orders to market\norders as a trader\u2019s aversion to execution risk (represented by \u03bbu,\u03bbo) increases. We also notice that\nan order allocation depends on V OLBAT S - since trading volume is positively autocorrelated in\ntime, a \u201chigh\u201d reading of volume in the previous minute predicts a higher volume in the next minute,\nmaking a limit order execution more likely on the BATS Z exchange. Therefore the numerical\nsolution places more limit orders there. Interestingly, the optimal allocation puts limit orders on\nboth NASDAQ and BATS Z when V OLBAT S is \u201cmedium\u201d, but when it is \u201chigh\u201d almost no orders\nare placed on NASDAQ because a BATS execution is already quite likely.\nThe average execution costs for our numerical solution was computed based on a separate data\nsample and compared to the costs of an equal split strategy XE =\n\u0000 S\n3 , S\n3 , S\n3\n\u0001\n. Despite a substantial\nsimpli\ufb01cation of the original optimization problem the approximate solution based on states out-\nperforms a static benchmark. Similarly to the previous section we \ufb01nd that a static approach is too\nin\ufb02exible - for example when execution risk aversion parameters are relatively low \u03bbu = \u03bbo = 0.5\ncents per share, a better execution can be achieved by using fewer market orders. Since more limit\norders are used in our strategy, its average \ufb01ll size 526 is lower than to 867 shares obtained with\nXE. However, our solution leads to a lower cost because execution risk parameters are relatively\nlow and overall we can achieve a lower cost by using more limit orders.\nThe computation presented here can be viewed only as a \ufb01rst step towards implementing an\norder routing system. There is a variety of practical issues that were not addressed, such as \ufb01ne-\ntuning risk aversion parameters \u03bbu,\u03bbo or de\ufb01ning meaningful state variables given intraday changes\nin distributions of queue sizes and order \ufb02ows. Still, we are hopeful that results and intuition\ndeveloped here will be useful for designing order routing systems.",
    "chunk_index": 27,
    "start_char": 65802,
    "end_char": 68626,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "and overall we can achieve a lower cost by using more limit orders.\nThe computation presented here can be viewed only as a \ufb01rst step towards implementing an\norder routing system. There is a variety of practical issues that were not addressed, such as \ufb01ne-\ntuning risk aversion parameters \u03bbu,\u03bbo or de\ufb01ning meaningful state variables given intraday changes\nin distributions of queue sizes and order \ufb02ows. Still, we are hopeful that results and intuition\ndeveloped here will be useful for designing order routing systems.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n30\n6. Conclusion\nWe have formulated the optimal order placement problem for a market participant able to submit\nmarket orders and limit orders across multiple exchanges, and studied its solution properties using\nhistorical data and numerical simulations. In the case when only one exchange is available we\nhave derived an optimal split between limit and market orders and showed that an optimal order\nallocation depends on trader\u2019s aversion to execution risk. For the general case of multiple exchanges,\nwe provide a characterization of the optimal order placement strategy in terms of execution shortfall\nprobabilities. To solve the problem in practical applications we propose a fast and straightforward\nnumerical algorithm that re-samples past order \ufb01ll data to optimize future order executions. Using\nthis algorithm, we have studied the sensitivities of an optimal order allocation to problem inputs\nand showed that a simultaneous placement of limit orders on multiple trading venues according to\nour method can lead to a substantial reduction of transaction costs.\nAppendix. A: Alternative problem formulation\nWe may also consider an alternative approach to order placement optimization, which turns out\nto be related to our original formulation by duality. Consider the following problem:\nProblem 2 (Alternative formulation: cost minimization under execution constraints)\nmin\nX\u2208RK+1\n+\nE\n\"\n(h + f)M \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+)\n+\u03b8\n \nM +\nK\nX\nk=1\nLk + (S \u2212A(X,\u03be))+\n!#\n(13)\nsubject to:\nE\n\u0002\n(S \u2212A(X,\u03be))+\n\u0003\n\u2264\u00b5u\n(14)\nE\n\u0002\n(A(X,\u03be) \u2212S)+\n\u0003\n\u2264\u00b5o\n(15)\nIn this alternative formulation a trader can specify his tolerance to execution risks using constraints\non expected order shortfalls and over\ufb02ows. The goal is to minimize an expectation of order execution\ncosts under expected shortfall constraints. The Problem 2 does not appear to be tractable, but it\nhas a convex objective and convex inequality constraints, and we can easily \ufb01nd its (Lagrangian)\ndual problem:\nProblem 3\nmax\n\u03bbu\u22650,\u03bbo\u22650{V \u22c6(\u03bbu,\u03bbo) \u2212\u03bbu\u00b5u \u2212\u03bbo\u00b5o}\n(16)\nwhere V \u22c6(\u03bbu,\u03bbo) =\nmin\nX\u2208RK+1\n+\nE[v(X,\u03be)] is the optimal objective value from Problem 1 given parameter\nvalues \u03bbu,\u03bbo.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n31\nWe see that the Problem 3 is related to our original order placement problem - solving the Problem\n3 (and therefore, the Problem 2) amounts to re-solving the Problem 1 for di\ufb00erent values of \u03bbu,\u03bbo.",
    "chunk_index": 28,
    "start_char": 68108,
    "end_char": 71136,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "where V \u22c6(\u03bbu,\u03bbo) =\nmin\nX\u2208RK+1\n+\nE[v(X,\u03be)] is the optimal objective value from Problem 1 given parameter\nvalues \u03bbu,\u03bbo.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n31\nWe see that the Problem 3 is related to our original order placement problem - solving the Problem\n3 (and therefore, the Problem 2) amounts to re-solving the Problem 1 for di\ufb00erent values of \u03bbu,\u03bbo.\nThis discussion also leads to a new interpretation of parameters \u03bbu,\u03bbo in the Problem 1 as shadow\nprices for expected shortfall and over\ufb02ow constraints in the related Problem 2. Hereafter we focus\non the (more tractable) Problem 1, but note that the optimal point for the Problem 2 can also be\nfound by solving its dual problem.\nAppendix. B: Proofs\nProof of Proposition 1 First, for any allocation \u02dcX that has\n\u02dc\nM > S, we automatically have\nA( \u02dcX) > S and we can show that the (random) costs of\n\u02dcX are larger than those of XM =\n(S,0,...,0) \u2208C:\nv( \u02dcX,\u03be) \u2212v(XM,\u03be) = (h + f)( \u02dc\nM \u2212S) \u2212\nK\nX\nk=1\n(h + rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212\u02dcLk)+)+\n\u03bbo\n \n\u02dc\nM \u2212S +\nK\nX\nk=1\n\u0010\n(\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212\u02dcLk)+\n\u0011!\n+ \u03b8(M \u2212S +\nK\nX\nk=1\n\u02dcLk) >\n(\u03bbo + h + f)( \u02dc\nM \u2212S) +\nK\nX\nk=1\n(\u03bbo \u2212h \u2212rk)((\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+) > 0,\nwhich holds for all random \u03be. Therefore, V ( \u02dcX) > V (XM). Similarly, for any allocation \u02dcX with\n\u02dcLk > S \u2212\u02dc\nM de\ufb01ne a new allocation \u02dcX\u2032 by setting \u02dc\nM \u2032 = \u02dc\nM, \u02dcL\u2032\nj = \u02dcLj,\u2200j \u0338= k and \u02dcL\u2032\nk = S \u2212\u02dc\nM. Then\nv( \u02dcX,\u03be) \u2212v( \u02dcX\u2032,\u03be) = \u03b8(\u02dcLk \u2212\u02dcL\u2032\nk) > 0 on the event B = {\u03c9|\u03bek(\u03c9) < Qk + S \u2212M}.\nOn a complementary event Bc,\nv( \u02dcX,\u03be) \u2212v( \u02dcX\u2032,\u03be) = \u2212(h + rk)((\u03bek \u2212Qk \u2212S + \u02dc\nM)+ \u2212(\u03bek \u2212Qk \u2212\u02dcLk)+)\n+\u03bbo((\u03bek \u2212Qk \u2212S + \u02dc\nM)+ \u2212(\u03bek \u2212Qk \u2212\u02dcLk)+) + \u03b8(\u02dcLk \u2212\u02dcL\u2032\nk).\nTherefore\nV ( \u02dcX) \u2212V ( \u02dcX\u2032) = E\nh\nv( \u02dcX,\u03be) \u2212v( \u02dcX\u2032,\u03be)|B\ni\nP(B) + E\nh\nv( \u02dcX,\u03be) \u2212v( \u02dcX\u2032,\u03be)|Bci\nP(Bc) =\n\u03b8(\u02dcLk \u2212\u02dcL\u2032\nk) + E\nh\n(\u03bbo \u2212(h + rk))((\u03bek \u2212Qk \u2212S + \u02dc\nM)+ \u2212(\u03bek \u2212Qk \u2212\u02dcLk)+)|Bci\nP(Bc) > 0\nIf \u02dcX\u2032 /\u2208C, we can continue truncating limit order sizes \u02dcL\u2032\nj > S \u2212\u02dc\nM \u2032 to S \u2212\u02dc\nM \u2032 following the same\nargument. Each time the truncation decreases the objective function and \ufb01nally we obtain a \u02dcX\u2032\u2032 \u2208C,\nsuch that V ( \u02dcX\u2032\u2032) < V ( \u02dcX).\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n32\nNext, if \u02dcX is such that \u02dc\nM \u2212PK\nk=1 \u02dcLk < S consider the di\ufb00erence s = S \u2212\u02dc\nM \u2212PK\nk=1 \u02dcLk and de\ufb01ne a\nnew allocation \u02dcX\u2032 as \u02dc\nM \u2032 = \u02dc\nM +s, \u02dcL\u2032\nk = \u02dcLk,k = 1,...,K.",
    "chunk_index": 29,
    "start_char": 70736,
    "end_char": 73031,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "the objective function and \ufb01nally we obtain a \u02dcX\u2032\u2032 \u2208C,\nsuch that V ( \u02dcX\u2032\u2032) < V ( \u02dcX).\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n32\nNext, if \u02dcX is such that \u02dc\nM \u2212PK\nk=1 \u02dcLk < S consider the di\ufb00erence s = S \u2212\u02dc\nM \u2212PK\nk=1 \u02dcLk and de\ufb01ne a\nnew allocation \u02dcX\u2032 as \u02dc\nM \u2032 = \u02dc\nM +s, \u02dcL\u2032\nk = \u02dcLk,k = 1,...,K. Then v( \u02dcX\u2032,\u03be)\u2212v( \u02dcX,\u03be) = (h+f \u2212\u03bbu)s < 0.\nAlthough the total quantity of orders in allocation \u02dcX is smaller than in \u02dcX\u2032, it has the same market\nimpact. The former always falls short of the target S and its remainder s must be \ufb01lled with a\nmarket order at time T, leading to the same impact. Since v( \u02dcX\u2032,\u03be) \u2212v( \u02dcX,\u03be) < 0, we also have\nV ( \u02dcX\u2032) < V ( \u02dcX). \u25a0\nProof of Proposition 2: First, note that (\u03bek \u2212Qk)+ \u2212(\u03bek \u2212Qk \u2212Lk)+ are concave functions of Lk.\nTherefore, A(X,\u03be) is concave as a sum of concave functions. Similarly, the cost term in v(X,\u03be) is\na sum of convex functions, as long as rk \u2265\u2212h,k = 1,...,K and is itself a convex function. Second,\nsince S \u2212A(X,\u03be) is a convex functon of X, and the function h(x) = \u03bbu(x)+ \u2212\u03bbo(\u2212x)+ is convex\nin x for positive \u03bbu,\u03bbo, so the penalty term h(S \u2212A(X,\u03be))) is also convex.\nUsing the assumption \u03bbo > h + max\nk {rk} it is clear that the function v(X,\u03be) is bounded from\nbelow by \u2212(h + max\nk {rk})S and therefore V (X) is also bounded from below.\nSince V (X) is convex, it is also continous and since it is bounded from below it reaches a\nlocal minimum Vmin on the compact set C at some point X\u22c6\u2208C. By convexity, Vmin is a global\nminimum of V (X) on C. Moreover, Proposition 1 guarantees that Vmin < V ( \u02dcX) for any \u02dcX /\u2208C, so\nVmin is also a global minimum of V (X) on RK+1\n+\n. \u25a0\nProof of Proposition 3:\nBy Proposition 1 there exists an optimal split (M \u22c6,L\u22c6) \u2208C between limit and market orders.\nMoreover for K = 1 the set C reduces to a line M \u22c6+ L\u22c6= S so it is su\ufb03cient to \ufb01nd M \u22c6. The\nrestriction L = S \u2212M implies that {A(X,\u03be) > S} = \u2205, {A(X,\u03be) < S,\u03be > Q + L} = \u2205, and we can\nrewrite the objective function as\nV (M) = E\nh\n(h + f)M \u2212(h + r)((\u03be \u2212Q)+ \u2212(\u03be \u2212Q \u2212S + M)+) + \u03b8S +\n(\u03bbu + \u03b8)(S \u2212M \u2212((\u03be \u2212Q)+ \u2212(\u03be \u2212Q \u2212S + M)+))))+\ni\n.\n(17)\nAlthough \u03b8S is a constant, we note that di\ufb00erent solutions (M,L) have di\ufb00erent total market impact\ndue to a terminal market order at time T.",
    "chunk_index": 30,
    "start_char": 72695,
    "end_char": 74939,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "V (M) = E\nh\n(h + f)M \u2212(h + r)((\u03be \u2212Q)+ \u2212(\u03be \u2212Q \u2212S + M)+) + \u03b8S +\n(\u03bbu + \u03b8)(S \u2212M \u2212((\u03be \u2212Q)+ \u2212(\u03be \u2212Q \u2212S + M)+))))+\ni\n.\n(17)\nAlthough \u03b8S is a constant, we note that di\ufb00erent solutions (M,L) have di\ufb00erent total market impact\ndue to a terminal market order at time T. For M \u2208(0,S) the expression under the expectation in\n(17) is bounded for all \u03be and Lipschitz with respect to M, so we can compute V \u2032(M) = dV (M)\ndM\nas:\nV \u2032(M) = E\nh\nh + f + (h + r)1{\u03be>Q+S\u2212M} \u2212(\u03bbu + \u03b8)1{\u03be<Q+S\u2212M}\ni\n= 2h + f + r \u2212(h + r + \u03bbu + \u03b8)F(Q + S \u2212M)\n(18)\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n33\nNote that if \u03bbu \u22642h + f + r\nF(Q + S) \u2212(h + r + \u03b8), then V \u2032(M) \u22650 for M \u2208(0,S) and therefore V is\nnon-decreasing at these points. Checking that\nV (S) \u2212V (0) \u2265(h + f \u2212\u03bbu \u2212\u03b8)S + (\u03bbu + \u03b8 + h + r)S(1 \u2212F(Q + S)) \u22650\nwe conclude that M \u22c6= 0. Similarly, if \u03bbu \u22652h + f + r\nF(Q)\n\u2212(h + r + \u03b8), then V \u2032(M) \u22640 for all M \u2208\n(0,S) and V (M) is non-increasing at these points. Checking that\nV (S) \u2212V (0) \u2264(h + f \u2212\u03bbu \u2212\u03b8)S + (\u03bbu + \u03b8 + h + r)S(1 \u2212F(Q)) \u22640\nwe conclude that M \u22c6= S. Finally, if \u03bbu is between these two values, \u2203\u03f5 > 0, such that\nV \u2032(\u03f5) < 0,V \u2032(S \u2212\u03f5) > 0 and by continuity of V \u2032 there is a point where V \u2032(M \u22c6) = 0. This M \u22c6is\noptimal by convexity of V (M) and (9) solves equations V \u2032(M \u22c6) = 0,L\u22c6= S \u2212M \u22c6. \u25a0\nProof of Proposition 4:\nProposition 2 implies the existence of an optimal order allocation X\u22c6\u2208C. First, we de\ufb01ne XM =\n(S,0,...,0) and prove that X\u22c6\u0338= XM by contradiction. If XM were optimal in the problem (6) it\nwould also be optimal in the same problem with a constraint Lk = 0,k \u0338= j, for any one j. In other\nwords, the solution (S,0) would be optimal for any one-exchange problem, de\ufb01ned by using only\nexchange j. But by our assumption, there exists a J such that \u03bbu < 2h + f + rJ\nFJ(QJ)\n\u2212(h + rJ) and\nProposition 3 implies that (S,0) is not optimal for the J-th single-exchange subproblem, leading\nto a contradiction.\nThe function v(X,\u03be) is bounded for X \u2208C and for all \u03be, di\ufb00erentiable with respect to M and\nLk,k = 1,...,K for X \u2208C\\{XM} for almost all \u03be. Therefore V (X) is di\ufb00erentiable for X \u2208C\\{XM}\nand we can compute all of its partial derivatives by interchanging the order of di\ufb00erentiation and\nintegration.",
    "chunk_index": 31,
    "start_char": 74683,
    "end_char": 76908,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "is not optimal for the J-th single-exchange subproblem, leading\nto a contradiction.\nThe function v(X,\u03be) is bounded for X \u2208C and for all \u03be, di\ufb00erentiable with respect to M and\nLk,k = 1,...,K for X \u2208C\\{XM} for almost all \u03be. Therefore V (X) is di\ufb00erentiable for X \u2208C\\{XM}\nand we can compute all of its partial derivatives by interchanging the order of di\ufb00erentiation and\nintegration. The KKT conditions for problem (6) and X \u2208C\\{XM} are\nh + f + \u03b8 \u2212(\u03bbu + \u03b8)P(A(X\u22c6,\u03be) < S) + \u03bboP(A(X\u22c6,\u03be) > S) \u2212\u00b50 = 0\n(19)\n\u2212(h + rk)P(\u03bek > Qk + L\u22c6\nk) + \u03b8 \u2212(\u03bbu + \u03b8)P(A(X\u22c6,\u03be) < S,\u03bek > Qk + L\u22c6\nk)+\n\u03bboP(A(X\u22c6,\u03be) > S,\u03bek > Qk + L\u22c6\nk) \u2212\u00b5k = 0,\nk = 1,...,K\n(20)\nM \u22650,\nLk \u22650,\n\u00b50 \u22650,\n\u00b5k \u22650,\n\u00b50M = 0,\n\u00b5kLk = 0,\nk = 1,...,K\n(21)\nSince the objective function V (\u00b7) is convex, conditions (19)\u2013(21) are both necessary and su\ufb03cient\nfor optimality. The \ufb01rst result of this proposition follows if we consider any \u02dcX with \u02dc\nM = 0 and use\nthe assumption \u03bbu \u2265\n2h+f+max\nk {rk}\np0\n\u2212(h + max\nk {rk}):\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n34\nV ( \u02dcX) \u2265\u2212(h+max\nk {rk})SP\n \\\nk\n{\u03bek \u2264Qk}\n!\n+\u03bbuSP\n \\\nk\n{\u03bek \u2264Qk}\n!\n+\u03b8S \u2265(h+f +\u03b8)S = V (XM).\nWe already argued that \u2203X\u22c6with V (X\u22c6) < V (XM), so X\u22c6\u0338= \u02dcX and therefore M \u22c6> 0.\nNext, we can rearrange terms in a j-th equality (20) as follows:\n\u03b8 + P(\u03bej > Qj + L\u22c6\nj)\n\u0002\n\u03bbo \u2212(h + rj) \u2212(\u03bbu + \u03bbo + \u03b8)P(A(X\u22c6,\u03be) < S|\u03bej > Qj + L\u22c6\nj)\n\u0003\n\u2212\u00b5j = 0.\n(22)\nSetting L\u22c6\nj = 0 and using the assumption (h + rj \u2212\u03bbo)P(\u03bej > Qj) > \u03b8 we see that equation (22)\ncannot hold because the left-hand side is negative, leading to a contradiction.\nWe showed that M \u22c6> 0,L\u22c6\nj > 0 for all j = 1,...,K and therefore, \u00b50 = \u00b51 = \u00b7\u00b7\u00b7 = \u00b5K = 0 by\ncomplementary slackness. Then the KKT conditions (19)\u2013(21) reduce to (10)\u2013(11). \u25a0\nCorollary In addition to assumptions of Proposition 4, assume that K = 2 and \u03be1,\u03be2 are\nindependent. Also, assume that F1,2(Q1,2) < 1 \u2212h + r2,1\n\u03bbo\nand (h + r1,2)P(\u03be1,2 > Q1,2 + S) > \u03b8. Then\nthere is an optimal order allocation X\u22c6= (M \u22c6,L\u22c6\n1,L\u22c6\n2) \u2208int{C} and it solves equations (12a-12c).\nProof: First we show that solutions on the boundary of C are sub-optimal: M \u22c6= 0, M \u22c6= S and\nL\u22c6\n1,2 = 0 are ruled out in Proposition 4. Solutions with M \u22c6+\nK\nP\nk=1\nL\u22c6\nk = S are ruled out by checking\n(20) and noting that since A(X\u22c6,\u03be) > S = \u2205and (h + rj)P(\u03bej > Qj + S) > \u03b8 by assumption, the\nleft-hand side of that equation is always negative, leading to a contradiction.",
    "chunk_index": 32,
    "start_char": 76528,
    "end_char": 78906,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "First we show that solutions on the boundary of C are sub-optimal: M \u22c6= 0, M \u22c6= S and\nL\u22c6\n1,2 = 0 are ruled out in Proposition 4. Solutions with M \u22c6+\nK\nP\nk=1\nL\u22c6\nk = S are ruled out by checking\n(20) and noting that since A(X\u22c6,\u03be) > S = \u2205and (h + rj)P(\u03bej > Qj + S) > \u03b8 by assumption, the\nleft-hand side of that equation is always negative, leading to a contradiction.\nFinally, solutions with L1,2 = S \u2212M are also ruled out by checking (20). For example, if L1 =\nS \u2212M > 0 it follows that \u00b51 = 0 and we can rewrite (20) as:\n\u2212(h + r1)P(\u03be1 > Q1 + L1) + \u03b8 + \u03bboP(\u03be2 > Q2)P(\u03be1 > Q1 + L1) = 0\nDividing by P(\u03be1 > Q1 + L1) and applying the assumption F1(Q1) < 1 \u2212h + r2\n\u03bbo\nwe note that the\nleft-hand side of the equation is always positive leading to a contradiction.\nHaving showed that an optimal solution belongs to the interior of C, we can simplify the descrip-\ntion of an event A(X,\u03be) > S. For any X \u2208int{C}, A(X,\u03be) > S if and only if all the following three\ninequalities are satis\ufb01ed:\n\u03be1 > Q1 + S \u2212M \u2212L2\n(23a)\n\u03be2 > Q2 + S \u2212M \u2212L1\n(23b)\n\u03be1 + \u03be2 > Q1 + Q2 + S \u2212M\n(23c)\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n35\nThese inequalities give a simple characterization of the event {A(X,\u03be) > S} which is directly\nveri\ufb01ed by considering subsets of (\u03be1,\u03be2) forming a complete partition of R2\n+.\nCase 1: \u03be1 > Q1 + L1,\u03be2 > Q2 + L2. Since L1 + L2 + M > S, we have A(X,\u03be) = L1 + L2 + M > S\nand at the same time all of the inequalities (23a-23c) are satis\ufb01ed, so they are trivially equivalent\nin this case.\nCase 2: \u03be1 > Q1 +L1,Q2 \u2264\u03be2 \u2264Q2 +L2. Because of the condition \u03be1 > Q1 +L1, (23a) is satis\ufb01ed.\nWe have in this case that A(X,\u03be) = L1 + \u03be2 \u2212Q2 + M and thus A(X,\u03be) > S if and only if (23b) is\nsatis\ufb01ed. Finally, \u03be1 > Q1 + L1 together with (23b) imply (23c), so A(X,\u03be) > S and (23a-23c) are\nequivalent in this case.\nCase 3: \u03be2 > Q2 + L2,Q1 \u2264\u03be1 \u2264Q1 + L1. Similarly to Case 2 we can show that inequalities\n(23a-23c) are satis\ufb01ed if and only if A(X,\u03be) > S.\nCase 4: Q1 + S \u2212M \u2212L2 < \u03be1 \u2264Q1 + L1,Q2 + S \u2212M \u2212L1 < \u03be2 \u2264Q2 + L2. This set is non-\nempty because 0 < S \u2212M \u2212L1 < L2 and similarly for L1,L2 reversed. Inequalities (23a)\u2013(23b) hold\ntrivially, only (23c) needs to be checked. We can write A(X,\u03be) = \u03be1 \u2212Q1 + \u03be2 \u2212Q2 + M > S if and\nonly if (23c) holds, so A(X,\u03be) > S is equivalent to (23a-23c).",
    "chunk_index": 33,
    "start_char": 78543,
    "end_char": 80851,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "if and only if A(X,\u03be) > S.\nCase 4: Q1 + S \u2212M \u2212L2 < \u03be1 \u2264Q1 + L1,Q2 + S \u2212M \u2212L1 < \u03be2 \u2264Q2 + L2. This set is non-\nempty because 0 < S \u2212M \u2212L1 < L2 and similarly for L1,L2 reversed. Inequalities (23a)\u2013(23b) hold\ntrivially, only (23c) needs to be checked. We can write A(X,\u03be) = \u03be1 \u2212Q1 + \u03be2 \u2212Q2 + M > S if and\nonly if (23c) holds, so A(X,\u03be) > S is equivalent to (23a-23c).\nCase 5: Outside of Cases 1-4, either (23a) or (23b) is not satis\ufb01ed. If \u03be1 \u2264Q1 +S \u2212M \u2212L2,\u03be2 \u2264\nQ2 + L2, then A(X,\u03be) \u2264S \u2212M \u2212L2 + L2 + M = S. The case \u03be2 \u2264Q2 + S \u2212M \u2212L1,\u03be1 \u2264Q1 + L1 is\ncompletely symmetric, and it shows that neither A(X,\u03be) > S nor (23a-23c) hold in this case.\nNext, we use inequalities (23a-23c) to characterize the set {A(X,\u03be) > S} in the \ufb01rst-order conditions\n(10)\u2013(11). We observe that in the two-exchange case\n{A(X,\u03be) > S,\u03be1 > Q1 + L1} = {\u03be1 > Q1 + L1,\u03be2 > Q2 + S \u2212M \u2212L1}\n{A(X,\u03be) > S,\u03be2 > Q2 + L2} = {\u03be2 > Q2 + L2,\u03be1 > Q1 + S \u2212M \u2212L2},\nand then use the independence of \u03be1 and \u03be2 to compute\nP(A(X,\u03be) > S|\u03be1 > Q1 + L1) = \u00afF2(Q2 + S \u2212M \u2212L1)\nP(A(X,\u03be) > S|\u03be2 > Q2 + L2) = \u00afF1(Q1 + S \u2212M \u2212L2)\nTogether with (11), this leads to a pair of equations for limit orders sizes:\n\u00afF2(Q2 + S \u2212M \u2212L1) = \u2212\u03b8/ \u00afF1(Q1 + L1) + \u03bbu + \u03b8 + h + r1\n\u03bbu + \u03bbo + \u03b8\n\u00afF1(Q1 + S \u2212M \u2212L2) = \u2212\u03b8/ \u00afF2(Q2 + L2) + \u03bbu + \u03b8 + h + r2\n\u03bbu + \u03bbo + \u03b8\nwhose solution is given by L\u22c6\n1,L\u22c6\n2 from (12a,12b). To obtain the equation (12c), we rewrite the \ufb01rst\nequation in (10,11) using the inequalities (23a-23c). Then P(A(X,\u03be) > S) may be computed as the\nintegral of the product measure F1 \u2297F2 over the region de\ufb01ned by\nU(Q,S,M,L1,L2) = {(x1,x2) \u2208R2,\nx1 > Q1+S\u2212M \u2212L2,\nx2 > Q2+S\u2212M \u2212L1,\nx1+x2 > Q1+Q2+S\u2212M}.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n36\nThis integral is given by\nP(A(X,\u03be) > S) = F1 \u2297F2 (U(Q,S,M,L1,L2))\n= \u00afF1(Q1 + L1) \u00afF2(Q2 + S \u2212M \u2212L1) +\nQ1+L1\nZ\nQ1+S\u2212M\u2212L2\n\u00afF2(Q1 + Q2 + S \u2212M \u2212x1)dF1(x1) = \u03bbu \u2212(h + f)\n\u03bbu + \u03bbo + \u03b8\n\u25a0\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n37\nReferences\nAlfonsi, Aur\u00b4elien, Antje Fruth, Alexander Schied. 2010. Optimal execution strategies in limit order books\nwith general shape functions. Quantitative Finance 10(2) 143\u2013157.\nAlmgren, R, N Chriss. 2000. Optimal execution of portfolio transactions. Journal of Risk 3 5\u201339.\nAlmgren, R, W. Harts. 2008. A Dynamic Algorithm for Smart Order Routing. Tech. rep., StreamBase.\nAlmgren, R, C Thum, E Hauptmann, H Li.",
    "chunk_index": 34,
    "start_char": 80488,
    "end_char": 82882,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "+ \u03bbo + \u03b8\n\u25a0\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n37\nReferences\nAlfonsi, Aur\u00b4elien, Antje Fruth, Alexander Schied. 2010. Optimal execution strategies in limit order books\nwith general shape functions. Quantitative Finance 10(2) 143\u2013157.\nAlmgren, R, N Chriss. 2000. Optimal execution of portfolio transactions. Journal of Risk 3 5\u201339.\nAlmgren, R, W. Harts. 2008. A Dynamic Algorithm for Smart Order Routing. Tech. rep., StreamBase.\nAlmgren, R, C Thum, E Hauptmann, H Li. 2005. Direct Estimation of Equity Market Impact. Risk 18 57.\nBaron, Matthew, Jonathan Brogaard, Andrei Kirilenko. 2014. Risk and returns in high frequency trading.\nTech. rep.\nBattalio, Robert, Shane Corwin, Robert Jennings. 2013. Can Brokers Have it all? On the Relation between\nMake Take Fees & Limit Order Execution Quality. Working paper.\nBayraktar, Erhan, Michael Ludkovski. 2011. Optimal Trade Execution in Illiquid Markets. Mathematical\nFinance 21(4) 681\u2013701.\nBertsimas, D, Andrew W Lo. 1998. Optimal control of execution costs. Journal of Financial Markets 1(1)\n1\u201350.\nBoehmer, Ekkehart, Robert Jennings. 2007. Public disclosure and private decisions: Equity market execution\nquality and order routing. Review of Financial Studies 20(2) 315\u2013358.\nBrogaard, Jonathan, Corey Garriott, Anna Pomeranets. 2014. High-Frequency Trading Competition.\nCao, Charles, Oliver Hansch, Xiaoxin Wang. 2008. Order placement strategies in a pure limit order book\nmarket. Journal of Financial Research 31(2) 113\u2013140.\nCont, Rama. 2011. Statistical modeling of high-frequency \ufb01nancial data. IEEE Signal Processing 28(5)\n16\u201325.\nCont, Rama, Adrien De Larrard. 2013. Price dynamics in a Markovian limit order market. SIAM Journal\non Financial Mathematics 4(1) 1\u201325.\nCont, Rama, Arseniy Kukanov, Sasha Stoikov. 2014. Price impact of order book events. Journal of Financial\nEconometrics 12(1) 47\u201388.\nEisler, Zoltan, Jean-Philippe Bouchaud, Julien Kockelkoren. 2012. The price impact of order book events:\nmarket orders, limit orders and cancellations. Quantitative Finance 12(9) 1395\u20131419.\nFoucault, T, A.J. Menkveld. 2008. Competition for Order Flow and Smart Order Routing Systems. Journal\nof Finance 63(1) 112.\nFoucault, Thierry. 1999. Order \ufb02ow composition and trading costs in a dynamic limit order market. Journal\nof Financial Markets 2(2) 99\u2013134.\nFoucault, Thierry, O Kadan, Eugene Kandel. 2005. Limit Order Book as a Market for Liquidity. Review of\nFinancial Studies 18(4) 1171\u20131217.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n38\nGanchev, Kuzman, Yuriy Nevmyvaka, Michael Kearns. 2010. Censored exploration and the dark pool prob-\nlem. Communications of the ACM 53(5) 99.\nGlosten, L, P Milgrom. 1985. Bid, ask and transaction prices in a specialist market with heterogeneously\ninformed traders. Journal of Financial Economics 14 71\u2013100.\nGueant, Olivier, C.A. Lehalle. 2014. General Intensity Shapes in Optimal Liquidation. Mathematical Finance\nto appear.\nGuilbaud, Fabien, Huyen Pham. 2014. Optimal high frequency trading in a pro-rata microstructure with\npredictive information. Mathematical Finance to appear.\nGuo, Xin, Adrien De Larrard, Z Ruan. 2013. Optimal Placement in a Limit Order Book.\nHarris, Lawrence, Joel Hasbrouck. 1996. Market vs. limit orders: the SuperDOT evidence on order submission\nstrategy. Journal of Financial and Quantitative Analysis 31(2) 213\u2013231.\nHuberman, Gur, Werner Stanzl. 2005.",
    "chunk_index": 35,
    "start_char": 82370,
    "end_char": 85819,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "Gueant, Olivier, C.A. Lehalle. 2014. General Intensity Shapes in Optimal Liquidation. Mathematical Finance\nto appear.\nGuilbaud, Fabien, Huyen Pham. 2014. Optimal high frequency trading in a pro-rata microstructure with\npredictive information. Mathematical Finance to appear.\nGuo, Xin, Adrien De Larrard, Z Ruan. 2013. Optimal Placement in a Limit Order Book.\nHarris, Lawrence, Joel Hasbrouck. 1996. Market vs. limit orders: the SuperDOT evidence on order submission\nstrategy. Journal of Financial and Quantitative Analysis 31(2) 213\u2013231.\nHuberman, Gur, Werner Stanzl. 2005. Optimal liquidity trading. Review of Finance 9(2) 165\u2013200.\nHuitema, Robert. 2012. Optimal Portfolio Execution using Market and Limit Orders.\nKushner, Harold, George Yin. 2003. Stochastic Approximation and Recursive Algorithms and Applications.\nSpringer, New York.\nKyle, A.S. 1985. Continuous auctions and insider trading. Econometrica 1315\u20131335.\nLaruelle, Sophie, Charles-Albert Lehalle, Gilles Pages. 2011. Optimal split of orders across liquidity pools:\na stochastic algorithm approach. SIAM Journal on Financial Mathematics 2(1) 1042\u20131076.\nLi, Tianhui Michael. 2013. Optimal Limit-versus-Market Order Slicing under a VWAP Bechmark - Contin-\nuous Case.\nLynch, Sarah, Emily Flitter. 2014. SEC probing brokerages over handling of retail orders. Reuters .\nMastromatteo, I, B. Toth, Jean-Philippe Bouchaud. 2013.\nAgent-based models for latent liquidity and\nconcave price impact.\nMenkveld, A.J. 2013. High frequency trading and the new-market makers. Journal of Financial Markets\n16(4) 712\u2013740.\nMoallemi, C.C., Constantinos Maglaras, Hua Zheng. 2012. Optimal Order Routing in a Fragmented Market.\nWorking Paper DRO-2012-02, Columbia University.\nNemirovski, A, A Juditsky, G Lan, A Shapiro. 2009. Robust stochastic approximation approach to stochastic\nprogramming. SIAM Journal on Optimization 19(4) 1574\u20131609.\nObizhaeva, Anna A, Jiang Wang. 2012. Optimal trading strategy and supply/demand dynamics. Journal of\nFinancial Markets 16(1) 1\u201332.\nParlour, Christine. 1998. Price dynamics in limit order markets. Review of Financial Studies 11(4) 789\u2013816.\n\nR Cont and A Kukanov: Optimal order placement and routing in limit order markets\n39\nPhillips, Matthew. 2014. What\u2019s E*Trade Doing With Its Customers\u2019 Orders Anyway? Bloomberg Business-\nweek .\nPredoiu, Silviu, Gennady Shaikhet, Steven Shreve. 2011. Optimal Execution in a General One-Sided Limit-\nOrder Book. SIAM Journal on Financial Mathematics 2(1) 183\u2013212.\nRosu, Ioanid. 2009. A Dynamic Model of the Limit Order Book. Review of Financial Studies 22(11) 4601.\nU.S. Senate. 2014. Con\ufb02icts of Interest in the U.S. Equity Markets: Testimony of Robert Battalio. Permanent\nSubcommittee on Investigations of the Committee on Homeland Security and Governmental A\ufb00airs .\nWilliamson, David P, David B Shmoys. 2011. The design of approximation algorithms. Cambridge University\nPress.",
    "chunk_index": 36,
    "start_char": 85246,
    "end_char": 88142,
    "paper_title": "Optimal order placement in limit order markets",
    "paper_category": "q-fin.TR",
    "paper_filename": "Optimal_order_placement_in_limit_order_markets.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Optimal_order_placement_in_limit_order_markets.pdf"
  },
  {
    "text": "Quantifying macroeconomic expectations in stock markets using \nGoogle Trends \n \nJohannes Bock \nDepartment of Finance, Warwick Business School, University of Warwick, Scarman \nRd, Coventry CV4 7AL, UK, email: j.bock@warwick.ac.uk \n \nMay 2018 \n \nAbstract \nAmong other macroeconomic indicators, the monthly release of U.S. unemployment rate \nfigures in the Employment Situation report by the U.S. Bureau of Labour Statistics gets a lot of \nmedia attention and strongly affects the stock markets. I investigate whether a profitable \ninvestment strategy can be constructed by predicting the likely changes in U.S. unemployment \nbefore the official news release using Google query volumes for related search terms. I find \nthat massive new data sources of human interaction with the Internet not only improves U.S. \nunemployment rate predictability, but can also enhance market timing of trading strategies \nwhen considered jointly with macroeconomic data. My results illustrate the potential of \ncombining extensive behavioural data sets with economic data to anticipate investor \nexpectations and stock market moves. \n \n \n\n2 \n \nIntroduction \nOnline data potentially offers new opportunities for economists, researchers and investors to \ninvestigate and observe shifts in behaviour ahead of more traditional and often backward-\nlooking sources such as consumer surveys or official data. Timely internet-based search \ndata helps to provide \u201cnew insights into different stages of large-scale collective decision \nmaking\u201d (Preis et al., 2013, p.5) and can be used to analyse people\u2019s beliefs, concerns and \npreferences to ultimately improve forecasts about future behaviour (Ettredge et al., 2005). \nFor example, investigations of Google Search data have been applied to predicting \nexchange rate volatility (Smith, 2012), private consumption (Vosen and Schmidt, 2011) and \ntrading behaviour (Preis et al., 2013). Moreover, Ettredge et al. (2005) were among the first \nresearchers using Google Search data to predict macroeconomic indicators including \nunemployment rates. For the United States, Choi and Varian (2009), and D\u2019Amuri and \nMarcucci (2017), find that recently laid off people are likely searching for unemployment \nbenefits, vacancies or welfare related topics online and therefore, unemployment forecasts \ncan be improved using Google data. Askitas and Zimmermann (2009), Smith (2016), and \nMcLaren (2011) find similar results for Germany and the UK. \nAs suggested by many researchers, better macroeconomic forecasts could be used to inform \neconomic decision making and compensate for the lagged nature of traditional data sources. \nGoing beyond past studies, this paper seeks to investigate the suggested benefits of \nincorporating Google based macroeconomic forecasts into the investment decision process \nby testing a hypothetical investment strategy based on monthly U.S. unemployment rate \npredictions. Since job growth is an important stimulus for the economy and thus, also a driver \nfor investment decisions in financial markets, I expect to be able to construct a profitable \ntrading strategy exploiting a Google based forecasting strategy. This study aims to contribute \nto the literature by shedding light on the impact of macroeconomic news on stock prices. It \ncan be interesting for investors seeking to understand the usefulness of online data for \nmarket timing. \nMethods and Results \nIn this study, I first analyse the possibility to predict monthly U.S.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3478,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "driver \nfor investment decisions in financial markets, I expect to be able to construct a profitable \ntrading strategy exploiting a Google based forecasting strategy. This study aims to contribute \nto the literature by shedding light on the impact of macroeconomic news on stock prices. It \ncan be interesting for investors seeking to understand the usefulness of online data for \nmarket timing. \nMethods and Results \nIn this study, I first analyse the possibility to predict monthly U.S. unemployment rates \n(\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) using monthly Google Search query volumes for a set of 34 search terms and \nsearch categories. Monthly and seasonally adjusted U.S. civilian unemployment rate data \nwas obtained for the period from January 2004 until December 2017 from the website of the \nFederal Reserve Bank of St. Louis1. For the same period, using the Google Trends service I \n \n1 U.S. Bureau of Labor Statistics, Civilian Unemployment Rate [UNRATE], retrieved from FRED, \nFederal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/UNRATE, March 5, 2018. \n\n3 \n \nobtained monthly Google Web Search volumes restricted to search requests of users located \nin the United States2. \nA two-step method was used to identify relevant Google Search terms for my analysis. First, \nI identified a base set of 8 search terms by considering past research (D\u2019Amuri and Marcucci, \n2017, p.9, Smith, 2016, p.265). Second, using the GloVe word embeddings3 of the root terms \n\u201cjobs\u201d, \u201cunemployment\u201d and \u201cunemployed\u201d, I identified an additional 22 semantically related \nwords by computing the Euclidean distance of 1.9 million English language words to these \nroot terms4. I considered word-word co-occurrence based word embeddings to be particularly \nhelpful in identifying search terms that are most typically searched along with the root terms \nor more generally, are related to unemployment. Additionally, following Choi and Varian \n(2009, p.1), I included four Google search categories5 namely \u201cJobs\u201d (60), \u201cWelfare & \nunemployment\u201d (706), \u201cWork & labour issues\u201d (703) and \u201cJob listings\u201d (960) in my analysis. \nNo particular transformation of the data was used, however it is important to note that Google \ndata, which is normalized to a range between 0 and 100 with respect to the maximum search \nvolume during a particular time period, is subject to certain limitations and needs to be \ntreated with caution. Google Trends data is provided using a sampling method causing it to \nvary from day to day (Choi and Varian, 2012, p.1) and therefore, there is potentially \nunobserved heterogeneity in the data. \nAs suggested in previous research, I will present all my forecasting results using the levels of \nthe monthly U.S. unemployment rate and Google Search data (D\u2019Amuri and Marcucci, 2017, \np.12-13). Similar to past studies, the \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 forecasting models are based on a linear \nregression formulation (Eq. 2) which includes a lagged autoregressive component and one \nlag of the exogeneous variables (\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc56\n), which in this study will be the Google Search \nvolumes for a particular search term or category \ud835\udc56 (McLaren, 2011, p.136). As in previous \nstudies, to see whether Google Trends data improves prediction accuracy, all models are \ncompared to a Baseline Model (Eq.",
    "chunk_index": 1,
    "start_char": 2990,
    "end_char": 6246,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "Search data (D\u2019Amuri and Marcucci, 2017, \np.12-13). Similar to past studies, the \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 forecasting models are based on a linear \nregression formulation (Eq. 2) which includes a lagged autoregressive component and one \nlag of the exogeneous variables (\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc56\n), which in this study will be the Google Search \nvolumes for a particular search term or category \ud835\udc56 (McLaren, 2011, p.136). As in previous \nstudies, to see whether Google Trends data improves prediction accuracy, all models are \ncompared to a Baseline Model (Eq. 1), which is a simple AR(1) autoregressive model that \nincludes unemployment level in the previous month as explanatory variable (McLaren, 2011, \np.136, Choi and Varian, 2009, p.2). \n \n \n \n2 Monthly Google search volumes for 34 search terms and categories, retrieved from Google Trends, \nhttps://trends.google.com/trends/, March 5, 2018. \n3 Common Crawl (42B tokens) GloVe word embeddings, retrieved from Stanford University, \nhttps://nlp.stanford.edu/projects/glove/, March 4, 2018. \n4 GloVe word embeddings are vector representations of words obtained by an unsupervised learning \nalgorithm trained on aggregated global word-word co-occurrence statistics from a text corpus of web \ndata (Pennington et al., 2014). \n5 Using Natural Language Processing Google Trends classifies search queries into about 30 broad \ncategories and 250 subcategories (Choi and Varian, 2012). \n\n4 \n \nBaseline Model: \n\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61= \ud835\udefd0 + \u03b21\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121 + \u03b5\ud835\udc61 \n \n(1) \nAlternative Model: \n\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61= \ud835\udf030 + \ud835\udf031\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121 + \ud835\udf032\n\ud835\udc56\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc56\n+ \ud835\udf02\ud835\udc61 \n \n(2) \nIt is crucial to note that different from previous research, I did not include the \ncontemporaneous search volumes \ud835\udc4b\ud835\udc61\n\ud835\udc56 in Eq. 2 due to constraints imposed by my later \nproposed investment strategy. Therefore, technically speaking this is not a \u201cnowcasting\u201d \nproblem like it has been extensively researched in the literature (Choi and Varian, 2009, p.2, \nSmith, 2016, p.267, D\u2019Amuri and Marcucci, 2017, p.14, McLaren, 2011, p.136). However, as \nstated by the U.S. Bureau of Labour Statistics (BLS), unemployment rates (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) \u201cin \nmonth t refer to individuals who do not have a job, but are available for work, in the week \nincluding the 12th day of month t and who have looked for a job in the prior 4 weeks ending \nwith the reference week\u201d (D\u2019Amuri and Marcucci, 2017, p.9). Thus, I decided to include one \nlag of Google Search volumes in Eq. 2 to be able to capture the research efforts of \nindividuals looking for a job, who eventually are included in the unemployment rate \ncalculations. In-sample estimation for both models is performed using the full data ranging \nfrom January 2004 until December 2017. The out-of-sample testing is conducted using a \nrolling window procedure, where the models are estimated using the past 36 months to \npredict \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 in the subsequent month. \nThe model\u2019s forecasting performance and its statistical significance are evaluated by \ncomparing the mean squared errors (MSE) and the squared residuals of each Alternative \nModel to the Baseline Model (Fig. 1). According to the Wilcoxon paired signed rank test of \nsquared model residuals, five of the proposed Alternative Models achieve statistically \nsignificant out-of-sample forecasting improvements, with the largest improvements by \nincluding Google Search volumes for the search term laid off (\ud835\udc59\ud835\udc4e\ud835\udc56\ud835\udc51 \ud835\udc5c\ud835\udc53\ud835\udc53: \ud835\udc4a= 5534, \ud835\udc5d<\n0.01;",
    "chunk_index": 2,
    "start_char": 5725,
    "end_char": 9060,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "models are estimated using the past 36 months to \npredict \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 in the subsequent month. \nThe model\u2019s forecasting performance and its statistical significance are evaluated by \ncomparing the mean squared errors (MSE) and the squared residuals of each Alternative \nModel to the Baseline Model (Fig. 1). According to the Wilcoxon paired signed rank test of \nsquared model residuals, five of the proposed Alternative Models achieve statistically \nsignificant out-of-sample forecasting improvements, with the largest improvements by \nincluding Google Search volumes for the search term laid off (\ud835\udc59\ud835\udc4e\ud835\udc56\ud835\udc51 \ud835\udc5c\ud835\udc53\ud835\udc53: \ud835\udc4a= 5534, \ud835\udc5d<\n0.01; \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b: \ud835\udc4a= 5789, \ud835\udc5d< 0.01; \ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc60: \ud835\udc4a= 5159, \ud835\udc5d< 0.06; \ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc52\ud835\udc52\ud835\udc5f\ud835\udc60: \ud835\udc4a= 5054, \ud835\udc5d<\n0.1; \ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc66\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61: \ud835\udc4a= 5235, \ud835\udc5d< 0.05). \n\n5 \n \n \nFigure 1 | Monthly U.S. unemployment rate forecasting improvements using Google Search data. (A) In- and \n(B) out-of-sample forecasting error as measured by the mean squared error (MSE) for 34 monthly U.S. \nunemployment rate (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) regression models incorporating Google Trends data. All 34 models include one \nautoregressive lag (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121), and one lag of monthly Google search volumes (\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc56\n) as explanatory variables for \n\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61. The regressions are estimated in-sample using the full data ranging from January 2004 until December \n2017. The out-of-sample testing is conducted using a rolling window procedure, where the models are estimated \nusing the past 36 months to predict \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 in the subsequent month. All models are benchmarked against the \nbaseline autoregressive forecasting model with one lag (vertical blue line). I find that Google search data improves \nMSE against the baseline model both in- and out-of-sample, with statistically significant improvements out-of-\nsample for five Google Trends models (\ud835\udc59\ud835\udc4e\ud835\udc56\ud835\udc51 \ud835\udc5c\ud835\udc53\ud835\udc53: \ud835\udc4a= 5534,\ud835\udc5d< 0.01;\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b: \ud835\udc4a= 5789, \ud835\udc5d< 0.01;\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc60: \ud835\udc4a=\n5159, \ud835\udc5d< 0.06;\ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc52\ud835\udc52\ud835\udc5f\ud835\udc60: \ud835\udc4a= 5054, \ud835\udc5d< 0.1;\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc5c\ud835\udc66\ud835\udc5a\ud835\udc52\ud835\udc5b\ud835\udc61: \ud835\udc4a= 5235, \ud835\udc5d< 0.05; Wilcoxon paired signed rank test of \nsquared model residuals). \nWith the Google Trends series, the out-of- sample MSE is decreased by 25.0% and 8.3% \nagainst the Baseline Model, for the search terms laid off and jobless, respectively \n(\ud835\udc40\ud835\udc46\ud835\udc38\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52= 0.036; \ud835\udc40\ud835\udc46\ud835\udc38\ud835\udc59\ud835\udc4e\ud835\udc56\ud835\udc51 \ud835\udc5c\ud835\udc53\ud835\udc53= 0.027; \ud835\udc40\ud835\udc46\ud835\udc38\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60\ud835\udc60= 0.033). Additionally, all parameter \nestimates are statistically significant, even though statistical conclusions about the coefficient \nestimates need to be treated with wariness, since distributional assumptions do not hold as I \nwill discuss below. My results are largely in line with past research, where D\u2019Amuri and \nMarcucci (2017, p.16) for instance, find an improvement of 23% over an AR(1) with no \nGoogle data (\ud835\udc40\ud835\udc46\ud835\udc38= 0.032) compared to their best model using Google (\ud835\udc40\ud835\udc46\ud835\udc38= 0.026). \nMoreover, note that while some of the Google Trends models, such as the Google Search \ncategory \u201cWelfare & unemployment\u201d, increased out-of-sample MSE (Fig. 1A), all of them at \nleast showed equal performance and mostly improved in-sample MSE (Fig. 1B), highlighting \nthe importance of an out-of-sample testing procedure. Overall, I can provide support for the \nidea that Google query volumes for most of the proposed search terms contain predictive \n\n6 \n \ninformation that might be related to job research efforts by individuals who are included in the \nunemployment statistics, as previously described.",
    "chunk_index": 3,
    "start_char": 8441,
    "end_char": 11724,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "out-of-sample MSE (Fig. 1A), all of them at \nleast showed equal performance and mostly improved in-sample MSE (Fig. 1B), highlighting \nthe importance of an out-of-sample testing procedure. Overall, I can provide support for the \nidea that Google query volumes for most of the proposed search terms contain predictive \n\n6 \n \ninformation that might be related to job research efforts by individuals who are included in the \nunemployment statistics, as previously described. Since my regression analysis focuses on \nunemployment rate and Google Search time series data, conclusions about the significance \nof parameter estimates depend on the stationarity and normality assumption. However, after \ntesting these with an Augmented Dickey-Fuller and Shapiro-Wilk test, respectively, I find that, \nonly for few exceptions, the null hypothesis of a unit root cannot be rejected and normality is \nstrongly rejected. Since S&P 500 returns are also non-normally distributed (Christoffersen, \n2011, p.9), I will only report distribution-free tests in the following analysis. \nThe second part of my study concerns the implementation of a hypothetical investment \nstrategy that is based on the previously introduced \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 forecasting models (Eq. 2). \nTherefore, I retrieved daily closing prices \ud835\udc43\ud835\udc51 of the S&P 500, which are adjusted for both \ndividends and splits, for the period from January 2, 2004 until December 29, 20176. Based on \n\ud835\udc43\ud835\udc51, cumulative returns \ud835\udc45\ud835\udc51 at time \ud835\udc51 \u220f\n[\n\ud835\udc43\ud835\udc58\n\ud835\udc43\ud835\udc58\u22121]\n\ud835\udc51\n\ud835\udc58=1\n\u22121 were calculated by the cumulative product \nof daily returns. Moreover, to make the trading strategy operable, I calculated the historical \nrelease dates \ud835\udc51\ud835\udc61\n\u2217 of the Employment Situation report for month \ud835\udc617. \nI propose an investment strategy, where the investment decision for month \ud835\udc61 is made 15 \ntrading days before the government data release (\ud835\udc51\ud835\udc61\n\u2217\u221215) based on the predicted change in \n\u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61= \n\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\n\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121 \u22121. If the unemployment rate is predicted to decrease (\u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61< 0), \nthe S&P 500 is bought at closing price \ud835\udc43\ud835\udc51\ud835\udc61\n\u2217\u221215\u22121. If however, \u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61> 0, I buy if \u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u2212\n\u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121 < 0, and short sell otherwise. This second order criteria is based on findings in \npsychology and behavioural finance that people tend to overweight positive news and \nunderreact to negative news (Easterwood and Nutt, 1999, p.1777). Hence, even though \nunemployment rates keep rising, if they do not rise as strongly as in the previous period, this \nis likely to be perceived as positive news and it is interpreted as a slowdown in the crisis, \nthus I buy. The logic is reversed in upstates of the economy and, therefore, nothing changes \nin the basic idea that decreasing unemployment is generally good news. In my setup long \nshort and no position if \u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61= 0 are allowed and the trading position is held until the next \ninvestment decision date. As imposed by the available Google data, the one month lagged \nmodel formulation and the 36 months of model estimation period, the first trading decision for \nthe March, 2007 government data release on Friday, April 6, 2007 is made at the end of \nMarch 15, 2007 and therefore, the investment period starts on March 16, 2007.",
    "chunk_index": 4,
    "start_char": 11253,
    "end_char": 14388,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "news. In my setup long \nshort and no position if \u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61= 0 are allowed and the trading position is held until the next \ninvestment decision date. As imposed by the available Google data, the one month lagged \nmodel formulation and the 36 months of model estimation period, the first trading decision for \nthe March, 2007 government data release on Friday, April 6, 2007 is made at the end of \nMarch 15, 2007 and therefore, the investment period starts on March 16, 2007. It is important \n \n6 Adjusted daily closing prices of the S&P 500, retrieved from Yahoo! Finance, \nhttps://finance.yahoo.com/quote/%5EGSPC/history/, March 5, 2018 \n7 UNEM release date calculations are based on the fact that data \u201cis typically released on the third \nFriday after the conclusion of the reference week, i.e., the week which includes the 12th of the month\u201d \n(also see: https://www.bls.gov/ces/ces_tabl.htm) \n\n7 \n \nto note that in my analysis I neglect transaction fees, since the maximum number of \ntransactions per year when using my strategy is only 24. However, such transaction fees \nwould certainly impact profit in a real-world implementation. \n \nFigure 2 | Correlation between U.S. unemployment predictability & trading profit. Correlation between out-of-\nsample mean squared error (MSE) in monthly U.S. unemployment rate (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) predictions and cumulative trading \nreturns. Returns are obtained by investment strategies which trade 15 trading days before the government data \nrelease and are based on 544 different \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 forecasting models incorporating Google Trends data from 34 search \nterms/categories. The prediction model specifications vary with respect to their number of autoregressive lags \n(\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u2212\ud835\udc58 ,\ud835\udc58 \ud835\udf16 [1,2]), Google search volume lags (\ud835\udc4b\ud835\udc61\u2212\ud835\udc58\n\ud835\udc56\n, \ud835\udc58 \ud835\udf16 [1,2]) and out-of-sample estimation window size \n(\ud835\udc5a \ud835\udf16 [12, 24, 36, 48]). I find a significant negative correlation between forecasting error and trading returns, \nsuggesting that the better U.S. unemployment predictability, the larger trading profits can be made \n(\ud835\udf0c= \u22120.15, \ud835\udc5d< 0.01, \ud835\udc46\ud835\udc5d\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc5b\u2019\ud835\udc60 \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b). \nThe trading strategy was implemented based on 544 variations of the \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 forecasting \nmodels. Differently from Eq. 2, these models varied with respect to their number of \nautoregressive lags (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u2212\ud835\udc58 , \ud835\udc58 \ud835\udf16 [1,2]), Google search volume lags (\ud835\udc4b\ud835\udc61\u2212\ud835\udc58\n\ud835\udc56\n, \ud835\udc58 \ud835\udf16 [1,2]) and \nout-of-sample estimation window size (\ud835\udc5a \ud835\udf16 [12, 24, 36, 48]). As shown in Fig. 2, I find that the \ncorrelation between out-of-sample MSE and cumulative trading returns over the whole \ninvestment period is significantly negative (\ud835\udf0c= \u22120.15, \ud835\udc5d< 0.01, Spearman\u2019s rank correlation). \nThis suggests that the better U.S. unemployment rate predictability, the larger trading profits \ncan be made. \nMoreover, the performance of the investment strategy is evaluated on the basis of the \ndifference in overall returns against four different benchmarks (Fig. 3). First, it is compared to \na strategy that is identical but based on forecasts of the Baseline Model. Second, I contrast a \n\n8 \n \nstrategy that buys or sells at closing price on the day of the data release, based on the actual \ndata. \n \nFigure 3 | Performances of Google Trends investment strategies against their benchmarks. Difference in \ncumulative returns between 34 investment strategies based on U.S.",
    "chunk_index": 5,
    "start_char": 13918,
    "end_char": 17210,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "the investment strategy is evaluated on the basis of the \ndifference in overall returns against four different benchmarks (Fig. 3). First, it is compared to \na strategy that is identical but based on forecasts of the Baseline Model. Second, I contrast a \n\n8 \n \nstrategy that buys or sells at closing price on the day of the data release, based on the actual \ndata. \n \nFigure 3 | Performances of Google Trends investment strategies against their benchmarks. Difference in \ncumulative returns between 34 investment strategies based on U.S. unemployment rate (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) forecasting \nmodels and their benchmark trading strategies. The 34 investment strategies are based on prediction models that \ninclude one autoregressive lag (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121), and one lag of monthly Google search volumes (\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc56\n) as explanatory \nvariables for \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 and trade 15 trading days before the government data release. The benchmarks include a \nstrategy based on the baseline autoregressive forecasting model with one lag (left), a strategy based on the actual \nchanges in monthly \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61, a strategy based purely on the Google search volumes of the respective search \nterm/category and a \u201cbuy and hold\u201d strategy (right). The difference in cumulative returns is calculated using the \nrespective overall portfolio returns over the entire investment period of my study from March 16, 2007 until \nDecember 29, 2017. I find that returns from the Google Trends strategies are significantly higher overall than returns \nfrom the benchmark strategies (< \ud835\udc45>\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52= 2.11;\ud835\udc4a= 414, \ud835\udc5d< 0.03; < \ud835\udc45>\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59\ud835\udc37\ud835\udc4e\ud835\udc61\ud835\udc4e= 2.11;\ud835\udc4a= 413, \ud835\udc5d<\n0.03; < \ud835\udc45>\ud835\udc35\ud835\udc62\ud835\udc66\ud835\udc3b\ud835\udc5c\ud835\udc59\ud835\udc51= 0.92;\ud835\udc4a= 595, \ud835\udc5d< 0.01, \ud835\udc5c\ud835\udc5b\ud835\udc52\u2212\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 \ud835\udc4a\ud835\udc56\ud835\udc59\ud835\udc50\ud835\udc5c\ud835\udc65\ud835\udc5c\ud835\udc5b \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b\ud835\udc52\ud835\udc51\u2212\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61; \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4a\ud835\udc43\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc54\ud835\udc59\ud835\udc52=\n595,\ud835\udc5d< 0.01, two-sample Wilcoxon paired signed rank test ). \nThird, I include a strategy, which is purely based on the change in Google Search volumes \nfor the respective search term, since similar strategies were proposed in past research (Preis \net al., 2013). In this case I sell at the beginning of month \ud835\udc61 if search volumes have increased \nin month \ud835\udc61\u22121 and vice versa for the long position. Finally, the \u201cbuy and hold strategy\u201d will \nserve as the benchmark for the market performance. I find that returns from the proposed \nGoogle Trends strategies are significantly higher than returns from the benchmarks \n(< \ud835\udc45>\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52= 2.11; \ud835\udc4a= 414, \ud835\udc5d< 0.03; < \ud835\udc45>\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59\ud835\udc37\ud835\udc4e\ud835\udc61\ud835\udc4e= 2.11;\ud835\udc4a= 413, \ud835\udc5d<\n0.03; < \ud835\udc45>\ud835\udc35\ud835\udc62\ud835\udc66\ud835\udc3b\ud835\udc5c\ud835\udc59\ud835\udc51= 0.92; \ud835\udc4a= 595, \ud835\udc5d< 0.01, \ud835\udc5c\ud835\udc5b\ud835\udc52\u2212\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52 \ud835\udc4a\ud835\udc56\ud835\udc59\ud835\udc50\ud835\udc5c\ud835\udc65\ud835\udc5c\ud835\udc5b \ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b\ud835\udc52\ud835\udc51\u2212\n\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61; \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4a\ud835\udc43\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc54\ud835\udc59\ud835\udc52= 595, \ud835\udc5d< 0.01, two-sample Wilcoxon paired signed rank test ). \nHowever, my results show that performance of the Google Trends strategy differs with the \n\n9 \n \nsearch term chosen. In Fig. 3 I investigate these differences and conclude that, even though \nthere is some variation, when compared to the \u201cbuy and hold\u201d and purely Google Trends \nbased strategy all 34 Alternative Models yield higher overall returns of up to 500% (laid off). \nBut, the Baseline Model and the actual data strategy are more competitive. To beat their \nperformance, more accurate \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 prediction models are required and therefore, the laid \noff and jobless models, which significantly improved out-of-sample MSE (see Fig. 1A), can \ncompete, for instance.",
    "chunk_index": 6,
    "start_char": 16673,
    "end_char": 19855,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "conclude that, even though \nthere is some variation, when compared to the \u201cbuy and hold\u201d and purely Google Trends \nbased strategy all 34 Alternative Models yield higher overall returns of up to 500% (laid off). \nBut, the Baseline Model and the actual data strategy are more competitive. To beat their \nperformance, more accurate \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 prediction models are required and therefore, the laid \noff and jobless models, which significantly improved out-of-sample MSE (see Fig. 1A), can \ncompete, for instance. \n \nFigure 4 | Performance of an investment strategy based on unemployment predictions using Google Trends \ndata. (A) Profit and loss for an investment strategy based on (B) the predicted month-over-month change in U.S. \nunemployment rates (\u2206\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61) plotted as a function of time. \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 predictions are based on Google search \nvolumes for the search term laid off, the keyword, which in my analysis gave the largest increase in out-of-sample \nforecasting accuracy (Fig. 1A). Investment decisions are made 15 trading days before the government data release \nand are based on prediction models that include one autoregressive lag (\ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61\u22121), and one lag of monthly Google \nsearch volumes (\ud835\udc4b\ud835\udc61\u22121\n\ud835\udc59\ud835\udc4e\ud835\udc56\ud835\udc51 \ud835\udc5c\ud835\udc53\ud835\udc53) as explanatory variables for \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61. This is compared to a strategy based purely on the \nGoogle search volumes for laid off (grey line), a strategy based on the actual changes in monthly \ud835\udc48\ud835\udc41\ud835\udc38\ud835\udc40\ud835\udc61 (dark blue \nline), a strategy based on the baseline autoregressive forecasting model with one lag (light blue line) and a \u201cbuy \nand hold\u201d strategy (black line). The Google Trends strategy using the search volume of the term laid off would have \nyielded a profit of 445% (red line). \nSome of the models, such as the one based on the search category \u201cjobs\u201d, would have \nyielded about 108% lower returns than the Baseline or actual data strategy. Fig. 4 shows the \ncumulative return as a function of time for the search term laid off, which in my analysis gave \nthe largest increase in out-of-sample forecasting accuracy (Fig. 1A). The use of this Google \nTrends strategy would have increased the value of a portfolio by 445%. Note that in hindsight \nall active trading strategies, including all benchmarks except the \u201cbuy and hold\u201d strategy, \n\n10 \n \nwould have avoided large losses during the financial crisis 2008-2010. However, only the \nAlternative Model strategy continued to benefit from rising stock markets, even though it \ncould also not prevent losses during 2015 and beginning of 2016. \nDiscussion \nMonthly government data releases on economic conditions are crucial information for \ninvestors, which drive investment decisions in financial markets, but unfortunately are lagged \nby several days and not readily available. Data on human interaction with Google Search has \nbeen shown to correlate with the current level of economic activity and predict certain \neconomic indicators such as unemployment rates. Therefore, I have investigated the \npossibility of exploiting this information in the present study by suggesting a hypothetical \ninvestment strategy based on monthly U.S. unemployment rate forecasts.",
    "chunk_index": 7,
    "start_char": 19351,
    "end_char": 22465,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "lagged \nby several days and not readily available. Data on human interaction with Google Search has \nbeen shown to correlate with the current level of economic activity and predict certain \neconomic indicators such as unemployment rates. Therefore, I have investigated the \npossibility of exploiting this information in the present study by suggesting a hypothetical \ninvestment strategy based on monthly U.S. unemployment rate forecasts. \nI have estimated and tested 34 forecasting models using Google query volumes for search \nterms and categories related to employment and have found that unemployment predictions \ncan significantly be improved using Google data. Additionally, I have demonstrated that my \nforecasting models could have been used in the construction of profitable trading strategies. I \nhave shown that portfolio returns of the proposed strategies not only significantly correlated \nwith unemployment forecasting accuracy, but also significantly outperform several \nbenchmark strategies including a \u201cbuy and hold\u201d strategy, and strategies purely based on \nactual unemployment or Google data. Hence, I find support for the idea that trading \nbehaviour in financial markets can be anticipated by forecasting changes in important \neconomic indicators. \nI offer one possible interpretation of my results within the context of investor expectations. \nInvestors closely monitor job growth since it is an important stimulus for economic growth \nand a general driver of consumer confidence and spending in the economy (Ludvigson, \n2004, p.31). Therefore, it is no surprise that changes in unemployment statistics are affecting \nstock markets. However, since by definition a profitable investment strategy needs to \nanticipate investor behaviour \u2013 or, more precisely, investor expectations -, my findings \nsuggest that expectations of unemployment levels might be already formed much before the \nrelease of new Employment Situation reports. Therefore, I suggest that Google search \nvolumes are a good measure of latent factors that ultimately impact investor expectations, \nand when jointly considered with unemployment data this results in a good approximation of \ninvestor expectations of unemployment levels. This is line with McLaren (2011), who argues \nthat Google Trends data may contain information above and beyond those provided by \nsurvey indicators. However, it is also important to note that Google Search behaviour is \nconstantly changing and issues like herd behaviour can make initially observed patterns \n\n11 \n \nobsolete. Such issues have considerable implications for forecasting ability and for example, \nhave been suggested as a key reason behind the persistent errors by Google Search data in \nflu predictions in recent years (Ormerod et al., 2014). \nTo conclude, my results illustrate the potential of combining extensive behavioural data sets \nwith economic data to anticipate investor expectations and stock market moves, but this \nshould also be treated with wariness due to fluctuations in Google Search behaviour. Future \nresearch could extend my analysis to other important economic statistics such as inflation or \nretail sales and investigate if investor expectations can be anticipated more widely. \n \n \n\n12 \n \nReferences \nASKITAS, N. & ZIMMERMANN, K. F. 2009. Google econometrics and unemployment forecasting. \nApplied Economics Quarterly, 55, 107-120. \nCHOI, H. & VARIAN, H. 2009. Predicting initial claims for unemployment benefits. Google Inc, 1-5.",
    "chunk_index": 8,
    "start_char": 22027,
    "end_char": 25520,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "and stock market moves, but this \nshould also be treated with wariness due to fluctuations in Google Search behaviour. Future \nresearch could extend my analysis to other important economic statistics such as inflation or \nretail sales and investigate if investor expectations can be anticipated more widely. \n \n \n\n12 \n \nReferences \nASKITAS, N. & ZIMMERMANN, K. F. 2009. Google econometrics and unemployment forecasting. \nApplied Economics Quarterly, 55, 107-120. \nCHOI, H. & VARIAN, H. 2009. Predicting initial claims for unemployment benefits. Google Inc, 1-5. \nCHOI, H. & VARIAN, H. 2012. Predicting the Present with Google Trends. Economic Record, 88, 2-9. \nCHRISTOFFERSEN, P. F. 2011. Elements of Financial Risk Management (2nd ed), Academic Press. \nD\u2019AMURI, F. & MARCUCCI, J. 2017. The predictive power of Google searches in forecasting US \nunemployment. International Journal of Forecasting, 33, 801-816. \nEASTERWOOD, J. C. & NUTT, S. R. 1999. Inefficiency in analysts' earnings forecasts: Systematic \nmisreaction or systematic optimism? The Journal of Finance, 54, 1777-1797. \nETTREDGE, M., GERDES, J. & KARUGA, G. 2005. Using Web-based Search Data to Predict: \nMACROECONOMIC STATISTICS. Communications of the ACM, 48, 87-92. \nLUDVIGSON, S. C. 2004. Consumer confidence and consumer spending. Journal of Economic \nperspectives, 18, 29-50. \nMCLAREN, N. 2011. Using Internet search data as economic indicators. Bank of England Quarterly \nBulletin, 51, 134-140. \nORMEROD, P., NYMAN, R. & BENTLEY, R. A. 2014. Nowcasting economic and social data: When and \nwhy search engine data fails, an illustration using Google Flu Trends. arXiv preprint \narXiv:1408.0699. \nPENNINGTON, J., SOCHER, R. & MANNING, C. Glove: Global vectors for word representation. \nProceedings of the 2014 conference on empirical methods in natural language processing \n(EMNLP), 2014. 1532-1543. \nPREIS, T., MOAT, H. S. & STANLEY, H. E. 2013. Quantifying trading behavior in financial markets using \nGoogle Trends. Scientific reports, 3, 1684. \nSMITH, G. P. 2012. Google Internet search activity and volatility prediction in the market for foreign \ncurrency. Finance Research Letters, 9, 103-110. \nSMITH, P. 2016. Google's MIDAS Touch: Predicting UK Unemployment with Internet Search Data. \nJournal of Forecasting, 35, 263-284. \nVOSEN, S. & SCHMIDT, T. 2011. Forecasting private consumption: survey\u2010based indicators vs. Google \ntrends. Journal of Forecasting, 30, 565-578.",
    "chunk_index": 9,
    "start_char": 24959,
    "end_char": 27403,
    "paper_title": "Quantifying macroeconomic expectations in stock ma",
    "paper_category": "q-fin.TR",
    "paper_filename": "Quantifying_macroeconomic_expectations_in_stock_ma.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Quantifying_macroeconomic_expectations_in_stock_ma.pdf"
  },
  {
    "text": "Systemic Risk in Market Microstructure of Crude Oil and\nGasoline Futures Prices: A Hawkes Flocking Model Approach\nHyun Jin Jang\u2217\nKiseop Lee\u2020\nKyungsub Lee\u2021\nAbstract\nWe propose the Hawkes \ufb02ocking model that assesses systemic risk in high-frequency\nprocesses at the two perspectives \u2013 endogeneity and interactivity. We examine the fu-\ntures markets of WTI crude oil and gasoline for the past decade, and perform a com-\nparative analysis with conditional value-at-risk as a benchmark measure.\nIn terms of\nhigh-frequency structure, we derive the empirical \ufb01ndings. The endogenous systemic risk\nin WTI was signi\ufb01cantly higher than that in gasoline, and the level at which gasoline\na\ufb00ects WTI was constantly higher than in the opposite case. Moreover, although the\nrelative in\ufb02uence\u2019s degree was asymmetric, its di\ufb00erence has gradually reduced.\nkeyword: Systemic risk; Hawkes process; \ufb02ocking; WTI crude oil futures; gasoline fu-\ntures; calibration; branching ratio; CoVaR\nJEL: G13, C13\n1\nIntroduction\nOver the past two decades, the systemic risk level has increased in \ufb01nancial markets due to the\ngrowth of securitization, hedge fund markets, and increase in intraday trading. Recently, the\nemergence of innovative technologies has accelerated the paradigm shift of trading activities\nin \ufb01nancial markets. Traditional trading platforms such as phone conversations or clicks on\na screen by humans has moved to automated trading by computers based on the ultra-low\nlatency electronic system. The increased trading speed enables execution of orders within\nmicroseconds by the use of sophisticated algorithms; this is called high-frequency trading.\nAccording to a report of the Commodity Futures Trading Commission (CFTC)1, the volume\n\u2217School of Business Administration, Ulsan National Institute of Science and Technology (UNIST), Ulsan,\nRepublic of Korea\n\u2020Department of Statistics, Purdue University, West Lafayette, Indiana, USA\n\u2021Corresponding author. Department of Statistics, Yeungnam University, Gyeongsan, Republic of Korea\n(Email:ksublee@yu.ac.kr, Tel:+82-53-810-2324, Fax:+82-53-810-2036)\n1CFTC, \u201cRemarks of Chairman Timothy Massad before the Conference on the Evolving Structure of the US\nTreasury Market,\u201d October 21, 2015, at http://www.cftc.gov/PressRoom/SpeechesTestimony/opamassad-\n30.\n1\narXiv:2012.04181v1 [q-fin.TR] 8 Dec 2020\n\nof high-frequency trading in futures markets has grown remarkably over the past decade. It\naccounts for 80% of foreign exchange futures, 67% of interest rate futures, 62% of equity\nfutures, and 47% of metals and energy futures trading volume. In addition, \ufb02ash crash events\nfrequently occur in security markets which are attributed to high-frequency trading2. Such\nan environmental change in trading potentially allows large price movements within a short\nperiod of time as well as the rapid risk propagation to di\ufb00erent assets, as mentioned in Miller\nand Shorter (2016).\nIn this context of high-frequency \ufb01nance, we develop a novel Hawkes process-based model\nto examine the level of systemic risk that exists within and between price dynamics at the\nmicroscopic level. The proposed model allows capturing contagious and clustered phenomena\nthat can be investigated in the excessive volatile and correlated markets. Studies related to\nsystemic risk in high-frequency trading are discussed under various aspects. Filimonov and\nSornette (2012) conduct event studies to investigate the changes in the systemic risk before\nand after the announcements of two extreme events: downgrading of Greece/Portugal and\nthe \ufb02ash crash event for the E-mini S&P500 futures in 2010.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3598,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "we develop a novel Hawkes process-based model\nto examine the level of systemic risk that exists within and between price dynamics at the\nmicroscopic level. The proposed model allows capturing contagious and clustered phenomena\nthat can be investigated in the excessive volatile and correlated markets. Studies related to\nsystemic risk in high-frequency trading are discussed under various aspects. Filimonov and\nSornette (2012) conduct event studies to investigate the changes in the systemic risk before\nand after the announcements of two extreme events: downgrading of Greece/Portugal and\nthe \ufb02ash crash event for the E-mini S&P500 futures in 2010. Hardiman et al. (2013) perform\na similar analysis with Filimonov and Sornette (2012) by taking power-law kernels. Chavez-\nDemoulin and McGill (2012) compute intraday value-at-risk (VaR) for stocks in New York\nStock Exchange (NYSE) using a peak-over-threshold model, and Jain et al. (2016) assess the\nextent to which a high-frequency system increases systemic risk in the Tokyo Stock Exchange.\nBormetti et al. (2015) use a multivariate Hawkes process with a common factor that controls\na large number of jumps in the transaction movement. Calcagnile et al. (2018) compute the\nnumber of co-jumps occurring in Russell 3000 index stocks to measure the frequency of the\ncollective instability at high-frequency.\nOn the other hand, there is little discussion on the increased systemic risk in energy mar-\nkets associated with high-frequency trading. However, energy futures markets are no longer\nexceptional on this matter. As noted in the beginning, almost the half of the trading volume\nin the energy markets is raised from high-frequency trading. Moreover, the CFTC examined\nhow frequently \ufb02ash events have occurred in the top-\ufb01ve most active futures contracts in\n2015, that is, corn, gold, West Texas Intermediate (WTI) crude oil, E-mini S&P 500 futures,\nand Euro FX3. Among them, surprisingly, more than 35 similar intraday \ufb02ash have occurred\njust for WTI crude oil futures4. This result implies that WTI crude oil futures are utilized\nactively as instruments of algorithmic trading strategies.\nIn this study, we attempt to discover empirical evidences of the systemic risk level in the\ndynamics of the two futures prices of WTI crude oil and gasoline observed at the intraday\ntransaction level over the past decade. The gasoline futures contracts are being traded most\n2For example, the DJIA index plunged roughly 1,100 points in the \ufb01rst \ufb01ve minutes of trading on August\n24, 2015.\n3In this research, the \ufb02ash crash is de\ufb01ned by the episodes in which a contract price moved at least 2%\nwithin an hour, but returned to within 0.75% of the original or starting price within that same hour.\n4https://www.cftc.gov/sites/default/files/idc/groups/public/@newsroom/documents/file/\nhourlyflashevents102115.pdf\n2\n\nactively in the New York Mercantile Exchange (NYMEX) in the energy sector, following the\nWTI crude oil futures. We consider two kinds of de\ufb01nitions for systemic risk in the market\nmicrostructure with the instability perspective. The \ufb01rst view is the degree of instability that\nexists within a price process. It is regarded as the term endogeneity, which is introduced in\nthe earlier literature (e.g.,Danielsson et al., 2012;",
    "chunk_index": 1,
    "start_char": 2948,
    "end_char": 6228,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "within 0.75% of the original or starting price within that same hour.\n4https://www.cftc.gov/sites/default/files/idc/groups/public/@newsroom/documents/file/\nhourlyflashevents102115.pdf\n2\n\nactively in the New York Mercantile Exchange (NYMEX) in the energy sector, following the\nWTI crude oil futures. We consider two kinds of de\ufb01nitions for systemic risk in the market\nmicrostructure with the instability perspective. The \ufb01rst view is the degree of instability that\nexists within a price process. It is regarded as the term endogeneity, which is introduced in\nthe earlier literature (e.g.,Danielsson et al., 2012; Filimonov and Sornette, 2012; Hardiman\net al., 20135). By estimating this level, we examine whether the trend of price decline leads\nto additional price decreases (or price rebounds). The second view is the degree of instability\nthat exists between price processes caused by interaction between two di\ufb00erent markets. In\nthat point of view, we investigate how the change in one price a\ufb00ects to the change in the\nother price, and vice versa. In addition, we observe how micro-movements of prices in the\ntwo markets are likely to close to each other when the price di\ufb00erence widens or narrows.\nMeanwhile, WTI crude oil and gasoline futures prices have maintained a strong depen-\ndence for a long time (EIA, 2014). From a macroeconomic perspective, the main causes of the\nprice di\ufb00erence between crude oil and gasoline are re\ufb01ning costs and supply/demand balance\nof each product. Such comovement has been studied in terms of market cointegration in\neconometrics or \ufb02ocking behavior. When the two markets are co-integrated or have a \ufb02ocking\nfeature, the associated prices are closely correlated. Furthermore, one price could lead the\nother, while the reverse also occurs from time to time, or all prices in a system could follow\nthe same behavior.\nCointegration refers to two or more non-stationary time series that are driven by one or\nmore common non-stationary time series, proposed in the seminal works by Granger (1981)\nand Engle and Granger (1987). Many \ufb01nancial data series are known to exhibit the coin-\ntegration, for example, international stock markets (Cerchi and Havenner, 1988; Taylor and\nTonks, 1989; Duan and Pliska, 2004), foreign exchange rates (Baillie and Bollerslev, 1989;\nKellard et al., 2010), futures and spot prices (Ng and Pirrong, 1994, 1996; Maslyuka and\nSmyth, 2009), especially, crude oil, gasoline, and heating oil futures prices (Serletis, 1992;\nChiu et al., 2015). As a similiar manner, \ufb02ocking is known to the collective motion of a large\nnumber of self-propelled entities. Reynolds (1987) \ufb01rstly proposed the breaking-through al-\ngorithm that makes it feasible to generate realistic computer simulation of \ufb02ocking agents.\nThe \ufb02ocking behavior appears in many contexts of physics, biology, engineering, and human\nsystems including \ufb01nancial markets (Rauch et al., 1995; Huepe and Aldana, 2008; Ha et al.,\n2015; Fang et al., 2017; among many others). Even though comovement propensity in two\nor more dynamics has been discussed with di\ufb00erent terms of cointegration or \ufb02ocking in an\namount of literature, at our knowledge there is no investigation of such tendency focusing on\n\u2018microstructure\u2019 movements.",
    "chunk_index": 2,
    "start_char": 5617,
    "end_char": 8859,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\ufb02ocking agents.\nThe \ufb02ocking behavior appears in many contexts of physics, biology, engineering, and human\nsystems including \ufb01nancial markets (Rauch et al., 1995; Huepe and Aldana, 2008; Ha et al.,\n2015; Fang et al., 2017; among many others). Even though comovement propensity in two\nor more dynamics has been discussed with di\ufb00erent terms of cointegration or \ufb02ocking in an\namount of literature, at our knowledge there is no investigation of such tendency focusing on\n\u2018microstructure\u2019 movements.\nBased on the notions of systemic risk and comovement tendency in high-frequency mar-\nkets, we propose a Hawkes \ufb02ocking model that enables us to quantify systemic risk embedded\nin price structures at a microscopic level. This model addresses how to measure the extent\n5In Filimonov and Sornette (2012) and Hardiman et al. (2013), this is referred to \u201cre\ufb02exivity\u201d instead of\nendogeneity\n3\n\nof both endogeneity and interactivity. By manipulating intensity processes that depend on\nthe relative level of a couple of prices, the proposed model describes a feedback mechanism\ncontaining self/mutually-exciting features as well as the \ufb02ocking behavior. Moreover, as a\ndirect indicator of systemic risk, we formulate branching ratios, which are generally used in a\nHawkes-based model to gauge how many additional jumps occur in the intensity process due\nto one exogenous event, and is employed for checking out the stability of the process. For the\nempirical analysis, we choose the nearest dated futures prices of WTI crude oil and gasoline\nthat are collected at a transaction level in the time period of January 2007 to December 2016.\nDuring this period, prices plunged three times in both markets.\nWe also compute the systemic risk level by employing an existing methodology, that is,\na conditional value-at-risk (CoVaR) approach, as a complementary measure of the proposed\nmodel. The concept of CoVaR is that the maximum loss can happen in an entity within a\ncon\ufb01dence level due to the e\ufb00ect of large loss from the other entity. This was \ufb01rstly proposed\nby Adrian and Brunnermeier (2016) and generalized by Girardi and Ergun (2013). In this\nstudy, we adopt the CoVaR de\ufb01ned by Girardi and Ergun (2013) and use a copula method\nto implement the CoVaR introduced by Reboredo and Ugolini (2015). Then, we simulate\nthe results from the CoVaR method and from the Hawkes \ufb02ocking model and compare the\nevolution of systemic risk in the high-frequency markets of WTI crude oil and gasoline, which\ninterplay actively.\nThe main contribution of this study is twofold. First, we develop a novel class of Hawkes-\nbased model that assesses two types of systemic risk in high-frequency price processes: the\nendogenous systemic risk in a single process and interactive systemic risk in a couple of\nprocesses. Second, we examine the existence of the systemic risk at a microscopic level via\nthe futures markets of WTI crude oil and gasoline that are most liquid in the US energy\nsector. Through the empirical test based on the proposed model, we obtain the following\nresults.",
    "chunk_index": 3,
    "start_char": 8365,
    "end_char": 11413,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "two types of systemic risk in high-frequency price processes: the\nendogenous systemic risk in a single process and interactive systemic risk in a couple of\nprocesses. Second, we examine the existence of the systemic risk at a microscopic level via\nthe futures markets of WTI crude oil and gasoline that are most liquid in the US energy\nsector. Through the empirical test based on the proposed model, we obtain the following\nresults. The overall systemic risk level that exists in the two futures markets was the highest\njust before the onset of the global credit crisis. For the past decade, the level of endogeneity\nin the WTI market was signi\ufb01cantly higher than that in the gasoline market. In particular,\nthe level at which gasoline price a\ufb00ects WTI price was steadily higher than in the opposite\ncase. Although the two markets have been interactive, their relative in\ufb02uences, that is, from\nWTI to gasoline and vice versa, were very asymmetric, but the degree of the di\ufb00erence has\nbeen gradually reducing over the study period.\nThis paper is organized as follows. In Section 2, we introduce the Hawkes \ufb02ocking model\nand derive branching ratios to check the stability condition of the process. Section 3 presents\nthe intraday transaction data for the two futures prices of WTI crude oil and gasoline from\n2007 to 2016 along with estimate results under the proposed model using the maximum\nlikelihood (ML) method. Section 4 presents a comparative analysis between the branching\nratios of the proposed model and the CoVaR measure. Section 5 concludes the paper, and\ntechnical proofs and additional \ufb01gures/tables are presented in the Appendix.\n4\n\n2\nThe Hawkes Flocking Model\nIn this section, we develop the Hawkes \ufb02ocking model. The proposed model can be categorized\nto generalized Hawkes processes, which is introduced in the serial papers (Hawkes, 1971a;\nHawkes, 1971b; Hawkes and Oakes, 1974). As the Hawkes process is based on a class of\nmultivariate counting processes, this model can account for the interaction of various types\nof Poisson-like events through its intensity process.\nBecause of their great \ufb02exibility and versatility, Hawkes-based models are very popular for\nmodeling high-frequency \ufb01nance. As a pioneering work, Bowsher (2007) introduce a bivariate\nHawkes processes to model the joint dynamics of trades and mid-price changes in NYSE\nstocks. After that, many studies related to high-frequency \ufb01nance have employed Hawkes\nprocesses (Bacry et al., 2012; Bacry et al., 2013; Da Fonseca and Zaatour, 2014; Bacry et al.,\n2015; Lee and Seo, 2017).\n2.1\nModel speci\ufb01cation\nAs noted in the introduction, the model we propose captures the interaction between two\nhighly correlated processes observed at the level of transaction data. Consider the bivariate\nprice processes that are de\ufb01ned by the di\ufb00erences between the two counting processes:\nC1(t) = \u03b41(Nu\n1 (t) \u2212Nd\n1 (t))\nC2(t) = \u03b42(Nu\n2 (t) \u2212Nd\n2 (t))\nwhere processes Nu\ni (t) and Nd\ni (t) count the number of events for upward and downward\nmovement in price Ci(t) up to time t, respectively, and \u03b4i are tick sizes.",
    "chunk_index": 4,
    "start_char": 10981,
    "end_char": 14061,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "the model we propose captures the interaction between two\nhighly correlated processes observed at the level of transaction data. Consider the bivariate\nprice processes that are de\ufb01ned by the di\ufb00erences between the two counting processes:\nC1(t) = \u03b41(Nu\n1 (t) \u2212Nd\n1 (t))\nC2(t) = \u03b42(Nu\n2 (t) \u2212Nd\n2 (t))\nwhere processes Nu\ni (t) and Nd\ni (t) count the number of events for upward and downward\nmovement in price Ci(t) up to time t, respectively, and \u03b4i are tick sizes.\nWe present a system of the counting process N and its intensity process \u03bb by employing\nthe following matrix form\nNt =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nNu\n1 (t)\nNd\n1 (t)\nNu\n2 (t)\nNd\n2 (t)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n\u03bbt =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03bbu\n1(t)\n\u03bbd\n1(t)\n\u03bbu\n2(t)\n\u03bbd\n2(t)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nwhere intensity process \u03bb represents the expected number of arrivals of counting events over\nan in\ufb01nitesimal time interval dt divided by dt. Let the intensity process be\n\u03bbt = \u00b5 +\n\u02c6 t\n\u2212\u221e\nh(t \u2212u)dNu\n(1)\nwhere \u00b5 = [\u00b51, \u00b51, \u00b52, \u00b52]\u22a4is a constant vector, and h is a four-by-four matrix. Here, the\nvector \u00b5 is called base intensity that accounts for the average frequency of exogenous events\ncoming into this system, which is independent of the other asset\u2019s movement as well as its\npast movement. Parameters \u00b51 and \u00b52 are interpreted as the pressure of orders for buying\nor selling at price C1 and C2, respectively. The matrix h is called a feedback kernel of the\n5\n\nHawkes process that decides the weight to be attributed to events dN occurring at lag u in\nthe past.\nWe set the feedback kernel by\nh(t \u2212u) = \u03a6(t \u2212u) + k(t) \u25e6\u03a8(t \u2212u)\n(2)\nwhere \u03a6, k, and \u03a8 are four-by-four matrices, and \u201c\u25e6\u201d denotes the element-wise multiplication\nof matrices. The kernel h consists of two components:\n(i) The matrix \u03a6 controls the self/mutually-exciting patterns between the two prices, which\nis de\ufb01ned as\n\u03a6(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b11se\u2212\u03b21t\n\u03b11ce\u2212\u03b21t\n0\n0\n\u03b11ce\u2212\u03b21t\n\u03b11se\u2212\u03b21t\n0\n0\n0\n0\n\u03b12se\u2212\u03b22t\n\u03b12ce\u2212\u03b22t\n0\n0\n\u03b12ce\u2212\u03b22t\n\u03b12se\u2212\u03b22t\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(3)\nwhere non-negative constants \u03b1is and \u03b1ic denote the self/mutually-exciting terms, re-\nspectively, and \u03b2i governs the speed of decay to the base intensity level of the i-th price\nprocess.\n(ii) The matrix k \u25e6\u03a8 controls the \ufb02ocking phenomenon according to which two price move-\nments interact, and where matrix k is de\ufb01ned by an indicator function matrix such\nas\nk(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1{C1(t)<C2(t)}\n1{C1(t)<C2(t)}\n1{C1(t)<C2(t)}\n1{C1(t)<C2(t)}\n1{C1(t)>C2(t)}\n1{C1(t)>C2(t)}\n1{C1(t)>C2(t)}\n1{C1(t)>C2(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n(4)\nand the matrix \u03a8(t) is de\ufb01ned by\n\u03a8(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u03b11we\u2212\u03b21t\n\u03b11ne\u2212\u03b21t\n0\n0\n\u03b11ne\u2212\u03b21t\n\u03b11we\u2212\u03b21t\n\u03b12we\u2212\u03b22t\n\u03b12ne\u2212\u03b22t\n0\n0\n\u03b12ne\u2212\u03b22t\n\u03b12we\u2212\u03b22t\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(5)\nwhere non-negative constants \u03b1iw and \u03b1in denote the \ufb02ocking exciting terms.\nIn part (i), the exponential decaying setup in the bivariate Hawkes process with symmetric\n\u03b1\u2019s shows a prototypical model for high-frequency \ufb01nance.",
    "chunk_index": 5,
    "start_char": 13598,
    "end_char": 16474,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "1{C1(t)>C2(t)}\n1{C1(t)>C2(t)}\n1{C1(t)>C2(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)<C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n1{C2(t)>C1(t)}\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n(4)\nand the matrix \u03a8(t) is de\ufb01ned by\n\u03a8(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u03b11we\u2212\u03b21t\n\u03b11ne\u2212\u03b21t\n0\n0\n\u03b11ne\u2212\u03b21t\n\u03b11we\u2212\u03b21t\n\u03b12we\u2212\u03b22t\n\u03b12ne\u2212\u03b22t\n0\n0\n\u03b12ne\u2212\u03b22t\n\u03b12we\u2212\u03b22t\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(5)\nwhere non-negative constants \u03b1iw and \u03b1in denote the \ufb02ocking exciting terms.\nIn part (i), the exponential decaying setup in the bivariate Hawkes process with symmetric\n\u03b1\u2019s shows a prototypical model for high-frequency \ufb01nance.\nIn part (ii), k deals with the\n\ufb02ocking phenomenon while considering an additional term under which intensity \u03bbu\n1 increases\nonly when C1 is less than C2 and \u03bbd\n1 increases only when C1 is larger than C2. \u03a8 captures\nthe degree of a \ufb02ocking phenomenon. The parameter \u03b1iw or \u03b1in is triggered depending on\nwhether the di\ufb00erence between the two prices is narrowed (n) or widened (w).\n6\n\n\u03b12w\n\u03b11s\n\u03b11c\n\u03b12s\n\u03b12c\n\u03b11n\nwidening up move by C1\nnarrowing up move by C2\n\u03bb1\nu\n\u03bb1\nd\n\u03bb2\nu\n\u03bb2\nd\ntime\nC1\nC2\nFigure 1: Illustration of the idea on the Hawkes \ufb02ocking model\nFigure 1 shows a descriptive idea on the intensity movements in a Hawkes \ufb02ocking model.\nIn this \ufb01gure, the paths represent the dynamics of two prices and the associated four inten-\nsities6. The black straight line is for price movement C1. The black curved line and curved\ndashed line represent the intensities for upward and downward movement, respectively. Ac-\ncordingly, the red lines are for price C2 and its intensities.\nAssume C1 > C2. Suppose that a widening upward jump of C1 occurs. Then, three\nsimultaneous jumps in intensities are instantly activated, \u03bbu\n1, \u03bbd\n1 and \u03bbu\n2. The jumps in \u03bbu\n1\nand \u03bbd\n1 are due to self/mutually-exciting tendency in the individual Hawkes model and jump\nsizes are given by \u03b11s and \u03b11c, respectively. The jump in \u03bbu\n2 is due to the \ufb02ocking feature to\naccelerate a upward movement of C2 resulting from the jump in C1 and a jump size is given\nas \u03b12w.\nLater on, a narrowing upward movement of C2 occurs. In a similar way, simultaneous\njumps arrive in intensities, \u03bbu\n2, \u03bbu\n2 and \u03bbd\n1. The jumps in \u03bbu\n2 and \u03bbd\n2 are due to self/mutually-\nexciting propensity, and the jump in \u03bbd\n1 is due to the \ufb02ocking mechanism attributed to a\nnarrowing event. Note that jump sizes of \u03bbd\n1 and \u03b11n, are intentionally expressed as quite\nsmall in the \ufb01gure, since the jumps in intensities due to the narrowing event are close to zero\nin the empirical analysis.\nRemark 1 (The role of k). We simply put \u03a6ij = 0 and \u03a8ij = 1 for all i, j. Then\n\u03bbu\n1(t) = \u00b51 +\n\u02c6 t\n\u2212\u221e\n1{C1(u)<C2(u)}d\n\u0010\nNu\n1 + Nd\n1 + Nu\n2 + Nd\n2\n\u0011\n(u)\n6This is for an illustrative purpose and the actual values of prices and intensities may be di\ufb00erent from the\n\ufb01gure.\n7",
    "chunk_index": 6,
    "start_char": 15927,
    "end_char": 18670,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "in intensities due to the narrowing event are close to zero\nin the empirical analysis.\nRemark 1 (The role of k). We simply put \u03a6ij = 0 and \u03a8ij = 1 for all i, j. Then\n\u03bbu\n1(t) = \u00b51 +\n\u02c6 t\n\u2212\u221e\n1{C1(u)<C2(u)}d\n\u0010\nNu\n1 + Nd\n1 + Nu\n2 + Nd\n2\n\u0011\n(u)\n6This is for an illustrative purpose and the actual values of prices and intensities may be di\ufb00erent from the\n\ufb01gure.\n7\n\nand this implies that the intensity of the up movement of C1 changes (typically increases)\nwhen C1(t) is less than C2(t).\nWhen C1(t) < C2(t), the intensity of the up movement of C1 increases, and C1 and C2\ntend to be close to each other, indicating a \ufb02ocking phenomenon. With similar arguments,\nwhen C1(t) < C2(t), the up movement of C1 and down movement of C2 tend to increase and\nwhen C1(t) > C2(t), the down movement of C1 and up movement of C2 tend to increase.\nRemark 2 (Comparison between \u03a6 and \u03a8). The matrices \u03a6 and \u03a8 have zero components in\npositions that do not overlap each other. This ensures each role of the matrix is separated:\n\u03a6 only a\ufb00ects the movements in the individual price process, and \u03a8 only controls the e\ufb00ect\nof the new information from another prices process. Therefore, the test of the signi\ufb01cance of\n\u03a8 veri\ufb01es the existence of interactions between the two prices, especially through the sign of\nC1 \u2212C2.\nThrough combination with k, \u03b1in, and \u03b1iw respectively represent the e\ufb00ects of the narrow-\ning and widening events of price di\ufb00erence on intensities. To see this, we simply put \u03a6ij = 0,\nand then\n\u03bbu\n1(t) = \u00b51 +\n\u02c6 t\n\u2212\u221e\n1{C1(u)<C2(u)}e\u2212\u03b21(t\u2212u) \u0010\n\u03b11wdNu\n2 (u) + \u03b11ndNd\n2 (u)\n\u0011\n.\nWhen C1(u) < C2(u), the increase of Nu\n2 (u) is the price di\ufb00erence widening event, and the\noccurrence of Nd\n2 is the price di\ufb00erence narrowing event.\nFrom the setup for the components in the kernel matrix h, the proposed model can be\nexpressed with a di\ufb00erential form using a Markov property. Let the decay parameter for C1\nand C2 be \ufb01xed as \u03b21 and \u03b22, respectively. Then, the intensity process satis\ufb01es the system of\nstochastic di\ufb00erential equations such as\nd\u03bbt = \u03b2 \u25e6(\u00b5 \u2212\u03bbt)dt + \u03b1dNt\nwhere \u03b1 and \u03b2 are given as\n\u03b1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b11s\n\u03b11c\n1{C1(t)<C2(t)}\u03b11w\n1{C1(t)<C2(t)}\u03b11n\n\u03b11c\n\u03b11s\n1{C1(t)>C2(t)}\u03b11n\n1{C1(t)>C2(t)}\u03b11w\n1{C2(t)<C1(t)}\u03b12w\n1{C2(t)<C1(t)}\u03b12n\n\u03b12s\n\u03b12c\n1{C2(t)>C1(t)}\u03b12n\n1{C2(t)>C1(t)}\u03b12w\n\u03b12c\n\u03b12s\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n\u03b2 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b21\n\u03b21\n\u03b22\n\u03b22\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nThis model can be understood as follows. Market orders (buy or sell) arrive with intensity \u00b5,\nand the arrival intensity jumps by the amount of \u03b1 instantly when an arrival event occurs,\nand then it decays to the base intensity level \u00b5 with the speed of \u03b2.\nThis model can be extended to the multi-dimensional case as the Remark 3 shows.\nRemark 3. (The multi-dimensional Hawkes \ufb02ocking model) Consider m-dimensional price\nprocesses such that\nC1(t) = Nu\n1 (t) \u2212Nd\n1 (t), \u00b7 \u00b7 \u00b7 , Cm(t) = Nu\nm(t) \u2212Nd\nm(t),\n(6)\n8",
    "chunk_index": 7,
    "start_char": 18314,
    "end_char": 21122,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b21\n\u03b21\n\u03b22\n\u03b22\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nThis model can be understood as follows. Market orders (buy or sell) arrive with intensity \u00b5,\nand the arrival intensity jumps by the amount of \u03b1 instantly when an arrival event occurs,\nand then it decays to the base intensity level \u00b5 with the speed of \u03b2.\nThis model can be extended to the multi-dimensional case as the Remark 3 shows.\nRemark 3. (The multi-dimensional Hawkes \ufb02ocking model) Consider m-dimensional price\nprocesses such that\nC1(t) = Nu\n1 (t) \u2212Nd\n1 (t), \u00b7 \u00b7 \u00b7 , Cm(t) = Nu\nm(t) \u2212Nd\nm(t),\n(6)\n8\n\nwhere processes Nu\ni (t) and Nd\ni (t) have intensity processes \u03bbu\ni (t) and \u03bbd\ni (t), respectively, for\neach i = 1, \u00b7 \u00b7 \u00b7 , m. For a system of 2m-dimensional counting process N and its intensity\nprocess \u03bb\nNt =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nNu\n1 (t)\nNd\n1 (t)\n...\nNu\nm(t)\nNd\nm(t)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\u03bbt =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03bbu\n1(t)\n\u03bbd\n1(t)\n...\n\u03bbu\nm(t)\n\u03bbd\nm(t)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwe can extend the intensity process in (1) with the feedback kernel in (2) to the multi-\ndimensional version as follows.\n(i) The matrix \u03a6 in (3) is given as the 2m-by-2m sized form,\n\u03a6(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b11se\u2212\u03b21t\n\u03b11ce\u2212\u03b21t\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03b11ce\u2212\u03b21t\n\u03b11se\u2212\u03b21t\n\u00b7 \u00b7 \u00b7\n0\n0\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b1mse\u2212\u03b2mt\n\u03b1mce\u2212\u03b2mt\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b1mce\u2212\u03b2mt\n\u03b1mse\u2212\u03b2mt\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(ii) The matrix k \u25e6\u03a8 in (4) and (5) is also given as the 2m-by-2m sized form such that\nk(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1{C1(t)< \u00afC(t)}\n1{C1(t)< \u00afC(t)}\n\u00b7 \u00b7 \u00b7\n1{C1(t)< \u00afC(t)}\n1{C1(t)< \u00afC(t)}\n1{C1(t)> \u00afC(t)}\n1{C1(t)> \u00afC(t)}\n\u00b7 \u00b7 \u00b7\n1{C1(t)> \u00afC(t)}\n1{C1(t)> \u00afC(t)}\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n1{Cm(t)< \u00afC(t)}\n1{Cm(t)< \u00afC(t)}\n\u00b7 \u00b7 \u00b7\n1{Cm(t)< \u00afC(t)}\n1{Cm(t)< \u00afC(t)}\n1{Cm(t)> \u00afC(t)}\n1{Cm(t)> \u00afC(t)}\n\u00b7 \u00b7 \u00b7\n1{Cm(t)> \u00afC(t)}\n1{Cm(t)> \u00afC(t)}\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\nwhere \u00afC(t) is given as the average of Ci(t)\u2019s such as\n\u00afC(t) = C1(t) + \u00b7 \u00b7 \u00b7 + Cm(t)\nm\n,\nand\n\u03a8(t) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b11we\u2212\u03b21t\n\u03b11ne\u2212\u03b21t\n0\n0\n\u00b7 \u00b7 \u00b7\n\u03b11ne\u2212\u03b21t\n\u03b11we\u2212\u03b21t\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u03b1mwe\u2212\u03b2mt\n\u03b1mne\u2212\u03b2mt\n\u00b7 \u00b7 \u00b7\n0\n0\n\u03b1mne\u2212\u03b2mt\n\u03b1mwe\u2212\u03b2mt\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n2.2\nStability condition\nThe proposed process can be shown to be well de\ufb01ned and to admit a version with stationary\nincrements under the condition holds of which all eigenvalues of the matrix\n\u02c6 \u221e\n0\nh(\u03c4)d\u03c4\n9\n\nare less than one (Hawkes, 1971b; Bacry et al., 2013).\nThe matrix is called a branching\nmatrix, which is used by, e.g. Filimonov and Sornette (2012), Hardiman et al. (2013). Such\nterminology is related to the ancestor-o\ufb00spring argument.",
    "chunk_index": 8,
    "start_char": 20591,
    "end_char": 22937,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "The proposed process can be shown to be well de\ufb01ned and to admit a version with stationary\nincrements under the condition holds of which all eigenvalues of the matrix\n\u02c6 \u221e\n0\nh(\u03c4)d\u03c4\n9\n\nare less than one (Hawkes, 1971b; Bacry et al., 2013).\nThe matrix is called a branching\nmatrix, which is used by, e.g. Filimonov and Sornette (2012), Hardiman et al. (2013). Such\nterminology is related to the ancestor-o\ufb00spring argument. The immigrant ancestor arrives at\nthe system exogenously at a basic intensity rate \u00b5, and this ancestor event produces internally\no\ufb00spring arrivals in the system with the intensity h that relies on the ancestor\u2019s arrivals. Both\nancestor and o\ufb00spring arrivals can increase likelihood of occurrence for additional events in\nthe system. Satisfying the stability condition indicates that each ancestor arrival generates\n\u201cless than one o\ufb00spring event\u201d on average, and hence the process can be stable. Otherwise,\nthe process can diverge to an in\ufb01nite value within a \ufb01nite time.\nFrom this argument, we investigate stability of the Hawkes \ufb02ocking process by computing\nspectral radius, which is de\ufb01ned by the largest absolute value among the eigenvalues of the\nbranching matrix, and checking out that the spectral radius is less than one.\nSince the\nbranching matrix still depends on the original process Nt, unlike the pure Hawkes model7, the\nstability condition gets to have a stochastic form. To make the stability condition be feasible\nto implement in computation, we take account of approximation of the branching matrix by\ntaking unconditional expectation on it. For the proposed model, we set a branching matrix\nM with a component\nMi,j =\n\u02c6 \u221e\n0\n|\u03a6ij(t) + E[kij(t)]\u03a8ij(t)|dt, for 1 \u2264i, j \u22644.\nIt implies the expected level of the number of o\ufb00spring events caused by one ancestor event\narriving at the rate \u00b5.\nFor the expectation E[kij(t)], we set the probabilities of {C1(t) < C2(t)} and {C1(t) >\nC2(t)} by p \u22640.5 and 1 \u2212p, respectively8. Thus, we have\nM =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b11s\n\u03b21\n\u03b11c\n\u03b21\np \u03b11w\n\u03b21\np \u03b11n\n\u03b21\n\u03b11c\n\u03b21\n\u03b11s\n\u03b21\n(1 \u2212p)\u03b11n\n\u03b21\n(1 \u2212p)\u03b11w\n\u03b21\n(1 \u2212p)\u03b12w\n\u03b22\n(1 \u2212p)\u03b12n\n\u03b22\n\u03b12s\n\u03b22\n\u03b12c\n\u03b22\np \u03b12n\n\u03b22\np \u03b12w\n\u03b22\n\u03b12c\n\u03b22\n\u03b12s\n\u03b22\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(7)\nSince it is empirically shown that p is quite close to 0.5, we may assume that both probabilities\nare identical. Under the setup, we obtain the spectral radius of the branching matrix M such\nthat\n\u03c1M = 1\n2\n\u0010\na +\np\na2 + 4(b \u2212c)\n\u0011\n,\n(8)\n7In the Hawkes process, the feedback kernel is usually given as a deterministic function.\n8This model does not consider occurrence for the case {C1(t) = C2(t)}.\n10\n\nwhere a, b, and c are given by\na = \u03b11s + \u03b11c\n\u03b21\n+ \u03b12s + \u03b12c\n\u03b22\n,\nb = \u03b11n\u03b12n + \u03b11n\u03b12w + \u03b11w\u03b12n + \u03b11w\u03b12w\n4\u03b21\u03b22\n,\nc = \u03b11s\u03b12s + \u03b11s\u03b12c + \u03b11c\u03b12s + \u03b11c\u03b12c\n\u03b21\u03b22\n,\nand hereafter we call \u03c1M by a branching ratio.",
    "chunk_index": 9,
    "start_char": 22518,
    "end_char": 25260,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\u0011\n,\n(8)\n7In the Hawkes process, the feedback kernel is usually given as a deterministic function.\n8This model does not consider occurrence for the case {C1(t) = C2(t)}.\n10\n\nwhere a, b, and c are given by\na = \u03b11s + \u03b11c\n\u03b21\n+ \u03b12s + \u03b12c\n\u03b22\n,\nb = \u03b11n\u03b12n + \u03b11n\u03b12w + \u03b11w\u03b12n + \u03b11w\u03b12w\n4\u03b21\u03b22\n,\nc = \u03b11s\u03b12s + \u03b11s\u03b12c + \u03b11c\u03b12s + \u03b11c\u03b12c\n\u03b21\u03b22\n,\nand hereafter we call \u03c1M by a branching ratio.\nThe branching ratio \u03c1M measures how fast the increased feedback kernel due to arrivals\ngenerated from internal and external sources is shrinking as time goes. If it shrinks quickly,\nit infers that instability in microscopic price dynamics stays low. Oppositely, if it does shrink\nreluctantly, it may infer that instability stays high. In this context, we may \ufb01nd a relevance\nof the branching ratio with a systemic risk level in price dynamics. More discussion on the\nbranching ratio with a systemic risk indicator will be in Section 4.1.\nTo summarize, the Hawkes \ufb02ocking model has the following characteristics. Each price\nprocess has a self/mutually-exciting terms that are a\ufb00ected by its original changes in tick price\ndynamics. In addition, both prices incorporate a \ufb02ocking feature describing as propensity of\ntwo dynamics moving together. Such phenomenon emerges when narrowing and widening\nevents between two prices occur. We obtain a direct mapping between an original process Nt\nand a feedback kernel with a branching matrix M, in which a main event occurs exogenously\nwith basic intensity \u00b5 and may give rise to additional following events endogenously with\nintensity M on average.\n3\nApplication to Empirical Data\nIn this section we examine the stylized facts of two major oil-related energy prices in the US.\nFigure 2 indicates the time series of daily closing prices for WTI crude oil futures and RBOB9\ngasoline futures from 2007 to 2016. In this section, we use the gasoline price multiplied by\na factor 42 because crude oil price is quoted per barrel, whereas gasoline price is quoted per\ngallon10. Each series is created by connecting the prices of the nearest maturity contracts.\nAs shown in Figure 2, there were a three times of signi\ufb01cant drops in both series \u2013 from\nSeptember 2008 to December 2008; from July 2014 to January 2015; from July 2015 to\nFebruary 2016 \u2013 representing a cyan-shaded area in order.\nWe investigate the change in\nparameters of the Hawkes \ufb02ocking model, especially focusing on the three speci\ufb01ed periods\nwhen the two prices plunged, and we compare them with the CoVaR values in the following\nsection. The data considered in this test are tick size change data in milliseconds for the WTI\ncrude oil and gasoline futures listed in NYMEX from 2007 to 2016.\n9RBOB stands for reformulated gasoline blendstock for oxygen blending.\n101 barrel = 42 gallons\n11\n\nFigure 2: The history of daily closing prices for WTI crude oil futures (black) and RBOB\ngasoline futures (orange) from January 1, 2007 to December 30, 2016.",
    "chunk_index": 10,
    "start_char": 24885,
    "end_char": 27811,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "and we compare them with the CoVaR values in the following\nsection. The data considered in this test are tick size change data in milliseconds for the WTI\ncrude oil and gasoline futures listed in NYMEX from 2007 to 2016.\n9RBOB stands for reformulated gasoline blendstock for oxygen blending.\n101 barrel = 42 gallons\n11\n\nFigure 2: The history of daily closing prices for WTI crude oil futures (black) and RBOB\ngasoline futures (orange) from January 1, 2007 to December 30, 2016. The cyan-shaded areas\nrepresent (i) September 2008 \u2013 December 2008 (ii) July 2014 \u2013 January 2015 (iii) July 2015\n\u2013 February 2016, in order.\n3.1\nEstimation method\nThis section explains the ML estimation method used for the empirical study and demon-\nstrates simulation to show the ML estimator\u2019s goodness-of-\ufb01t to the Hawkes \ufb02ocking model.\nBy following the idea in the algorithm by Ogata (1978, 1981), we perform the ML method\nusing the log-likelihood function represented by conditional intensities as follows.\nL(\u03b8) =\nNu\n1 (T)\nX\nj=1\nlog \u03bbu\n1(tu\n1,j) +\nNd\n1 (T)\nX\nj=1\nlog \u03bbd\n1(td\n1,j) +\nNu\n2 (T)\nX\nj=1\nlog \u03bbu\n2(tu\n2,j) +\nNd\n2 (T)\nX\nj=1\nlog \u03bbd\n2(td\n2,j)\n\u2212\n\u02c6 T\n0\n\u0010\n\u03bbu\n1(u) + \u03bbd\n1(u) + \u03bbu\n2(u) + \u03bbd\n2(u)\n\u0011\ndu\n(9)\nwhere \u03bb\u00b7\ni indicates the left-continuous versions of the conditional intensity processes, and t\u00b7\ni,j\nindicates the associated event times. The parameter set \u03b8 = (\u00b5i, \u03b2i, \u03b1is, \u03b1ic, \u03b1in, \u03b1iw) for all\ni = 1, 2 is estimated by maximizing the log-likelihood function numerically.\nSince the estimation proceeds numerically with twelve parameters without the assurance\nof the convexity of the log-likelihood function, it is not mathematically guaranteed to \ufb01nd\nthe global maximum. It is therefore worthwhile to check under various situations whether\nthe numerical optimizer can \ufb01nd the correct estimates. Using the present parameters, 500\nsample paths are generated for two price processes under the Hawkes \ufb02ocking model. The\n12\n\npresumed values are presented in the column titled \u201cTrue\u201d in Table 1. The column \u201cMean\u201d\npresents the means of the estimates of 500 estimation procedures, which are quite close to\nthe true values. Since the above results show that the numerical optimizer that Henningsen\nand Toomet (2011) used works well, we apply this procedure in empirical studies.\nTable 1: Simulation using ML estimation for a Hawkes \ufb02ocking model with 500 sample paths\nTrue\nMean\nStd.\nTrue\nMean\nStd.\nTrue\nMean\nStd.\n\u00b51\n0.0800\n0.0803\n0.0039\n0.0500\n0.0503\n0.0025\n0.1000\n0.1000\n0.0074\n\u03b11n\n0.0000\n-0.0009\n0.0124\n0.2000\n0.2001\n0.0206\n0.3000\n0.3015\n0.0324\n\u03b11w\n0.2000\n0.2007\n0.0142\n0.3500\n0.3509\n0.0200\n0.3500\n0.3501\n0.0247\n\u03b11s\n0.4000\n0.4013\n0.0083\n0.1500\n0.1505\n0.0140\n0.2000\n0.1988\n0.0176\n\u03b11c\n0.0000\n-0.0004\n0.0141\n0.4000\n0.4005\n0.0168\n0.2000\n0.2005\n0.0157\n\u03b21\n0.6000\n0.6008\n0.0212\n1.0500\n1.0513\n0.0368\n0.9000\n0.9011\n0.0402\n\u00b52\n0.0500\n0.0500\n0.0023\n0.0700\n0.0702\n0.0027\n0.1200\n0.1207\n0.0072\n\u03b12n\n0.0000\n-0.0001\n0.0105\n0.3500\n0.3495\n0.0269\n0.0000\n0.0010\n0.0215\n\u03b12w\n0.1000\n0.1009\n0.0115\n0.1000\n0.1001",
    "chunk_index": 11,
    "start_char": 27334,
    "end_char": 30303,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "0.0025\n0.1000\n0.1000\n0.0074\n\u03b11n\n0.0000\n-0.0009\n0.0124\n0.2000\n0.2001\n0.0206\n0.3000\n0.3015\n0.0324\n\u03b11w\n0.2000\n0.2007\n0.0142\n0.3500\n0.3509\n0.0200\n0.3500\n0.3501\n0.0247\n\u03b11s\n0.4000\n0.4013\n0.0083\n0.1500\n0.1505\n0.0140\n0.2000\n0.1988\n0.0176\n\u03b11c\n0.0000\n-0.0004\n0.0141\n0.4000\n0.4005\n0.0168\n0.2000\n0.2005\n0.0157\n\u03b21\n0.6000\n0.6008\n0.0212\n1.0500\n1.0513\n0.0368\n0.9000\n0.9011\n0.0402\n\u00b52\n0.0500\n0.0500\n0.0023\n0.0700\n0.0702\n0.0027\n0.1200\n0.1207\n0.0072\n\u03b12n\n0.0000\n-0.0001\n0.0105\n0.3500\n0.3495\n0.0269\n0.0000\n0.0010\n0.0215\n\u03b12w\n0.1000\n0.1009\n0.0115\n0.1000\n0.1001\n0.0162\n0.1000\n0.1003\n0.0216\n\u03b12s\n0.5000\n0.5014\n0.0242\n0.4500\n0.4505\n0.0213\n0.3000\n0.2995\n0.0224\n\u03b12c\n0.3000\n0.3005\n0.0178\n0.2500\n0.2497\n0.0142\n0.6000\n0.6006\n0.0282\n\u03b22\n1.2000\n1.2028\n0.0446\n1.3000\n1.2999\n0.0465\n1.1500\n1.1519\n0.0467\n3.2\nData\nData on WTI crude oil and gasoline futures\u2019 trade prices are obtained from the database of\nTickdatamarket (www.tickdatamarket.com). We choose the futures data on \u201cLight Sweet\nCrude Oil\u201d for WTI crude oil (referred to as \u201cCL\u201d henceforth) and \u201cRBOB Gasoline\u201d for\ngasoline (referred to as \u201cRB\u201d henceforth) in the energy sector of North America. We consider\nthe data for 10 years from January 1, 2007 to December 30, 2016, and each year has twelve\ndelivery months. To construct a single time series over that period for each futures contract,\nwe select data with the nearest delivery month from the observation date, which is usually\nthe most liquid among contracts with maturity longer than one month.\nTo ensure that raw data can be more feasibly applied to the proposed model, a data\nwrangling procedure is needed. Without loss of generality, we transform the raw data into a\nmore appropriate format in terms of the following aspects.\n(i) Di\ufb00erent price level: The two futures prices of CL and RB have di\ufb00erent price levels. For\ninstance, the CL price at January 13, 2016 with maturity in January 2016 is around $31, but\nthe RB price is around $1.07 per gallon, which is $44.94 per barrel. Minimum tick sizes are\nalso di\ufb00erent. The tick size is 0.01 for CL and 0.0001 for RB. Therefore, we need to adjust\nprices to similar levels. For adjustment, we consider the sample mean ratio, \u00afX/ \u00afY where \u00afX\nimplies the sample mean of prices, for example, of the day.\n13\n\n36.0\n36.2\n36.4\n36.6\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\nTime\nprice\nCL\nRB\n36.0\n36.2\n36.4\n36.6\n10:00\n11:00\n12:00\n13:00\n14:00\n15:00\nTime\nprice\nCL\nRB\nFigure 3: Before (left) and after adjustment (right) for CL and RB prices traded at March\n15, 2016\nLet X and Y be the original price processes. De\ufb01ne C1 = X, C2 = ( \u00afX/ \u00afY )Y . However, the\nadjusted prices on a daily basis are not applicable for the Hawkes \ufb02ocking model as presented\nin the left of Figure 3, where C1 denotes CL and C2 denotes RB. Data for March 15, 2016 with\nmaturity March 2016 were used. In the early part of the day, it is almost always C1 < C2, but\nin the later part, it is almost always C2 > C1.",
    "chunk_index": 12,
    "start_char": 29767,
    "end_char": 32665,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "original price processes. De\ufb01ne C1 = X, C2 = ( \u00afX/ \u00afY )Y . However, the\nadjusted prices on a daily basis are not applicable for the Hawkes \ufb02ocking model as presented\nin the left of Figure 3, where C1 denotes CL and C2 denotes RB. Data for March 15, 2016 with\nmaturity March 2016 were used. In the early part of the day, it is almost always C1 < C2, but\nin the later part, it is almost always C2 > C1. Thus, the sample mean should be computed\nunder a shorter time interval. For example, we calculate the sample mean every 10 minutes,\nand the prices are adjusted during each 10-minute interval. Then the price processes are more\napplicable, as in the right of Figure 3.\n(ii) Multiple price changes in unit time: The minimum resolution time of the data is one\nsecond, and multiple price changes can be observed in one second. In this case, we assume\nthat each change occurs at the equi-distant time interval that divides one second with the\nsame number of observations.\n(iii) Simultaneous changes in the two prices: The probability that the two prices change at\nthe same time is almost zero in our model, but simultaneous changes in both prices can be\nrecorded in practice. In this case, one price change was assumed to have occurred slightly\nearlier than the other change. Since this simultaneous jump can be observed several times a\nday, we consider that one jump to have occurred before the other.\n3.3\nRobustness test for calibration\nFrom the time series of the CL and RB futures prices after data pre-processing, we estimate\nthe model parameters using the ML estimator presented in Section 3.1. The estimation is\nperformed for up to 10 years. We investigate the dynamics of all parameters for the proposed\nmodel over time, especially focusing on the periods of plunges in the CL and RB prices.\nBefore implementing model calibration, we conduct a test for the relation between the\ntwo parts in the intensity kernel h: a self/mutually-exciting factor and a \ufb02ocking factor. We\nconsider two models with di\ufb00erent kernels: (i) a symmetric Hawkes model without a \ufb02ocking\nterm versus (ii) a Hawkes \ufb02ocking model. Then, we compare the parameter estimation results\nderived from each model.\n14\n\nFirst, we assume that CL and RB futures prices follow a symmetric Hawkes process that\nhas an exponential decaying kernel without a \ufb02ocking term, that is\n\u03bbt = \u00b5 +\n\u02c6 t\n\u2212\u221e\n\u03a6(t \u2212u)dNu\n(10)\nwhere the matrix \u03a6 is symmetric with parameters \u03b1is, \u03b1ic, and \u03b2i for i = 1, 2, as de\ufb01ned in\n(3). In this setup, since the \ufb02ocking phenomenon between the CL and RB futures prices is\nnot implicated, the two processes are independent.\nSecond, we assume that CL and RB futures prices follow the Hawkes \ufb02ocking process,\nthat is,\n\u03bbt = \u00b5 +\n\u02c6 t\n\u2212\u221e\n\u03a6(t \u2212u) + k(u) \u25e6\u03a8(t \u2212u)dNu\n(11)\nwhere the matrix k and \u03a8 are de\ufb01ned in (4) and (5), respectively, with additional parameters\n\u03b1in, \u03b1iw for i = 1, 2.",
    "chunk_index": 13,
    "start_char": 32265,
    "end_char": 35122,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\u03a6 is symmetric with parameters \u03b1is, \u03b1ic, and \u03b2i for i = 1, 2, as de\ufb01ned in\n(3). In this setup, since the \ufb02ocking phenomenon between the CL and RB futures prices is\nnot implicated, the two processes are independent.\nSecond, we assume that CL and RB futures prices follow the Hawkes \ufb02ocking process,\nthat is,\n\u03bbt = \u00b5 +\n\u02c6 t\n\u2212\u221e\n\u03a6(t \u2212u) + k(u) \u25e6\u03a8(t \u2212u)dNu\n(11)\nwhere the matrix k and \u03a8 are de\ufb01ned in (4) and (5), respectively, with additional parameters\n\u03b1in, \u03b1iw for i = 1, 2.\nExample 1. [Multicollinearity for \u03b1s and \u03b1c]. We estimate the parameters \u03b1s and \u03b1c from\nCL and RB futures prices under each model assumption. The test is performed on a daily\nbasis on the futures prices with maturity in February 2016, and the observation period is\nfrom January 4 to February 22, 2016. Figure 4 illustrates the results of \u03b1s and \u03b1c for CL\nand RB under the symmetric Hawkes model (red solid line) and the Hawkes \ufb02ocking model\n(black dashed line).\nWe \ufb01nd that the estimates under di\ufb00erent model assumptions are almost identical over\nthe observed period. This implies that k(u) \u25e6\u03a8(t \u2212u) in the Hawkes \ufb02ocking model does not\na\ufb00ect the existing self/mutually-exciting parts \u03a6(t \u2212u). This concept can be considered as\nbeing similar to the non-existence of multicollinearity in linear regression.\nExample 2. [Multicollinearity for \u00b5 and \u03b2].\nWe conduct the same test as conducted in\nExample 1 on the base intensity parameter \u00b5i and the resilience speed to the base intensity\n\u03b2i over the same sample period. Figures 5 and 6 show the results of parameters \u00b5i and \u03b2i\nbased on the two models, respectively.\nFor \u00b5, we \ufb01nd that both CL and RB futures prices of the symmetric Hawkes model have\nlarger \u00b5 than those of the Hawkes \ufb02ocking model. The reason is that additional \ufb02uctuations\nin price processes due to the \ufb02ocking phenomenon are considered as a part of exogenous\ndynamics, and hence they are inherent to \u00b5 under the symmetric Hawkes models where\n\ufb02ocking terms, \u03b1n and \u03b1w, do not exist.\nFor \u03b2, we see that results from the symmetric Hawkes and Hawkes \ufb02ocking models are\nvery similar for the CL futures price. Meanwhile, the \u03b2 in the Hawkes \ufb02ocking model is\nslightly larger than \u03b2 in the symmetric Hawkes model for the RB futures price. This implies\nthat \u201c\u03b2 due to \u03b1n and \u03b1w\u201d is larger than \u201c\u03b2 due to \u03b1s and \u03b1c\u201d in RB. Note that a large \u03b2\nimplies weak persistence. We do not rule the possibility out that \u03b2 depends on \u03b1\u2019s but merely\nconsider uni\ufb01ed \u03b2 for model parsimony.\nMore relevant examples for other sample periods appear in Appendix D.\n15\n\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n(a) \u03b1s in CL\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n(b) \u03b1c in CL\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\n(c) \u03b1s in",
    "chunk_index": 14,
    "start_char": 34652,
    "end_char": 37392,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\u03b2\nimplies weak persistence. We do not rule the possibility out that \u03b2 depends on \u03b1\u2019s but merely\nconsider uni\ufb01ed \u03b2 for model parsimony.\nMore relevant examples for other sample periods appear in Appendix D.\n15\n\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n(a) \u03b1s in CL\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n(b) \u03b1c in CL\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\n(c) \u03b1s in RB\n0.0\n0.2\n0.4\n0.6\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\n(d) \u03b1c in RB\nFigure 4: Comparison of estimates of \u03b1s and \u03b1c under a symmetric Hawkes model (red solid\nline) and the Hawkes \ufb02ocking model (black dotted line) for CL and RB futures prices (with\nmaturity in February 2016) from January 4 to February 22, 2016\n3.4\nCalibration\nBased on the argument of the model\u2019s robustness check, we conduct calibration for all 12\nparameters in the Hawkes \ufb02ocking model on the partial period in 2016, where the results are\npresented in Tables 4 and 5. In particular, we investigate signi\ufb01cance of \ufb02ocking parameters \u03b1n\nand \u03b1w with more attention. Figure 7 compares \u03b1n and \u03b1w under the Hawkes \ufb02ocking model\nover the same sample period. The narrowing events\u2019 parameters \u03b1in are depicted with plus\nminus two times of standard errors (dotted lines) to check the signi\ufb01cance of the estimates.\nThe result shows that \u03b1in are close to zero in the selected time period and this means that\nthe price di\ufb00erence of narrowing events does not substantially a\ufb00ect the intensities.\nOn the other hand, \u03b1iw are signi\ufb01cant. This means that the widening event signi\ufb01cantly\nincreases intensities associated with the \ufb02ocking so that the two price processes tend to\nconverge toward each other after widening events. The standard errors of \u03b1iw are omitted for\nclarity of the graph but the estimates of \u03b1iw are statistically signi\ufb01cant for all time period. For\nthe graph, selected maturity for the futures is in February 2016 and estimates are computed\non a daily basis. In addition, \u03b1w in CL is larger than that in RB.\nFrom now, we calibrate the Hawkes \ufb02ocking model by expanding the test period from a\nsample month (February 2016) to the recent decade from January 2007 to December 2016.\nUsing the transaction data of CL and RB futures prices, the estimates are computed on a\ndaily basis and the daily estimates are averaged over a month for better visualization. The\n16\n\n0.00\n0.05\n0.10\n0.15\n0.20\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n0.00\n0.05\n0.10\n0.15\n0.20\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\nFigure 5: Comparison of estimates of \u00b51 and \u00b52 under the symmetric Hawkes model (red\nline) and the Hawkes \ufb02ocking model (black dotted line) for CL and RB futures prices (with\nmaturity in February 2016) from January 4 to February 22, 2016\n0.0\n0.5\n1.0\n1.5\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n0.0\n0.5\n1.0\n1.5\n01/15\n02/01\n02/15\ndate\nestimate\nRB",
    "chunk_index": 15,
    "start_char": 36968,
    "end_char": 39820,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "02/15\ndate\nestimate\nCL\nflocking\n0.00\n0.05\n0.10\n0.15\n0.20\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\nFigure 5: Comparison of estimates of \u00b51 and \u00b52 under the symmetric Hawkes model (red\nline) and the Hawkes \ufb02ocking model (black dotted line) for CL and RB futures prices (with\nmaturity in February 2016) from January 4 to February 22, 2016\n0.0\n0.5\n1.0\n1.5\n01/15\n02/01\n02/15\ndate\nestimate\nCL\nflocking\n0.0\n0.5\n1.0\n1.5\n01/15\n02/01\n02/15\ndate\nestimate\nRB\nflocking\nFigure 6: Comparison of estimates of \u03b21 and \u03b22 under the symmetric Hawkes model (red\nline) and the Hawkes \ufb02ocking model (black dotted line) for CL and RB futures prices (with\nmaturity in February 2016) from January 4 to February 22, 2016\nassociated results are displayed in Figures 8, 9, and 10.\nFigure 8 exhibits the \ufb02ocking parameters \u03b1n (red solid line) and \u03b1w (black dotted line)\nfor CL (left panel) and RB (right panel). For each futures price, the level of \u03b1n is much\nsmaller than that of \u03b1w and it is close to zero for a long time period. This seems reasonable\nbecause widening events have a strong role causing the \ufb02ocking phenomenon, while narrowing\nevents have no or relatively small e\ufb00ects. In CL, \u03b1w has the maximum value near the fourth\nquarter of 2008, and it gradually decreases and then increases again around 2015. In RB, \u03b1w\ngradually increases.\nFigure 9 shows the behavior of \u03b1s (black dotted line) and \u03b1c (red solid line) for CL (left\npanel) and RB (right panel). The self-exciting parameter \u03b1s in CL gradually decreases over\ntime and is close to zero in 2016. All other parameters are far from zero and do not show any\nparticular trend. In general, \u03b1c is greater than \u03b1s in CL and \u03b1c is less than \u03b1s in RB over\nthe sample period. It is known that the self-exciting pattern is due to order splitting and the\nmutually-exciting pattern is due to microstructure noise.\nFigure 10 plots the behavior of exogenous \ufb02uctuation parameter \u00b5 (left panel) and persis-\ntence parameter \u03b2 (right panel). As mentioned before, the dynamics of \u00b5 seem related to the\n17\n\n0.0\n0.2\n0.4\n01/15\n02/01\n02/15\ndate\nestimate\nn\nw\nCL\n0.0\n0.2\n0.4\n01/15\n02/01\n02/15\ndate\nestimate\nn\nw\nRB\nFigure 7: Comparison of \u03b1n and \u03b1w for CL (left) and RB (right) futures prices (with maturity\nin February 2016) from January 4 to February 22, 2016\ndynamics of \u03b1w. In general, \u00b5 in CL is larger than RB but the gap is closing. Meanwhile, the\npersistence parameter \u03b2 is smaller in CL, and this implies that persistence in CL is stronger\nthan in RB. In addition, there is no particular trend in \u03b2.\n3.5\nInterpretation of the estimation results\nThrough the calibration results discussed in Section 3.4, we \ufb01gure out the stylized behavioral\ncharacteristics exhibited in WTI and gasoline futures prices. First, we observe that the main\nsource for comovement of the two price processes is CL by the fact that \u03b1w in CL is greater\nthan \u03b1w in RB as shown in Figure 8.",
    "chunk_index": 16,
    "start_char": 39371,
    "end_char": 42258,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "RB but the gap is closing. Meanwhile, the\npersistence parameter \u03b2 is smaller in CL, and this implies that persistence in CL is stronger\nthan in RB. In addition, there is no particular trend in \u03b2.\n3.5\nInterpretation of the estimation results\nThrough the calibration results discussed in Section 3.4, we \ufb01gure out the stylized behavioral\ncharacteristics exhibited in WTI and gasoline futures prices. First, we observe that the main\nsource for comovement of the two price processes is CL by the fact that \u03b1w in CL is greater\nthan \u03b1w in RB as shown in Figure 8. Both prices have comovement propensity if when a\nwidening event occurs driven by either of them. However, in terms of the absolute magnitude\nfor power to move, CL is greater than RB.\nNext, in a di\ufb00erent viewpoint, we consider the following situation.\nSuppose that the\nCL price is larger than RB, and a widening event happens by a RB\u2019s down movement, as\nillustrated in the left of Figure 11. In this case, the narrowing tendency can be facilitated by\n(i) increase of the up intensity in RB or (ii) increase of the down intensity in CL. Increase of\nthe up intensity in RB price is measured by \u03b12c, since it is caused by the previous downward\nmovement in RB which is captured by the individual Hawkes price model within the RB price.\nSimilarly, increase of the down intensity in CL is represented by \u03b11w, since this jumping is\ncaused by a widening event a\ufb00ecting the intensity of CL.\nWhen comparing \u03b12c in RB and \u03b11w in CL in Figures 8 and 9, it is observed that generally\n\u03b11w is larger than \u03b12c. Although it does not strictly imply that the two price paths are more\nlikely to be narrowed by CL, we deduce that the in\ufb02uence of CL on narrowing is quite\nsigni\ufb01cant.\nWe also suppose that CL is greater than RB and a widening event is activated by a CL\u2019s\nup movement, as in the right of Figure 11. In the same manner, the narrowing tendency can\nbe facilitated by (i) increase of the up intensity in RB or (ii) increase of the down intensity\nin CL. Increase of the up intensity in RB is captured by \u03b12w, since it is caused by a widening\nevent a\ufb00ecting the intensity of RB. The increase of the down intensity in CL is represented\n18\n\nby \u03b11c, since it is caused by an upward movement of CL.\nComparing \u03b12w in RB and \u03b11c in CL in Figures 8 and 9, it is shown that \u03b11c has a\nmuch larger value than \u03b1w generally. This means that if the two price levels get widened\nthen tendency to converge toward each other increases for both prices, but the magnitude\nof power to move is much more signi\ufb01cant in CL than in RB. We can deduce that, in many\ncases, the \ufb02ocking feature between CL and RB is more likely to be owing to CL.",
    "chunk_index": 17,
    "start_char": 41701,
    "end_char": 44364,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "CL in Figures 8 and 9, it is shown that \u03b11c has a\nmuch larger value than \u03b1w generally. This means that if the two price levels get widened\nthen tendency to converge toward each other increases for both prices, but the magnitude\nof power to move is much more signi\ufb01cant in CL than in RB. We can deduce that, in many\ncases, the \ufb02ocking feature between CL and RB is more likely to be owing to CL. One possible\ninterpretation of this result is that participants in WTI crude oil futures market seem to\nuse information from the gasoline market more actively than do participants in the gasoline\nfutures market from the WTI crude oil futures market.\n4\nSystemic Risk in Market Microstructure\nSo far, we proposed the Hawkes \ufb02ocking model with an approximative stability condition given\nas the branching ratio \u03c1M of the branching matrix M in Section 2, and we then calibrated\nthe proposed model from the high-frequency data for WTI crude oil and gasoline futures in\nSection 3.\nIn this section, we discuss how to quantify systemic risk existing within and between the\ntwo price processes in a microscopic level based on the de\ufb01nition of the branching ratio. We\n\ufb01nally compare the systemic risk levels embedded in the two futures prices by using di\ufb00erent\nmeasurements through (i) a branching ratio analysis in the proposed model and (ii) CoVaR\nwhich is a widely used method for systemic risk. To implement the CoVaR for the empirical\ndata of WTI crude oil and gasoline futures prices, we adopt a CoVaR-copula approach, which\nis developed by Reboredo and Ugolini (2015) and Reboredo (2015).\n4.1\nThe Hawkes \ufb02ocking model and its relevance of systemic risk\nUnderstanding systemic risk is one of major topics in modern \ufb01nancial risk management in\nterms of de\ufb01nition, quanti\ufb01cation, and regulation. A wide range of literature discusses on\nsystemic risk. One strand of the literature relates to a market-based risk measure, which\nare CoVaR proposed by Adrian and Brunnermeier (2016); marginal expected shortfall by\nAcharya et al. (2017); SRISK by Brownlees and Engle (2017); distress insurance measure\nby Huang et al. (2009). Another strand is under a network-based approach developed by\nElliott et al. (2014), Rogers and Veraart (2013), Capponi and Chen (2015) based on the\nidea by Eisenberg and Noe (2001). In addition, ones empirically observe systemic risk in a\ntime-varying perspective through various data set. For example, Lucas et al. (2014), Oh and\nPatton (2017) use CDS spread; Reboredo (2015) takes stock prices; Jammazi et al. (2015) uses\nstock-bond returns; Choro\u00b4s-Tomczyk et al. (2014), Okhrin and Xu (2017) employ portfolio\ncredit derivative prices.\nIn terms of de\ufb01ning systemic risk, this study is inspired by the idea proposed by Danielsson\nand Shin (2003), Danielsson et al. (2012); and it is also related to Filimonov and Sornette\n19\n\n(2012), Hardiman et al. (2013). Danielsson et al. (2012) \ufb01rstly characterize systemic risk by\nintroducing the concept of \u201cendogenous risk\u201d, which is de\ufb01ned as the additional risk that\nthe \ufb01nancial system adds on top of equilibrium risk as commonly understood.",
    "chunk_index": 18,
    "start_char": 43971,
    "end_char": 47068,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "al. (2015) uses\nstock-bond returns; Choro\u00b4s-Tomczyk et al. (2014), Okhrin and Xu (2017) employ portfolio\ncredit derivative prices.\nIn terms of de\ufb01ning systemic risk, this study is inspired by the idea proposed by Danielsson\nand Shin (2003), Danielsson et al. (2012); and it is also related to Filimonov and Sornette\n19\n\n(2012), Hardiman et al. (2013). Danielsson et al. (2012) \ufb01rstly characterize systemic risk by\nintroducing the concept of \u201cendogenous risk\u201d, which is de\ufb01ned as the additional risk that\nthe \ufb01nancial system adds on top of equilibrium risk as commonly understood. In addition,\nprice movements would be consistent with price e\ufb03ciency if they were entirely driven by\npayo\ufb00-relevant fundamental news. However, a large part of volatility/correlation is due to a\nnumber of feedback e\ufb00ects. Although the volatility/correlation stem from exogenous factors,\na large part of eventual realized magnitude is due to the ampli\ufb01cation within the system by\nthe exogenous news.\nTaking this argument into the market microstucture, the Hawkes intensity process in (1)\nmay have the following meanings: the base intensity is related to a portion due to the incor-\nporation of fundamental news; and the feedback kernel has a role of an endogenous feedback\ndue to the trading patterns of market participants over the incorporation of fundamental\nnews. In this context, we employ the stability condition estimated with the branching ratio\nin (8) as an indicator of systemic risk in two price processes. In other words, we may assess\nthat a systemic risk level stays higher as the branching ratio is closer to one and does lower\nas it is closer to zero. Thus, we observe how empirical values of (8) change over time using\nthe estimated parameters in Section 3.\nWith more precise investigation for the proposed feedback kernel, it is composed of two\nparts, the self/mutually-exciting kernel de\ufb01ned in (3) and the \ufb02ocking kernel in (4) and (5).\nWe analyze the systemic risk level by distinguishing it into two factors \u2013 endogeneity within a\nsingle price process and interaction between two price processes. As a systemic risk indicator\nfor microstructure dynamics, we compute a quarter-wise branching ratio from the feedback\nkernel, which is related to each factor of endogeneity and interaction in each WTI and gasoline\nfutures price.\nThe quarter-wise branching ratio is described as follows. The branching matrix M is of\nfour-by-four size that contains 16 components as follows.\nM =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b11s\n\u03b21\n\u03b11c\n\u03b21\n\u03b11w\n2\u03b21\n\u03b11n\n2\u03b21\n\u03b11c\n\u03b21\n\u03b11s\n\u03b21\n\u03b11n\n2\u03b21\n\u03b11w\n2\u03b21\n\u03b12w\n2\u03b22\n\u03b12n\n2\u03b22\n\u03b12s\n\u03b22\n\u03b12c\n\u03b22\n\u03b12n\n2\u03b22\n\u03b12w\n2\u03b22\n\u03b12c\n\u03b22\n\u03b12s\n\u03b22\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(12)\nThe matrix can be divided by a four distinct quadrants (indicated by dash lines) according to\nthe role of parameters, as shown in (12). The components in the \ufb01rst and fourth quadrants\nrepresent the branching ratios that a\ufb00ect the self/mutually-exciting factors in CL and RB\nprice processes, respectively. Similarly, the components in the second and third quadrants\nrepresent the branching ratios that a\ufb00ect the \ufb02ocking behavior (widening and narrowing\nevents) in CL and RB price processes, respectively.",
    "chunk_index": 19,
    "start_char": 46489,
    "end_char": 49608,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\u03b22\n\u03b12n\n2\u03b22\n\u03b12w\n2\u03b22\n\u03b12c\n\u03b22\n\u03b12s\n\u03b22\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(12)\nThe matrix can be divided by a four distinct quadrants (indicated by dash lines) according to\nthe role of parameters, as shown in (12). The components in the \ufb01rst and fourth quadrants\nrepresent the branching ratios that a\ufb00ect the self/mutually-exciting factors in CL and RB\nprice processes, respectively. Similarly, the components in the second and third quadrants\nrepresent the branching ratios that a\ufb00ect the \ufb02ocking behavior (widening and narrowing\nevents) in CL and RB price processes, respectively.\nTo measure the extent of ampli\ufb01cation caused by the self/mutually-exciting factor ex-\nplained by \u03b1is, \u03b1ic and \ufb02ocking factor by \u03b1in, \u03b1iw separately, we examine the branching ratios\n20\n\nby component. By taking average of four components belonging to each quadrant, we obtain\n\u03b11s + \u03b11c\n\u03b21\n,\n\u03b12s + \u03b12c\n\u03b22\n,\n\u03b11n + \u03b11w\n2\u03b21\n,\n\u03b12n + \u03b12w\n2\u03b22\n.\n(13)\nEach value has the following interpretation: First, (\u03b1is + \u03b1ic)/\u03b2i indicates the average fre-\nquency of the occurrence of o\ufb00spring events due to price upward or downward movements\nout of total arrivals for each CL or RB futures price, for i = 1, 2. This can be interpreted as\nthe level of endogeneity that exists in WTI crude oil futures market for i = 1 and gasoline\nfuture market for i = 2. Next, (\u03b1in + \u03b1iw)/(2\u03b2i) corresponds to the average frequency of the\noccurrence of o\ufb00spring events due to widening and narrowing events between the two prices\nout of total arrivals. This can be interpreted as the level of interaction from gasoline to WTI\ncrude oil futures markets for i = 1 and the opposite direction for i = 2.\n4.2\nA CoVaR-copula approach and its implementation\nA systemic risk measure focuses on a tail distribution for potential losses of given portfolios\nin order to investigate a spillover e\ufb00ect from one to another. Among the aforementioned\nsystemic measures in practice, CoVaR is a most widely used systemic risk measure, proposed\nby Adrian and Brunnermeier (2016).\nThe CoVaR is the VaR for the \ufb01nancial system conditional on the fact that an individual\n\ufb01nancial institution is under stress. Let Ri\nt be the return for the \ufb01nancial market as a whole\nat time t and let Rj\nt be the return for market j at time t. The original de\ufb01nition of CoVaR\nis given by\nP\n\u0010\nRi\nt \u2264CoVaRi|j,\u03b1=q\n\u03b2,t\n|Rj\nt = VaRj\nq,t\n\u0011\n= \u03b2.\n(14)\nThis is the VaR when the return of market j stands at the VaR with the q-percent con\ufb01dence\nlevel. After that, by replacing the condition to make it more realistic, the de\ufb01nition was\nextended to the following form.\nP\n\u0010\nRi\nt \u2264CoVaRi|j\n\u03b2,t|Rj\nt \u2264VaRj\n\u03b1,t\n\u0011\n= \u03b2,\n(15)\nwhere VaRj\n\u03b1,t is the VaR for market j, measuring the maximum loss that market j may\nexperience for con\ufb01dence level 1\u2212\u03b1 and a speci\ufb01c time horizon, that is, the \u03b1-quantile of the\nreturn distribution for the market j: P(Rj\nt \u2264VaRj\n\u03b1,t) = \u03b1.",
    "chunk_index": 20,
    "start_char": 49059,
    "end_char": 51880,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "market j stands at the VaR with the q-percent con\ufb01dence\nlevel. After that, by replacing the condition to make it more realistic, the de\ufb01nition was\nextended to the following form.\nP\n\u0010\nRi\nt \u2264CoVaRi|j\n\u03b2,t|Rj\nt \u2264VaRj\n\u03b1,t\n\u0011\n= \u03b2,\n(15)\nwhere VaRj\n\u03b1,t is the VaR for market j, measuring the maximum loss that market j may\nexperience for con\ufb01dence level 1\u2212\u03b1 and a speci\ufb01c time horizon, that is, the \u03b1-quantile of the\nreturn distribution for the market j: P(Rj\nt \u2264VaRj\n\u03b1,t) = \u03b1.\nUsing the CoVaR, the systemic risk can be measured by the delta CoVaR (\u2206CoVaR),\nwhich is the di\ufb00erence between the VaR of whole market conditional on the distressed state\nof market j, that is, Rj\nt \u2264VaRj\n\u03b1,t, and the VaR of the the whole market conditional on the\nnormal state of market j, that is, Rj\nt = VaRj\n\u03b1=50%,t. Note that usually the median quantile\nis considered.\nTo implement the de\ufb01ned \u2206CoVaR as presented, we consider a copula function approach\nto implement \u2206CoVaR. By consolidating the de\ufb01nition of CoVaR proposed by the relevant\nliterature, we present the formula as an analytic form using a copula function. The following\nproposition shows the formula for computing \u2206CoVaR with the relevant proof in Appendix B.\n21\n\nCopula\nDistribution C(u, v; \u00b7)\nRange of \u03b8\n\u03bbL\n\u03bbU\nGenerator \u03c8(\u00b7)\nGuassian\n\u03a6\u03b8(\u03a6\u22121(u), \u03a6\u22121(v))\n(\u22121, 1)\n0\n0\n\u00d7\nStudent t\nT\u03bd,\u03b8(t\u22121\n\u03bd (u), t\u22121\n\u03bd (v))\n(\u22121, 1)\n2t\u03bd+1\n\u0010\n\u2212\n\u221a\u03bd+1\n\u221a\n1\u2212\u03b8\n\u221a\n1+\u03b8\n\u0011\n\u00d7\nGumbel\nexp\n\u0000\u2212[(ln u)\u03b8 + (ln v)\u03b8]1/\u03b8\u0001\n[1, \u221e)\n0\n2 \u221221/\u03b8\n(\u2212ln t)\u03b8\nClayton\n\u0000u\u2212\u03b8 + v\u2212\u03b8 \u22121\n\u0001\u22121\n\u03b8\n(0, \u221e)\n2 \u221221/\u03b8\n0\n(t\u2212\u03b8 \u22121)/\u03b8\nTable 2: Bivariate copula models with correlation parameter \u03b8, upper tail dependence \u03bbL,\nlower tail \u03bbU dependence parameters\nProposition 4. For a uniform vector (U, V ) with a copula function C, let \u03b6v(u) denote the\nconditional distribution by\n\u03b6v(u) = P(U \u2264u|V = v) = \u2202C(u, v)\n\u2202v\n,\n(16)\nand C\u22121\n\u03b1 (\u00b7) denote the inverse of C\u03b1 : x \u2192C(\u00b7, \u03b1). Then, the \u03b2-quantile \u2206CoVaRi|j\nt\nof asset\ni\u2019s return Ri conditional on asset j\u2019s return Rj is given as an analytic form:\n\u2206CoVaRi|j\nt\n= CoVaRi|j\n\u03b2,t \u2212CoVaRi|j,\u03b1=0.5\n\u03b2,t\n.\n(17)\nEach part of (17) is computed by\nCoVaRi|j\n\u03b2,t = F \u22121\nRi\nt\n\u0000C\u22121\n\u03b1 (\u03b1\u03b2)\n\u0001\nand CoVaRi|j,\u03b1=q\n\u03b2,t\n= F \u22121\nRi\nt\n\u0000h\u22121\nq (\u03b2)\n\u0001\n,\n(18)\nwhere FRi\nt is the marginal distribution function of Ri\nt.\nTo apply the notion of \u2206CoVaR to our study, we compute it based on daily returns for\ntwo price dynamics. This measure captures the level of systemic risk in a day. We replicate\nthe computation presented in Proposition 4 to the transaction data over a regular time stamp\nt.\nLet R1\nt and R2\nt be the returns for daily observations of C1(t) and C2(t), respectively, that\nis,\nR1\nt = C1(t + \u2206t) \u2212C1(t)\nC1(t)\n,\nR2\nt = C2(t + \u2206t) \u2212C2(t)\nC2(t)\n,\n(19)\nwhere \u2206t is given by a one-day length.",
    "chunk_index": 21,
    "start_char": 51412,
    "end_char": 54092,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "apply the notion of \u2206CoVaR to our study, we compute it based on daily returns for\ntwo price dynamics. This measure captures the level of systemic risk in a day. We replicate\nthe computation presented in Proposition 4 to the transaction data over a regular time stamp\nt.\nLet R1\nt and R2\nt be the returns for daily observations of C1(t) and C2(t), respectively, that\nis,\nR1\nt = C1(t + \u2206t) \u2212C1(t)\nC1(t)\n,\nR2\nt = C2(t + \u2206t) \u2212C2(t)\nC2(t)\n,\n(19)\nwhere \u2206t is given by a one-day length.\nWe consider the following four kinds of copula with di\ufb00erent tail dependencies and sym-\nmetries: the Gaussian copula with tail independence; Student t copula with symmetric tail\ndependence; Gumbel copula with upper tail dependence and lower tail independence; Clay-\nton copula with upper tail independence and lower tail dependence. The details are speci\ufb01ed\nin Table 2. Appendix A presents how to implement time-varying CoVaR using the copula\nfunctions.\nFor application of the CoVaR\u2019s de\ufb01nition to our data set, we set R1\nt and R2\nt by the daily\nreturns of CL and RB futures prices, respectively, such as de\ufb01ned in (19). We compute one-day\n22\n\n\u2206CoVaR1|2\nt\nand one-day \u2206CoVaR2|1\nt\nbased on R1\nt , R2\nt\n11. The value of one-day \u2206CoVaR1|2\nt\nis interpreted as the extent to which extreme downward changes in gasoline futures price\n(conditioned variable) contribute to the systemic risk in WTI crude oil futures price for a\nday at time t. Conversely, the value of \u2206CoVaR2|1\nt\nindicates the contribution of extreme\ndownward changes in WTI crude oil futures price (conditioned variable) to systemic risk in\ngasoline futures price.\nBy \ufb01nding the best \ufb01tting copula among the aforementioned ones (the detail procedure\nis explained in Appendix C), we compute time-varying CoVaRt using Proposition 4 with an\nanalytic form of the \u03b6\u03b1 function. Since the Student t copula is chosen as having the best \ufb01t\nto our data over the whole test period by the AIC and BIC tests, function (24) is employed\nonly in our analysis. We pick the 95% quantile level for computing the CoVaR and VaR used\nin the conditioned part of CoVaR, that is, \u03b1 = 5% and \u03b2 = 5%.\nFigure 20 (Appendix E) shows the dynamics of one-day 95% CoVaR1|2\nt\nand CoVaR2|1\nt\nwhen the conditional variable is given as the distressed situation and when it is given as the\nnormal situation \u03b1 = 50% as presented in (14) and (15), respectively, from 2007 to 2016.\nThe shaded areas represent the three large drops in the WTI crude oil and gasoline futures\nprices as mentioned in Figure 2. We \ufb01nd that both CoVaR and \u2206CoVaR values signi\ufb01cantly\nincrease in distressed time periods compared with other normal times. Moreover, relative\ncontributions of the WTI crude oil futures to the systemic risk in gasoline futures, and vice\nversa, change almost similarly over time.",
    "chunk_index": 22,
    "start_char": 53614,
    "end_char": 56395,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "2007 to 2016.\nThe shaded areas represent the three large drops in the WTI crude oil and gasoline futures\nprices as mentioned in Figure 2. We \ufb01nd that both CoVaR and \u2206CoVaR values signi\ufb01cantly\nincrease in distressed time periods compared with other normal times. Moreover, relative\ncontributions of the WTI crude oil futures to the systemic risk in gasoline futures, and vice\nversa, change almost similarly over time.\n4.3\nComparison of branching ratios with CoVaRs\nWe simulate the branching ratio \u03c1M in (8) and the quarter-wise branching ratios in (13)\nusing the best \ufb01tting kernel parameters \u03b1is, \u03b1ic, \u03b1in, \u03b1iw, and \u03b2i for i = 1, 2 in the Hawkes\n\ufb02ocking model with p = 1/2, as discussed in Subsection 3.3. Moreover, the branching ratios\nare compared with VaR and CoVaR as a benchmark of the systemic risk. The relevant results\nare displayed in Figures 12, 13, and 14.\nFigure 12 illustrates time-varying \u03c1M. We observe that mid-2008 had the highest level\nof spectral radius at around 85% just before the collapse of Lehman Brothers in September\n2008. With the onset of the global credit crisis, the overall level decreased until the beginning\nof 2011 when it was the lowest at around 63% during the test period up to December 2016.\nFigure 13 presents the evolution of the quarter-wise branching ratios (\u03b11s + \u03b11c)/\u03b21 for\nCL futures, (\u03b12s + \u03b12c)/\u03b22 for RB futures and their one-day 95% VaR values from January\n2007 to December 2016. For the CL futures, it varies between 58% and 82%; however, for\nRB futures, it varies between 32% and 60%. This implies that the level of endogeneity in CL\nfutures market was consistently higher than that in RB futures prices for the past decade. In\nCL futures in mid-2008, the highest level of endogeneity was recorded just before the onset\n11Since pro\ufb01t returns (not loss returns) are used in the computation of CoVaR and VaR, the CoVaR and\nVaR values with the minus sign are considered throughout the test. The minus VaR is usually given as a\npositive value when the quantile level is greater than 50%.\n23\n\nof the global crisis. Meanwhile, there were no signi\ufb01cant changes for this level in the second\nand third plunge periods in the CL and RB markets in 2014 and 2016, respectively. On the\nother hand, for one-day 95% VaR values, precipitous rises occurred on mid-2008, 2014, and\n2016 in both CL and RB markets, and the overall \ufb02ows of VaR in CL and RB markets were\nsimilar.\nFigure 14 presents the evolution of the quarter-wise branching ratios (\u03b11n + \u03b11w)/(2\u03b21)\nfor the CL process a\ufb00ected by the RB price \ufb02uctuation, (\u03b12n +\u03b12w)/(2\u03b22) for the RB process\na\ufb00ected by CL price \ufb02uctuation, and one-day 95% \u2206CoVaR1|2\nt\nand \u2206CoVaR2|1\nt\nfrom January\n2007 to December 2016. At a microscopic level, the degree to which RB a\ufb00ects CL has values\nranging between 10% and 55%; however, the degree to which CL a\ufb00ects RB is between 2%\nand 8%.",
    "chunk_index": 23,
    "start_char": 55979,
    "end_char": 58840,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "Figure 14 presents the evolution of the quarter-wise branching ratios (\u03b11n + \u03b11w)/(2\u03b21)\nfor the CL process a\ufb00ected by the RB price \ufb02uctuation, (\u03b12n +\u03b12w)/(2\u03b22) for the RB process\na\ufb00ected by CL price \ufb02uctuation, and one-day 95% \u2206CoVaR1|2\nt\nand \u2206CoVaR2|1\nt\nfrom January\n2007 to December 2016. At a microscopic level, the degree to which RB a\ufb00ects CL has values\nranging between 10% and 55%; however, the degree to which CL a\ufb00ects RB is between 2%\nand 8%. It infers that the level to which RB price a\ufb00ects CL price has been consistently\nhigher than that in its opposite direction over the past 10 years. Moreover, a reverse pattern\nwas observed between the relative in\ufb02uences on the two high-frequency price processes. For\nthe degree of the impact of the change in RB price on CL price, the highest level was recorded\nin late 2008, it gradually decreased after that but rose slightly during the second and third\nplunge periods. On the other hand, the level to which the CL price a\ufb00ects RB price was the\nlowest in late 2008, but it increased to the highest level during the second and third plunge\nperiods. For delta CoVaR values, the extent of their relative contribution to systemic risk\ndue to each futures market was almost symmetric.\nThroughout the test, we \ufb01nd some remarkable facts about the futures markets of WTI\ncrude oil and gasoline in terms of high-frequency structure. First, the overall systemic risk\nlevel in the two futures markets was the highest before the onset of the global credit crisis,\nand there was no considerable change in overall systemic risk in these markets when the\nprices plunge occurred in 2014 and 2016. Second, when we compare the levels of endogeneity\nembedded in each futures market, the level of the WTI market was a signi\ufb01cantly higher\nthan that in the gasoline market. Moreover, since the WTI crude oil market is more actively\na\ufb00ected by change in the gasoline market, it is more likely to react promptly to a delicate\nchange in the gasoline market than in the opposite case. Last, the levels of the risk interaction\nbetween the two markets, that is, from WTI crude oil to gasoline and vice versa, were very\nasymmetric. However, the degree of the di\ufb00erence has been reducing steadily over the past\ndecade.\n5\nConcluding Remark\nWe propose the Hawkes \ufb02ocking model to quantify systemic risk in high-frequency markets.\nThe model is designed to capture self/mutually-exciting features as well as cross-exciting on\nthe intensity processes depending on the relative position of asset prices as the price di\ufb00erence\nis narrowed or widened. In the empirical study, we observe a micro-level behavior between\nthe two futures markets of WTI crude oil and gasoline. We see that when the di\ufb00erence of the\ntwo prices narrows, no additional \ufb02ocking phenomenon occurs, but, when they get widened, a\n24\n\nstrong \ufb02ocking phenomenon occurred. The Hawkes \ufb02ocking model-based assessment is highly\nsuitable for application of tick-by-tick data, and it is also feasible to capture a delicate change\nin the level of systemic risk that appears in highly correlated data.",
    "chunk_index": 24,
    "start_char": 58389,
    "end_char": 61468,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "on the relative position of asset prices as the price di\ufb00erence\nis narrowed or widened. In the empirical study, we observe a micro-level behavior between\nthe two futures markets of WTI crude oil and gasoline. We see that when the di\ufb00erence of the\ntwo prices narrows, no additional \ufb02ocking phenomenon occurs, but, when they get widened, a\n24\n\nstrong \ufb02ocking phenomenon occurred. The Hawkes \ufb02ocking model-based assessment is highly\nsuitable for application of tick-by-tick data, and it is also feasible to capture a delicate change\nin the level of systemic risk that appears in highly correlated data.\nIn terms of the assessment of systemic risk, we compare the results of the branching\nratios derived from the Hawkes \ufb02ocking model with the delta CoVaR, which is introduced\nas a benchmark for the proposed metric of the systemic risk. Estimating the best \ufb01t kernel\nusing a ML estimator from the given data set, we obtain the following empirical results.\nThe systemic risk level in the WTI crude oil futures price has been consistently higher than\nthat in the gasoline futures price for the test period. Furthermore, the change in gasoline\nfutures price has a signi\ufb01cantly greater impact on WTI crude oil futures price than in the\nopposite case, which implies that the relative contribution of each price is asymmetric at the\nmicroscopic level of price structure.\nData Availability Statement\nThe part of data that support the \ufb01ndings of this study are openly available in \ufb01gshare at\nhttps://doi.org/10.6084/m9.figshare.9114383.v4, refer to Lee et al. (2019).\nAcknowledgements\nThis work was supported by \u201cHuman Resources Program in Energy Technology\u201d of the Korea\nInstitute of Energy Technology Evaluation and Planning (KETEP), granted \ufb01nancial resource\nfrom the Ministry of Trade, Industry & Energy, Republic of Korea. (No. 20184010201680);\nand the National Research Foundation of Korea(NRF) grant funded by the Korea govern-\nment(MSIT) (No.2017R1C1B5017338).\nReferences\nAas, K., C. Czado, A. Frigessi, and H. Bakken (2009). Pair-copula constructions of multiple\ndependence. Insurance: Mathematics and Economics 44(2), 182\u2013198.\nAcharya, V. V., L. H. Pedersen, T. Philippon, and M. Richardson (2017). Measuring systemic\nrisk. The Review of Financial Studies 30(1), 2\u201347.\nAdrian, T. and M. K. Brunnermeier (2016). Covar. American Economic Review 106(7),\n1705\u201341.\nBacry, E., K. Dayri, and J.-F. Muzy (2012). Non-parametric kernel estimation for symmetric\nhawkes processes. application to high frequency \ufb01nancial data.\nThe European Physical\nJournal B 85(5), 157.\n25\n\nBacry, E., S. Delattre, M. Ho\ufb00mann, and J.-F. Muzy (2013). Modelling microstructure noise\nwith mutually exciting point processes. Quantitative Finance 13(1), 65\u201377.\nBacry, E., I. Mastromatteo, and J.-F. Muzy (2015). Hawkes processes in \ufb01nance. Market\nMicrostructure and Liquidity 1(01), 1550005.\nBaillie, R. and T. Bollerslev (1989). The message in daily exchange rates: A conditional-\nvariance tale. Journal of Business & Economic Statistics 7(3), 197\u2013305.\nBormetti, G., L. M. Calcagnile, M. Treccani, F. Corsi, S. Marmi, and F. Lillo (2015). Mod-\nelling systemic price cojumps with hawkes factor models.\nQuantitative Finance 15(7),\n1137\u20131156.\nBowsher, C.-G. (2007). Modelling security market events in continuous time: Intensity based,\nmultivariate point process models.",
    "chunk_index": 25,
    "start_char": 60869,
    "end_char": 64201,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "(2015). Hawkes processes in \ufb01nance. Market\nMicrostructure and Liquidity 1(01), 1550005.\nBaillie, R. and T. Bollerslev (1989). The message in daily exchange rates: A conditional-\nvariance tale. Journal of Business & Economic Statistics 7(3), 197\u2013305.\nBormetti, G., L. M. Calcagnile, M. Treccani, F. Corsi, S. Marmi, and F. Lillo (2015). Mod-\nelling systemic price cojumps with hawkes factor models.\nQuantitative Finance 15(7),\n1137\u20131156.\nBowsher, C.-G. (2007). Modelling security market events in continuous time: Intensity based,\nmultivariate point process models. Journal of Econometrics 141(2), 876\u2013912.\nBrownlees, C. and R.-F. Engle (2017).\nSrisk: A conditional capital shortfall measure of\nsystemic risk. The Review of Financial Studies 30(1), 48\u201379.\nCalcagnile, L.-M., G. Bormetti, M. Treccani, S. Marmi, and F. Lillo (2018).\nCollective\nsynchornization and high frequency systemic instabilties in \ufb01nancial markets. Quantitative\nFinance 18, 237\u2013247.\nCapponi, A. and P.-C. Chen (2015). Systemic risk mitigation in \ufb01nancial networks. Journal\nof Economic Dynamics and Control 58, 152\u2013166.\nCerchi, M. and A. Havenner (1988). Cointegration and stock prices: The random walk and\nwall street revisited. Journal of Economic Dynamics and Control 12(-), 333\u2013346.\nChavez-Demoulin, V. and J. McGill (2012). High-frequency \ufb01nancial data modeling using\nhawkes processes. Journal of Banking & Finance 36(12), 3415\u20133426.\nChiu, M. C., H. Y. Wong, and J. Zhao (2015). Commodity derivatives pricing with coin-\ntegration and stochastic covariances. European Journal of Operational Research 246(2),\n476\u2013486.\nChoro\u00b4s-Tomczyk, B., W. K. H\u00a8ardle, and L. Overbeck (2014). Copula dynamics in CDOs.\nQuantitative Finance 14(9), 1573\u20131585.\nDa Fonseca, J. and R. Zaatour (2014). Hawkes process: Fast calibration, application to trade\nclustering, and di\ufb00usive limit. Journal of Futures Market 34(6), 548\u2013579.\nDanielsson, J. and H. S. Shin (2003). Endogenous risk. pp. 297\u2013314.\nDanielsson, J., H. S. Shin, and J.-P. Zigrand (2012). Endogenous and systemic risk. pp.\n73\u201394.\n26\n\nDuan, J.-C. and S. Pliska (2004). Option valuation with co-integrated asset prices. Journal\nof Economic Dynamics & Control 28(2), 727\u2013754.\nEIA (2014).\nWhat drives us gasoline prices?\nEIA, Independent Statistics & Analysis -\n(October), \u2013.\nEisenberg, L. and T. H. Noe (2001). Systemic risk in \ufb01nancial systems. Management Sci-\nence 47(2), 236\u2013249.\nElliott, M., B. Golub, and M. O. Jackson (2014). Financial networks and contagion. American\nEconomic Review 104(10), 3115\u201353.\nEngle, R. F. and C. W. Granger (1987). Co-integration and error correction: representation,\nestimation, and testing. Econometrica: journal of the Econometric Society, 251\u2013276.\nFang, F., Y. Sun, and K. Spiliopoulos (2017). On the e\ufb00ect of heterogeneity on \ufb02ocking\nbehavior and systemic risk. Statistics & Risk Modeling 34(3-4), \u2013.\nFern\u00b4andez, C. and M. F. Steel (1998). On bayesian modeling of fat tails and skewness. Journal\nof the American Statistical Association 93(441), 359\u2013371.\nFilimonov, V. and D. Sornette (2012). Quantifying re\ufb02exivity in \ufb01nancial markets: Toward\na prediction of \ufb02ash crashes. Physical Review E 85(5), 056108.\nGirardi, G. and A.-T. Ergun (2013). Systemic risk measurement: Multivariate garch estima-\ntion of covar. Journal of Banking and Finance 37(11), 3169\u20133180.\nGranger, C. W. (1981). Some properties of time series data and their use in econometric\nmodel speci\ufb01cation. Journal of Econometrics 16(1), 121\u2013130.\nHa, S.-Y., K.-K.",
    "chunk_index": 26,
    "start_char": 63637,
    "end_char": 67112,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "fat tails and skewness. Journal\nof the American Statistical Association 93(441), 359\u2013371.\nFilimonov, V. and D. Sornette (2012). Quantifying re\ufb02exivity in \ufb01nancial markets: Toward\na prediction of \ufb02ash crashes. Physical Review E 85(5), 056108.\nGirardi, G. and A.-T. Ergun (2013). Systemic risk measurement: Multivariate garch estima-\ntion of covar. Journal of Banking and Finance 37(11), 3169\u20133180.\nGranger, C. W. (1981). Some properties of time series data and their use in econometric\nmodel speci\ufb01cation. Journal of Econometrics 16(1), 121\u2013130.\nHa, S.-Y., K.-K. Kim, and K. Lee (2015). A mathematical model for multi-name credit based\non community \ufb02ocking. Quantitative Finance 15(5), 841\u2013851.\nHardiman, S. J., N. Bercot, and J.-P. Bouchaud (2013).\nCritical re\ufb02exivity in \ufb01nancial\nmarkets: a hawkes process analysis. The European Physical Journal B 86(10), 442.\nHawkes, A. G. (1971a). Point spectra of some mutually exciting point processes. Journal of\nthe Royal Statistical Society. Series B (Methodological) 33(3), 438\u2013443.\nHawkes, A. G. (1971b). Spectra of some self-exciting and mutually exciting point processes.\nBiometrika 58(1), 83\u201390.\nHawkes, A. G. and D. Oakes (1974). A cluster process representation of a self-exciting process.\nJournal of Applied Probability 11(3), 493\u2013503.\nHenningsen, A. and O. Toomet (2011). maxlik: A package for maximum likelihood estimation\nin r. Computational Statistics 26(3), 443\u2013458.\n27\n\nHuang, X., H. Zhou, and H. Zhu (2009). A framework for assessing the systemic risk of major\n\ufb01nancial institutions. Journal of Banking and Finance 33(11), 2036\u20132049.\nHuepe, C. and M. Aldana (2008). New tools for characterizing swarming systems: A com-\nparison of minimal models. Physic A: Statistical Mechanics and its Applications 387(12),\n2809\u20132822.\nJain, P. K., P. Jain, and T. H. McInish (2016). Does high-frequency trading increase systemic\nrisk? Journal of Financial Markets 31, 1\u201324.\nJammazi, R., A. K. Tiwari, R. Ferrer, and P. Moya (2015). Time-varying dependence between\nstock and government bond returns: International evidence with dynamic copulas. The\nNorth American Journal of Economics and Finance 33, 74\u201393.\nKellard, N., C. Dunis, and N. Sarantis (2010). Foreign exchange, fractional cointegration and\nthe implied\u2013realized volatility relation. Journal of Banking & Finance 34(-), 882\u2013891.\nLee, K., H. J. Jang, and K. Lee (2019). WTI crude oil and RBOB gasoline futures dataset.\nLee, K. and B.-K. Seo (2017). Modeling microstructure price dynamics with symmetric hawkes\nand di\ufb00usion model using ultra-high-frequency stock data. Journal of Economic Dynamics\nand Control 79(6), 154\u2013183.\nLucas, A., B. Schwaab, and X. Zhang (2014). Conditional euro area sovereign default risk.\nJournal of Business & Economic Statistics 32(2), 271\u2013284.\nMaslyuka, S. and R. Smyth (2009).\nCointegration between oil spot and future prices of\nthe same and di\ufb00erent grades in the presence of structural change. Energy Policy 37(-),\n1687\u20131963.\nMiller, R.-S. and G. Shorter (2016). High frequency trading: Overview of recent developments.\nCongressional Research Service.\nNg, V. K. and S. C. Pirrong (1994). Fundamentals and volatility: Storage, spreads, and the\ndynamics of metals prices. The Journal of Business 67(2), 203\u2013230.\nNg, V. K. and S. C. Pirrong (1996). Price dynamics in re\ufb01ned petroleum spot and futures\nmarkets. Journal of Empirical Finance 2(4), 359\u2013388.\nOgata, Y.",
    "chunk_index": 27,
    "start_char": 66551,
    "end_char": 69943,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "same and di\ufb00erent grades in the presence of structural change. Energy Policy 37(-),\n1687\u20131963.\nMiller, R.-S. and G. Shorter (2016). High frequency trading: Overview of recent developments.\nCongressional Research Service.\nNg, V. K. and S. C. Pirrong (1994). Fundamentals and volatility: Storage, spreads, and the\ndynamics of metals prices. The Journal of Business 67(2), 203\u2013230.\nNg, V. K. and S. C. Pirrong (1996). Price dynamics in re\ufb01ned petroleum spot and futures\nmarkets. Journal of Empirical Finance 2(4), 359\u2013388.\nOgata, Y. (1978). The asymtotic behaviour of maximum liklihood estimators for stationary\npoint processes. Ann. Inst. Statist. Math. 30(Part A), 243\u2013261.\nOgata, Y. (1981). On lewis simulation method for point processes. IEEE Inform. Theory 27,\n23\u201331.\nOh, D. H. and A. J. Patton (2017, jan). Modeling dependence in high dimensions with factor\ncopulas. Journal of Business & Economic Statistics 35(1), 139\u2013154.\n28\n\nOkhrin, O. and Y. F. Xu (2017). A comparison study of pricing credit default swap index\ntranches with convex combination of copulae. The North American Journal of Economics\nand Finance 42, 193\u2013217.\nRauch, E. M., M. M. Millonas, and D. R. Chialvo (1995). Pattern formation and functionality\nin swarm models. Physics Letters A 207(3-4), 185\u2013193.\nReboredo, J. C. (2015). Is there dependence and systemic risk between oil and renewable\nenergy stock prices? Energy Economics 48, 32\u201345.\nReboredo, J. C. and A. Ugolini (2015). Systemic risk in european sovereign debt markets: A\ncovar-copula approach. Journal of International Money and Finance 51, 214\u2013244.\nReynolds, C. W. (1987). Flocks, herds and schools: A distributed behavioral model. ACM\nSIGGRAPH Computer Graphics 21(-), 25\u201334.\nRogers, L. C. and L. A. Veraart (2013). Failure and rescue in an interbank network. Man-\nagement Science 59(4), 882\u2013898.\nSchepsmeier, U. and J. St\u00a8ober (2014). Derivatives and \ufb01sher information of bivariate copulas.\nStatistical Papers 55(2), 525\u2013542.\nSerletis, A. (1992). Maturity e\ufb00ects in energy futures. Energy Economics 14(2), 150\u2013157.\nSklar, A. (1959). Fonctions de r\u00b4epartition `a n dimensions et leurs marges. Publ. Inst. Statist.\nUniv. Paris 8, 229\u2013234.\nTaylor, M. and I. Tonks (1989). The internationalisation of stock markets and the abolition\nof uk exchange control. The Review of Economics and Statistics 71(2), 332\u2013336.\nA\nA CoVaR-Copula Approach\nUnder the copula speci\ufb01cations in Table 2, we follow a three-step procedure to implement the\ntime-varying CoVaR.\nStep 1. Estimating marginal distributions for returns.\nTo estimate the marginal distributions for each return R\u2113\nt for \u2113= 1, 2, we use an ARMA(p, q)\u2212\nTGARCH(r, m) model, that is,\nRt = \u03c60 +\np\nX\nj=1\n\u03c6jRt\u2212j + \u03f5t \u2212\nq\nX\ni=1\n\u03b8i\u03f5t\u2212i,\n(20)\nwhere p and q are non-negative integers and \u03c6 and \u03b8 are the ARMA parameters, respectively.\nHere, \u03f5t = \u03c3tzt, and \u03c32\nt is the conditional variance given by a TGARCH speci\ufb01cation:\n\u03c32\nt = \u03c9 +\nr\nX\nk=1\n\u03b2\u03c32\nt\u2212k +\nm\nX\nh=1\n\u03b1h\u03f52\nt\u2212h +\nm\nX\nh=1\n\u03bbh\u03f52\nt\u2212h1{t\u2212h>0},\n(21)\n29\n\nwhere \u03c9 is a constant, \u03c32\nt\u2212k is the GARCH component, \u03f5t\u2212h is the ARCH component, and \u03bb\ncaptures asymmetric e\ufb00ects.",
    "chunk_index": 28,
    "start_char": 69414,
    "end_char": 72504,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "(20)\nwhere p and q are non-negative integers and \u03c6 and \u03b8 are the ARMA parameters, respectively.\nHere, \u03f5t = \u03c3tzt, and \u03c32\nt is the conditional variance given by a TGARCH speci\ufb01cation:\n\u03c32\nt = \u03c9 +\nr\nX\nk=1\n\u03b2\u03c32\nt\u2212k +\nm\nX\nh=1\n\u03b1h\u03f52\nt\u2212h +\nm\nX\nh=1\n\u03bbh\u03f52\nt\u2212h1{t\u2212h>0},\n(21)\n29\n\nwhere \u03c9 is a constant, \u03c32\nt\u2212k is the GARCH component, \u03f5t\u2212h is the ARCH component, and \u03bb\ncaptures asymmetric e\ufb00ects. If \u03bb > 0, then the future conditional variance will increase more\nfollowing a negative shock than following a positive shock of the same magnitude. Here, zt is\nan independent and identically distributed random variable with zero mean and unit variance\nthat follows a skewed-t distribution, given by Fern\u00b4andez and Steel (1998):\nf(zj,t; \u03b3) =\n2\n\u03b3 + 1\n\u03b3\n\u001a\nf\u03bd\n\u0012zj,t\n\u03b3\n\u0013\n1[0,\u221e)(zj,t) + f\u03bd(\u03b3zj,t)1(\u2212\u221e,0)(zj,t)\n\u001b\nwhere \u03b3 is a skew parameter, and f\u03bd is the density of the t distribution with \u03bd degree of\nfreedom.\nStep 2. Finding the best \ufb01tting copula.\nAmong the copulas, we \ufb01nd one with the best \ufb01t in Table 2 with empirical marginal\ndistributions of R\u2113\nt. We use an ML estimation method, that is,\n\u03b8i = argmax\u03b8\ni+d\nX\nt=i\nln c(\u02c6ut, \u02c6vt; \u03b8)\nwhere c(\u00b7, \u00b7; \u03b8) is a copula density obtained by \u22022C(u, v; \u03b8)/\u2202u\u2202v, \u02c6ut, and \u02c6vt are samples\ntransformed from observations R\u2113\nt by their empirical distributions obtained in step (1). To\nestimate \u03b8i, d days of samples (\u02c6ut, \u02c6vt) are used.\nStep 3. Computing \u2206CoVaR.\nWe compute the time-varying \u2206CoVaRt with the best \ufb01tting copula obtained in step (2)\nand marginal distributions in step (1), as stated in Proposition 4. To quantify the systemic\nimpact of an asset price return on another asset price return, we compute the \u03b2-quantile\nCoVaR1|2\n\u03b2,t and CoVaR2|1\n\u03b2,t that are de\ufb01ned by\nP\n\u0010\nR1\nt \u2264CoVaR1|2\n\u03b2,t|R2\nt \u2264VaR2\n\u03b1,t\n\u0011\n= \u03b2 and P\n\u0010\nR2\nt \u2264CoVaR2|1\n\u03b2,t|R1\nt \u2264VaR1\n\u03b1,t\n\u0011\n= \u03b2,\nrespectively. Then, we compute CoVaR1|2,\u03b1=0.5\n\u03b2,t\nand CoVaR2|1,\u03b1=0.5\n\u03b2,t\n.\nTo implement the \ufb01rst term of \u2206CoVaR in Proposition 4, the numerical inversion is needed\nfor the Gaussian and Student t copulas. For the Gumbel and Clayton copulas with generator\n\u03c8, C\u22121\n\u03b1\ncan be easily derived in an explicit form as \u03c8\u22121(\u03c8(x) \u2212\u03c8(\u03b1)) for x \u2208(0, \u03b1). Thus,\n(33) is written as\nCoVaR1|2\n\u03b2,t = F \u22121\nR1\nt\n\u0000\u03c8\u22121(\u03c8(\u03b1\u03b2) \u2212\u03c8(\u03b1))\n\u0001\n,\n(22)\nwhere \u03b1 and \u03b2 are the given levels.\nNext, for the second term of \u2206CoVaR in Proposition 4, computing the conditional copula\nfunction \u03b6\u03b1(u) with a given level \u03b1 is required as de\ufb01ned in (16). Depending on a choice of\ncopulas, the functions \u03b6\u03b1(u) are obtained as analytic forms, which are derived by Aas et al.\n(2009) and Schepsmeier and St\u00a8ober (2014), as follows.\n\u2022 Gaussian copula:\n\u03b6\u03b1(u) = \u03a6\n\u0012\u03a6\u22121(u) \u2212\u03b8\u03a6\u22121(\u03b1)\n\u221a\n1 \u2212\u03b82\n\u0013\n(23)\n30",
    "chunk_index": 29,
    "start_char": 72124,
    "end_char": 74752,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "for the second term of \u2206CoVaR in Proposition 4, computing the conditional copula\nfunction \u03b6\u03b1(u) with a given level \u03b1 is required as de\ufb01ned in (16). Depending on a choice of\ncopulas, the functions \u03b6\u03b1(u) are obtained as analytic forms, which are derived by Aas et al.\n(2009) and Schepsmeier and St\u00a8ober (2014), as follows.\n\u2022 Gaussian copula:\n\u03b6\u03b1(u) = \u03a6\n\u0012\u03a6\u22121(u) \u2212\u03b8\u03a6\u22121(\u03b1)\n\u221a\n1 \u2212\u03b82\n\u0013\n(23)\n30\n\n\u2022 Student t copula with the degree of freedom \u03bd:\n\u03b6\u03b1(u) = t\u03bd+1\n\uf8eb\n\uf8ed\nt\u22121\n\u03bd (u) \u2212\u03b8t\u22121\n\u03bd (\u03b1)\nq\u0000\u03bd + [t\u22121\n\u03bd (\u03b1)]2\u0001\n(1 \u2212\u03b82)/(\u03bd + 1)\n\uf8f6\n\uf8f8\n(24)\n\u2022 Gumbel copula: for x = (\u2212ln u)\u03b8 and y = (\u2212ln \u03b1)\u03b8\n\u03b6\u03b1(u) = \u2212\nexp\n\u0010\n\u2212(x + y)\n1\n\u03b8\n\u0011\n\u00b7 (x + y)\n1\n\u03b8 \u22121 \u00b7 y\n\u03b1 ln \u03b1\n(25)\n\u2022 Clayton copula\n\u03b6\u03b1(u) = \u03b1\u2212\u03b8\u22121 \u00b7\n\u0010\nu\u2212\u03b8 + \u03b1\u2212\u03b8 \u22121\n\u0011\u22121\u22121\n\u03b8\n(26)\nDependence parameter \u03b8 is the value estimated in step (2), and the level of \u03b1 is chosen as the\nmedian (i.e., \u03b1 = 0.5).\nBy conducting the three-step procedure described above, we compute the time-varying\n\u2206CoVaR\u2019s. Then, we compare them with the calibrated parameters from the proposed model\nin Section 2 to examine the consistency of systemic risk measures for high frequency data.\nB\nProof of Proposition 4\nAs de\ufb01ned in (17), computing \u2206CoVaR consists of determining two types of CoVaR speci\ufb01ed\nin (15) and (14). Each part is derived in the following step (i) and (ii).\n(i) The CoVaR de\ufb01ned in (14).\nIt can be computed by using the property of a copula function. Then (14) can be written\nas\n\u03b6q\n\u0010\nFRj\nt\n\u0010\nCoVaRd|j,\u03b1=q\n\u03b2,t\n\u0011\u0011\n= \u03b2.\n(27)\nThus, the CoVaR is given by\nCoVaRi|j,\u03b1=q\n\u03b2,t\n= F \u22121\nRj\nt\n\u0000\u03b6\u22121\nq (\u03b2)\n\u0001\n.\n(28)\n(ii) The CoVaR de\ufb01ned in (15).\nIn this case, the quantile value of a conditional distribution, or, alternatively, of an un-\nconditional bivariate distribution is needed if we express in (15) as\nP\n\u0010\nRi\nt \u2264CoVaRi|j\n\u03b2,t, Rj\nt \u2264VaRj\n\u03b1,t\n\u0011\nP(Rj\nt \u2264VaRj\n\u03b1,t)\n= \u03b2.\n(29)\nGiven that P(Rj\nt \u2264VaRj\n\u03b1,t) = \u03b1, the CoVaR in (29) can be expressed as:\nP\n\u0010\nRi\nt \u2264CoVaRi|j\n\u03b2,t, Rj\nt \u2264VaRj\n\u03b1,t\n\u0011\n= \u03b1\u03b2.\n(30)\n31\n\nHere, the form (29) can be expressed in terms of the joint distribution function of Ri\nt and Rj\nt,\nFRi\nt,Rj\nt, as\nFRi\nt,Rj\nt\n\u0010\nCoVaRi|j\n\u03b2,t, VaRj\n\u03b1,t\n\u0011\n= \u03b1\u03b2,\n(31)\nand that, according to Sklar\u2019s theorem (Sklar, 1959), the joint distribution function of two\ncontinuous variables can be expressed in terms of a copula function.\nHence, (31) can be\nwritten as\nC(u, v) = \u03b1\u03b2,\n(32)\nwhere C(\u00b7, \u00b7) is a copula function, u = FRi\nt(CoVaRi|j\n\u03b2,t) and v = FRj\nt(VaRj\n\u03b1,t), and where FRi\nt\nand FRj\nt are the marginal distribution function of Ri\nt and Rj\nt, respectively.",
    "chunk_index": 30,
    "start_char": 74368,
    "end_char": 76820,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "\u03b1\u03b2,\n(31)\nand that, according to Sklar\u2019s theorem (Sklar, 1959), the joint distribution function of two\ncontinuous variables can be expressed in terms of a copula function.\nHence, (31) can be\nwritten as\nC(u, v) = \u03b1\u03b2,\n(32)\nwhere C(\u00b7, \u00b7) is a copula function, u = FRi\nt(CoVaRi|j\n\u03b2,t) and v = FRj\nt(VaRj\n\u03b1,t), and where FRi\nt\nand FRj\nt are the marginal distribution function of Ri\nt and Rj\nt, respectively. Given its copula\nrepresentation in (32), the CoVaR can be computed from that equation through copulas in\na two-step procedure. First, we obtain the value of u = FRi\nt(CoVaRi|j\n\u03b2,t). Since C(u, v) = \u03b1\u03b2,\nwhere \u03b1, \u03b2, and v are given (note that v = \u03b1), from the copula function speci\ufb01cation we\ncan solve to determine the value of u. Next, taking u, we can obtain the CoVaR value as\nthe quantile of distribution Ri\nt, with a cumulative probability equal to u, by inverting the\nmarginal distribution function of Ri\nt: CoVaRi|j\n\u03b2,t = F \u22121\nRi\nt (u). Letting C\u22121\n\u03b1 (\u00b7) be the inverse of\nC\u03b1 : x \u2192C(\u00b7, \u03b1), then the CoVaR can be expressed as an analytic form\nCoVaRi|j\n\u03b2,t = F \u22121\nRi\nt\n\u0000C\u22121\n\u03b1 (\u03b1\u03b2)\n\u0001\n.\n(33)\nC\nSelection of the best \ufb01tting copula in Section 4.2\nWe estimated marginal distributions for each return series of CL and RB futures prices using\nthe ARMA(1,1)-TGARCH(1,1) model with skewed-t distribution. Then, we transformed the\ndaily return series into uniform variables such that \u02c6ut = \u02c6F1(xt) and \u02c6vt = \u02c6F2(yt) where \u02c6Fi\nis the estimated distribution and xt, yt are the standardized returns of R1\nt , R2\nt , respectively.\nFigure 15 displays the scatter plot of 2,000 days of pseudo-sample observations, \u02c6ut, \u02c6vt under\nthe marginal distribution model.\nWe extracted dependence parameters of the copula functions reported in Table 2 using\nthe consecutive d days of series pairs. Figure 18 (Appendix E) illustrates the dynamics of the\nestimated parameter \u03b8 of CL and RB futures prices using given copulas with the standard\nerror during the test period from 2007 to 2016.\nTo select the best \ufb01tting copula among them, we compare di\ufb00erent copula speci\ufb01cations\nusing the commonly used error measures of Akaike information criterion (AIC) and Bayesian\ninformation criterion (BIC) in the model selection based on an ML estimation. Figure 16\nshows the results of AIC and BIC values from estimations of the Gaussian, Student t, Gumbel,\nand Clayton copulas over time. We \ufb01nd that the Student t copula provides the lowest values\nby both measures on the entire timeline.\nIn addition, the degrees of freedom \u03bd for the Student t copula are estimated over time\nand displayed in Figure 19 (Appendix E), which provides evidence of fat tails of the joint\n32\n\nTable 3: Estimates for CL (left) and RB (right) under the Hawkes-based model without the\n\ufb02ocking-related parameters\nWTI crude oil\nRBOB gasoline\nDate/Maturity\n\u00b51\n\u03b11s\n\u03b11c\n\u03b21\n\u00b52\n\u03b12s\n\u03b12c\n\u03b22\n2016-03-16",
    "chunk_index": 31,
    "start_char": 76419,
    "end_char": 79253,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "the Student t copula provides the lowest values\nby both measures on the entire timeline.\nIn addition, the degrees of freedom \u03bd for the Student t copula are estimated over time\nand displayed in Figure 19 (Appendix E), which provides evidence of fat tails of the joint\n32\n\nTable 3: Estimates for CL (left) and RB (right) under the Hawkes-based model without the\n\ufb02ocking-related parameters\nWTI crude oil\nRBOB gasoline\nDate/Maturity\n\u00b51\n\u03b11s\n\u03b11c\n\u03b21\n\u00b52\n\u03b12s\n\u03b12c\n\u03b22\n2016-03-16\n0.0780\n0.0400\n0.3710\n0.5120\n0.0577\n0.3763\n0.2001\n0.9109\nMarch 2016\n(0.0025)\n(0.0050)\n(0.0122)\n(0.0176)\n(0.0017)\n(0.0183)\n(0.0110)\n(0.0439)\n2016-04-15\n0.0731\n0.0128\n0.3455\n0.4827\n0.0593\n0.5337\n0.2126\n1.3083\nApril 2016\n(0.0025)\n(0.0044)\n(0.0119)\n(0.0184)\n(0.0014)\n(0.0204)\n(0.0118)\n(0.0446)\n2016-05-17\n0.0872\n-0.0206\n0.3527\n0.5106\n0.0568\n0.4200\n0.2712\n1.1314\nMay 2016\n(0.0025)\n(0.0037)\n(0.0112)\n(0.0195)\n(0.0015)\n(0.0204)\n(0.0144)\n(0.0510)\n2016-06-16\n0.1001\n-0.0033\n0.4315\n0.6022\n0.0574\n0.3405\n0.1954\n0.8668\nJune 2016\n(0.0027)\n(0.0041)\n(0.0124)\n(0.0189)\n(0.0016)\n(0.0151)\n(0.0101)\n(0.0367)\ndistribution of the return pair of CL and RB futures prices. The empirical result indicates\nthe existence of positive and symmetric dependence with fat tails between the two futures\nprices. The extent of the overall positive dependency and extreme tail dependency has varied\nover time.\nD\nMore tests for calibration in Section 3\nSome of selected estimates are presented in Table 3 with the numerically computed standard\nerrors in the parentheses. In this period of time, the self-exciting term \u03b11s of CL is close to\nzero and all other parameters are signi\ufb01cant. Since we performed non-constraint parameter\nestimation, sometimes negative \u03b1s are observed, but it is better to be considered as zero by\nthe Hawkes-based model de\ufb01nition. In general, \u00b5 in CL is larger than that in RB implying\nlarger trade frequency in CL and \u03b2 in RB is larger than that in CL implying longer persistence\nin RB.\nThe estimates and numerically computed standard errors in parenthesis are presented in\nTable 4. Note that \u03b1is and \u03b1ic are similar to the estimates in Table 3 for i = 1, 2, which means\nthat the additionally introduced \u03b1in and \u03b1iw do not a\ufb00ect the self/mutually-exciting terms.\nThe result also shows that \u03b1in are close to zero in the selected time period, which means that\nthe price di\ufb00erence for narrowing events does not a\ufb00ect the intensities. We expand the time\nrange, nonzero positive \u03b11n of CL is also observed, but overall \u03b1in is quite small and close to\nzero. In addition, \u03b1iw are signi\ufb01cant, which means that the price di\ufb00erence widening events\nincrease the intensities so that the two price processes tend to converge to each other.\nBy data visualization, we con\ufb01rm the previous discussion. For the graph, the selected\nmaturity for the futures is February 2016 and estimates are computed on a daily basis over\nthe sample period from January 4 to February 22, 2016. Figure 7 compares \u03b1n and \u03b1w. For\nboth CL and RB, the estimated \u03b1n are almost zero, but \u03b1w are far from zero and \u03b1w in CL\n33",
    "chunk_index": 32,
    "start_char": 78786,
    "end_char": 81826,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "that the two price processes tend to converge to each other.\nBy data visualization, we con\ufb01rm the previous discussion. For the graph, the selected\nmaturity for the futures is February 2016 and estimates are computed on a daily basis over\nthe sample period from January 4 to February 22, 2016. Figure 7 compares \u03b1n and \u03b1w. For\nboth CL and RB, the estimated \u03b1n are almost zero, but \u03b1w are far from zero and \u03b1w in CL\n33\n\nTable 4: Estimates for CL (top) and RB (bottom) with the Hawkes \ufb02ocking model\nWTI crude oil\ndate/maturity\n\u00b51\n\u03b11n\n\u03b11w\n\u03b11s\n\u03b11c\n\u03b21\n2016-03-16\n0.0755\n-0.0120\n0.2265\n0.0105\n0.4333\n0.6079\nMarch 2016\n(0.0039)\n(0.0086)\n(0.0286)\n(0.0053)\n(0.0321)\n(0.0523)\n2016-04-15\n0.0345\n0.0046\n0.1993\n0.0076\n0.2992\n0.4084\nApril 2016\n(0.0021)\n(0.0078)\n(0.0118)\n(0.0038)\n(0.0108)\n(0.0160)\n2016-05-17\n0.0670\n-0.0085\n0.2053\n-0.0318\n0.3761\n0.5699\nMay 2016\n(0.0024)\n(0.0069)\n(0.0119)\n(0.0032)\n(0.0121)\n(0.0217)\n2016-06-16\n0.0706\n-0.0189\n0.2748\n-0.0121\n0.4348\n0.6142\nJune 2016\n(0.0026)\n(0.0093)\n(0.0160)\n(0.0032)\n(0.0154)\n(0.0245)\nRBOB gasoline\ndate/maturity\n\u00b52\n\u03b12n\n\u03b12w\n\u03b12s\n\u03b12c\n\u03b22\n2016-03-16\n0.0499\n-0.0109\n0.1335\n0.4384\n0.2306\n1.2513\nMarch, 2016\n(0.0037)\n(0.0083)\n(0.0269)\n(0.1005)\n(0.0471)\n(0.3109)\n2016-04-15\n0.0403\n0.0098\n0.1719\n0.4450\n0.1836\n1.5166\nApril, 2016\n(0.0013)\n(0.0073)\n(0.0112)\n(0.0212)\n(0.0125)\n(0.0598)\n2016-05-17\n0.0413\n0.0347\n0.2229\n0.4555\n0.3085\n1.4429\nMay, 2016\n(0.0013)\n(0.0096)\n(0.0121)\n(0.0200)\n(0.0151)\n(0.0492)\n2016-06-16\n0.0348\n0.0186\n0.1302\n0.2950\n0.1882\n0.9146\nJune, 2016\n(0.0014)\n(0.0065)\n(0.0077)\n(0.0133)\n(0.0096)\n(0.0352)\n34\n\nTable 5: Means of estimates for CL and RB\nWTI crude oil\nRBOB gasoline\nmaturity\n\u03b11n\n\u03b11w\n\u03b11s\n\u03b11c\n\u03b12n\n\u03b12w\n\u03b12s\n\u03b12c\n2016-01\n-0.0229\n0.1828\n0.0003\n0.3919\n0.0197\n0.1683\n0.5835\n0.2900\n2016-02\n-0.0332\n0.2782\n0.0106\n0.4662\n-0.0059\n0.1290\n0.4887\n0.2605\n2016-03\n-0.0237\n0.2481\n-0.0129\n0.4621\n-0.0059\n0.1321\n0.4625\n0.2491\n2016-04\n-0.0234\n0.2161\n-0.0266\n0.4359\n-0.0020\n0.1517\n0.4600\n0.2333\n2016-05\n-0.0229\n0.2497\n-0.0233\n0.4611\n-0.0026\n0.1480\n0.4570\n0.2577\n2016-06\n-0.0187\n0.1725\n-0.0250\n0.4136\n0.0174\n0.1618\n0.4737\n0.2731\n2016-07\n-0.0157\n0.2321\n-0.0199\n0.4300\n0.0185\n0.1872\n0.4588\n0.2579\n2016-08\n-0.0205\n0.2013\n-0.0149\n0.4707\n0.0026\n0.1787\n0.4498\n0.2497\n2016-09\n-0.0145\n0.2223\n-0.0178\n0.4870\n-0.0015\n0.1834\n0.4574\n0.2237\n2016-10\n-0.0115\n0.2205\n-0.0222\n0.4970\n0.0005\n0.1738\n0.4053\n0.2212\n2016-11\n-0.0137\n0.2123\n-0.0164\n0.4798\n-0.0049\n0.1576\n0.4033\n0.2123\n2016-12\n-0.0073\n0.2756\n-0.0242\n0.4879\n0.0028\n0.1363\n0.4169\n0.2217\nis larger than that in RB.\nFigure 17 compares \u03b1s and \u03b1c. For CL, \u03b1s are close to zero and for other cases, \u03b1c in CL\nand \u03b1s, \u03b1c in RB, the estimates are signi\ufb01cantly positive. In addition, \u03b1s is less than \u03b1c in\nCL, but \u03b1s is greater than \u03b1c in RB over the sample period.\nIn Table 5, for each maturity, we calculated the averages of estimates on a daily basis using\n20 days\u2019 data. In this result, we also observe that \u03b1in are close to zero for all maturities.\nE\nFigures related to Section 4\n35\n\n0.00\n0.25\n0.50\n0.75\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nn\nw\nCL\n0.00\n0.25\n0.50\n0.75\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nn\nw\nRB\nFigure 8: Comparison of changes in the \ufb02ocking parameters \u03b1n",
    "chunk_index": 33,
    "start_char": 81410,
    "end_char": 84550,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "In Table 5, for each maturity, we calculated the averages of estimates on a daily basis using\n20 days\u2019 data. In this result, we also observe that \u03b1in are close to zero for all maturities.\nE\nFigures related to Section 4\n35\n\n0.00\n0.25\n0.50\n0.75\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nn\nw\nCL\n0.00\n0.25\n0.50\n0.75\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nn\nw\nRB\nFigure 8: Comparison of changes in the \ufb02ocking parameters \u03b1n (red line) and \u03b1w (black\ndotted line) for CL (left) and RB (right) from January 2007 to December 2016\n0.0\n0.2\n0.4\n0.6\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nc\ns\nCL\n0.0\n0.2\n0.4\n0.6\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nc\ns\nRB\nFigure 9: Comparison of changes in the self/mutually exciting parameters \u03b1s (red line) and\n\u03b1c (black dotted line) for CL (left) and RB (right) from January 2007 to December 2016\n0.04\n0.08\n0.12\n0.16\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nCL\nRB\nmu\n0.4\n0.8\n1.2\n1.6\n2008\n2010\n2012\n2014\n2016\nyear\nestimate\nCL\nRB\nbeta\nFigure 10: Comparison of changes in the exogenous \ufb02uctuation parameter \u00b5 (left) and the\npersistence \u03b2 (right) for CL (black dotted line) and RB (red line) from January 2007 to\nDecember 2016\n36\n\nwidening event by RB\nnarrowed by CL\ntime\nprice\nCL\nRB\nwidening event by CL\nnarrowed by CL\ntime\nprice\nCL\nRB\nFigure 11: Examples of widening events and their possible consequences\n0.65\n0.70\n0.75\n0.80\n0.85\n2008\n2010\n2012\n2014\n2016\nyear\nspectral radius\nFigure 12: Illustration of spectral radius from January 2007 to December 2016\n0.60\n0.65\n0.70\n0.75\n0.80\n2008\n2010\n2012\n2014\n2016\nyear\nratio\nCL\n0.4\n0.5\n0.6\n2008\n2010\n2012\n2014\n2016\nyear\nratio\nRB\n0.03\n0.06\n0.09\n2008\n2010\n2012\n2014\n2016\ndate\nVaR for CL\n0.025\n0.050\n0.075\n0.100\n2008\n2010\n2012\n2014\n2016\ndate\nVaR for RB\nFigure 13: Illustration of evolution of (\u03b11s + \u03b11c)/\u03b21 for CL (top, left), (\u03b12s + \u03b12c)/\u03b22 for RB\n(top, right), time-varying one-day 95% VaR for CL (bottom, left), and VaR for RB (bottom,\nright) from January 2007 to December 2016\n37\n\n0.1\n0.2\n0.3\n0.4\n0.5\n2008\n2010\n2012\n2014\n2016\nyear\nratio\n0.02\n0.04\n0.06\n0.08\n2008\n2010\n2012\n2014\n2016\nyear\nratio\n0.00\n0.05\n0.10\n0.15\n2008\n2010\n2012\n2014\n2016\ndate\n\u2206CoVaR for CL\n0.00\n0.05\n0.10\n0.15\n2008\n2010\n2012\n2014\n2016\ndate\n\u2206CoVaR fo RB\nFigure 14: Illustration of evolution of (\u03b11n+\u03b11w)/2\u03b21 (top, left), (\u03b12n+\u03b12w)/2\u03b22 (top, right),\n\u2206CoVaR1|2\nt\n(bottom, left), and \u2206CoVaR2|1\nt\n(bottom, right) where R1\nt and R2\nt are given by\nthe CL and RB daily returns at time t, respectively, from January 2007 to December 2016\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nRB\nCL\nFigure 15: Scatter plot of \u02c6ut, \u02c6vt using the ARMA(1,1)-TGARCH(1,1) model with skewed-t\ndistribution\n38",
    "chunk_index": 34,
    "start_char": 84135,
    "end_char": 86752,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "(\u03b11n+\u03b11w)/2\u03b21 (top, left), (\u03b12n+\u03b12w)/2\u03b22 (top, right),\n\u2206CoVaR1|2\nt\n(bottom, left), and \u2206CoVaR2|1\nt\n(bottom, right) where R1\nt and R2\nt are given by\nthe CL and RB daily returns at time t, respectively, from January 2007 to December 2016\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nRB\nCL\nFigure 15: Scatter plot of \u02c6ut, \u02c6vt using the ARMA(1,1)-TGARCH(1,1) model with skewed-t\ndistribution\n38\n\n\u2212400\n\u2212300\n\u2212200\n\u2212100\n2008\n2010\n2012\n2014\n2016\nyear\nt\nGaussian\nGumbel\nClayton\nAIC\n\u2212300\n\u2212200\n\u2212100\n2008\n2010\n2012\n2014\n2016\nyear\nt\nGaussian\nGumbel\nClayton\nBIC\nFigure 16: Results of AIC (left) and BIC (right) measures for Gaussian, Student t, Gumbel,\nand Clayton copulas from January 2007 to December 2016\nFigure 17: Comparison of \u03b1s and \u03b1c for CL (left) and RB (right) futures prices with maturity\nin February 2016\n0.6\n0.7\n0.8\n0.9\n2008\n2010\n2012\n2014\n2016\nyear\nrho\ns.e.\nGaussian copula\n0.55\n0.65\n0.75\n0.85\n0.95\n2008\n2010\n2012\n2014\n2016\nyear\nrho\ns.e.\nt copula\n2\n3\n4\n2008\n2010\n2012\n2014\n2016\nyear\ndelta\ns.e.\nGumbel copula\n0\n1\n2\n3\n4\n5\n2008\n2010\n2012\n2014\n2016\nyear\ntheta\ns.e.\nClayton copula\nFigure 18: The evolution of the estimated dependence parameter \u03b8 of CL and RB futures\nprices using Gaussian copula (top, right), Student t copula (top, left), Gumbel copula (bottom,\nleft), and Clayton copula (bottom, right) with the standard error (red dotted line) from\nJanuary 2007 to December 2016\n39\n\nFigure 19: The evolution of the estimated degree of freedom parameter \u03bd in the Student t\ncopula for the return pairs of CL and RB futures prices from January 2007 to December 2016\n0.00\n0.05\n0.10\n0.15\n0.20\n2008\n2010\n2012\n2014\n2016\ndate\nCoVaR for CL conditional on distressed RB\n0.00\n0.05\n0.10\n0.15\n0.20\n2008\n2010\n2012\n2014\n2016\ndate\nCoVaR for RB conditional on distressed CL\n0.00\n0.02\n0.04\n0.06\n2008\n2010\n2012\n2014\n2016\ndate\nCoVaR for CL conditional on normal RB\n0.00\n0.02\n0.04\n0.06\n2008\n2010\n2012\n2014\n2016\ndate\nCoVaR for RB conditional on normal CL\nFigure 20: Illustration of time-varying one-day CoVaR1|2\n95%,t (top, left), \u2206CoVaR2|1\n95%,t (top,\nright), CoVaR1|2,\u03b1=50%\n95%,t\n(bottom, left), and \u2206CoVaR2|1,\u03b1=50%\n95%,t\n(bottom, right) where R1\nt and\nR2\nt are given by the CL and RB daily returns at time t, respectively, from January 2007 to\nDecember 2016\n40",
    "chunk_index": 35,
    "start_char": 86357,
    "end_char": 88601,
    "paper_title": "Systemic Risk in Market Microstructure of Crude Oi",
    "paper_category": "q-fin.TR",
    "paper_filename": "Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Systemic_Risk_in_Market_Microstructure_of_Crude_Oi.pdf"
  },
  {
    "text": "The Hybrid Forecast of S&P 500 Volatility ensembled from VIX, GARCH and LSTM models\nNatalia Roszyk1 and Robert \u00b4Slepaczuk2\n1University of Warsaw, Faculty of Economic Sciences, Quantitative Finance Research Group, Ul. D\u0142uga\n44/50, 00-241 Warsaw, Poland.\n2University of Warsaw, Faculty of Economic Sciences, Department of Quantitative Finance and Machine\nLearning, Quantitative Finance Research Group, Ul. D\u0142uga 44/50, 00-241 Warsaw, Poland. ORCID:\nhttps://orcid.org/0000-0001-5527-2014, corresponding author: rslepaczuk@wne.uw.edu.pl\nAbstract\nPredicting the S&P 500 index\u2019s volatility is crucial for investors and financial analysts as it\nhelps assess market risk and make informed investment decisions. Volatility represents the level\nof uncertainty or risk related to the size of changes in a security\u2019s value, making it an essential\nindicator for financial planning. This study explores four methods to improve the accuracy of\nvolatility forecasts for the S&P 500: the established GARCH model, known for capturing his-\ntorical volatility patterns; an LSTM network that utilizes past volatility and log returns; a hybrid\nLSTM-GARCH model that combines the strengths of both approaches; and an advanced version\nof the hybrid model that also factors in the VIX index to gauge market sentiment. This analysis is\nbased on a daily dataset that includes data for S&P 500 and VIX index, covering the period from\nJanuary 3, 2000, to December 21, 2023. Through rigorous testing and comparison, we found that\nmachine learning approaches, particularly the hybrid LSTM models, significantly outperform the\ntraditional GARCH model. Including the VIX index in the hybrid model further enhances its\nforecasting ability by incorporating real-time market sentiment. The results of this study offer\nvaluable insights for achieving more accurate volatility predictions, enabling better risk manage-\nment and strategic investment decisions in the volatile environment of the S&P 500.\nKeywords: volatility forecasting, LSTM-GARCH, S&P 500 index, hybrid forecasting models,\nVIX index, machine learning, financial time series analysis, walk-forward process, hyperparame-\nters tuning, deep learning, recurrent neural networks.\nJEL Codes: C4, C45, C55, C65, G11\nNote: This research did not receive any specific grant from funding agencies in the public, commer-\ncial, or not-for-profit sectors.\n1\narXiv:2407.16780v1 [q-fin.TR] 23 Jul 2024\n\nIntroduction\nThe financial market\u2019s inherent volatility presents opportunities and challenges for investors\nand analysts, dictating the need for precise and adaptable forecasting models. Volatility, reflecting\nthe degree of variation in trading prices, is a critical measure of market risk and uncertainty. This\npaper embarks on an exploration of advanced forecasting methodologies to predict the S&P 500 in-\ndex\u2019s volatility, a benchmark for U.S. equity market performance. Through a comprehensive analysis\nincorporating time series and neural network models, this study aims to enhance the accuracy of\nvolatility forecasts, thereby offering valuable insights for strategic investment decision-making and\nrisk management.\nThe primary objective of this study was to examine the possibility of forecasting the volatility\nof the stock market, specifically the S&P 500 index, utilizing historical data and machine learning\ntechniques.\nThe key research hypotheses explored in this study include:\n\u2022 Research Hypothesis 1 (RH1): Market prices do not reflect all available information, impact-\ning the predictability of market movements.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3546,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "this study aims to enhance the accuracy of\nvolatility forecasts, thereby offering valuable insights for strategic investment decision-making and\nrisk management.\nThe primary objective of this study was to examine the possibility of forecasting the volatility\nof the stock market, specifically the S&P 500 index, utilizing historical data and machine learning\ntechniques.\nThe key research hypotheses explored in this study include:\n\u2022 Research Hypothesis 1 (RH1): Market prices do not reflect all available information, impact-\ning the predictability of market movements.\n\u2022 Research Hypothesis 2 (RH2): The GARCH model effectively identifies patterns of historical\nvolatility and serves as a benchmark for newer forecasting approaches.\n\u2022 Research Hypothesis 3 (RH3): LSTM networks outperform traditional models such as GARCH\nin forecasting the S&P 500 index\u2019s volatility.\n\u2022 Research Hypothesis 4 (RH4): A hybrid LSTM-GARCH model surpasses the performance\nof standalone LSTM and GARCH models.\n\u2022 Research Hypothesis 5 (RH5): The inclusion of VIX inputs enhances the accuracy of volatility\nforecasts.\n\u2022 Research Hypothesis 6 (RH6): The Local Interpretable Model-agnostic Explanations (LIME)\ntechnique enhances the interpretability of the hybrid LSTM-GARCH model with VIX input.\nSide Hypotheses for Sensitivity Testing:\n\u2022 Side Research Hypothesis 1 (sRH1): Changing the loss function from MSE to MAE will\nresult in more robust or less sensitive forecasts of market volatility.\n\u2022 Side Research Hypothesis 2 (sRH2): Replacing input Log Returns with Daily Percentage\nChanges will improve the accuracy of the volatility forecasts.\n\u2022 Side Research Hypothesis 3 (sRH3): Decreasing the sequence length to 5 days will affect\nthe model\u2019s ability to capture long-term dependencies in the data, potentially reducing forecast\naccuracy.\n\u2022 Side Research Hypothesis 4 (sRH4): Increasing the sequence length to 66 days will enhance\nthe model\u2019s capacity to capture long-term trends, thereby improving the accuracy of volatility\nforecasts.\n\u2022 Side Research Hypothesis 5 (sRH5): Reducing the number of LSTM layers to 1 will decrease\nthe model\u2019s complexity and might reduce its ability to capture complex patterns in the data.\n2\n\n\u2022 Side Research Hypothesis 6 (sRH6): Increasing the number of LSTM layers to 3 will en-\nhance the model\u2019s ability to learn more complex patterns, potentially increasing the forecasting\naccuracy.\n\u2022 Side Research Hypothesis 7 (sRH7): The performance of LSTM models is significantly im-\npacted by the choice of activation function, affecting both learning efficiency and forecasting\naccuracy.\nThis analysis is based on a dataset that includes data from the S&P 500 and the VIX index,\ncovering the period from January 3, 2000, to December 21, 2023. It consists of 2,252 data points, all\nof which have been obtained from Yahoo Finance.\nFour approaches were studied to enhance the accuracy of volatility predictions for the S&P\n500. These include the well-established GARCH model, recognized for its ability to detect historical\nvolatility patterns; an LSTM network that processes past volatility and log returns; a hybrid LSTM-\nGARCH model that merges the advantages of both methods; and an advanced iteration of the hybrid\nmodel, which incorporates the VIX index to assess market sentiment.",
    "chunk_index": 1,
    "start_char": 2977,
    "end_char": 6266,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "have been obtained from Yahoo Finance.\nFour approaches were studied to enhance the accuracy of volatility predictions for the S&P\n500. These include the well-established GARCH model, recognized for its ability to detect historical\nvolatility patterns; an LSTM network that processes past volatility and log returns; a hybrid LSTM-\nGARCH model that merges the advantages of both methods; and an advanced iteration of the hybrid\nmodel, which incorporates the VIX index to assess market sentiment.\nThis study introduces several novel contributions to the field of financial volatility forecasting.\nFirstly, it is among the first studies to examine the combined effects of LSTM and GARCH models\nthrough a hybrid approach, leveraging the strengths of both machine learning and traditional time\nseries econometric models to enhance prediction accuracy. Secondly, the incorporation of the VIX\nindex as an input in a hybrid model represents an innovative attempt to integrate market sentiment\ninto volatility forecasts. This approach not only improves the predictive accuracy but also deepens\nour understanding of how market emotions influence volatility. Additionally, the application of the\nLocal Interpretable Model-agnostic Explanations (LIME) technique to these complex models is a pi-\noneering effort to uncover the \u2019black box\u2019 nature of deep learning in finance. By providing clear,\ninterpretable insights into the decision-making processes of these models, this study not only ad-\nvances academic knowledge but also enhances the practical applicability of neural network models\nin financial decision-making. Collectively, these advancements push the boundaries of existing re-\nsearch and offer valuable frameworks for both future academic inquiries and real-world financial risk\nmanagement.\nThe paper is organized into seven main chapters. The first chapter introduces the key research\nhypotheses. The second chapter reviews existing research on predicting market volatility, looking at\nboth traditional models and newer, more complex neural network models. The third chapter briefly\ngoes over the modeling setup, describes the data used in this study, and explains how it was pre-\nprocessed for analysis. Additionally, it presents the error metrics utilized to gauge the accuracy of the\nmodel\u2019s predictions. The fourth chapter gets into the details of the methodology for the four different\nmodels used, covering how each model was chosen and how the parameters were tuned, how predic-\ntions were made on walk forward basis, and how the results were evaluated and compared. It ends\nwith a side-by-side comparison of the results from the different methods. The fifth chapter is about\ntesting how stable and reliable the results are by changing some of the parameters that were initially\ndecided on in the methodology. The sixth chapter demonstrates the application of LIME to enhance\nthe transparency and interpretability of the complex predictive model developed in this study. By\napplying LIME, this chapter offers a detailed example of how explanations of predictions can be gen-\nerated, making the LSTM-GARCH model with VIX inputs more understandable. The final chapter\nwraps up the research, looks at whether the initial hypotheses were correct based on what was found,\nand suggests what could be looked into in future research, pointing out new directions to take.\n3",
    "chunk_index": 2,
    "start_char": 5772,
    "end_char": 9144,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "methodology. The sixth chapter demonstrates the application of LIME to enhance\nthe transparency and interpretability of the complex predictive model developed in this study. By\napplying LIME, this chapter offers a detailed example of how explanations of predictions can be gen-\nerated, making the LSTM-GARCH model with VIX inputs more understandable. The final chapter\nwraps up the research, looks at whether the initial hypotheses were correct based on what was found,\nand suggests what could be looked into in future research, pointing out new directions to take.\n3\n\nI. LITERATURE REVIEW\nMarket volatility is a fundamental concept in finance that captures the degree of variation or\ndispersion of a financial instrument over time. It is often regarded as a critical indicator of risk and\nuncertainty within financial markets. The study of market volatility extends beyond merely tracking\nprice movements; it covers an understanding of the mechanisms that drive these fluctuations and\ntheir implications for investors, traders, and policymakers. Our choice to study market volatility\narises from its significant implications for investment strategies and economic policy, motivating us\nto further explore its predictive patterns and impacts.\nThe idea that market volatility is entirely unpredictable is challenged by empirical evidence,\nwhich identifies several consistent patterns. Volatility clustering is one such pattern, indicating that\nperiods of high or low volatility tend to follow each other. This concept significantly influenced the\ncreation of the ARCH and GARCH models by Engle [1982] and Bollerslev [1986], respectively.\nAnother key observation is the leverage effect, which shows an inverse relationship between stock\nprices and volatility, suggesting a complex interaction between the two [Black, 1976]. Moreover,\nvolatility exhibits long memory, meaning market shocks can affect future volatility for an extended\nperiod, and it displays seasonal trends influenced by trading activities and economic news. These\ncharacteristics, while indicating some level of predictability, underline the complexity of financial\nmarkets [Andersen and Bollerslev, 1998].\nTraditional approaches to estimating market volatility have primarily centered around historical\nand implied volatility measures. Historical volatility is calculated based on past price fluctuations of\nan asset, serving as a backward-looking indicator that reflects the degree of variation in trading prices\nover a certain period [Engle, 1982]. Despite its utility, historical volatility is limited by its reliance\non past data, failing to account for future market expectations. Implied volatility, on the other hand,\nis derived from the pricing of options and represents the market\u2019s forecast of a security\u2019s volatility\nover the life of the option [Black, 1976]. It is considered a forward-looking measure, capturing in-\nvestor sentiment and expectations about future volatility. The Chicago Board Options Exchange\u2019s\nVolatility Index (VIX), a popular implied volatility indicator, is often referred to as the \u2019fear index\u2019\ndue to its ability to reflect market uncertainty and investor sentiment [Whaley, 2000]. While these\ntraditional volatility estimates provide valuable insights, they are not without limitations. Historical\nvolatility does not necessarily predict future volatility and implied volatility can be influenced by\nfactors beyond mere expectations of future price movements, such as supply and demand dynamics\nfor options.",
    "chunk_index": 3,
    "start_char": 8577,
    "end_char": 12088,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "and expectations about future volatility. The Chicago Board Options Exchange\u2019s\nVolatility Index (VIX), a popular implied volatility indicator, is often referred to as the \u2019fear index\u2019\ndue to its ability to reflect market uncertainty and investor sentiment [Whaley, 2000]. While these\ntraditional volatility estimates provide valuable insights, they are not without limitations. Historical\nvolatility does not necessarily predict future volatility and implied volatility can be influenced by\nfactors beyond mere expectations of future price movements, such as supply and demand dynamics\nfor options. This has led researchers to explore more sophisticated models, such as the Generalized\nAutoregressive Conditional Heteroskedasticity (GARCH) model, which allows for varying volatility\nover time. The evolution of GARCH models has included model variations to better capture mar-\nket characteristics like the introduction of the Fractionally Integrated GARCH (FIGARCH) model,\nwhich addresses the long memory characteristic of volatility, providing a more accurate representa-\ntion of how past shocks can influence future volatility over extended periods [Baillie et al., 1996].\nAdditionally, the GJR-GARCH model by Glosten, Jagannathan, and Runkle integrates an asymmetry\ncomponent, better capturing the leverage effect where negative market returns lead to higher sub-\nsequent volatility compared to positive returns [Glosten et al., 1993]. Then, Awartani and Corradi\n[2005] performed a pairwise comparison of various GARCH-type models versus the GARCH(1,1)\nmodel. The findings proved the superior predictive performance of asymmetric GARCH models over\nthe GARCH(1,1) model for both one-day ahead and longer forecast horizons. Conversely, among\nmodels not accounting for asymmetries, GARCH(1,1) showcased enhanced predictive capabilities,\nunderlining the complex dynamics of volatility forecasting and the pivotal role of model features like\nasymmetry in achieving forecasting accuracy.\nRecent advancements in machine learning have significantly improved the prediction accuracy\n4\n\nof financial market volatility. Particularly, Long Short-Term Memory (LSTM) networks, a type of\nrecurrent neural network, have shown promising results in capturing the temporal dependencies and\nnonlinear patterns in market data. The LSTM\u2019s ability to process data sequences makes it particularly\nsuitable for time-series forecasting tasks, such as predicting the volatility of the S&P 500 index. Chris-\ntensen et al. [2021] demonstrate the efficacy of LSTM models in forecasting one-day-ahead volatility\nof the Dow Jones Industrial Average (DJIA), comparing their performance against traditional mod-\nels like the Heterogeneous AutoRegressive (HAR) model and other machine learning approaches,\nincluding regularization and tree-based algorithms. The study highlights LSTMs\u2019 superior ability to\nmodel complex dynamics in financial markets, owing to their architecture that can remember infor-\nmation over long periods and dynamically adjust to new information. This is particularly relevant\nfor financial markets where volatility clustering and leverage effects present challenging forecasting\nconditions. Furthermore, a comprehensive examination of machine learning models in algorithmic\ntrading strategies has shown that diverse approaches, such as Neural Networks, K Nearest Neighbor,\nRegression Trees, Random Forests, Na\u00efve Bayes classifiers, Bayesian Generalized Linear Models,\nand Support Vector Machines, have been successful in forecasting market volatility across various eq-\nuity indices, including the S&P 500 [Grudniewicz and \u00b4Slepaczuk, 2023].",
    "chunk_index": 4,
    "start_char": 11490,
    "end_char": 15125,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "markets where volatility clustering and leverage effects present challenging forecasting\nconditions. Furthermore, a comprehensive examination of machine learning models in algorithmic\ntrading strategies has shown that diverse approaches, such as Neural Networks, K Nearest Neighbor,\nRegression Trees, Random Forests, Na\u00efve Bayes classifiers, Bayesian Generalized Linear Models,\nand Support Vector Machines, have been successful in forecasting market volatility across various eq-\nuity indices, including the S&P 500 [Grudniewicz and \u00b4Slepaczuk, 2023]. These models were found\nparticularly adept at utilizing technical analysis indicators to predict volatility, contributing to en-\nhanced trading signals. The empirical evidence suggests that Linear Support Vector Machines and\nBayesian Generalized Linear Models, in particular, provided the best risk-adjusted returns, thereby\naffirming the potential of integrating sophisticated machine learning models for financial market fore-\ncasting. Among the neural network architectures studied, recurrent neural networks, including Long\nShort-Term Memory (LSTM) networks and Nonlinear Autoregressive Networks with Exogenous In-\nputs (NARX), stand out. Research by Bucci (2020) demonstrates that these advanced neural network\nmodels surpass traditional econometric techniques in forecasting accuracy. The enhanced capability\nof LSTM and NARX networks to recognize and model long-range dependencies offers a significant\nadvantage, particularly in environments marked by high volatility, thereby positioning them as highly\neffective tools for predictive tasks in finance [Bucci, 2020]. Exploring the effectiveness of machine\nlearning in financial forecasting, Rahimikia and Poon (2020) dive into the predictive capabilities of\nthese technologies specifically for realized volatility. Their comprehensive study assesses the perfor-\nmance of machine learning models using diverse data sets, including those typically associated with\nHeterogeneous Autoregressive (HAR) models, data from the limit order book (LOB), and news senti-\nment analysis. Remarkably, after training and evaluating 3.7 million models, the findings suggest that\nmachine learning approaches, especially those managing high-dimensional data across multiple time\nlags, consistently surpass traditional HAR models in non-extreme volatility conditions. This indicates\na superior ability of machine learning models to adapt and respond to evolving market dynamics over\ntime, highlighting their potential to enhance forecasting accuracy in financial markets [Rahimikia\nand Poon, 2020]. Jia and Yang [2021] particularly focus on improving forecasting precision by in-\ntegrating a likelihood-based loss function within deep neural network (DNN) and Long Short-Term\nMemory (LSTM) models. Their research contrasts traditional econometric approaches with modern\nmachine learning techniques, demonstrating that the deep learning models equipped with this novel\nloss function outperform standard econometric models and traditional deep learning models using a\ndistance loss function. This finding is critical as it confirms the potential of tailored loss functions in\nenhancing model performance in highly volatile market conditions, making a significant contribution\nto the literature on financial volatility forecasting.\nMoreover, recent studies have explored the integration of GARCH models with machine learn-\ning techniques to enhance predictive accuracy. Research by Kumar and Thenmozhi [2020], Hansen\nand Lunde [2021] on combining GARCH models with Artificial Neural Networks (ANN) and Long\nShort-Term Memory (LSTM) networks has shown promising results in capturing complex patterns in\nvolatility data that traditional models may miss. These hybrid approaches leverage the time-varying\nvolatility modeling capabilities of GARCH models and the pattern recognition strengths of machine\n5",
    "chunk_index": 5,
    "start_char": 14574,
    "end_char": 18461,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "volatile market conditions, making a significant contribution\nto the literature on financial volatility forecasting.\nMoreover, recent studies have explored the integration of GARCH models with machine learn-\ning techniques to enhance predictive accuracy. Research by Kumar and Thenmozhi [2020], Hansen\nand Lunde [2021] on combining GARCH models with Artificial Neural Networks (ANN) and Long\nShort-Term Memory (LSTM) networks has shown promising results in capturing complex patterns in\nvolatility data that traditional models may miss. These hybrid approaches leverage the time-varying\nvolatility modeling capabilities of GARCH models and the pattern recognition strengths of machine\n5\n\nlearning algorithms, offering a powerful toolkit for forecasting market volatility. Furthermore, in the\nstudy by Amirshahi and Lahmiri [2023], the efficacy of integrating GARCH-type models with deep\nlearning approaches for volatility forecasting across diverse markets was rigorously examined. This\nresearch shows how features extracted from GARCH-type models, when utilized as inputs for deep\nlearning models like DFFNN and LSTM, significantly boost predictive performance. The integration\nof Long Short-Term Memory (LSTM) networks with GARCH-type models represents a significant\nadvancement in the prediction of financial market volatility. Kim and Won [2018] developed a hy-\nbrid model that combines the strengths of LSTM networks in capturing long-term dependencies and\nGARCH-type models in modeling time-varying volatility. Their study utilized the KOSPI 200 index\ndata to demonstrate that the hybrid model, particularly the GEW-LSTM model combining LSTM with\nthree GARCH-type models (GARCH, EGARCH, and EWMA), significantly outperforms traditional\nGARCH models and other machine learning approaches in predicting stock price index volatility. The\nfindings underscore the potential of leveraging machine learning techniques alongside econometric\nmodels to enhance the predictive accuracy of market volatility.\nThe Chicago Board Options Exchange Volatility Index (VIX), often referred to as the \u2019fear\ngauge\u2019 plays a pivotal role in market volatility prediction. Fleming, Ostdiek, and Whaley (1995)\nlaid the groundwork by illustrating the dynamic relationship between the VIX and market returns,\nhighlighting its utility in forecasting market volatility [Fleming et al., 1995]. Building on this foun-\ndation, Whaley [2000] underscored the significance of the VIX as a leading indicator for market\nstress and investor uncertainty, demonstrating its predictive capacity for future market movements.\nFurther exploring the integration of VIX data into volatility models, Todorov and Bollerslev [2010]\nenhanced the prediction of volatility by incorporating market expectations, thereby providing a more\nnuanced understanding of market dynamics. These studies collectively affirm the VIX\u2019s crucial role\nin providing insights into future market volatility, serving as an invaluable tool for investors, portfo-\nlio managers, and policymakers in navigating financial markets. Based on the recent study, Bhandari\net al. [2022] leveraged the power of artificial intelligence and machine learning, specifically utilizing a\nLong Short-Term Memory (LSTM) network, to forecast the S&P 500 index\u2019s next-day closing prices.\nThey meticulously selected a combination of nine predictors, such as market data, macroeconomic\ndata, and technical indicators to construct a holistic view of the stock market\u2019s dynamics. Their\nresearch compared single-layer and multilayer LSTM models across various performance metrics,\nconcluding that single-layer models offer superior accuracy and fit.",
    "chunk_index": 6,
    "start_char": 17775,
    "end_char": 21429,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "the recent study, Bhandari\net al. [2022] leveraged the power of artificial intelligence and machine learning, specifically utilizing a\nLong Short-Term Memory (LSTM) network, to forecast the S&P 500 index\u2019s next-day closing prices.\nThey meticulously selected a combination of nine predictors, such as market data, macroeconomic\ndata, and technical indicators to construct a holistic view of the stock market\u2019s dynamics. Their\nresearch compared single-layer and multilayer LSTM models across various performance metrics,\nconcluding that single-layer models offer superior accuracy and fit. Notably, the VIX was included\namong the predictors, underscoring its significance in capturing market volatility insights.\nConsidering the pivotal role of volatility in financial decision-making, this research aims to\nstudy the efficacy of standard econometric models and contemporary machine learning techniques in\npredicting the volatility of the S&P 500 index. The integration of traditional models with advanced\npredictive techniques allows for a comprehensive examination of volatility\u2019s dynamic nature. Addi-\ntionally, this study explores the application of the Local Interpretable Model-agnostic Explanations\n(LIME) technique on the best-performing model, aiming to enhance the interpretability of machine\nlearning predictions in financial markets, thereby bridging the gap between complex model outputs\nand actionable financial insights. Hence, guided by the insights from previous studies, our research\ncontrasts the traditional GARCH model with complex LSTM models, which utilize a diverse array of\npredictors as outlined in Table1, to forecast the S&P 500\u2019s volatility.\nII. MODELLING SETUP\nThis research incorporates a multifaceted approach to estimate the volatility of S&P 500, em-\nploying time series as well as a neural network model with a variety of data inputs to capture the com-\nplexity and nuances of market behavior. This includes the analysis of S&P 500 log returns, which\n6\n\nserve as a fundamental gauge of market movement; the examination of S&P 500 lagged volatility,\nproviding insights into historical volatility patterns; the utilization of GARCH model predictions, of-\nfering forward-looking estimates of S&P 500 volatility for a given day; and the incorporation of VIX\nprices, known as the market\u2019s \u2019fear gauge\u2019 to assess expected market volatility.\n2.1. Data inputs\nThe analysis within this study is predicated upon the daily data of the S&P 500, encompassing\n6032 data points spanning from January 3, 2000, to December 21, 2023, sourced from Yahoo Finance.\nThis dataset serves as the foundation for extracting pivotal variables critical for the analysis and\npredictive modeling of financial market behaviors, as highlighted in Table 1. Specifically, the study\nemploys two principal variables derived from the S&P 500 data: Log Returns and Lagged Volatility.\nLog returns, illustrated in Figure 1 and Figure 2, formulated in Equation 1, compute the log-\narithmic difference between consecutive closing prices of the S&P 500. This metric is preferred in\nfinancial analyses for its capability to normalize and facilitate comparison of price movements over\ntime. Conversely, (Lagged) Volatility, shown in Figure 3 and Figure 4, outlined in Equation 3, cap-\ntures historical volatility trends within the market, offering insights into past market dynamics and\nserving as a predictor of potential future volatility and market sentiment.",
    "chunk_index": 7,
    "start_char": 20842,
    "end_char": 24291,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "in Figure 1 and Figure 2, formulated in Equation 1, compute the log-\narithmic difference between consecutive closing prices of the S&P 500. This metric is preferred in\nfinancial analyses for its capability to normalize and facilitate comparison of price movements over\ntime. Conversely, (Lagged) Volatility, shown in Figure 3 and Figure 4, outlined in Equation 3, cap-\ntures historical volatility trends within the market, offering insights into past market dynamics and\nserving as a predictor of potential future volatility and market sentiment.\nFurther enhancing the study\u2019s analytical depth, GARCH volatility predictions are integrated\nas inputs into the LSTM-GARCH model, as discussed in Section 3.1. The inclusion of GARCH-\nderived volatility predictions allows the model to emulate the temporal evolution of market uncer-\ntainty, thereby crafting a prediction framework that accounts for financial markets\u2019 intrinsic volatility.\nAdditionally, the study leverages the VIX index (Figure 5 and Figure 6) to gain insights into\ninvestor sentiment and anticipated market volatility. As elaborated in Section 3.4, the VIX input\ninfuses the models with a prospective view on volatility, derived from S&P 500 index options, hence\nintroducing a forward-looking element to the predictive analyses. This integration ensures the models\nnot only rely on historical data but also consider the market\u2019s current expectations, enhancing the\nprecision and relevance of the forecasts.\nTable 1. Summary of Variables Used in the Models\nGroup\nVariable\nUsed in Model\nS&P 500\nLog Returns\nLSTM,\nLSTM-GARCH,\nLSTM-GARCH with VIX input\nS&P 500\nLagged Volatility\nLSTM,\nLSTM-GARCH,\nLSTM-GARCH with VIX input\nGARCH\nGARCH volatility estimates\nLSTM-GARCH\nLSTM-GARCH with VIX input\nVIX\nVIX implied volatility\nLSTM-GARCH with VIX input\nNote: This table enumerates all the variables utilized across the four models analyzed in this study: log returns, lagged volatility,\nGARCH volatility estimates, and VIX implied volatility as discussed above in Section 2.1. The log returns and lagged volatility are\nderived from the S&P 500 dataset. GARCH volatility estimates are calculated as described in Section 3.1. VIX index close prices are\nused as the VIX implied volatility variable.\nTable 2 presents the descriptive statistics for close prices and log returns. The normality of a\n7\n\ndistribution is often assessed by the proximity of its mean to the median. The discrepancy between\nthe mean and the median indicates a potential skew in the distribution of close prices. On the other\nhand, a closer mean to the median for log returns suggests a distribution with a tendency toward\nnormality. Extreme values, as indicated by the distance between the maximum and minimum as well\nas the 25th and 75th percentiles, imply a distribution with heavy tails, thus higher kurtosis. The\nmean log return is 0.0002, indicating a marginal average daily increase in asset price, with a standard\ndeviation of 0.0124, which reflects the extent of return volatility. The range of log returns spans from\na minimum of \u22120.1277 to a maximum of 0.1096, showcasing significant fluctuations in asset price\nmovements. Quartile values further delineate the distribution, with 25%, 50% (median), and 75%\npercentiles located at \u22120.0049, 0.0006, and 0.0059, respectively.",
    "chunk_index": 8,
    "start_char": 23745,
    "end_char": 27051,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "25th and 75th percentiles, imply a distribution with heavy tails, thus higher kurtosis. The\nmean log return is 0.0002, indicating a marginal average daily increase in asset price, with a standard\ndeviation of 0.0124, which reflects the extent of return volatility. The range of log returns spans from\na minimum of \u22120.1277 to a maximum of 0.1096, showcasing significant fluctuations in asset price\nmovements. Quartile values further delineate the distribution, with 25%, 50% (median), and 75%\npercentiles located at \u22120.0049, 0.0006, and 0.0059, respectively.\nThe log returns distribution\u2019s normality was challenged through the Shapiro-Wilk test, which\nyielded a statistic of 0.9005 and a p-value of 0.0, indicating a slight deviation from a normal distri-\nbution. These results are confirmed by measures of skewness and kurtosis, with values of \u22120.37859\nand 10.29295 respectively, indicating a left-skewed distribution and a leptokurtic shape, which are\ncharacteristic of financial return data with its pronounced tails and peak.\nTable 2. Descriptive Statistics of Close Prices and Log Returns of S&P500\nStatistic\nClose Prices\nLog Returns\nCount\n6014\n6054\nMean\n1985.29\n0.0002\nStandard Deviation\n1072.75\n0.0124\nMinimum\n676.53\n-0.1277\n25% Percentile\n1191.15\n-0.0049\n50% Percentile (Median)\n1462.46\n0.0006\n75% Percentile\n2624.44\n0.0059\nMaximum\n4894.16\n0.1096\nShapiro-Wilk Statistic\n0.66\n0.90\nShapiro-Wilk p-value\n0.00\n0.00\nNote: Panel A presents the descriptive statistics for close prices, showing a discrepancy between the mean and the median, indicating\na potential skew in the distribution. Panel B presents the descriptive statistics for log returns. The closer mean to median in log returns\n(compared to close prices) suggests a distribution with tendencies toward normality, despite significant fluctuations and heavy tails.\nThe Shapiro-Wilk test results confirm a deviation from normal distribution, indicated by the skewness and kurtosis measures. However,\nsuch characteristics are typical for financial returns data.\n8\n\nFigure 1. S&P 500 Log Returns Distribution\nNote: This figure compares the distribution of the S&P 500 log\nreturns (period from January 3, 2000, to December 21, 2023)\nwith normal distribution.\nFigure 2. S&P 500 Log Returns Over Time\nNote: This figure presents the time series of log-returns of the S&P\n500, mapping the return fluctuations over time. It serves to indicate\nthe volatility and the temporal patterns in the log returns across the\nobserved period from January 3, 2000, to December 21, 2023.\nFigure 3. S&P 500 Volatility Distribution\nNote: This figure compares the distribution of the S&P 500\nvolatility (period from January 3, 2000, to December 21, 2023)\nwith normal distribution.\nFigure 4. S&P 500 Volatility Over Time\nNote: This figure presents the time series of the volatility of the\nS&P 500, mapping the fluctuations over time. It serves to indicate\nthe changes in volatility across the observed period from January 3,\n2000, to December 21, 2023.\nFigure 5. VIX Distribution\nNote: This figure compares the distribution of VIX (period from\nJanuary 3, 2000 to December 21, 2023) with normal distribution.\nFigure 6. VIX Over Time\nNote: This figure presents the time series of VIX across the ob-\nserved period from January 3, 2000 to December 21, 2023.\n9",
    "chunk_index": 9,
    "start_char": 26494,
    "end_char": 29784,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "series of the volatility of the\nS&P 500, mapping the fluctuations over time. It serves to indicate\nthe changes in volatility across the observed period from January 3,\n2000, to December 21, 2023.\nFigure 5. VIX Distribution\nNote: This figure compares the distribution of VIX (period from\nJanuary 3, 2000 to December 21, 2023) with normal distribution.\nFigure 6. VIX Over Time\nNote: This figure presents the time series of VIX across the ob-\nserved period from January 3, 2000 to December 21, 2023.\n9\n\n2.2. Estimation of log returns and volatility\nThe log returns of S&P 500 on a day t are calculated as a natural logarithm of the ratio of the\nclosing price on day t to the closing price on the previous day (t \u22121).\nLog_Returnst = log\n\u0012 Closet\nCloset\u22121\n\u0013\n(1)\nThen, the historical volatility estimate on day t is calculated as the standard deviation of log\nreturns over a specified rolling window.\nVolatilityt =\ns\n1\nN \u22121\nt\n\u2211\ni=t\u2212N+1\n(Log_Returnst \u2212\u00b5)2\n(2)\nwhere:\n\u2022 N denotes the 22 trading days window, a model time frame commonly approximating a trading\nmonth,\n\u2022 \u00b5 is the mean average of the log returns over the rolling window.\nNext, the lagged volatility for a given day is calculated by shifting the volatility values by a\nspecified number of lag days. For a lag of 1 day, the calculation can be represented as follows:\nLagged_Volatilityt = Volatilityt\u22121\n(3)\nwhere:\n\u2022 Volatilityt\u22121 represents the volatility on the previous day (t \u22121).\n\u2022 Lagged_Volatilityt represents the lagged volatility value assigned to day t.\n2.3. Data standardization\nSince standardization is a key step before inputting data into the LSTM models, the Min-Max\nscaling method was used to normalize the raw input data. This normalization process is applied\ndynamically to each prediction window within our time series dataset, ensuring that our model is\ntrained on data that reflects the most recent conditions. The formula for Min-Max scaling is given by:\nXscaled =\nX \u2212Xmin\nXmax \u2212Xmin\n(4)\nwhere: X is the original value, Xmin is the minimum value in the feature column, Xmax is the\nmaximum value in the feature column and Xscaled is the resulting normalized value.\nThe scalers are fitted on the current training data segment. This process recalculates the min-\nimum and maximum values used for scaling, based on the training data available up to the current\nprediction point. After fitting, the training, validation, and subsequent test datasets are transformed\nusing these dynamically adjusted scalers. The model is either trained or loaded from previous states,\n10\n\nand predictions are made on the normalized test data. Post-prediction, the output is inversely trans-\nformed to revert it back to the original data scale for accurate error calculation and comparison.\n2.4. Error Metrics\nError metrics are crucial for evaluating the performance of predictive models, providing quan-\ntitative measures of the accuracy of the predictions. Two commonly used error metrics are the Mean\nAbsolute Error (MAE) and the Root Mean Squared Error (RMSE).",
    "chunk_index": 10,
    "start_char": 29286,
    "end_char": 32301,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "either trained or loaded from previous states,\n10\n\nand predictions are made on the normalized test data. Post-prediction, the output is inversely trans-\nformed to revert it back to the original data scale for accurate error calculation and comparison.\n2.4. Error Metrics\nError metrics are crucial for evaluating the performance of predictive models, providing quan-\ntitative measures of the accuracy of the predictions. Two commonly used error metrics are the Mean\nAbsolute Error (MAE) and the Root Mean Squared Error (RMSE).\n2.4.1. Mean Absolute Error (MAE)\nThe Mean Absolute Error (MAE) is a measure of the average magnitude of the errors in a set of\npredictions, without considering their direction. It calculates the average of the absolute differences\nbetween the predicted values and the actual values. The MAE is given by the equation:\nMAE = 1\nn\nn\n\u2211\ni=1\n|yi \u2212\u02c6yi|\n(5)\nwhere:\n\u2022 n is the number of observations,\n\u2022 yi is the actual value for the ith observation,\n\u2022 \u02c6yi is the predicted value for the ith observation,\n\u2022 |\u00b7| denotes the absolute value.\nThe MAE provides a straightforward measure of prediction error magnitude, with lower values\nindicating better model performance.\n2.4.2. Root Mean Squared Error (RMSE)\nThe Root Mean Squared Error (RMSE) quantifies the square root of the average of the squared\ndifferences between the predicted and actual values. Unlike MAE, RMSE gives more weight to larger\nerrors due to the squaring of the error terms. The RMSE is defined as:\nRMSE =\ns\n1\nn\nn\n\u2211\ni=1\n(yi \u2212\u02c6yi)2\n(6)\nwhere the variables are as previously defined. RMSE is sensitive to outliers and tends to penal-\nize large errors more heavily than MAE. Lower RMSE values denote higher accuracy of the predictive\nmodel.\nBoth MAE and RMSE are scale-dependent metrics, meaning their values are influenced by the\nscale of the data. They are best used for comparative purposes\u2014to compare the performance of dif-\nferent models or model configurations on the same dataset.\n11\n\nIII. MODELLING APPROACHES\n3.1. GARCH\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model, introduced\nby Bollerslev (1986), extends the foundational Autoregressive Conditional Heteroskedasticity (ARCH)\nmodel proposed by Engle (1982). The GARCH model articulates the conditional variance as a linear\nfunction of past squared values of the series, enabling it to adeptly capture the volatility clustering\nphenomenon prevalent in financial markets. Such a pattern, characterized by periods of high volatility\nfollowed by similar periods, highlights the GARCH model\u2019s efficacy in financial time series analysis\n[Bollerslev, 1986, Engle, 1982].\n3.1.1. Model Selection and Parameterization\nAs described by [Francq and Zako\u00efan, 2010], the GARCH model is defined by its two parame-\nters, p and q, which govern the number of lagged conditional variances and squared shocks, respec-\ntively, included in the volatility equation. Specifically, q represents the order of the ARCH terms,\ncapturing the short-term volatility due to recent shocks, while p represents the order of the GARCH\nterms, encapsulating the volatility\u2019s persistence across time.",
    "chunk_index": 11,
    "start_char": 31776,
    "end_char": 34908,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "As described by [Francq and Zako\u00efan, 2010], the GARCH model is defined by its two parame-\nters, p and q, which govern the number of lagged conditional variances and squared shocks, respec-\ntively, included in the volatility equation. Specifically, q represents the order of the ARCH terms,\ncapturing the short-term volatility due to recent shocks, while p represents the order of the GARCH\nterms, encapsulating the volatility\u2019s persistence across time. The model\u2019s conditional variance \u03c32\nt is\nexpressed through these parameters as follows:\n\u03c32\nt = \u03c9 +\nq\n\u2211\ni=1\n\u03b1i\u03b52\nt\u2212i +\np\n\u2211\nj=1\n\u03b2 j\u03c32\nt\u2212j\n(7)\nwhere \u03c9 is the intercept term, \u03b1i are the coefficients of the ARCH component, and \u03b2j are\nthe coefficients of the GARCH component. The backshift operator B simplifies the representation,\nallowing for the compact expression of the model as:\n\u03c32\nt = \u03c9 +\u03b1(B)\u03b52\nt +\u03b2(B)\u03c32\nt\n(8)\nwith the polynomials \u03b1(B) and \u03b2(B) defined as \u03b1(B) = \u2211q\ni=1 \u03b1iBi and \u03b2(B) = \u2211p\nj=1 \u03b2jBj, re-\nspectively. In the special case where \u03b2(z) = 0, the GARCH model simplifies to an ARCH model with\nno autoregressive conditional variances.\nTo identify the optimal parameters for the GARCH(p,q) model in forecasting financial mar-\nket volatility, an exhaustive exploration of parameter combinations was undertaken. Parameters p\nand q were varied within the range from 0 to 4 in a systematic loop. This analysis pinpointed the\nGARCH(2,2) model as the optimal choice, marked by the lowest Akaike Information Criterion (AIC)\nscore of 16750.227, hence balancing fitting accuracy with model simplicity effectively. The Akaike\nInformation Criterion is a measure used for model selection where lower values suggest a better model\nfit, adjusted for the number of parameters used, as introduced by Akaike [1974]. It is calculated as\nAIC = 2k \u22122ln(L), where k is the number of parameters and L is the maximum likelihood of the\nmodel. The model is mathematically expressed as:\n\u03c32\nt = \u03c9 +\u03b11\u03b52\nt\u22121 +\u03b12\u03b52\nt\u22122 +\u03b21\u03c32\nt\u22121 +\u03b22\u03c32\nt\u22122\n(9)\n12\n\n3.1.2. GARCH Volatility Forecasting Methodology\nThe forecast method uses a GARCH(2,2) model to predict future changes in the S&P 500\nindex\u2019s volatility.\nThe analysis initiates with S&P 500 index data, particularly focusing on daily log returns (as\nper Equation 1) for their advantageous properties in financial modeling Tsay [2010]. Log returns are\npreferred for their additivity over time, which allows for straightforward aggregation of returns across\ntime intervals. Additionally, log returns are often more homoskedastic, providing a more consistent\nvariance that is conducive to the assumptions underlying various volatility models. The symmetry and\nnormalization features of log returns, along with their tendency to align closer to a normal distribution,\nfurther validate their use in predictive financial models.\nFollowing the model\u2019s parameterization, the dataset was segmented into an initial training pe-\nriod (January 1, 1985, to January 2, 2000) and a testing period (January 3, 2000, to December 21,\n2023). The prediction approach involves updating the training set with the most recent data at each\nstep, thereby allowing for the prediction of the subsequent day\u2019s (t +1) volatility.",
    "chunk_index": 12,
    "start_char": 34456,
    "end_char": 37632,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "assumptions underlying various volatility models. The symmetry and\nnormalization features of log returns, along with their tendency to align closer to a normal distribution,\nfurther validate their use in predictive financial models.\nFollowing the model\u2019s parameterization, the dataset was segmented into an initial training pe-\nriod (January 1, 1985, to January 2, 2000) and a testing period (January 3, 2000, to December 21,\n2023). The prediction approach involves updating the training set with the most recent data at each\nstep, thereby allowing for the prediction of the subsequent day\u2019s (t +1) volatility. At every iteration,\nthe model is trained on the current dataset to forecast the volatility for the next day. This process\nnot only ensures that the model remains dynamically aligned with the latest market trends but also\nenables a robust evaluation of the model\u2019s predictive accuracy in a real-time context.\nFor the GARCH model to deliver trustworthy estimates and predictions, the time series data\nneed to be stationary, which means its statistical properties, like mean and variance, don\u2019t change\nover time. We applied the Augmented Dickey-Fuller (ADF) test to the scaled log returns to check\nfor stationarity. The ADF test\u2019s null hypothesis suggests that the time series has a unit root and is\nnon-stationary. With a test statistic of \u221233.4127 and a p-value of 0.0, we have strong evidence to\nreject the null hypothesis, confirming that the data is indeed stationary.\n3.1.3. GARCH Results\nThe performance of the GARCH model\u2019s volatility predictions can be visually assessed in Fig-\nure 7, where the model\u2019s predictions are plotted alongside the actual market volatility over a testing\nperiod from February 13, 2015, to December 21, 2023. Generally, the predicted volatility, represented\nby the solid blue line, tends to be higher than the actual market volatility, shown with a dashed line,\nacross most periods. This trend of overestimation is particularly pronounced during periods of market\nstress, such as the significant spike observed around 2020. While this conservative bias in volatility\nforecasting could be beneficial for risk management by preparing for potential higher volatility, it\nalso highlights an overarching tendency of the model to overestimate, especially evident in capturing\nextreme market movements, suggesting areas for improvement in its predictive accuracy.\nA quantitative assessment of the model\u2019s out-of-sample prediction accuracy is presented in\nTable 3. The MAE was 1.56 \u00d7 10\u22123 and the RMSE was 2.39 \u00d7 10\u22123. These error metrics are rela-\ntively low, which corroborates the visual assessment that the GARCH model predictions are in close\nagreement with the observed volatility, although the model\u2019s performance during extreme market\nconditions could be further examined and improved.\n13\n\nFigure 7. GARCH Out of Sample prediction\nNote: This figure presents the GARCH Out of Sample prediction over the period February 13, 2015 to December 21, 2023.\nTable 3. GARCH Out-of-Sample Error Metrics\nModel\nMAE\nRMSE\nGARCH\n1.56 \u00d710\u22123\n2.39 \u00d710\u22123\nNote: This table presents the GARCH Out of Sample Error Metrics, for walk-forward predictions on a t+1 basis over the period from\nFebruary 13, 2015 to December 21, 2023.",
    "chunk_index": 13,
    "start_char": 37022,
    "end_char": 40269,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "the model\u2019s performance during extreme market\nconditions could be further examined and improved.\n13\n\nFigure 7. GARCH Out of Sample prediction\nNote: This figure presents the GARCH Out of Sample prediction over the period February 13, 2015 to December 21, 2023.\nTable 3. GARCH Out-of-Sample Error Metrics\nModel\nMAE\nRMSE\nGARCH\n1.56 \u00d710\u22123\n2.39 \u00d710\u22123\nNote: This table presents the GARCH Out of Sample Error Metrics, for walk-forward predictions on a t+1 basis over the period from\nFebruary 13, 2015 to December 21, 2023.\n3.2. LSTM\nLong Short-Term Memory (LSTM) networks, introduced by Hochreiter and Schmidhuber [1997],\nmark a pivotal advancement in recurrent neural network (RNN) architectures, enabling the learning\nof order dependencies in sequence prediction problems. Unlike conventional time-series forecasting\nmethods, LSTMs excel in capturing and retaining information across extended sequences of data.\nThis capability makes them particularly well-suited for analyzing financial time series data, which is\noften characterized by noise and non-stationarity.\nLSTMs stand out for their memory cells, which capture long-term dependencies and temporal\ndynamics inherent in financial market volatility. Unlike traditional RNNs, LSTMs have a complex\narchitecture that includes multiple gates to control the flow of information, addressing some of the\nkey challenges that RNNs face, such as the difficulty in making use of distant information and the\nvanishing gradient problem.\nIn the LSTM cell architecture the input at the current timestep, denoted as xt, and the hidden\nstate from the preceding timestep, ht\u22121, are aggregated and fed into three critical gates: the forget\ngate, the input gate, and the output gate. These gates regulate the flow of information through the cell\n14\n\nby selectively updating the cell state and the hidden state based on the inputs they receive.\n\u2022 Forget Gate: Here, the activation function is given by ft = \u03c3(Uf \u00b7 xt +Vf \u00b7 ht\u22121 + b f ), where\nft signifies the activation output of the forget gate, and bf , Uf , and Vf represent the biases,\ninput weights, and recurrent weights, respectively. This sigmoid function, outputting values\nbetween 0 and 1, determines the degree to which information from the previous cell state, Ct\u22121,\nis retained or discarded in the new cell state, with the updated cell state being C\u2032\nt = ft \u00b7Ct\u22121.\n\u2022 Input Gate: This gate determines the new information to be incorporated into the cell state\nfrom the current input and the previous hidden state. The process involves two steps: calculating\nit = \u03c3(Ui \u00b7xt +Vi \u00b7ht\u22121 +bi) to decide which parts of the new information are significant, and\nthen forming a new candidate cell state, C+\nt = tanh(Uc \u00b7 xt +Vc \u00b7 ht\u22121 + bc). The cell state is\nupdated as Ct = C\u2032\nt +it \u00b7C+\nt , blending the old and new information.\n\u2022 Output Gate: Employing a sigmoid function, ot = \u03c3(Uo \u00b7 xt +Vo \u00b7 ht\u22121 + bo), this gate influ-\nences the update of the hidden state for the current timestep.",
    "chunk_index": 14,
    "start_char": 39754,
    "end_char": 42725,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "and\nthen forming a new candidate cell state, C+\nt = tanh(Uc \u00b7 xt +Vc \u00b7 ht\u22121 + bc). The cell state is\nupdated as Ct = C\u2032\nt +it \u00b7C+\nt , blending the old and new information.\n\u2022 Output Gate: Employing a sigmoid function, ot = \u03c3(Uo \u00b7 xt +Vo \u00b7 ht\u22121 + bo), this gate influ-\nences the update of the hidden state for the current timestep. It uses inputs from the previous\nhidden state and the current input to determine how the current cell state, Ct, influences the new\nhidden state, ht = ot \u00b7 tanh(Ct), thus deciding what portion of the cell state is conveyed in the\noutput.\nFigure 8. Long Short-Term Memory Model Architecture\nNote: Illustration from \"A Review on the Long Short-Term Memory Model\" by Van Houdt et al. (2020), showcasing the architecture\nof an LSTM network. Reprinted under \u00a9 Springer Nature B.V. 2020.\n3.2.1. Model Selection and Parameterization\nTo enhance the LSTM model\u2019s predictive accuracy for financial market volatility, a comprehen-\nsive hyperparameter tuning process was conducted on a dataset spanning fifteen years, from 2000 to\n2015. The Keras Tuner library in Python, implementing a RandomSearch strategy, was used for this\noptimization. This methodical approach involved setting the search to execute 50 trials, each with\n3 executions, to ensure the reliability of the results. Additionally, EarlyStopping was employed to\nprevent model overfitting, optimizing the model\u2019s performance while safeguarding against the loss of\ngeneralizability.\n15\n\nThe tuning process included: the number of layers (1, 2, or 3), the number of neurons in the\nLSTM layers (32, 64, or 128), activation functions for the LSTM layers (\u2019tanh\u2019 and \u2019relu\u2019 activations),\ndropout rates (varied from 0.0 to 0.3 in increments of 0.1), learning rates for the Adam optimizer (0.01,\n0.001, 0.0001), loss function (MSE or MAE).\nTable 4. Hyperparameter search space of LSTM model\nHyperparameter\nValue Range\nNumber of Layers in the LSTM network\n1, 2, 3\nNumber of Neurons in the LSTM layers\n32, 64, 128\nActivation Functions for the LSTM layers\n\u2019tanh\u2019, \u2019relu\u2019\nDropout Rates\n0.0, 0.1, 0.2, 0.3\nLearning Rates for the Adam Optimizer\n0.01, 0.001, 0.0001\nLoss function\nMSE, MAE\nNote: This table presents hyperparameters search space for LSTM model architecture. RandomSearch was used with max trials 50,\nexecution per trial 3, epochs 50, and with an early stopping mechanism with patience 10. The search was performed on the 15 years of\ndata, from January 2000 to January 2015.\nFollowing the tuning phase, the architecture selected for the LSTM network comprised two\nLSTM layers, accompanied by a dense output layer activated by a ReLU function. Given the com-\nplexity of LSTM models and the considerable duration required for their execution, the research was\nconstrained to employing a single, optimally tuned LSTM model for the entire testing period due to\ntime and computing power limitations. Table 5 shows the final configuration of hyperparameters for\nthe LSTM model.\nTable 5.",
    "chunk_index": 15,
    "start_char": 42396,
    "end_char": 45358,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "two\nLSTM layers, accompanied by a dense output layer activated by a ReLU function. Given the com-\nplexity of LSTM models and the considerable duration required for their execution, the research was\nconstrained to employing a single, optimally tuned LSTM model for the entire testing period due to\ntime and computing power limitations. Table 5 shows the final configuration of hyperparameters for\nthe LSTM model.\nTable 5. Final Hyperparameter Configuration of LSTM Model\nHyperparameter\nConfiguration\nNumber of Layers in the LSTM network\n2\nNumber of neurons in the first hidden layer\n128\nNumber of neurons in the second hidden layer\n128\nActivation function for the first hidden layer\n\u2019tanh\u2019\nActivation function for the second hidden layer\n\u2019tanh\u2019\nDropout rate after first hidden layer\n0.1\nDropout rate after second hidden layer\n0.1\nLearning rate for the Adam optimizer\n0.001\nLoss function\nMSE\nEpochs\n100\nBatch size\n64\nNote: This table presents the final LSTM architecture that was selected during hyperparameters tuning on the 15 years of data, from\nJanuary 2000 to January 2015.\n3.2.2. LSTM Volatility Forecasting Methodology\nThe tuned LSTM architecture includes two LSTM layers with 128 neurons each. The first\nlayer includes a \u2019tanh\u2019 activation function and a recurrent dropout of 0.1, while the second layer does\n16\n\nnot return sequences and also employs a \u2019tanh\u2019 activation. After each LSTM layer, a dropout of\n0.1 is applied to prevent overfitting. The network concludes with a dense output layer activated by\nReLU, known for its ability to maintain non-linearity in output while ensuring positive predictions,\nan essential characteristic for forecasting metrics like volatility that are inherently positive.\nThe dataset utilized for forecasting volatility comprises log returns (Equation 1) and lagged\nvolatility (Equation 3) measures. Volatility, as defined in Equation 2, is set as the target variable.\nData preprocessing involves scaling the feature variables with a Min-Max Scaler (Equation 4). The\nscaled features and target are then structured into sequences to be LSTM-friendly, with a lookback\nperiod of 22-time steps. The 22-day lookback period reflects the approximate number of trading\ndays in a month, considering financial markets usually operate five days a week. This standard is\nused to align model inputs with monthly market cycles effectively. To enhance reproducibility across\ncomputational environments, random seeds were consistently set across various libraries. The model\nwas compiled using the Adam optimizer combined with a mean squared error loss function. This\nconfiguration is particularly well-suited for the regression nature of volatility forecasting, aiming to\nminimize discrepancies between actual and predicted values.\nTo mitigate the risk of overtraining- a common issue where the model overly specializes on the\ntraining data to the detriment of its generalization to unseen data- an early stopping mechanism was\nintegrated into the training routine. This technique monitors the model\u2019s performance on a validation\nset, stopping the training process if there is no improvement in predictive accuracy for a predefined\nnumber of epochs, set in this case to 10.",
    "chunk_index": 16,
    "start_char": 44938,
    "end_char": 48136,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "of overtraining- a common issue where the model overly specializes on the\ntraining data to the detriment of its generalization to unseen data- an early stopping mechanism was\nintegrated into the training routine. This technique monitors the model\u2019s performance on a validation\nset, stopping the training process if there is no improvement in predictive accuracy for a predefined\nnumber of epochs, set in this case to 10. This parameter, known as the patience parameter, strikes a\nbalance between sufficient model training and the prevention of overfitting, ensuring that the model\nremains generalizable to new market conditions.\nA walk-forward methodology (Figure 9) was employed as the foundation of the model training\nprocess. Initially, the training dataset consisted of 12 years of data, totaling 3,024 days (12 \u00d7 252\ndays), and the initial validation set was comprised of 756 days (3 \u00d7 252 days). The model was refitted\nevery 252 observations, which is a common practice reflecting the typical number of trading days in\na year. This approach ensures that the model\u2019s weights are consistently updated with the most recent\nmarket data, enhancing its responsiveness to new information. Furthermore, to integrate new observa-\ntions without retraining from scratch, the model incrementally adds them, maintaining an up-to-date\ndataset. Predictions are then generated for the immediate future time step (t + 1), aligning the fore-\ncasting process with practical, short-term market predictions. This process not only maintained the\nrelevance of the model to current market conditions but also enhanced the robustness of the forecasts\nby continuously integrating new information into the training and validation phases.\nThe model\u2019s predictive performance is assessed through two key metrics: the MAE and the\nRMSE. Both metrics offer a straightforward evaluation of prediction quality by averaging the discrep-\nancies between predicted and actual values. Specifically, MAE calculates the average of the absolute\ndifferences, providing a clear measure of the magnitude of prediction errors, while RMSE squares\nthese differences before averaging, thus giving more weight to larger errors and offering insight into\nthe variance of the predictions.\n17\n\nFigure 9. Walk Forward Validation\nNote: Initially, the training dataset consisted of 12 years, or 3,024 days (12 \u00d7 252 days), and the validation set included 756 days (3 \u00d7\n252 days). The model was refitted every 252 days, which is the size of one walk-forward window. New observations were incrementally\nadded to the datasets to ensure currency without full retraining. Predictions were generated for the next day (t +1).\n3.2.3. LSTM Results\nThe LSTM model\u2019s performance in predicting volatility was assessed over a time span from\nFebruary 13, 2015, to December 21, 2023, for out-of-sample observations. The graph provided in\nFigure 10 illustrates the model\u2019s predicted values in comparison with the actual market volatility\nvalues. The green solid line represents the LSTM predictions, while the red dashed line denotes the\nactual values.\nThe LSTM model\u2019s predictions, as visualized in Figure 10, exhibit a high degree of closeness to\nthe actual volatility trend across the observed period.",
    "chunk_index": 17,
    "start_char": 47716,
    "end_char": 50952,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "over a time span from\nFebruary 13, 2015, to December 21, 2023, for out-of-sample observations. The graph provided in\nFigure 10 illustrates the model\u2019s predicted values in comparison with the actual market volatility\nvalues. The green solid line represents the LSTM predictions, while the red dashed line denotes the\nactual values.\nThe LSTM model\u2019s predictions, as visualized in Figure 10, exhibit a high degree of closeness to\nthe actual volatility trend across the observed period. Notably, during the high volatility peak in 2020,\nthe model accurately mirrors the surge in market volatility, underscoring its responsiveness to market\ndynamics and its ability to capture sudden market movements. However, upon closer examination,\nparticularly during more stable periods such as the span from 2017 to 2018, the model appears to\noverestimate the volatility. The predictions in these intervals show more fluctuations compared to\nthe actual values, which are relatively smooth. This overestimation suggests that the model may be\nsensitive to minor variations in the input data, resulting in a higher perceived volatility.\n18\n\nFigure 10. LSTM Out-of-sample prediction\nNote: This figure presents the LSTM Out of Sample prediction over the period February 13, 2015 to December 21, 2023.\nTo quantify the performance of the LSTM model, two error metrics were calculated: MAE\nof 1.24 \u00d7 10\u22123 and a RMSE of 1.55 \u00d7 10\u22123. The results are presented in Table 6. These low error\nmetrics underscore the model\u2019s effectiveness in closely tracking actual market trends, including during\nperiods of significant volatility.\nTable 6. Out-of-Sample Error Metrics for LSTM Model\nModel\nMAE\nRMSE\nLSTM\n1.24 \u00d710\u22123\n1.55 \u00d710\u22123\nNote: This table presents the LSTM Out of Sample Error Metrics, for walk-forward predictions on a t+1 basis over the period from\nFebruary 13, 2015, to December 21, 2023.\n3.3. LSTM-GARCH\nThe hybrid LSTM- GARCH approach enhances the predictive accuracy of volatility forecasting\nmodels by extending the LSTM framework to integrate GARCH model predictions as an additional\nindependent variable. This methodology, referred to as hybrid LSTM-GARCH, builds on the founda-\ntional LSTM model, which initially incorporates log returns and lagged volatility as predictors. The\ninclusion of GARCH predictions is based on the hypothesis that the combination of LSTM models\u2019\nmemory capabilities with the volatility modeling strengths of GARCH can yield a more nuanced\nunderstanding of market dynamics.\n19\n\n3.3.1. LSTM-GARCH Volatility Forecasting Methodology\nThe LSTM-GARCH model represents a sophisticated hybrid forecasting approach, specifically\ndesigned for financial market volatility. This methodology integrates the strengths of both GARCH\nand LSTM models to enhance prediction accuracy for future market volatility on the day t +1.\nInitially, the GARCH model, detailed in Section 3.1.2, is employed to generate preliminary\nvolatility forecasts for day t + 1. These forecasts leverage the GARCH model\u2019s proficiency in cap-\nturing short-term volatility patterns, serving as an initial estimation. Subsequently, these GARCH-\ngenerated forecasts are utilized as integral components of the LSTM-GARCH model\u2019s input dataset.\nAlong with log returns and lagged volatility values, the GARCH predictions enrich the LSTM-\nGARCH model\u2019s feature set.",
    "chunk_index": 18,
    "start_char": 50470,
    "end_char": 53798,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "on the day t +1.\nInitially, the GARCH model, detailed in Section 3.1.2, is employed to generate preliminary\nvolatility forecasts for day t + 1. These forecasts leverage the GARCH model\u2019s proficiency in cap-\nturing short-term volatility patterns, serving as an initial estimation. Subsequently, these GARCH-\ngenerated forecasts are utilized as integral components of the LSTM-GARCH model\u2019s input dataset.\nAlong with log returns and lagged volatility values, the GARCH predictions enrich the LSTM-\nGARCH model\u2019s feature set. By incorporating the GARCH model\u2019s volatility predictions for day\nt + 1 into the LSTM-GARCH model, this hybrid approach aims to significantly enhance the predic-\ntive performance.\n3.3.2. LSTM- GARCH Results\nThe performance of the LSTM-GARCH model in predicting market volatility was evaluated\nover the period extending from February 13, 2015, to December 21, 2023. In Figure 11, the model\u2019s\nforecasts are compared with the actual market volatility. The solid line illustrates the predictions\ngenerated by the LSTM-GARCH model, while the dashed line represents the actual market volatility.\nAs observed in Figure 11, the LSTM-GARCH model predictions demonstrate a high correlation\nwith the actual market volatility trajectory throughout the considered timespan. This is especially\nprominent during the market volatility peak of 2020, wherein the model precisely reflects the sharp\nincrease in volatility, showcasing its acute sensitivity to rapid market shifts and its capability to model\ndrastic changes in market conditions effectively.\nNevertheless, a closer look shows that the LSTM-GARCH model, much like the earlier LSTM\nmodel (Section 3.2.3), sometimes predicts higher volatility than what actually happens. This is espe-\ncially true for the second half of 2022 and the first half of 2023, where the model\u2019s predictions show\nmore ups and downs compared to the actual, more consistent market trends. This similarity suggests\nthat the LSTM-GARCH model, like its LSTM counterpart, might be too responsive to small changes\nin the data, leading to predictions of higher volatility than is realistic.\n20\n\nFigure 11. LSTM- GARCH Out-of-sample prediction\nNote: This figure presents the LSTM- GARCH Out of Sample prediction over the period February 13, 2015 to December 21, 2023.\nTo quantify the performance of the LSTM-GARCH model, two error metrics were calculated:\nMAE of 1.01\u00d710\u22123 and an RMSE of 1.31\u00d710\u22123. The results are presented in Table 7.\nTable 7. Out-of-Sample Error Metrics for GARCH, LSTM, and LSTM-GARCH Models\nModel\nMAE\nRMSE\nLSTM-GARCH\n1.01 \u00d710\u22123\n1.31 \u00d710\u22123\nNote: This table presents the LSTM- GARCH Out of Sample Error Metrics, for walk-forward predictions on a t+1 basis over the period\nfrom February 13, 2015, to December 21, 2023.\n3.4. LSTM- GARCH with VIX input\nBuilding upon the hybrid LSTM-GARCH framework, the fourth model in this study, referred\nto as the LSTM-GARCH with VIX input, introduces an additional feature to further refine volatility\npredictions\u2014the closing prices of the Volatility Index (VIX). The VIX, often referred to as the \u2019fear\nindex\u2019 is a real-time market index representing the market\u2019s expectations for volatility over the coming\n30 days [Exchange].",
    "chunk_index": 19,
    "start_char": 53276,
    "end_char": 56491,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "21, 2023.\n3.4. LSTM- GARCH with VIX input\nBuilding upon the hybrid LSTM-GARCH framework, the fourth model in this study, referred\nto as the LSTM-GARCH with VIX input, introduces an additional feature to further refine volatility\npredictions\u2014the closing prices of the Volatility Index (VIX). The VIX, often referred to as the \u2019fear\nindex\u2019 is a real-time market index representing the market\u2019s expectations for volatility over the coming\n30 days [Exchange].\nThe methodological integrity of the LSTM-GARCH approach is preserved in this enhanced\nmodel, ensuring that the addition of VIX data complements rather than complicates the forecasting\nprocess. By extending the model to include VIX closing prices, the LSTM-GARCH with VIX in-\nput model aims to capture not only the quantitative aspects of market data but also the qualitative\nsentiment reflected by VIX, thus promising a more holistic approach to market volatility forecasting.\n21\n\n3.4.1. LSTM-GARCH with VIX input Volatility Forecasting Methodology\nInitially, the standard GARCH model processes historical volatility data to produce prelimi-\nnary forecasts for day t + 1. Concurrently, the VIX closing prices (Figure 6), alongside log returns\n(Equation 1) and lagged volatility (Equation 3), are fed into the LSTM model, enriching the set of\npredictive features. The LSTM model then synthesizes the GARCH-derived volatility predictions\nwith the VIX-informed insights to generate a consolidated forecast for a day t +1.\nThe introduction of VIX closing prices is expected to enhance the model\u2019s capability to capture\nbroader market moods and stress levels, potentially leading to more accurate anticipations of sudden\nmarket shifts. Given the VIX\u2019s status as a gauge of investor sentiment, its inclusion in the LSTM-\nGARCH model is hypothesized to offer additional depth to the predictive signals, especially in the\nface of market uncertainty.\n3.4.2. LSTM-GARCH with VIX input Results\nVisual inspection of the forecasted versus actual volatility values (Figure 12) highlights the\nLSTM-GARCH with the VIX input model\u2019s proficiency in capturing volatility spikes, mirroring the\nsignificant peaks observed in the 2020 market stress period. The model\u2019s forecasts show an elevated\nalignment with the actual volatility, signifying an improved response to market changes with the\naddition of VIX data.\nFigure 12 also illustrates the model\u2019s ability to smooth out predictions during stable periods,\nsuggesting a mitigation of the overestimation tendency noted in the earlier models. This is indicative\nof a more balanced sensitivity to input data fluctuations, thanks to the informative nature of VIX\npricing.\nFigure 12. LSTM- GARCH with VIX input Out-of-sample prediction\nNote: This figure presents the LSTM- GARCH with VIX input Out of Sample prediction over the period February 13, 2015 to December\n21, 2023.\nTo quantify the performance of the LSTM-GARCH model with VIX input, two error metrics\n22\n\nwere calculated: MAE of 1.02 \u00d7 10\u22123 and an RMSE of 1.30 \u00d7 10\u22123. The results are presented in\nTable 8.\nTable 8. Out-of-Sample Error Metrics for LSTM-GARCH with VIX Input Models\nModel\nMAE\nRMSE\nLSTM-GARCH with VIX Input\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nNote: This table presents the LSTM- GARCH with VIX input Out",
    "chunk_index": 20,
    "start_char": 56036,
    "end_char": 59283,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "prediction over the period February 13, 2015 to December\n21, 2023.\nTo quantify the performance of the LSTM-GARCH model with VIX input, two error metrics\n22\n\nwere calculated: MAE of 1.02 \u00d7 10\u22123 and an RMSE of 1.30 \u00d7 10\u22123. The results are presented in\nTable 8.\nTable 8. Out-of-Sample Error Metrics for LSTM-GARCH with VIX Input Models\nModel\nMAE\nRMSE\nLSTM-GARCH with VIX Input\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nNote: This table presents the LSTM- GARCH with VIX input Out of Sample Error Metrics, for walk-forward predictions on a t+1\nbasis over the period from February 13, 2015, to December 21, 2023.\n3.4.3. Models comparison\nThe improvements in the error metrics (Table 9 and Table 11) for the LSTM-GARCH with\nVIX input model, specifically the lower Root Mean Squared Error (RMSE), suggest a more accurate\nmodeling of market volatility, potentially due to the inclusion of VIX as a sentiment indicator. Com-\npared to the GARCH model, the LSTM-GARCH with VIX input model achieves a lower MAE and\nRMSE by approximately 34.62% and 46.03%, respectively, as the MAE reduces from 1.56\u00d710\u22123 to\n1.02\u00d710\u22123 and the RMSE decreases from 2.39\u00d710\u22123 to 1.30\u00d710\u22123. Relative to the LSTM model,\nthe LSTM-GARCH with VIX input model records a reduction in RMSE by approximately 16.13%,\nwith the LSTM model posting an RMSE of 1.55\u00d710\u22123. In contrast, there is a slight increase in MAE\ncompared to the LSTM-GARCH model, moving from 1.01\u00d710\u22123 to 1.02\u00d710\u22123, reflecting a minor\ndegradation of about -0.99%. This contrast indicates that while the inclusion of VIX improves the\nRMSE, indicating lower volatility errors, it slightly worsens the average error (MAE) by a small mar-\ngin. Additionally, the Mann-Whitney U tests (Table 10) confirm the statistical significance of these\nimprovements, highlighting that the differences in MAE are not only numerically but also statistically\nsignificant, lending further credence to the superiority of the LSTM-GARCH with VIX input model\nover the GARCH and LSTM models.\nTable 9. Error Metrics for Selected Models\nModel\nMAE\nRMSE\nGARCH\n1.56\u00d710\u22123\n2.39\u00d710\u22123\nLSTM\n1.24\u00d710\u22123\n1.55\u00d710\u22123\nLSTM-GARCH\n1.01\u00d710\u22123\n1.31\u00d710\u22123\nLSTM-GARCH with VIX input\n1.02\u00d710\u22123\n1.30\u00d710\u22123\nNote: This table presents the Out of Sample Error Metrics for our four models: GARCH, LSTM, LSTM-GARCH, and LSTM-GARCH\nwith VIX input. Each t+1 prediction was calculated on a walk-forward basis over the period from February 13, 2015, to December 21,\n2023.\n23\n\nTable 10. Statistical Comparison of MAE Using Mann-Whitney U Test\nModel Comparison\nU Statistic\np-value\nLSTM-GARCH with VIX input vs. GARCH\n1963057\n< 0.001\nLSTM-GARCH with VIX input vs. LSTM\n1991434\n< 0.001\nLSTM-GARCH with VIX input vs. LSTM-GARCH\n2487891\n0.257\nNote: This table presents the results of Mann-Whitney U tests comparing the MAE of the LSTM-GARCH with the VIX input model\nagainst other models. A p-value of less than 0.05 indicates a significant difference, suggesting one model has statistically better\nperformance. Significant results highlight the superior accuracy of the LSTM-GARCH with VIX input model over the GARCH and\nLSTM models.\nTable 11.",
    "chunk_index": 21,
    "start_char": 58827,
    "end_char": 61906,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "LSTM-GARCH with VIX input vs. LSTM-GARCH\n2487891\n0.257\nNote: This table presents the results of Mann-Whitney U tests comparing the MAE of the LSTM-GARCH with the VIX input model\nagainst other models. A p-value of less than 0.05 indicates a significant difference, suggesting one model has statistically better\nperformance. Significant results highlight the superior accuracy of the LSTM-GARCH with VIX input model over the GARCH and\nLSTM models.\nTable 11. Percentage Improvement of LSTM-GARCH with VIX Input Over Other Models\nCompared Model\n% Improvement in MAE\n% Improvement in RMSE\nGARCH\n34.62\n46.03\nLSTM\n17.74\n16.13\nLSTM-GARCH\n\u22120.99\n0.76\nNote: This table presents the percentage improvement in MAE and RMSE of the LSTM-GARCH with VIX input model over the\nother three models: GARCH, LSTM, LSTM-GARCH. Calculations are based on actual improvements recorded between the respective\nmodels, reflecting more accurate and updated data.\nQualitatively, the models differ in their approach to capturing market behavior. The GARCH\nmodel, while less complex, may not fully capture the nonlinear patterns present in financial time series\ndata. On the other hand, LSTM networks are known for their ability to learn long-term dependen-\ncies, giving them an edge in modeling the temporal aspects of market volatility. The LSTM-GARCH\nmodel aims to harness both these strengths, providing a nuanced understanding of market fluctuations.\nBy incorporating VIX as a measure of market sentiment, the LSTM-GARCH with VIX input model\nextends this capability further, potentially allowing for more informed predictions that consider both\nhistorical data and current market sentiment.\nTo deepen our understanding of predictive models\u2019 effectiveness across varied market condi-\ntions, a systematic analysis was performed by segmenting prediction data based on the volatility\npresent in actual market values. This segmentation, categorizing market conditions into four distinct\nquartiles\u2014Lowest, Low-Medium, Medium-High, and Highest\u2014enables an examination of model\nperformance across the entire spectrum of market dynamics. These categories were intentionally de-\nfined to span the full range of market volatilities, encompassing both periods of relative calm and\nphases of significant turbulence.\nFor each volatility quartile, two error metrics, the MAE and the RMSE, were used to evaluate\nthe models\u2019 performance (Table 12). By comparing these metrics across the quartiles, as detailed in\nthe accompanying tables, we can assess the predictive accuracy and consistency of each model under\ndifferent market scenarios.\nFor the Lowest Volatility Quartile, the LSTM-GARCH with VIX input model is identified as the\nbest performer with the lowest MAE and RMSE, suggesting its suitability for environments with min-\nimal market fluctuations. Moving to the Low-Medium Volatility Quartile, the LSTM-GARCH model\nachieves the lowest both MAE and RMSE, advocating its use in slightly more volatile conditions. In\nthe Medium-High Volatility Quartile, the LSTM-GARCH model once again shows the lowest values\nfor both MAE and RMSE, underscoring its robustness in handling medium to high market volatility\n24\n\neffectively. Finally, for the Highest Volatility Quartile, this LSTM-GARCH with VIX input model\noutperforms others in terms of both metrics, reinforcing its suitability for the most volatile market\nconditions.",
    "chunk_index": 22,
    "start_char": 61451,
    "end_char": 64830,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "LSTM-GARCH model\nachieves the lowest both MAE and RMSE, advocating its use in slightly more volatile conditions. In\nthe Medium-High Volatility Quartile, the LSTM-GARCH model once again shows the lowest values\nfor both MAE and RMSE, underscoring its robustness in handling medium to high market volatility\n24\n\neffectively. Finally, for the Highest Volatility Quartile, this LSTM-GARCH with VIX input model\noutperforms others in terms of both metrics, reinforcing its suitability for the most volatile market\nconditions. Overall, the evaluation showcases an ongoing battle between the LSTM-GARCH and\nthe LSTM-GARCH with VIX input models, with each model excelling in different volatility envi-\nronments. Across the volatility spectrum, the LSTM-GARCH with VIX input model consistently\npresents as the optimal choice for effectively predicting market movements, especially in low and\nhigh-volatility environments. Its balanced performance in terms of both MAE and RMSE across all\nquartiles makes it the preferred model for general application in varying market conditions.\nTable 12. Error Metrics by Volatility Quartile for All Models\nModel\nMAE\nRMSE\nLowest Volatility Quartile\nGARCH\n1.42 \u00d710\u22123\n1.53 \u00d710\u22123\nLSTM\n1.06 \u00d710\u22123\n1.19 \u00d710\u22123\nLSTM-GARCH\n7.95 \u00d710\u22124\n8.97 \u00d710\u22124\nLSTM-GARCH with VIX Input\n7.63 \u00d710\u22124\n8.73 \u00d710\u22124\nLow-Medium Volatility Quartile\nGARCH\n8.96 \u00d710\u22124\n1.17 \u00d710\u22123\nLSTM\n1.11 \u00d710\u22123\n1.30 \u00d710\u22123\nLSTM-GARCH\n8.62 \u00d710\u22124\n1.05 \u00d710\u22123\nLSTM-GARCH with VIX Input\n9.19 \u00d710\u22124\n1.07 \u00d710\u22123\nMedium-High Volatility Quartile\nGARCH\n1.20 \u00d710\u22123\n1.54 \u00d710\u22123\nLSTM\n1.26 \u00d710\u22123\n1.53 \u00d710\u22123\nLSTM-GARCH\n1.05 \u00d710\u22123\n1.33 \u00d710\u22123\nLSTM-GARCH with VIX Input\n1.08 \u00d710\u22123\n1.35 \u00d710\u22123\nHigh Volatility Quartile\nGARCH\n2.69 \u00d710\u22123\n4.07 \u00d710\u22123\nLSTM\n1.40 \u00d710\u22123\n1.95 \u00d710\u22123\nLSTM-GARCH\n1.32 \u00d710\u22123\n1.79 \u00d710\u22123\nLSTM-GARCH with VIX Input\n1.30 \u00d710\u22123\n1.73 \u00d710\u22123\nNote: This table presents Out of Sample Error Metrics by Volatility Quartile. The best values for MAE and RMSE in each quartile are\nhighlighted in bold. The Out of Sample observations were categorized based on volatility magnitude and they were divided into four\ndistinct quartiles: Lowest, Low-Medium, Medium-High, and High.\nAdditionally, the study employs a directional accuracy test across the four models, evaluating\ntheir ability to predict market trend directions over periods of 1, 5, and 22 days. This assessment\nquantifies each model\u2019s precision in forecasting the trend\u2019s course, thus providing a detailed view of\ntheir predictive power in short-, medium-, and longer-term market movements. It is crucial to note\nthat these models are fundamentally designed for regression tasks rather than the classification of\nvolatility direction. This distinction highlights their intent to predict continuous outcomes, providing\na deeper analytical framework for understanding market behaviors rather than merely categorizing\ntrend directions. Thus, the results from Table 13 are for informational purposes.\nShort-term (1-day) forecasts show higher accuracy for the LSTM and LSTM-GARCH with VIX\ninput models, suggesting their effectiveness in capturing immediate market trends. The accuracy gen-\nerally increases for the medium-term (5-day) forecasts, except for the LSTM model which performs\nslightly worse, indicating their robustness over slightly longer durations. For long-term (22-day)\n25",
    "chunk_index": 23,
    "start_char": 64312,
    "end_char": 67611,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "directions. Thus, the results from Table 13 are for informational purposes.\nShort-term (1-day) forecasts show higher accuracy for the LSTM and LSTM-GARCH with VIX\ninput models, suggesting their effectiveness in capturing immediate market trends. The accuracy gen-\nerally increases for the medium-term (5-day) forecasts, except for the LSTM model which performs\nslightly worse, indicating their robustness over slightly longer durations. For long-term (22-day)\n25\n\nforecasts, the accuracies tend to converge, with no model showing a distinct advantage, highlighting\nthe inherent complexities in long-duration market prediction, where LSTM\u2019s performance notably\ndeclines, reflecting the escalating difficulty in sustaining prediction accuracy over extended periods.\nTable 13. Directional Prediction Accuracy of Models for Different Periods\nModel\n1-Day (%)\n5-Day (%)\n1-Month (22 Days) (%)\nGARCH\n50.73\n53.58\n51.31\nLSTM\n55.31\n55.12\n49.39\nLSTM-GARCH\n54.97\n57.72\n50.77\nLSTM-GARCH with VIX Input\n55.02\n59.27\n51.94\nNote: This table presents the directional prediction accuracy for 1, 5, and 22 days ahead. It is crucial to note that these models\nare fundamentally designed for regression tasks rather than the classification of volatility direction, hence these results are only for\ninformational purposes.\nIV. SENSITIVITY ANALYSIS\nThe sensitivity of the LSTM-GARCH model with VIX input to its setup and parameters was\nclosely examined. This analysis specifically aimed to understand the impact of alterations in the\nloss function, sequence length, number of layers in the LSTM model, and the type of activation\nfunctions. In each scenario analyzed, a single parameter was modified with the remainder of the\nmodel\u2019s configuration held constant to isolate the effects of each change. The below scenarios were\nconsidered:\n\u2022 Use MAE as a loss function instead of MSE\n\u2022 Replace input Log Returns with Daily Percentage Changes\n\u2022 Sequence length\n\u2013 Decrease sequence length to 5 days\n\u2013 Increase sequence length to 66 days\n\u2022 LSTM Architecture Number of Layers\n\u2013 Reduce the number of layers to 1\n\u2013 Increase the number of layers to 3\n\u2022 ReLu Activation Function for both LSTM Layers\n26\n\nTable 14. Error Metrics for LSTM-GARCH with VIX Input Model Under Different Sensitivity Cases\nSensitivity Case\nMAE\nRMSE\nUse MSE as a loss function *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nUse MAE as a loss function\n1.02 \u00d710\u22123\n1.33 \u00d710\u22123\nLog Returns as Input *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nPercentage Change in Price as Input\n9.80 \u00d710\u22124\n1.26 \u00d710\u22123\nSequence length of 22 days *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nDecrease sequence length to 5 days\n8.90 \u00d710\u22124\n1.19 \u00d710\u22123\nIncrease sequence length to 66 days\n1.09 \u00d710\u22123\n1.38 \u00d710\u22123\nDual- Layer LSTM Architecture *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nSingle- Layer LSTM Architecture\n8.23 \u00d710\u22124\n1.15 \u00d710\u22123\nThree- Layer LSTM Architecture\n1.23 \u00d710\u22123\n1.50 \u00d710\u22123\nActivation function: 1st Layer- Tanh, 2nd Layer- Tanh *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nActivation function: 1st Layer- ReLU, 2nd Layer- ReLU\n1.16 \u00d710\u22123\n1.60 \u00d710\u22123\nNote: This table presents the Out of Sample Error Metrics for the chosen sensitivity cases for our best performing LSTM- GARCH\nwith VIX input model. The (*) denotes the setup that was used for the base model.",
    "chunk_index": 24,
    "start_char": 67149,
    "end_char": 70328,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "Architecture *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nSingle- Layer LSTM Architecture\n8.23 \u00d710\u22124\n1.15 \u00d710\u22123\nThree- Layer LSTM Architecture\n1.23 \u00d710\u22123\n1.50 \u00d710\u22123\nActivation function: 1st Layer- Tanh, 2nd Layer- Tanh *\n1.02 \u00d710\u22123\n1.30 \u00d710\u22123\nActivation function: 1st Layer- ReLU, 2nd Layer- ReLU\n1.16 \u00d710\u22123\n1.60 \u00d710\u22123\nNote: This table presents the Out of Sample Error Metrics for the chosen sensitivity cases for our best performing LSTM- GARCH\nwith VIX input model. The (*) denotes the setup that was used for the base model. The bolded font indicates values better than for the\nbase case scenario.\n4.1. Sensitivity- Using MAE as a loss function instead of MSE\nThe sensitivity analysis of the LSTM-GARCH with VIX input model with respect to the choice\nof the loss function is illustrated in Table 14 and Figure 13. When transitioning from the MSE to\nMAE as the loss function, a slight increase in both the MAE and RMSE is observed, suggesting that\nthe model is more precise with MSE. This could be attributed to MSE\u2019s propensity to heavily penal-\nize larger errors, thereby being more sensitive to outliers than MAE [Chai and Draxler, 2014]. The\nsubtleties of this effect are captured in the graphical representation (Figure 13), where the predictions\nunder MSE adherence exhibit a closer adherence to actual values. The findings underscore the signifi-\ncance of aligning the loss function with the underlying error distribution and the specific requirements\nof the predictive modeling task at hand.\n27\n\nFigure 13. LSTM-GARCH with VIX Input sensitivity to loss function\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to different loss functions: MAE (blue) and MSE\n(green) over the period February 13, 2015 to December 21, 2023.\n4.2. Sensitivity- Replacing Log Returns with Daily Percentage Changes\nThe LSTM-GARCH model\u2019s performance under various input data types was scrutinized, with\na focus on log returns and percentage changes in price. As illustrated in Figure 14, the model exhibited\nnotable consistency when leveraging log returns as input data. In contrast, the use of percentage\nchanges in price as input resulted in marginally lower MAE and RMSE, as can be seen in Table 9. This\nmarginal reduction indicates a potential preference for percentage changes in price over log returns\nwhen precision is paramount. However, this preference is model and context-specific, considering log\nreturns generally provide a more stable and normalized measure for financial time series data [Tsay,\n2010].\n28\n\nFigure 14. LSTM- GARCH with VIX input sensitivity to different forms of input data: Log Returns\nversus simple Percentage Changes in Price\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to different forms of input data: percentage change\nin price (green) and log returns (yellow) over the period February 13, 2015 to December 21, 2023.\n4.3. Sensitivity- Sequence Length\nA detailed assessment was carried out to examine how well the LSTM-GARCH with VIX\ninput model performs with different input sequence lengths, specifically comparing 5-day and 66-\nday periods against the chosen 22-day period.",
    "chunk_index": 25,
    "start_char": 69822,
    "end_char": 72965,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "versus simple Percentage Changes in Price\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to different forms of input data: percentage change\nin price (green) and log returns (yellow) over the period February 13, 2015 to December 21, 2023.\n4.3. Sensitivity- Sequence Length\nA detailed assessment was carried out to examine how well the LSTM-GARCH with VIX\ninput model performs with different input sequence lengths, specifically comparing 5-day and 66-\nday periods against the chosen 22-day period. The findings, presented in Table 14 and Figure 15,\nshowed that the model consistently provided accurate predictions across these variations, indicating\nits flexibility.\nShortening the sequence to 5 days decreased the MAE and RMSE, implying a rapid reaction\nto market volatility. Conversely, an extended 66-day sequence saw a marked increase in errors, in-\ndicating a potential dilution of predictive relevancy with overextended historical data incorporation.\nThis analysis underscores the importance of selecting an appropriate sequence length that reflects the\ndesired balance between responsiveness to recent market events and the incorporation of long-term\ntrends, an aspect further discussed in the survey by [Dama and Sinoquet, 2021].\n29\n\nFigure 15. LSTM-GARCH with VIX Input sensitivity to sequence length\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to sequence length: 66 days (green), 5 days\n(yellow), 22 days (blue) over the period February 13, 2015 to December 21, 2023.\n4.4. Sensitivity- Number of LSTM layers\nWe also studied how the architecture\u2019s number of LSTM layers influences the model\u2019s predic-\ntive accuracy. The foundational structure of the base model includes two LSTM layers, each with\n128 neurons and a \u2019tanh\u2019 activation function, interspersed with dropout layers set at a rate of 0.1 to\nmitigate overfitting. This dual-layer configuration is the benchmark against which other architectural\nmodifications were measured.\nResults from the sensitivity analysis, as shown in Table 14 and Figure 16, reveal that reducing\nthe model to a single-layer LSTM architecture not only improves computational efficiency but also\nenhances performance, with a decrease in MAE to 8.23 \u00d7 10\u22124 and RMSE to 1.15 \u00d7 10\u22123. This\nsuggests that one LSTM layer is sufficient for capturing the essential temporal dependencies in the\ndata, indicating that additional layers might lead to overfitting or unnecessary complexity without\nproportionate gains in accuracy. In contrast, expanding the model to include a third LSTM layer\nresults in elevated error metrics, with the MAE increasing to 1.23\u00d710\u22123 and RMSE to 1.50\u00d710\u22123.\nThis heightened error suggests that the additional layer could be introducing unnecessary complexity,\npotentially leading to overfitting and a diminished ability to generalize from the input data. The\nsensitivity analysis thus highlights the effectiveness of the single-layer LSTM model in outperforming\nthe multi-layer configurations by providing a more parsimonious, yet robust approach to modeling\nfinancial time series data.\n30\n\nFigure 16. LSTM- GARCH with VIX input sensitivity to the number of LSTM layers\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to the number of LSTM hidden layers: 3 layers\n(purple), 1 layer (green), 2 layers (yellow) over the period February 13, 2015 to December 21, 2023.",
    "chunk_index": 26,
    "start_char": 72436,
    "end_char": 75857,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "potentially leading to overfitting and a diminished ability to generalize from the input data. The\nsensitivity analysis thus highlights the effectiveness of the single-layer LSTM model in outperforming\nthe multi-layer configurations by providing a more parsimonious, yet robust approach to modeling\nfinancial time series data.\n30\n\nFigure 16. LSTM- GARCH with VIX input sensitivity to the number of LSTM layers\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to the number of LSTM hidden layers: 3 layers\n(purple), 1 layer (green), 2 layers (yellow) over the period February 13, 2015 to December 21, 2023.\n4.5. Sensitivity- Activation Functions\nIn analyzing the sensitivity of the LSTM-GARCH model to various activation functions, a dis-\ntinct impact on performance metrics is observed in Table 14 and Figure 17. The standard configura-\ntion utilizing hyperbolic tangent (tanh) activation functions for both LSTM layers results in a MAE\nof 1.02 \u00d7 10\u22123 and an RMSE of 1.30 \u00d7 10\u22123, which serves as a baseline for comparison. Modify-\ning both layers to a Rectified Linear Unit (ReLU) activation function leads to an increased MAE of\n1.16\u00d710\u22123 and RMSE of 1.60\u00d710\u22123. This reduction in prediction accuracy may be attributable to\nthe unbounded nature of ReLU, which can potentially result in gradient instability.\nThe tanh nonlinearity is particularly advantageous because its outputs are zero-centered, con-\ntributing to more predictable and stable gradient descent trajectories during training. Unlike tanh, the\nReLU function can cause gradients to either explode or vanish during backpropagation, particularly\nwhen dealing with deep networks or complex patterns. Moreover, the zero-centered nature of tanh\noutputs tends to yield more efficient learning progress in layers deep within a network. This char-\nacteristic of tanh is beneficial for modeling complex nonlinear relationships, such as those found in\nfinancial time series data, enhancing the reliability of the predictions without the risks associated with\nthe unbounded nature of ReLU [Goodfellow et al., 2016].\n31\n\nFigure 17. LSTM-GARCH with VIX Input sensitivity to the choice of activation function\nNote: This figure presents the sensitivity of LSTM- GARCH with VIX input model to the choice of activation function for the two\nhidden layers: ReLU/ Tanh (blue), ReLU/ ReLU (green), Tanh/ Tanh (yellow) over the period February 13, 2015 to December 21,\n2023.\nV. LSTM EXPLAINABILITY THROUGH LIME\nLocal Interpretable Model-agnostic Explanations (LIME) revolutionize the interpretability of\nmachine learning models, making the inner workings of complex models like Long Short-Term Mem-\nory (LSTM) networks more transparent. LIME, as introduced by [Ribeiro et al., 2016], approximates\ncomplex model predictions locally with interpretable models by generating and analyzing permuta-\ntions of input data. This approach, particularly beneficial for \u2019black box\u2019 models, illuminates the in-\nfluence of individual features on predictions, enhancing our understanding of model decisions. LIME\nis characterized by three foundational principles:\n1. Interpretable: It generates explanations that are easy for humans to understand, using simple\nmodels to approximate the predictions of potentially complex machine learning models.\n2.",
    "chunk_index": 27,
    "start_char": 75222,
    "end_char": 78527,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "locally with interpretable models by generating and analyzing permuta-\ntions of input data. This approach, particularly beneficial for \u2019black box\u2019 models, illuminates the in-\nfluence of individual features on predictions, enhancing our understanding of model decisions. LIME\nis characterized by three foundational principles:\n1. Interpretable: It generates explanations that are easy for humans to understand, using simple\nmodels to approximate the predictions of potentially complex machine learning models.\n2. Local Fidelity: LIME focuses on providing accurate explanations in the local context around\nthe prediction, ensuring that the explanations are faithful to what the model computes in the\nvicinity of the instance being examined.\n3. Model Agnostic: Its application is not limited by the type of machine learning model, making\nLIME versatile and widely applicable across various models and algorithms.\nLIME generates local surrogate models that closely mimic a complex model\u2019s behavior within a\nspecified vicinity, thereby revealing the contribution of each feature to the prediction. This is achieved\nthrough data permutation and observing prediction changes, which are then used to train a simpler\nmodel, such as Lasso or a decision tree. The mathematical essence of LIME, presented in Equation\n10, seeks to minimize the loss function L, representing the difference between the complex model\n32\n\nf and the interpretable model g, while maintaining the simplicity of the explanation model through\n\u2126(g).\nexplanation(x) = argmin\ng\u2208G L(f,g,\u03c0x)+\u2126(g)\n(10)\nHere, g is the interpretable model chosen to minimize L, such as mean squared error, which\nmeasures the approximation accuracy of the explanation to f\u2019s prediction. The complexity \u2126(g)\nis kept low to prefer simpler explanations. The set G encompasses all potential explanatory models,\nwith \u03c0x defining the proximity of sampled instances to the instance of interest, x. LIME\u2019s optimization\nprimarily addresses the loss, allowing users to adjust model complexity, for example, by limiting the\nnumber of features.\nIn applying LIME to our LSTM- GARCH with VIX input model for explainability, we first\npreprocessed our dataset for compatibility, scaling features, and reshaping data to fit LSTM\u2019s input\nstructure. For LIME\u2019s analysis, we adapted our LSTM\u2019s input by flattening the 3D time-sequenced\ndata into a 2D format. Utilizing the LimeTabularExplainer, we crafted a custom prediction function\nthat reshapes and scales input data accordingly, enabling LIME to probe our LSTM- GARCH with\nVIX input model effectively.\nAs a use case example, the visualization provided by LIME for February 13, 2015, (Figure 18)\nillustrates the impact of various features on the LSTM model\u2019s prediction of market volatility. The\n\u2019Predicted value\u2019 section (located on the left side in Figure 18) displays the predicted median value\nof volatility for the analyzed date, which is 2.10, with the prediction interval ranging from a mini-\nmum of 1.40 to a maximum of 2.96. The \u2019Negative and Positive\u2019 section (situated in the middle of\nFigure 18) identifies the features contributing to a decrease (left - negative) or increase (right - posi-\ntive) in the predicted value, along with their respective weights. These features have been identified\nas significantly influential for this specific local prediction on the selected day.",
    "chunk_index": 28,
    "start_char": 78016,
    "end_char": 81378,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "in Figure 18) displays the predicted median value\nof volatility for the analyzed date, which is 2.10, with the prediction interval ranging from a mini-\nmum of 1.40 to a maximum of 2.96. The \u2019Negative and Positive\u2019 section (situated in the middle of\nFigure 18) identifies the features contributing to a decrease (left - negative) or increase (right - posi-\ntive) in the predicted value, along with their respective weights. These features have been identified\nas significantly influential for this specific local prediction on the selected day. In detail, features\nsuch as 0.06 < t21 lagged_volatility, 0.06 < t14 Close_vix, and t19 log_returns \u22640.52 are shown\nto negatively impact the volatility prediction, indicating an association with lower volatility levels.\nConversely, 0.06 < t18 lagged_volatility has a positive impact on the predicted volatility, suggesting\na link with higher volatility. The \u2019Feature Value\u2019 section presents the actual values of the top features\nfor the specific date. For instance, the value of t21 lagged_volatility is 0.09, which is greater than\n0.06, thereby negatively influencing the predicted value as per the model\u2019s analysis.\n33\n\nFigure 18. LIME local explainability for February 13, 2015\nNote: This figure presents the LIME local explainability for February 13, 2015. On the left, the \u2019Predicted value\u2019 section shows the\npredicted median volatility of 2.10, with a range from 1.40 to 2.96. In the middle, the \u2019Negative and Positive\u2019 section highlights the\nfeatures that either decrease or increase the predicted volatility. On the right, the \u2019Feature Value\u2019 section lists the actual values of these\ninfluential features on the specific date.\nDespite the invaluable perspectives LIME provides on model interpretability, it is not without\nits limitations. The balance between simplicity and accuracy in the interpretable models used poses a\nsignificant challenge, as these models may not capture the full complexity of the original predictions.\nThis simplification can lead to partially accurate or oversimplified explanations. Moreover, the local\nnature of LIME\u2019s explanations limits their applicability to a broader data set or differing contexts,\nconstraining the generalizability of the insights offered. The assumptions underlying feature selec-\ntion and perturbation generation also introduce potential biases, further highlighting the importance\nof critical evaluation and understanding of LIME-generated explanations within specific application\ncontexts.\nVI. CONCLUSION\nThis paper provided a comprehensive examination of volatility forecasting for the S&P 500\nindex through the application and integration of various models, including GARCH, LSTM, hybrid\nLSTM-GARCH, and an advanced hybrid model incorporating VIX inputs. Each model was designed\nto leverage different aspects of the data, from historical volatility patterns captured by GARCH mod-\nels to deep learning\u2019s capability to recognize complex nonlinear patterns in the LSTM and hybrid\nmodels. The inclusion of the VIX index in the final hybrid model aimed to introduce market senti-\nment into the forecasting process, thereby enriching the models\u2019 contextual awareness and predictive\naccuracy.\nOur findings revealed that hybrid models, especially those incorporating VIX inputs, signif-\nicantly outperformed traditional GARCH models. These advanced models demonstrated superior\ncapability in managing the complexities inherent in financial time series data, showcasing enhanced\npredictive accuracy and robustness.",
    "chunk_index": 29,
    "start_char": 80835,
    "end_char": 84351,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "by GARCH mod-\nels to deep learning\u2019s capability to recognize complex nonlinear patterns in the LSTM and hybrid\nmodels. The inclusion of the VIX index in the final hybrid model aimed to introduce market senti-\nment into the forecasting process, thereby enriching the models\u2019 contextual awareness and predictive\naccuracy.\nOur findings revealed that hybrid models, especially those incorporating VIX inputs, signif-\nicantly outperformed traditional GARCH models. These advanced models demonstrated superior\ncapability in managing the complexities inherent in financial time series data, showcasing enhanced\npredictive accuracy and robustness. The integration of LSTM networks with GARCH models allowed\nfor an effective synthesis of short-term historical volatility and long-term dependencies, which was\nfurther augmented by the real-time sentiment analysis provided by the VIX index.\nThe following conclusions are drawn from the hypothesis testing conducted in this study:\n34\n\n\u2022 RH1: The results support the hypothesis (RH1) that market prices do not fully reflect all avail-\nable information, indicating some level of predictability in market movements.\n\u2022 RH2: The GARCH model was found to effectively identify patterns of historical volatility, sup-\nporting the hypothesis (RH2) and establishing it as a reliable benchmark for newer forecasting\napproaches.\n\u2022 RH3: LSTM networks were confirmed to outperform traditional models like GARCH in fore-\ncasting the S&P 500 index\u2019s volatility, supporting the hypothesis (RH3).\n\u2022 RH4: The hybrid LSTM-GARCH model surpassed the performance of standalone LSTM and\nGARCH models, validating the hypothesis (RH4).\n\u2022 RH5: Including VIX inputs enhanced the accuracy of volatility forecasts, supporting the hy-\npothesis (RH5).\n\u2022 RH6: The use of the Local Interpretable Model-agnostic Explanations (LIME) technique en-\nhanced the interpretability of the hybrid LSTM-GARCH model with VIX input, substantiating\nthe hypothesis (RH6).\nResults from Side Hypotheses for Sensitivity Testing:\n\u2022 sRH1: Changing the loss function from MSE to MAE did not lead to more robust forecasts,\nrefuting this specific hypothesis.\n\u2022 sRH2: Replacing input Log Returns with Daily Percentage Changes improved the accuracy of\nvolatility forecasts, supporting this hypothesis.\n\u2022 sRH3: Decreasing the sequence length to 5 days had mixed results: it improved the MAE but\nslightly worsened the RMSE, providing partial support for the hypothesis.\n\u2022 sRH4: Increasing the sequence length to 66 days did not improve the accuracy of forecasts,\nrefuting this hypothesis.\n\u2022 sRH5: Reducing the number of LSTM layers to 1 had mixed outcomes: it improved the MAE\nbut worsened the RMSE, leading to partial support for the hypothesis.\n\u2022 sRH6: Increasing the number of LSTM layers to 3 worsened the model\u2019s performance signifi-\ncantly, contradicting the hypothesis.\n\u2022 sRH7: The performance of LSTM models was significantly impacted by the choice of activa-\ntion function, confirming this hypothesis.\nThis study introduced significant advancements in financial volatility forecasting by pioneering\na hybrid approach that combines LSTM and GARCH models. This integration enhances prediction\naccuracy by incorporating the VIX index to reflect market sentiment. Additionally, we applied the\nLocal Interpretable Model-agnostic Explanations (LIME) technique to the LSTM- GARCH with VIX\nInput model, offering insights into its decision-making processes and improving the interpretability\nof deep learning in finance.",
    "chunk_index": 30,
    "start_char": 83712,
    "end_char": 87210,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "\u2022 sRH7: The performance of LSTM models was significantly impacted by the choice of activa-\ntion function, confirming this hypothesis.\nThis study introduced significant advancements in financial volatility forecasting by pioneering\na hybrid approach that combines LSTM and GARCH models. This integration enhances prediction\naccuracy by incorporating the VIX index to reflect market sentiment. Additionally, we applied the\nLocal Interpretable Model-agnostic Explanations (LIME) technique to the LSTM- GARCH with VIX\nInput model, offering insights into its decision-making processes and improving the interpretability\nof deep learning in finance. These contributions provide valuable frameworks for both academic\nresearch and practical applications in financial risk management.\nFuture research could expand in several promising directions to build on the groundwork laid\nby this paper. Firstly, incorporating macroeconomic indicators such as GDP growth rates, unemploy-\nment rates, and inflation figures could provide a more holistic view of the factors influencing market\n35\n\nvolatility. These macroeconomic factors could be integrated into the existing models to examine their\npredictive power in conjunction with market data. Secondly, the exploration of advanced machine\nlearning architectures such as Transformer models, which have excelled in processing sequential data\nin other domains, could offer new insights and methodologies for financial time series analysis. These\nmodels could be particularly adept at handling the multi-scale temporal dynamics often observed in\nmarket data. Thirdly, enhancing the models to process real-time data would allow for dynamic up-\ndating of predictions, a critical feature for applications in high-frequency trading and real-time risk\nmanagement. This would involve developing techniques for online learning where the model con-\ntinuously updates itself as new data becomes available. Furthermore, extending the hybrid modeling\napproach to other financial indices or asset classes could provide a comparative analysis of model\nperformance across different markets. This would help in understanding if the models\u2019 effectiveness\nis consistent across various market conditions and asset types. Lastly, deepening the focus on model\ninterpretability and explainability is crucial, especially in finance where decision-making involves\nsignificant economic stakes. Advanced techniques in explainable artificial intelligence could be ap-\nplied to these models to uncover the underlying decision processes, thereby making the models more\ntransparent and trustworthy for users. These areas not only promise to refine the predictive capabili-\nties of volatility models but also enhance their applicability in practical financial settings, supporting\nmore informed and effective decision-making in the finance industry.\nReferences\nHirotugu Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic\nControl, 19(6):716\u2013723, 1974.\nBahareh Amirshahi and Salim Lahmiri. Hybrid deep learning and garch-family models for forecasting\nvolatility of cryptocurrencies. Machine Learning with Applications, 12, 2023. 10.1016/j.mlwa\n.2023.100465.\nTorben G. Andersen and Tim Bollerslev. Answering the skeptics: Yes, standard volatility models do\nprovide accurate forecasts. International Economic Review, 39(4):885\u2013905, 1998.\nBasel M.A. Awartani and Valentina Corradi. Predicting the volatility of the s&p-500 stock index\nvia garch models: The role of asymmetries. International Journal of Forecasting, 21(1):167\u2013183,\n2005.\nRichard T. Baillie, Tim Bollerslev, and Hans Ole Mikkelsen. Fractionally integrated generalized\nautoregressive conditional heteroskedasticity. Journal of Econometrics, 74(1):3\u201330, 1996.",
    "chunk_index": 31,
    "start_char": 86567,
    "end_char": 90324,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "forecasting\nvolatility of cryptocurrencies. Machine Learning with Applications, 12, 2023. 10.1016/j.mlwa\n.2023.100465.\nTorben G. Andersen and Tim Bollerslev. Answering the skeptics: Yes, standard volatility models do\nprovide accurate forecasts. International Economic Review, 39(4):885\u2013905, 1998.\nBasel M.A. Awartani and Valentina Corradi. Predicting the volatility of the s&p-500 stock index\nvia garch models: The role of asymmetries. International Journal of Forecasting, 21(1):167\u2013183,\n2005.\nRichard T. Baillie, Tim Bollerslev, and Hans Ole Mikkelsen. Fractionally integrated generalized\nautoregressive conditional heteroskedasticity. Journal of Econometrics, 74(1):3\u201330, 1996.\nHum Nath Bhandari, Binod Rimal, Nawa Raj Pokhrel, Ramchandra Rimal, Keshab R. Dahal, and Ra-\njendra K.C. Khatri. Predicting stock market index using lstm. Machine Learning with Applications,\n2022. 10.1016/j.mlwa.2022.100320.\nFischer Black. Studies of stock price volatility changes. In Proceedings of the American Statistical\nAssociation, Business and Economic Statistics Section, pages 177\u2013181, 1976.\nTim Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics,\n31(3):307\u2013327, 1986.\nAndrea Bucci. Realized volatility forecasting with neural networks. Journal of Financial Economet-\nrics, 2020. URL https://api.semanticscholar.org/CorpusID:201885407.\nTianfeng Chai and Roland R. Draxler. Root mean square error (rmse) or mean absolute error (mae)?\n\u2013 arguments against avoiding rmse in the literature. Geoscientific Model Development, 7(3):1247\u2013\n1250, 2014.\n36\n\nKim Christensen, Mathias Siggaard, and Bezirgen Veliyev. A machine learning approach to volatil-\nity forecasting. Presented at the 13th International Conference on Computational and Financial\nEconometrics (CFE 2019), 2021.\nFatoumata Dama and Christine Sinoquet. Time series analysis and modeling to forecast: a survey,\n2021. URL https://arxiv.org/abs/2104.00164.\nRobert F. Engle.\nAutoregressive conditional heteroskedasticity with estimates of the variance of\nunited kingdom inflation. Econometrica, 50(4):987\u20131007, 1982.\nChicago Board Options Exchange. Vix index. URL https://www.cboe.com/vix.\nJeff Fleming, Barbara Ostdiek, and Robert E. Whaley. Predicting stock market volatility: A new\nmeasure. Journal of Futures Markets, (3):265\u2013302, 1995.\nChristian Francq and Jean-Michel Zako\u00efan. GARCH Models: Structure, Statistical Inference and\nFinancial Applications. John Wiley & Sons, 2010.\nLawrence R. Glosten, Ravi Jagannathan, and David E. Runkle. On the relation between the expected\nvalue and the volatility of the nominal excess return on stocks. The Journal of Finance, 48(5):\n1779\u20131801, 1993.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\nJan Grudniewicz and Robert \u00b4Slepaczuk. Application of machine learning in algorithmic investment\nstrategies on global stock markets. Research in International Business and Finance, 66:102052,\n2023. URL www.elsevier.com/locate/ribaf.\nPeter R. Hansen and Asger Lunde. A forecast comparison of volatility models: does anything beat a\ngarch(1,1)? Journal of Applied Econometrics, 36(7):866\u2013885, 2021.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n1735\u20131780, 1997.\nFang Jia and Boli Yang. Forecasting volatility of stock index: Deep learning model with likelihood-\nbased loss function.\nComplexity, 2021:5511802:1\u20135511802:13, 2021.\nURL https://api\n.semanticscholar.org/CorpusID:232283592.\nHa Young Kim and Chang Hyun Won. Forecasting the volatility of stock price index: A hybrid model\nintegrating lstm with multiple garch-type models. Expert Systems with Applications, pages 25\u201337,\n2018.\nDilip Kumar and M. Thenmozhi. Forecasting stock index volatility with garch models: International\nevidence. Quantitative Finance, 20(8):1267\u20131289, 2020.\nEghbal Rahimikia and Ser-Huang Poon. Machine learning for realised volatility forecasting. Psy-\nchology Research Methods eJournal, 2020.",
    "chunk_index": 32,
    "start_char": 89644,
    "end_char": 93623,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "index: Deep learning model with likelihood-\nbased loss function.\nComplexity, 2021:5511802:1\u20135511802:13, 2021.\nURL https://api\n.semanticscholar.org/CorpusID:232283592.\nHa Young Kim and Chang Hyun Won. Forecasting the volatility of stock price index: A hybrid model\nintegrating lstm with multiple garch-type models. Expert Systems with Applications, pages 25\u201337,\n2018.\nDilip Kumar and M. Thenmozhi. Forecasting stock index volatility with garch models: International\nevidence. Quantitative Finance, 20(8):1267\u20131289, 2020.\nEghbal Rahimikia and Ser-Huang Poon. Machine learning for realised volatility forecasting. Psy-\nchology Research Methods eJournal, 2020. URL https://api.semanticscholar.org/\nCorpusID:233759481.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cwhy should i trust you?\u201d explaining the\npredictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, pages 1135\u20131144. ACM, 2016.\nViktor Todorov and Tim Bollerslev. Jumps and betas: A new framework for disentangling and esti-\nmating systematic risks. Journal of Econometrics, 157(2):220\u2013235, 2010.\n37\n\nRuey S. Tsay. Analysis of Financial Time Series. Wiley, 3 edition, 2010. ISBN 978-0-470-41435-4.\nRobert E. Whaley. The investor fear gauge. Journal of Portfolio Management, 26(3):12\u201317, 2000.\n38",
    "chunk_index": 33,
    "start_char": 92967,
    "end_char": 94306,
    "paper_title": "The Hybrid Forecast of SP 500 Volatility ensembled",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_Hybrid_Forecast_of_SP_500_Volatility_ensembled.pdf"
  },
  {
    "text": "arXiv:0904.0900v3 [q-fin.TR] 17 Sep 2010\nThe price impact of order book events: market orders, limit orders and cancellations\nZolt\u00b4an Eisler,1 Jean-Philippe Bouchaud,1 and Julien Kockelkoren1\n1Capital Fund Management, Paris, France\n(Dated: October 23, 2018)\nWhile the long-ranged correlation of market orders and their impact on prices has been relatively\nwell studied in the literature, the corresponding studies of limit orders and cancellations are scarce.\nWe provide here an empirical study of the cross-correlation between all these di\ufb00erent events, and\ntheir respective impact on future price changes. We de\ufb01ne and extract from the data the \u201cbare\u201d\nimpact these events would have, if they were to happen in isolation.\nFor large tick stocks, we\nshow that a model where the bare impact of all events is permanent and non-\ufb02uctuating is in good\nagreement with the data.\nFor small tick stocks, however, bare impacts must contain a history\ndependent part, re\ufb02ecting the internal \ufb02uctuations of the order book. We show that this e\ufb00ect can\nbe accurately described by an autoregressive model on the past order \ufb02ow. This framework allows\nus to decompose the impact of an event into three parts: an instantaneous jump component, the\nmodi\ufb01cation of the future rates of the di\ufb00erent events, and the modi\ufb01cation of the jump sizes of\nfuture events. We compare in detail the present formalism with the temporary impact model that\nwas proposed earlier to describe the impact of market orders when other types of events are not\nobserved. Finally, we extend the model to describe the dynamics of the bid-ask spread.\nKeywords: price impact, market orders, limit orders, cancellations, market microstructure, order \ufb02ow\nContents\n1. Introduction\n2\n2. Impact of market orders: a short review\n3\n2.1. The transient impact model\n3\n2.2. History dependence of the impact function\n4\n2.3. The role of hidden events\n5\n2.4. Relation with Hasbrouck\u2019s VAR model\n5\n3. Data and notations\n6\n4. Correlation and response functions\n7\n4.1. The autocorrelation of \u01eb and s\n8\n4.2. The signed event-event correlation functions\n9\n4.3. The unsigned event-event correlation functions\n11\n4.4. The response function\n11\n5. The temporary impact model\n12\n6. A constant impact model\n13\n7. The gap dynamics of small tick stocks\n16\n7.1. A linear model for gap \ufb02uctuations\n16\n7.2. A linear model for gap \ufb02uctuations\n17\n7.3. The \ufb01nal model for small ticks\n18\n7.4. Interpretation: direct impact vs. induced impact\n20\n8. Conclusions\n22\nReferences\n23\nA. The dependence of the spread on the event \ufb02ow\n24\nB. Plots of various correlation functions\n25\n\n2\n1.\nINTRODUCTION\nThe relation between order \ufb02ow and price changes has attracted considerable attention in the recent years [1\u20136]. To\nthe investors\u2019 dismay, trades on average impact the price in the direction of their transactions, i.e. buys push the price\nup and sells drive the price down. Although this sounds very intuitive, a little re\ufb02ection shows that such a statement\nis far from trivial, for any buy trade in fact meets a sell trade, and vice-versa! On the other hand, there must indeed\nbe a mechanism allowing information to be included into and re\ufb02ected by prices.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3167,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "functions\n25\n\n2\n1.\nINTRODUCTION\nThe relation between order \ufb02ow and price changes has attracted considerable attention in the recent years [1\u20136]. To\nthe investors\u2019 dismay, trades on average impact the price in the direction of their transactions, i.e. buys push the price\nup and sells drive the price down. Although this sounds very intuitive, a little re\ufb02ection shows that such a statement\nis far from trivial, for any buy trade in fact meets a sell trade, and vice-versa! On the other hand, there must indeed\nbe a mechanism allowing information to be included into and re\ufb02ected by prices. This is well illustrated by the Kyle\nmodel [7], where the trading of an insider progressively reveals his information by impacting the price. Traditionally,\nthe above \u201cone sell for one buy\u201d paradox is resolved by arguing that there are in fact two types of traders coexisting\nin the ecology of \ufb01nancial markets: (i) \u201cinformed\u201d traders who place market orders for immediate execution, at the\ncost of paying half the bid-ask spread, and (ii) uninformed (or less informed) market makers who provide liquidity\nby placing limit orders on both sides of the order book, hoping to earn part of the bid-ask spread. In this setting,\nthere is indeed an asymmetry between a buyer, placing a market order at the ask, and the corresponding seller with\na limit order at the ask, and one can speak about a well de\ufb01ned impact of buy/sell (market) orders. The impact of\nmarket orders has therefore been empirically studied in great detail since the early nineties. As reviewed below, many\nsurprising results have been obtained, such as a very weak dependence of impact on the volume of the market order,\nthe long-range nature of the sign of the trades, and the resulting non-permanent, power-law decay of impact.\nThe conceptual problem is that the distinction between informed trader and market maker is no longer obvious in\nthe present electronic markets, where each participant can place both limit and market orders, depending on his own\nstrategies, the current state of the order book, etc. Although there is still an asymmetry between a buy market order\nand a sell limit order that enables one to de\ufb01ne the direction of the trade, \u201cinformed\u201d traders too may choose to place\nlimit orders, aiming to decrease execution costs. Limit orders must therefore also have an impact: adding a buy limit\norder induces extra upwards pressure, and cancelling a buy limit order decreases this pressure. Surprisingly, there are\nvery few quantitative studies of the impact of these orders \u2013 partly due to the fact that more detailed data, beyond\ntrades and quotes, is often needed to conduct such studies. As this paper was under review, we became aware of ref.\n[8], where a similar empirical study of the impact of limit orders is undertaken.\nThe aim of the present paper is to provide a uni\ufb01ed framework for the description of the impact of all order book\nevents, at least at the best limits: market orders, limit orders and cancellations.",
    "chunk_index": 1,
    "start_char": 2578,
    "end_char": 5579,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "impact of these orders \u2013 partly due to the fact that more detailed data, beyond\ntrades and quotes, is often needed to conduct such studies. As this paper was under review, we became aware of ref.\n[8], where a similar empirical study of the impact of limit orders is undertaken.\nThe aim of the present paper is to provide a uni\ufb01ed framework for the description of the impact of all order book\nevents, at least at the best limits: market orders, limit orders and cancellations. We study the correlations between\nall events types and signs. Assuming an additive model of impact, we map out from empirical data (consisting purely\nof trades and quotes information) the average individual impact of these orders. We \ufb01nd that the impact of limit\norders is similar (albeit somewhat smaller) to that of market orders.\nWe then compare these results to a simple model which assumes that all impacts are permanent in time. This\nworks well for large tick stocks, for which the bid-ask spread is nearly constant, with no gaps in the order book. The\ndiscrepancies between this simple model and data from small tick stocks are then scrutinized in detail and attributed\nto the history dependence of the impact, which we are able to model successfully using a linear regression of the gaps\non the past order \ufb02ow. Our \ufb01nal model is speci\ufb01ed in Sec. 7, Eq. (30). This framework allows us to measure more\naccurately the average impact of all types of orders, and to assess precisely the importance of impact \ufb02uctuations due\nto changes in the gaps behind the best quotes.\nWe want to insist on the fact that our study is mostly empirical and phenomenological, in the sense that we aim\nat establishing some stylized facts and building a parsimonious mathematical model to describe them without at this\nstage referring to any precise economic reasoning about the nature and motivations of the agents who place the orders.\nFor recent papers along this latter direction, see e.g. [9, 10]. We, however, tend not to believe in the possibility, for\nnow, to come up with a model that economists would like, with agents, equilibrium, etc. It seems to us that before\nachieving this, a more down to earth (but comprehensive) description of the data is needed, on which intuition can\nbe built. This is what we try to provide in this paper. We are also aware of the cultural gap between communities,\nand that our work will appear to some researchers as \u201ceyeball econometrics\u201d. We, however, are deeply convinced that\na graphical representation of data is needed to foster intuition, before any rigourous calibration is attempted.\nThe outline of this paper is as follows. We \ufb01rst review (Sec.\n2) the relevant results on the impact of market\norders and set the mathematical framework within which we will analyze our order book event data. We explain in\nparticular why the market order impact function measured in previous studies is in fact \u201cdressed\u201d by the impact of\nother events (limit orders, cancellations), and by the history dependence of the impact.",
    "chunk_index": 2,
    "start_char": 5104,
    "end_char": 8122,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "econometrics\u201d. We, however, are deeply convinced that\na graphical representation of data is needed to foster intuition, before any rigourous calibration is attempted.\nThe outline of this paper is as follows. We \ufb01rst review (Sec.\n2) the relevant results on the impact of market\norders and set the mathematical framework within which we will analyze our order book event data. We explain in\nparticular why the market order impact function measured in previous studies is in fact \u201cdressed\u201d by the impact of\nother events (limit orders, cancellations), and by the history dependence of the impact. We also relate our formalism\nto Hasbrouck\u2019s Vector Autoregression framework. We then turn to the presentation of the data we have analyzed (Sec.\n3), and of the various correlation functions that one can measure (Sec. 4). From these we determine the individual (or\n\u201cbare\u201d), lag-dependent impact functions of the di\ufb00erent events occuring at the bid price or at the ask price (Sec. 5).\nWe introduce a simpli\ufb01ed model where these impact functions are constant in time, and show that this gives an good\napproximate account of our data for large tick stocks, while signi\ufb01cant discrepancies appear for small tick stocks (Sec.\n6). The systematic di\ufb00erences are explained by the dynamics of order \ufb02ow deeper in the book, which can be modeled\nas a history dependent correction to the linear impact model (Sec. 7, see Eq. (30)). Our results are summarized in\n\n3\nthe conclusion, with open issues that would deserve more detailed investigation. In the Appendices we also show how\nthe bid-ask spread dynamics can be accounted for within the framework introduced in the main text (Appendix A)\nand some supplementary information concerning the di\ufb00erent empirical correlations that can be measured (Appendix\nB).\n2.\nIMPACT OF MARKET ORDERS: A SHORT REVIEW\n2.1.\nThe transient impact model\nQuantitative studies of the price impact of market orders have by now \ufb01rmly established a number of stylized facts,\nsome of which appear rather surprising at \ufb01rst sight. The salient points are (for a recent review and references, see\n[6]):\n\u2022 Buy (sell) trades on average impact the price up (down). In other words, there is a strong correlation between\nprice returns over a given time interval and the market order imbalance on the same interval.\n\u2022 The impact curve as a function of the volume of the trade is strongly concave. In other words, large volumes\nimpact the price only marginally more than small volumes.\n\u2022 The sign of market orders is strongly autocorrelated in time. Despite this, the dynamics of the midpoint is very\nclose to being purely di\ufb00usive.\nA simple model encapsulating these empirical facts assumes that the mid-point price pt can be written at (trade) time\nt as a linear superposition of the impact of past trades [3, 4]:1\npt =\nX\nt\u2032<t\n\u0002\nG(t \u2212t\u2032)\u01ebt\u2032v\u03b8\nt\u2032 + nt\u2032\u0003\n+ p\u2212\u221e,\n(1)\nwhere vt\u2032 is the volume of the trade at time t\u2032, \u01ebt\u2032 the sign of that trade (+ for a buy, \u2212for a sell), and nt is an\nindependent noise term that models any price change not induced by trades (e.g.",
    "chunk_index": 3,
    "start_char": 7530,
    "end_char": 10585,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "the dynamics of the midpoint is very\nclose to being purely di\ufb00usive.\nA simple model encapsulating these empirical facts assumes that the mid-point price pt can be written at (trade) time\nt as a linear superposition of the impact of past trades [3, 4]:1\npt =\nX\nt\u2032<t\n\u0002\nG(t \u2212t\u2032)\u01ebt\u2032v\u03b8\nt\u2032 + nt\u2032\u0003\n+ p\u2212\u221e,\n(1)\nwhere vt\u2032 is the volume of the trade at time t\u2032, \u01ebt\u2032 the sign of that trade (+ for a buy, \u2212for a sell), and nt is an\nindependent noise term that models any price change not induced by trades (e.g. jumps due to news). The exponent \u03b8\nis small; the dependence in v might in fact be logarithmic (\u03b8 \u21920). The most important object in the above equation\nis the function G(t \u2212t\u2032) which describes the temporal evolution of the impact of a single trade, which can be called\na \u2018propagator\u2019: how does the impact of the trade at time t\u2032 < t propagate, on average, up to time t? We discuss in\nsection 2.4 below how Eq. (1) is related to Hasbrouck\u2019s VAR model [1, 12]\nAn important result, derived in [3], is that G(t \u2212t\u2032) must decay with time in a very speci\ufb01c way, such as to o\ufb00-set\nthe autocorrelation of the trades, and maintain the (statistical) e\ufb03ciency of prices.\nClearly, if G(t \u2212t\u2032) did not\ndecay at all, the returns would simply be proportional to the sign of the trades, and therefore would themselves be\nstrongly autocorrelated in time. The resulting price dynamics would then be highly predictable, which is not the case.\nConversely, if G(t \u2212t\u2032) decayed to zero immediately, the price as given by Eq. (1) would oscillate within a limited\nrange, and the long-term volatility would be zero. The result of [3] is that if the correlation of signs C(\u2113) = \u27e8\u01ebt\u01ebt+\u2113\u27e9\ndecays at large \u2113as \u2113\u2212\u03b3 with \u03b3 < 1 (as found empirically), then G(t \u2212t\u2032) must decay as |t \u2212t\u2032|\u2212\u03b2 with \u03b2 = (1 \u2212\u03b3)/2\nfor the price to be exactly di\ufb00usive at long times. The impact of single trades is therefore predicted to decay as a\npower-law (at least up to a certain time scale), at variance with simple models that assume that the impact decays\nexponentially to a non-zero\u201cpermanent\u201dvalue. More generally, one can use the empirically observable impact function\nR(\u2113), de\ufb01ned as:\nR(\u2113) = \u27e8(pt+\u2113\u2212pt) \u00b7 \u03bet\u27e9,\n(2)\nand the time correlation function C(\u2113) of the variable \u03bet = \u01ebtv\u03b8\nt to map out, numerically, the complete shape of\nG(t \u2212t\u2032). This was done in [4], using the exact relation:\nR(\u2113) =\nX\n0<n\u2264\u2113\nG(n)C(\u2113\u2212n) +\nX\nn>\u2113\nG(n)C(n \u2212\u2113) \u2212\nX\nn>0\nG(n)C(n).\n(3)\n1 In the following, we only focus on price changes over small periods of time, so that the following additive model is adequate. For longer\ntime scales, one should worry about multiplicative e\ufb00ects, which in this formalism would naturally arise from the fact that the bid-ask\nspread, and the gaps in the order book, are a fraction of the price.",
    "chunk_index": 4,
    "start_char": 10087,
    "end_char": 12838,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "was done in [4], using the exact relation:\nR(\u2113) =\nX\n0<n\u2264\u2113\nG(n)C(\u2113\u2212n) +\nX\nn>\u2113\nG(n)C(n \u2212\u2113) \u2212\nX\nn>0\nG(n)C(n).\n(3)\n1 In the following, we only focus on price changes over small periods of time, so that the following additive model is adequate. For longer\ntime scales, one should worry about multiplicative e\ufb00ects, which in this formalism would naturally arise from the fact that the bid-ask\nspread, and the gaps in the order book, are a fraction of the price. Therefore, the impact itself, G, is expected to be proportional to a\nmoving average of the price. See [11] for a discussion of this point.\n\n4\nThis analysis is repeated in a more general setting below (see Sec. 5 and Eq. (16)). The above model, however, is\napproximate and incomplete in two, interrelated ways.\n\u2022 First, Eq. (1) neglects the \ufb02uctuations of the impact: one expects that G(t\u2032 \u2192t), which is the impact of trade\nat some time t\u2032 measured until a later time t, to depend both on t and t\u2032 and not only on t \u2212t\u2032. Its formal\nde\ufb01nition is given by:\nG(t\u2032 \u2192t) = \u2202pt\n\u2202\u03bet\u2032 ,\n\u03bet \u2261\u01ebtv\u03b8\nt .\n(4)\nImpact can indeed be quite di\ufb00erent depending on the state of the order book and the market conditions\nat t\u2032. As a consequence, if one blindly uses Eq. (1) to compute the second moment of the price di\ufb00erence,\nD(\u2113) = \u27e8(pt+\u2113\u2212pt)2\u27e9, with a non-\ufb02uctuating G(\u2113) calibrated to reproduce the impact function R(\u2113), the result\nclearly underestimates the empirical price variance: see Fig. 1. Adding a di\ufb00usive noise nt \u0338= 0 would only shift\nD(\u2113)/\u2113upwards, but this is insu\ufb03cient to reproduce the empirical data.\n\u2022 Second, other events of the order book can also change the mid-price, such as limit orders placed inside the\nbid-ask spread, or cancellations of all the volume at the bid or the ask. These events do indeed contribute to\nthe price volatility and should be explicitly included in the description. A simpli\ufb01ed description of price changes\nin terms of market orders only attempts to describe other events of the order book in an e\ufb00ective way, through\nthe non-trivial time dependence of G(\u2113).\n 0\n 0.5\n 1\n 1.5\n 2\n100\n101\n102\n103\n104\nD(l)/l (ticks2)\nl (trades)\ndata\nmodel\nFigure 1: D(\u2113)/\u2113and its approximation with the temporary impact model with only trades as events, with nt = 0 and for\nsmall tick stocks. Results are shown when assuming that all trades have the same, non \ufb02uctuating, impact G(\u2113), calibrated\nto reproduce R(\u2113). This simple model accounts for \u223c2/3 of the long term volatility. Other events and/or the \ufb02uctuations of\nimpact G(\u2113) must therefore contribute to the market volatility as well.\n2.2.\nHistory dependence of the impact function\nLet us make the above statements more transparent on toy-models. First, the assumption of a stationary impact\nfunction G(t\u2032 \u2192t) = G(t \u2212t\u2032) is clearly an approximation.",
    "chunk_index": 5,
    "start_char": 12383,
    "end_char": 15151,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "to reproduce R(\u2113). This simple model accounts for \u223c2/3 of the long term volatility. Other events and/or the \ufb02uctuations of\nimpact G(\u2113) must therefore contribute to the market volatility as well.\n2.2.\nHistory dependence of the impact function\nLet us make the above statements more transparent on toy-models. First, the assumption of a stationary impact\nfunction G(t\u2032 \u2192t) = G(t \u2212t\u2032) is clearly an approximation. The past order \ufb02ow (< t\u2032) should a\ufb00ect the way the trade\nat time t\u2032 impacts the price, or, as argued by Lillo and Farmer, that liquidity may be history dependent [6, 13, 14].\nSuppose for simplicity that the variable \u03bet = \u01ebtv\u03b8\nt is Gaussian (which turns out to be a good approximation) and that\nits impact is permanent but history dependent. If we assume that the past order \ufb02ow has a small in\ufb02uence on the\nimpact, we can formally expand G in powers of all past \u03be\u2019s to get:\nG(t\u2032 \u2192t) = G0 + G1\nX\nt1<t\u2032\ng1(t\u2032 \u2212t1)\u03bet1 + G2\nX\nt1,t2<t\u2032\ng2(t\u2032 \u2212t1; t\u2032 \u2212t2)\u03bet1\u03bet2 + \u00b7 \u00b7 \u00b7 .\n(5)\nIf buys and sells play a symmetric role, G1 = 0. Using the fact the \u03be\u2019s are Gaussian with zero mean, one \ufb01nds that\nthe impact function R(\u2113) within this toy-model is given by:\nR(\u2113) =\nX\n0<n\u2264\u2113\nC(n)\n\"\nG0 + G2\nX\nn1,n2>0\ng2(n1; n2)C(n1 \u2212n2)\n#\n+ 2G2\nX\n0<n\u2264\u2113\nX\nn1,n2>0\ng2(n1; n2)C(n1)C(n \u2212n2).\n(6)\n\n5\nIf one compares this expression with Eq. (3) to extract an e\ufb00ective propagator G(\u2113), it is clear that the resulting\nsolution will have some non-trivial time dependence induced by the third term, proportional to G2.\n2.3.\nThe role of hidden events\nImagine now that two types of events are important for the dynamics of the price. Events of the \ufb01rst type are\ncharacterized by a random variable \u03bet (e.g., \u03bet = \u01ebtv\u03b8\nt in the above example), whereas events of the second type (say\nlimit orders) are charaterized by another random variable \u03b7t. The \u201cfull\u201d dynamical equation for the price is given by:\npt =\nX\nt\u2032<t\nG1(t \u2212t\u2032)\u03bet\u2032 +\nX\nt\u2032<t\nG2(t \u2212t\u2032)\u03b7t\u2032 + p\u2212\u221e,\n(7)\nImagine, however, that events of the second type are not observed. If for simplicity \u03be and \u03b7\u2019s are correlated Gaussian\nrandom variables, one can always express the \u03b7\u2019s as linear superposition of past \u03be\u2019s and \ufb01nd a model in terms of \u03be\u2019s\nonly, plus an uncorrelated \u2018noise\u2019 component nt coming from the unobserved events:\npt =\nX\nt\u2032<t\n\uf8ee\n\uf8f0G1(t \u2212t\u2032)\u03bet\u2032 + G2(t \u2212t\u2032)\nX\nt\u2032\u2032\u2264t\u2032\n\u039e(t\u2032 \u2212t\u2032\u2032)\u03bet\u2032\u2032\n\uf8f9\n\uf8fb+ nt + p\u2212\u221e.\n(8)\n\u039e is the linear \ufb01lter allowing to predict the \u03b7\u2019s in terms of the past \u03be\u2019s. It can be expressed in a standard way in\nterms of the correlation function of the \u03b7\u2019s and the cross-correlation between \u03be\u2019s and \u03b7\u2019s. Notice that the previous\nequation can be recast in the form of Eq. (1) plus noise, with an e\ufb00ective propagator \u201cdressed\u201d by the in\ufb02uence of\nthe unobserved events:\nG(\u2113) = G1(\u2113) +\nX\n0<\u2113\u2032\u2264\u2113\nG2(\u2113\u2032)\u039e(\u2113\u2212\u2113\u2032).",
    "chunk_index": 6,
    "start_char": 14742,
    "end_char": 17478,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "\u2212t\u2032)\nX\nt\u2032\u2032\u2264t\u2032\n\u039e(t\u2032 \u2212t\u2032\u2032)\u03bet\u2032\u2032\n\uf8f9\n\uf8fb+ nt + p\u2212\u221e.\n(8)\n\u039e is the linear \ufb01lter allowing to predict the \u03b7\u2019s in terms of the past \u03be\u2019s. It can be expressed in a standard way in\nterms of the correlation function of the \u03b7\u2019s and the cross-correlation between \u03be\u2019s and \u03b7\u2019s. Notice that the previous\nequation can be recast in the form of Eq. (1) plus noise, with an e\ufb00ective propagator \u201cdressed\u201d by the in\ufb02uence of\nthe unobserved events:\nG(\u2113) = G1(\u2113) +\nX\n0<\u2113\u2032\u2264\u2113\nG2(\u2113\u2032)\u039e(\u2113\u2212\u2113\u2032).\n(9)\nFrom this equation, it is clear that a non-trivial dependence of G can arise even if the \u2018true\u2019 propagators G1 and G2\nare time independent \u2013 in other words the decay of the impact of a single market order is in fact a consequence of the\ninterplay of market and limit order \ufb02ow. As a trivial example, suppose both bare propagators are equal and constant\nin time (G1(\u2113) = G2(\u2113) = G) and \u03b7t \u2261\u2212\u03bet, \u2200t. This means that the two types of events impact the price but exactly\ncancel each other. Then, \u039e(\u2113) = \u03b4\u2113,0 and G(\u2113) \u22610, as it should: the dressed impact of events of the \ufb01rst type is zero.\nThis is an idealized version of the asymmetric liquidity model of Lillo and Farmer mentioned above [13].\nThe aim of this paper is to investigate a model of impact similar to Eqs. (1) and (7), but where a wider class\nof order book events are explicitly taken into account. This will allow us to extract the corresponding single event\nimpact functions, and study their time evolution. As a test for the accuracy of the model, the time behavior of\nother observables, such as the second moment of the price di\ufb00erence should be correctly accounted for. We start by\npresenting the data and extra notations which will be useful in the sequel. We then discuss the di\ufb00erent correlation\nand response functions that can be measured on the data.\n2.4.\nRelation with Hasbrouck\u2019s VAR model\nAt this stage, it is interesting to relate the above \u2018propagator\u2019 framework encoded in Eq. (1) and the econometric\nVector Autoregressive (VAR) model proposed by Hasbrouck, and that became a standard in the microstructure\nliterature. In its original formulation, the VAR model is a joint linear regression of the present price return rt and\nsigned volume xt = \u01ebtvt onto their past realisations, or more precisely:\nrt =\nX\nt\u2032<t\nBrr(t \u2212t\u2032)rt\u2032 + Bxr(0)xt +\nX\nt\u2032<t\nBxr(t \u2212t\u2032)xt\u2032 + nr(t),\nxt =\nX\nt\u2032<t\nBrx(t \u2212t\u2032)rt\u2032 +\nX\nt\u2032<t\nBxx(t \u2212t\u2032)xt\u2032 + nx(t),\n(10)\nwhere nr,x are i.i.d. noises and the B(t \u2212t\u2032) are regression coe\ufb03cients, to be determined. Eq. (1) can be seen\nas a special case of the VAR model, Eq. (10), provided the following identi\ufb01cations/modi\ufb01cations are made: a)\nxt \u2192\u03bet = \u01ebtv\u03b8\nt ; b) the coe\ufb03cients Brr(\u2113) are assumed to be zero; c) since Eq. (1) models prices and not returns, one",
    "chunk_index": 7,
    "start_char": 17020,
    "end_char": 19728,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "+\nX\nt\u2032<t\nBxx(t \u2212t\u2032)xt\u2032 + nx(t),\n(10)\nwhere nr,x are i.i.d. noises and the B(t \u2212t\u2032) are regression coe\ufb03cients, to be determined. Eq. (1) can be seen\nas a special case of the VAR model, Eq. (10), provided the following identi\ufb01cations/modi\ufb01cations are made: a)\nxt \u2192\u03bet = \u01ebtv\u03b8\nt ; b) the coe\ufb03cients Brr(\u2113) are assumed to be zero; c) since Eq. (1) models prices and not returns, one\n\n6\nhas G(\u2113) = P\n0\u2264\u2113\u2032<\u2113Bxr(\u2113\u2032). G(\u221e) is called the information content of a trade in Hasbrouck\u2019s framework; d) \ufb01nally,\nalthough the autocorrelation C(\u2113) of the \u03bet is measured, the dynamical model for the \u03bet is left unspeci\ufb01ed.\nAlthough the two models are very similar at the formal level, the major distinction lies in the interpretation, which\nin fact illustrates the di\ufb00erence between econometric models and \u201cmicroscopic\u201d models. Whereas the VAR model\npostulates a general, noisy linear relation between two sets (or more) of variables and determines the coe\ufb03cients\nvia least squares, we insist on a microscopic mechanism that leads to an a priori structure of the model and an\ninterpretation of the coe\ufb03cients. Eq. (1) is a causal model for impact, which postulates that the current price is a\nresult of the impact of all past trades, plus some noise contribution nt that represents price moves not related to trades\n(for example, quote revisions after some news announcements). In this context, there is no natural interpretation\nfor the Brr coe\ufb03cients, which must be zero: past price changes cannot by themselves in\ufb02uence the present price,\nalthough these may of course a\ufb00ect the order \ufb02ow \u03bet, which in turn impacts \u2018physically\u2019 the price. On the other\nhand, the interpretation in terms of impact allows one to anticipate the limitations of the model and to suggest\npossible improvements, by including more events or by allowing for some history dependence, as discussed in the\nabove subsections.\nThe aim of the present paper is to justify fully this modeling strategy by accounting for all events in the order\nbook. In this case, the variation of the price can be tautologically decomposed in terms of these events, and the\ncorresponding regression coe\ufb03cients have a transparent interpretation. Furthermore, the limitations of a purely linear\nmodel appear very clearly as the history dependence of impact may induce explicit non-linearities (see Sec. 7).\n3.\nDATA AND NOTATIONS\nIn this paper we analyze data on 14 randomly selected liquid stocks traded at NASDAQ during the period\n03/03/2008 \u2013 19/05/2008, a total of 53 trading days (see Table II for details).\nThe particular choice of market\nis not very important, many of our results were also veri\ufb01ed on other markets (such as CME Futures, US Treasury\nBonds and stocks traded at London Stock Exchange2), as well as on other time periods and they appear fairly robust.\nWe only consider the usual trading time between 9:30\u201316:00, all other periods are discarded. We will always use\nticks (0.01 US dollars) as the units of price.",
    "chunk_index": 8,
    "start_char": 19352,
    "end_char": 22312,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "details).\nThe particular choice of market\nis not very important, many of our results were also veri\ufb01ed on other markets (such as CME Futures, US Treasury\nBonds and stocks traded at London Stock Exchange2), as well as on other time periods and they appear fairly robust.\nWe only consider the usual trading time between 9:30\u201316:00, all other periods are discarded. We will always use\nticks (0.01 US dollars) as the units of price. We will use the name \u201cevent\u201d for any change that modi\ufb01es the bid or\nask price, or the volume quoted at these prices. Events deeper in the order book are unobserved and will not be\ndescribed: although they do not have an immediate e\ufb00ect on the best quotes, our description will still be incomplete;\nin line with the previous section, we know that these unobserved events may\u201cdress\u201dthe impact of the observed events.\nFurthermore, we note that the liquidity is fragmented and that the stocks we are dealing with are traded on multiple\nplatforms. The activity on these other platforms will also \u201cdress\u201d the impact of observable events, in the sense of the\nprevious section. This may account for some of the residual discrepancies reported below.\nEvents will be used as the unit of time. This\u201cevent time\u201dis similar, but more detailed than the notion of transaction\ntime used in many recent papers. Since the dependence of impact on the volume of the trades is weak [6, 15], we have\nchosen to classify events not according to their volume but according to whether they change the mid-point or not.\nThis strong dichotomy is another approximation to keep in mind. It leads to six possible types of events3:\n\u2022 market orders4 that do not change the best price (noted MO0) or that do (noted MO\u2032),\n\u2022 limit orders at the current bid or ask (LO0) or inside the bid-ask spread so that they change the price (LO\u2032),\n\u2022 and cancellations at the bid or ask that do not remove all the volume quoted there (CA0) or that do (CA\u2032).\nThe upper index \u2019 (\u201cprime\u201d) will thus denote that the event changed any of the best prices, and the upper index 0 that\nit did not. Abbreviations without the upper index (MO, CA, LO) refer to both the price changing and the non-price\nchanging event type. The type of the event occuring at time t will be denoted by \u03c0t.\nOur sample of stocks can be divided into two groups: large tick and small tick stocks. Large tick stocks are such\nthat the bid-ask spread is almost always equal to one tick, whereas small tick stocks have spreads that are typically\n2 The results for these markets are not reproduced here, for lack of space, but the corresponding data is available on request.\n3 Our data also included a small number (\u22480.3%) of marketable (or crossing) limit orders. In principle these could have been treated as\na market order (and a consequent limit order for the remaining volume if there was any).",
    "chunk_index": 9,
    "start_char": 21884,
    "end_char": 24723,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "that the bid-ask spread is almost always equal to one tick, whereas small tick stocks have spreads that are typically\n2 The results for these markets are not reproduced here, for lack of space, but the corresponding data is available on request.\n3 Our data also included a small number (\u22480.3%) of marketable (or crossing) limit orders. In principle these could have been treated as\na market order (and a consequent limit order for the remaining volume if there was any). Due to technical limitations we decided to\ninstead remove these events and the related price changes.\n4 To identify multiple trades that are initiated by the same market order, we consider as one market order all the trades in a given stock\nthat occur on the same side of the book within a millisecond. Such a time resolution is su\ufb03cient for distinguishing trades initiated by\ndi\ufb00erent parties even at times of very intense trading activity.\n\n7\n\u03c0\nevent de\ufb01nition\nevent sign de\ufb01nition\ngap de\ufb01nition (\u2206\u03c0,\u01eb)\n\u03c0 = MO0 market order, volume < outstand-\ning volume at the best\n\u01eb = \u00b11 for buy/sell\nmarket orders\n0\n\u03c0 = MO\u2032 market order, volume \u2265outstand-\ning volume at the best\n\u01eb = \u00b11 for buy/sell\nmarket orders\nhalf of \ufb01rst gap behind the ask (\u01eb = 1)\nor bid (\u01eb = \u22121)\n\u03c0 = CA0 partial cancellation of the bid/ask\nqueue\n\u01eb = \u22131 for buy/sell\nside cancellation\n0\n\u03c0 = LO0 limit order at the current best\nbid/ask\n\u01eb = \u00b11 for buy/sell\nlimit orders\n0\n\u03c0 = CA\u2032 complete cancellation of the best\nbid/ask\n\u01eb = \u22131 for buy/sell\nside cancellation\nhalf of \ufb01rst gap behind the ask (\u01eb = 1)\nor bid (\u01eb = \u22121)\n\u03c0 = LO\u2032 limit order inside the spread\n\u01eb = \u00b11 for buy/sell\nlimit order\nhalf distance of limit order from the\nearlier best quote on the same side\nTable I: Summary of the 6 possible event types, the corresponding de\ufb01nitions of the event signs and gaps.\na few ticks.The behavior of the two groups is quite di\ufb00erent, and this will be emphasized throughout the paper. For\nexample, the events which change the best price have a relatively low probability for large tick stocks (about 3%\naltogether), but not for small tick stocks (up to 40%). Table II shows a summary of stocks, and some basic statistics.\nNote that there is a number of stocks with intermediate tick sizes, which to some extent possess the characteristics of\nboth groups. Technically, they can be treated in exactly the same way as small tick stocks, and all our results remain\nvalid. However, for the clarity of presentation, we will not consider them explicitly in this paper.\nEvery event is given a sign \u01ebt according to its expected long-term e\ufb00ect on the price.\nFor market orders this\ncorresponds to usual order signs, i.e., \u01ebt = 1 for buy market orders (at the ask price) and \u22121 for sell market orders (at\nthe bid price).",
    "chunk_index": 10,
    "start_char": 24253,
    "end_char": 26985,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "treated in exactly the same way as small tick stocks, and all our results remain\nvalid. However, for the clarity of presentation, we will not consider them explicitly in this paper.\nEvery event is given a sign \u01ebt according to its expected long-term e\ufb00ect on the price.\nFor market orders this\ncorresponds to usual order signs, i.e., \u01ebt = 1 for buy market orders (at the ask price) and \u22121 for sell market orders (at\nthe bid price). Cancelled sell limit orders and incoming buy limit orders both have \u01ebt = 1, while others have \u01ebt = \u22121.\nThe above de\ufb01nitions are summarized in Table I. Note that the table also de\ufb01nes the gaps \u2206\u03c0,\u01eb, which will be used\nlater.\nIt will also be useful to de\ufb01ne another sign variable corresponding to the \u201cside\u201d of the event at time t, which will\nbe denoted by st. It indicates whether the event t took place at the bid (st = \u22121) or the ask (st = 1), thus:\nst =\n\u001a\n\u01ebt if \u03c0t = MO0, MO\u2032, CA0 or CA\u2032\n\u2212\u01ebt if \u03c0t = LO0 or LO\u2032\n(11)\nThe di\ufb00erence between \u01eb and s is because limit orders correspond to the addition not the removal of volume, and\nthus they push prices away from the side of the book where they occur.\nIn the following calculations we will sometimes rely on indicator variables denoted as I(\u03c0t = \u03c0). This expression is\n1 if the event at t is of type \u03c0 and zero otherwise. In other words, I(\u03c0t = \u03c0) = \u03b4\u03c0t,\u03c0, where \u03b4 is the Kronecker-delta.\nWe will also use the notation \u27e8\u00b7\u27e9to denote the time average of the quantity between the brackets. For example, the\nunconditional probability of the event type \u03c0 can be, by de\ufb01nition, calculated as P(\u03c0) = \u27e8I(\u03c0t = \u03c0)\u27e9.\nThe indicator notation, although sometimes heavy, simpli\ufb01es the formal calculation of some conditional expectations.\nFor example if a quantity X\u03c0,t depends on the event type \u03c0 and the time t, then its conditional expectation at times\nof \u03c0-type events is\n\u27e8X\u03c0t,t|\u03c0t = \u03c0\u27e9= \u27e8X\u03c0,tI(\u03c0t = \u03c0)\u27e9\nP(\u03c0)\n.\nAlso, by de\ufb01nition\nX\n\u03c0\nI(\u03c0t = \u03c0) = 1;\nand\nX\n\u03c0\nX\u03c0,tI(\u03c0t = \u03c0) = X\u03c0t,t.\n(12)\n4.\nCORRELATION AND RESPONSE FUNCTIONS\nIn this section, we study the empirical temporal correlation of the di\ufb00erent events de\ufb01ned above, and the response\nfunction to these events.\n\n8\nticker\nP(MO0) P(MO\u2032) P(CA0) P(LO0) P(CA\u2032) P(LO\u2032) mean spread mean price time/event\n(ticks)\n(USD)\n(sec)\nlarge tick\nAMAT\n0.042\n0.011\n0.39\n0.54\n0.0018\n0.013\n1.11\n17.45\n0.16\nCMCSA\n0.040\n0.0065\n0.41\n0.53\n0.0021\n0.0087\n1.12\n20.29\n0.15\nCSCO\n0.051\n0.0085\n0.40\n0.53\n0.0010\n0.0096\n1.08\n67.77\n0.10\nDELL\n0.042\n0.0087\n0.40\n0.54\n0.0019\n0.011\n1.10\n20.22\n0.17\nINTC\n0.052\n0.0073\n0.40\n0.54\n0.00080 0.0081\n1.08\n19.43\n0.12\nMSFT\n0.054\n0.0087\n0.40\n0.53\n0.0012\n0.010\n1.09\n27.52\n0.098\nORCL\n0.050\n0.0090\n0.40\n0.54\n0.0012\n0.010\n1.09\n20.86\n0.16\nsmall tick\nAAPL\n0.043\n0.076\n0.32\n0.33\n0.077\n0.16\n3.35\n140.56\n0.068\nAMZN\n0.038",
    "chunk_index": 11,
    "start_char": 26556,
    "end_char": 29285,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "0.54\n0.0018\n0.013\n1.11\n17.45\n0.16\nCMCSA\n0.040\n0.0065\n0.41\n0.53\n0.0021\n0.0087\n1.12\n20.29\n0.15\nCSCO\n0.051\n0.0085\n0.40\n0.53\n0.0010\n0.0096\n1.08\n67.77\n0.10\nDELL\n0.042\n0.0087\n0.40\n0.54\n0.0019\n0.011\n1.10\n20.22\n0.17\nINTC\n0.052\n0.0073\n0.40\n0.54\n0.00080 0.0081\n1.08\n19.43\n0.12\nMSFT\n0.054\n0.0087\n0.40\n0.53\n0.0012\n0.010\n1.09\n27.52\n0.098\nORCL\n0.050\n0.0090\n0.40\n0.54\n0.0012\n0.010\n1.09\n20.86\n0.16\nsmall tick\nAAPL\n0.043\n0.076\n0.32\n0.33\n0.077\n0.16\n3.35\n140.56\n0.068\nAMZN\n0.038\n0.077\n0.26\n0.31\n0.12\n0.20\n3.70\n70.68\n0.21\nAPOL\n0.042\n0.080\n0.24\n0.33\n0.11\n0.20\n3.78\n55.24\n0.40\nCOST\n0.054\n0.069\n0.27\n0.36\n0.082\n0.16\n2.62\n67.77\n0.39\nESRX\n0.042\n0.074\n0.24\n0.32\n0.12\n0.20\n4.12\n60.00\n0.63\nGILD\n0.052\n0.043\n0.34\n0.46\n0.032\n0.077\n1.64\n48.23\n0.23\nTable II: Summary statistics for all stocks, showing the probability of the di\ufb00erent events, the mean spread in ticks, the mean\nprice in dollars and the average time between events in seconds. The last column shows the total number of events in the\nsample.\n4.1.\nThe autocorrelation of \u01eb and s\nWe \ufb01rst investigate the autocorrelation function of the event signs, calculated as \u27e8\u01ebt+\u2113\u00b7 \u01ebt\u27e9. These are found to be\nshort-ranged, see Fig. 2, where the correlation function dies out after 10-100 trades, corresponding to typically 10\nseconds in real time. This is in contrast with several other papers [3, 4, 6, 16], where \u01eb\u2019s are calculated for market\norders only (\u03c0 = MO0, MO\u2032), and those signs are known to be strongly persistent among themselves, with, as recalled\nin Sec. 2, a correlation decaying as a slow power law. However, the direction of incoming limit orders is negatively\ncorrelated with cancellations and market orders. Because the \u01eb time series contains all types of events, the mixture\nof long-range positive and negative correlations balances such that only short-range persistence remains. Any other\nresult would be incompatible with little predictability in price returns. As illustrated by the toy example of Sec. 2,\nEq. (9), this mixing process in fact maintains statistical market e\ufb03ciency, i.e. weak autocorrelation of price changes.\nWhen limit orders and cancellations are included, one can independently analyze the persistence of the side st\nof the events. According to Eq. (11) this means \ufb02ipping the event signs of limit orders in the \u01eb time series, while\nkeeping the rest unchanged. This change reverses the compensation mechanism discussed above, and s is found to\nhave long-range correlations in time: \u27e8st+\u2113\u00b7 st\u27e9is shown in Fig. 2 and decays as \u2113\u2212\u03b3 with \u03b3 \u22480.7. This long range\ndecay is akin to the long range persistence of market order signs discussed throughout the literature: since market\norders tend to persistently hit one side of the book, one expects more limit orders and cancellations on the same side\nas well. Intuitively, if a large player splits his order and buys or sells using market orders for a long period of time,\nthis will attract compensating limit orders on the same side of the book.",
    "chunk_index": 12,
    "start_char": 28826,
    "end_char": 31776,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "This long range\ndecay is akin to the long range persistence of market order signs discussed throughout the literature: since market\norders tend to persistently hit one side of the book, one expects more limit orders and cancellations on the same side\nas well. Intuitively, if a large player splits his order and buys or sells using market orders for a long period of time,\nthis will attract compensating limit orders on the same side of the book.\n10-4\n10-3\n10-2\n10-1\n100\n100\n101\n102\n103\n104\n105\n106\n<\u03b5t\u03b5t+l> and <st st+l>\nl (events)\n\u03b5, large tick\n\u03b5, small tick\ns, large tick\ns, small tick\nl-0.7\nFigure 2: \u27e8\u01ebt+\u2113\u00b7 \u01ebt\u27e9and \u27e8st+\u2113\u00b7 st\u27e9, averaged for large and small tick stocks.\n\n9\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO\u2019, \u03c02(l)\nFigure 3: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113), (left) \u03c01 = MO0, (right) \u03c01 = MO\u2032. The curves are\nlabeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n4.2.\nThe signed event-event correlation functions\nWe will see in the following, that for describing price impact the most important correlation functions are those\nde\ufb01ned between two (not necessarily di\ufb00erent) signed event types. For some \ufb01xed \u03c01 and \u03c02 one can de\ufb01ne the\nnormalized correlation between these signed events as:\nC\u03c01,\u03c02(\u2113) = \u27e8I(\u03c0t = \u03c01)\u01ebtI(\u03c0t+\u2113= \u03c02)\u01ebt+\u2113\u27e9\nP(\u03c01)P(\u03c02)\n.\n(13)\nOur convention is that the \ufb01rst index corresponds to the \ufb01rst event in chronological order.\nBecause we have 6\nevent types, altogether there are 62 = 36 of these event-event correlation functions. There are no clearly apparent,\nsystematic di\ufb00erences between large and small tick stocks, hence we give results averaged over both groups in Fig. 3\nfor \u03c01 = MO0 and \u03c01 = MO\u2032. (Other correlation functions are plotted in Appendix B.) Trades among themselves and\nregardless of group are long range correlated as it is well known and was recalled above, and con\ufb01rmed again in Fig.\n3. For other cases, the sign of the correlations between event types varies and in many cases one observes a similarly\nslow decay that can be \ufb01tted by a power law with an exponent around 0.5. Furthermore, there are two distinctly\ndi\ufb00erent regimes. For \u2113\u2272100 events (which means up to 10 \u221220 seconds in real time) returns are still autocorrelated\n(cf. Fig. 2). In this regime CMO0,\u03c02(\u2113) is positive for any event type \u03c02, so small trades are followed by a ballistic\nmove in the same direction by other trades and also by limit orders, while at the same time cancellations also push\nthe price in the same direction.",
    "chunk_index": 13,
    "start_char": 31330,
    "end_char": 34139,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "decay that can be \ufb01tted by a power law with an exponent around 0.5. Furthermore, there are two distinctly\ndi\ufb00erent regimes. For \u2113\u2272100 events (which means up to 10 \u221220 seconds in real time) returns are still autocorrelated\n(cf. Fig. 2). In this regime CMO0,\u03c02(\u2113) is positive for any event type \u03c02, so small trades are followed by a ballistic\nmove in the same direction by other trades and also by limit orders, while at the same time cancellations also push\nthe price in the same direction. CMO\u2032,\u03c02(\u2113) is also positive except for LO\u2032, where it is negative except for very small\nlags5. This means that if a market order removes a level, it is followed by further trades and cancellations in the\nsame direction, but the level is re\ufb01lled very quickly by incoming limit orders inside the spread. For longer times some\ncorrelation functions change sign. For example in Fig. 3(left) one can see this reversal for limit orders. Market orders\n\u201cattract\u201d limit orders, as noted in [4, 14, 17]. This \u201cstimulated re\ufb01ll\u201d process ensures a form of dynamic equilibrium:\nthe correlated \ufb02ow of market orders is o\ufb00set by an excess in\ufb02ow of opposing limit orders, such as to maintain the\ndi\ufb00usive nature of the price. This is the same process causing the long-range correlations of st noted above.\nIn general, there are no reasons to expect time reversal symmetry, which would impose C\u03c01,\u03c02(\u2113) = C\u03c02,\u03c01(\u2113).\nHowever, some pairs of events appear to obey this symmetry at least approximately, for example MO0 and CA0 or\n5 There is some sign of oscillations for small tick stocks.\n\n10\nMO\u2032 and CA\u2032, see Fig. 4. On the other hand, for the pair MO\u2032, LO\u2032 one can see that limit orders that move the price\nare immediately followed by opposing market orders. The dual compensation, i.e. a stimulated re\ufb01ll of liquidity after\na price moving market order MO\u2032, only happens with some delay. MO0 and limit orders also lead to some asymmetry,\nsee Fig. 5; here we see that after a transient, non-aggressive market orders induce compensating limit orders more\ne\ufb03ciently than the reverse process.\n10-2\n10-1\n100\n100\n101\n102\n103\n104\n-C\u03c01, \u03c02(l)\nl (events)\nCA0, LO0\nLO0, CA0\nCA\u2019, LO\u2019\nLO\u2019, CA\u2019\nMO\u2019, LO\u2019\nLO\u2019, MO\u2019\nFigure 4: Examples for time reversal symmetry for normalized, signed event correlations for small tick stocks, note that it is\n\u2212C\u03c01,\u03c02(\u2113) plotted. Lines and points of the same color correspond to the same event pairs. The curves are labeled by their\nrespective \u03c01\u2019s and \u03c02\u2019s in the legend.\n-10-3\n-10-2\n-10-1\n-100\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100 100\n101\n102\n103\n104\nMO0, LO\u2019\nLO\u2019, MO0\nMO0, LO0\nLO0, MO0\nFigure 5: Examples for time reversal asymmetry for normalized, signed event correlations for small tick stocks. Lines and\npoints of the same color correspond to the same event pairs. The curves are labeled by their respective \u03c01\u2019s and \u03c02\u2019s in the\nlegend.",
    "chunk_index": 14,
    "start_char": 33650,
    "end_char": 36497,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "and \u03c02\u2019s in the legend.\n-10-3\n-10-2\n-10-1\n-100\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100 100\n101\n102\n103\n104\nMO0, LO\u2019\nLO\u2019, MO0\nMO0, LO0\nLO0, MO0\nFigure 5: Examples for time reversal asymmetry for normalized, signed event correlations for small tick stocks. Lines and\npoints of the same color correspond to the same event pairs. The curves are labeled by their respective \u03c01\u2019s and \u03c02\u2019s in the\nlegend.\n\n11\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO\u2019, \u03c02(l)\nFigure 6: The normalized, unsigned event correlation functions \u03a0\u03c01,\u03c02(\u2113), (left) \u03c01 = MO0, (right) \u03c01 = MO\u2032. The curves are\nlabeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n4.3.\nThe unsigned event-event correlation functions\nA similar de\ufb01nition of a correlation function is possible purely between event occurences, without the signs:\n\u03a0\u03c01,\u03c02(\u2113) = P(\u03c0t+\u2113= \u03c02|\u03c0t = \u03c01)\nP(\u03c02)\n\u22121 \u2261\u27e8I(\u03c0t = \u03c01)I(\u03c0t+\u2113= \u03c02)\u27e9\nP(\u03c01)P(\u03c02)\n\u22121,\n(14)\nwhere we have subtracted 1 such as to make the function decay to zero at large times. This quantity expresses the\nexcess probability of \u03c02-type events in comparison to their stationary probability, given that there was a \u03c01-type event\n\u2113lags earlier. Examples of this quantity for averages over all stocks are plotted in Fig. 6. One \ufb01nds that generally\n\u03a0\u03c01,\u03c02(\u2113) decays slower when both \u03c01 and \u03c02 move the price. This implies that events which change the best price\nare clustered in time: aggressive orders induce and reinforce each other.\n4.4.\nThe response function\nLet us now turn to the response of the price to di\ufb00erent types of orders. The average behavior of price after events\nof a particular type \u03c0 de\ufb01nes the corresponding response function (or average impact function):\nR\u03c0(\u2113) = \u27e8(pt+\u2113\u2212pt) \u00b7 \u01ebt|\u03c0t = \u03c0\u27e9.\n(15)\nThis is a correlation function between \u201csign times indicator\u201d \u01ebtI(\u03c0t = \u03c0) at time t and the price change from t to t+ \u2113,\nnormalized by the stationary probability of the event \u03c0, denoted as P(\u03c0) = \u27e8I(\u03c0t = \u03c0)\u27e9. This normalized response\nfunction gives the expected directional price change after an event \u03c0. Its behavior for all \u03c0\u2019s is shown in Fig. 7. We\nnote that all type of events lead, on average, to a price change in the expected direction. Tautologically, R\u03c0(\u2113= 1) > 0\nfor price changing events and R\u03c0(\u2113= 1) = 0 for other events. As the time lag \u2113increases, the impact of market orders\ngrows signi\ufb01cantly, specially for small tick stocks, whereas it remains roughly constant for limit orders/cancellations\nthat do change the price. However, as emphasized in [3], the response function is hard to interpret intuitively, and\nin particular is not equal to the bare impact of an event since the correlations between events contribute to R\u03c0(\u2113),\nsee Eq.",
    "chunk_index": 15,
    "start_char": 36086,
    "end_char": 39027,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "type of events lead, on average, to a price change in the expected direction. Tautologically, R\u03c0(\u2113= 1) > 0\nfor price changing events and R\u03c0(\u2113= 1) = 0 for other events. As the time lag \u2113increases, the impact of market orders\ngrows signi\ufb01cantly, specially for small tick stocks, whereas it remains roughly constant for limit orders/cancellations\nthat do change the price. However, as emphasized in [3], the response function is hard to interpret intuitively, and\nin particular is not equal to the bare impact of an event since the correlations between events contribute to R\u03c0(\u2113),\nsee Eq. (3) above. We now attempt to deconvolute the e\ufb00ect of correlations and extract these bare impact functions\nfrom the data.\n\n12\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\n 1.6\n100\n101\n102\n103\nR\u03c0 (ticks)\nl (events)\n\u03c0=MO0\n\u03c0=MO\u2019\n\u03c0=CA0\n\u03c0=LO0\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\n 1.6\nl (events)\nFigure 7: The normalized response function R\u03c0(\u2113) for (left) large tick stocks and (right) small tick stocks. The curves are\nlabeled according to \u03c0 in the legend.\n5.\nTHE TEMPORARY IMPACT MODEL\nMarket orders move prices, but so do cancellations and limit orders. As reviewed in Sec. 2 above, one can try to\ndescribe the impact of all these events in an e\ufb00ective way in terms of a \u201cdressed\u201d propagator of market orders only,\nG(\u2113), as de\ufb01ned by Eq. (1). Let us extend this formalism to include any number of events in the following way. We\nassume, that after a lag of \u2113events, an event of type \u03c0 has a remaining impact G\u03c0(\u2113). The price is then expressed as\nthe sum of the impacts of all past events, plus some initial reference price:\npt =\nX\nt\u2032<t\nG\u03c0\u2032\nt(t \u2212t\u2032)\u01ebt\u2032 + p\u2212\u221e,\n(16)\nwhere the term with the indicators selects exactly one propagator for each t\u2032, the one corresponding to the particular\nevent type at that time. After straightforward calculations, the response function (15) can be expressed through Eq.\n(16) and (12) as\nR\u03c01(\u2113) =\nX\n\u03c02\nP(\u03c02)\n\uf8ee\n\uf8f0X\n0<n\u2264\u2113\nG\u03c02(n)C\u03c01,\u03c02(\u2113\u2212n) +\nX\nn>\u2113\nG\u03c02(n)C\u03c02,\u03c01(n \u2212\u2113) \u2212\nX\nn>0\nG\u03c02(n)C\u03c02,\u03c01(n)\n\uf8f9\n\uf8fb.\n(17)\nThis is a direct extension of Eq. (3), which was obtained in Ref. [4]. One can invert the system of equations in (17),\nto evaluate the unobservable G\u03c0\u2019s in terms of the observable R\u03c0\u2019s and C\u03c01,\u03c02\u2019s. In order to do this, one rewrites the\nabove in a matrix form, as\nR\u03c01(\u2113) =\nX\n\u03c02\n\u221e\nX\nn=0\nA\u03c01,\u03c02\n\u2113,n\nG\u03c02(n),\n(18)\nwhere\nA\u03c01,\u03c02\n\u2113,n\n= P(\u03c02)\n(\nC\u03c01,\u03c02(\u2113\u2212n) \u2212C\u03c02,\u03c01(n),\nif 0 < n \u2264\u2113\u2264L\nC\u03c02,\u03c01(n \u2212\u2113) \u2212C\u03c02,\u03c01(n),\nif 0 < \u2113< n \u2264L\n(19)\nand \u221ewas replaced by a large enough cuto\ufb00L, convenient for numerical purposes. In the following, we use L = 1000,\nwhich allows to determine the functions G\u03c0 with a good precision up to \u2113\u223c300, see Fig. 8.\nAs discussed in Sec. 2, the origin of the decay of market order price impact is that incoming limit orders maintain\nan equilibrium with market order \ufb02ow.",
    "chunk_index": 16,
    "start_char": 38442,
    "end_char": 41248,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "(18)\nwhere\nA\u03c01,\u03c02\n\u2113,n\n= P(\u03c02)\n(\nC\u03c01,\u03c02(\u2113\u2212n) \u2212C\u03c02,\u03c01(n),\nif 0 < n \u2264\u2113\u2264L\nC\u03c02,\u03c01(n \u2212\u2113) \u2212C\u03c02,\u03c01(n),\nif 0 < \u2113< n \u2264L\n(19)\nand \u221ewas replaced by a large enough cuto\ufb00L, convenient for numerical purposes. In the following, we use L = 1000,\nwhich allows to determine the functions G\u03c0 with a good precision up to \u2113\u223c300, see Fig. 8.\nAs discussed in Sec. 2, the origin of the decay of market order price impact is that incoming limit orders maintain\nan equilibrium with market order \ufb02ow. In order to keep prices di\ufb00usive, limit orders introduce a reverting force into\nprices, and this precisely o\ufb00-sets the persistence in market order \ufb02ow. However, our present extended formalism\nexplicitly includes these limit orders (and also cancellations) as events. If all order book events were described, one\nnaively expects that the G\u03c0\u2019s should be lag-independent constants for events that change the price, and zero otherwise.\nSolving the above equation for G\u03c0\u2019s, however, leads to functions that still depend on the lag \u2113, particularly for small\n\n13\n-0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n100\n101\n102\nG\u03c0 (ticks)\nl (events)\n\u03c0=MO0\n\u03c0=MO\u2019\n\u03c0=CA0\n\u03c0=LO0\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103-0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\nl (events)\nFigure 8: The bare propagators G\u03c0(\u2113) in the temporary impact model for (left) large tick stocks and (right) small tick stocks.\ntick stocks: see Fig. 8. We see in particular that market orders that do not change the price immediately do impact\nthe price on longer time scales. We also notice that the impact of single MO\u2032, MO0 events \ufb01rst grows with lag and\nthen decays slowly. The impact of limit orders, although clearly measurable, seems to be signi\ufb01cantly smaller than\nthat of market orders, in particular for small tick stocks (see [8] for a related discussion).\nIn the rest of the paper, we will try to understand in more detail where the lag dependence of G\u03c0\u2019s comes from. The\ndiscussion of Sec. 2 already suggested that some history dependence of impact is responsible for this e\ufb00ect. Before\ndwelling into this, it is interesting to see how well the above augmented model predicts the volatility of the stocks once\nall the G\u03c0\u2019s have been calibrated on the empirical R\u03c0\u2019s. As just mentioned, Eq. (16) neglects the \ufb02uctuations of the\nimpact, and we therefore expect some discrepancies. In order to make such a comparison, we \ufb01rst express exactly the\nvariance of the price at lag \u2113, D(\u2113) =\n\n(pt+\u2113\u2212pt)2\u000b\nin terms of the G\u2019s and the C\u2019s, generalizing the corresponding\nresult obtained in [3]:\nD(\u2113) =\n\n(pt+\u2113\u2212pt)2\u000b\n=\nX\n0\u2264n<\u2113\nX\n\u03c01\nG\u03c01(\u2113\u2212n)2P(\u03c01) +\nX\nn>0\nX\n\u03c01\n[G\u03c01(\u2113+ n) \u2212G\u03c01(n)]2 P(\u03c01)\n+ 2\nX\n0\u2264n<n\u2032<\u2113\nX\n\u03c01,\u03c02\nG\u03c01(\u2113\u2212n)G\u03c02(\u2113\u2212n\u2032)C\u03c01,\u03c02(n\u2032 \u2212n)\n+ 2\nX\n0<n<n\u2032<\u2113\nX\n\u03c01,\u03c02\n[G\u03c01(\u2113+ n) \u2212G\u03c01(n)] [G\u03c02(\u2113+ n\u2032) \u2212G\u03c02(n\u2032)] C\u03c01,\u03c02(n \u2212n\u2032)\n+ 2\nX\n0\u2264n<\u2113\nX\nn\u2032>0\nX\n\u03c01,\u03c02\nG\u03c01(\u2113\u2212n) [G\u03c02(\u2113+ n\u2032) \u2212G\u03c02(n\u2032)] C\u03c02,\u03c01(n\u2032 + n).\n(20)\nThe function D(\u2113)/\u2113, which should be constant for a strictly di\ufb00usive process, is plotted in Fig.",
    "chunk_index": 17,
    "start_char": 40776,
    "end_char": 43667,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "G\u2019s and the C\u2019s, generalizing the corresponding\nresult obtained in [3]:\nD(\u2113) =\n\n(pt+\u2113\u2212pt)2\u000b\n=\nX\n0\u2264n<\u2113\nX\n\u03c01\nG\u03c01(\u2113\u2212n)2P(\u03c01) +\nX\nn>0\nX\n\u03c01\n[G\u03c01(\u2113+ n) \u2212G\u03c01(n)]2 P(\u03c01)\n+ 2\nX\n0\u2264n<n\u2032<\u2113\nX\n\u03c01,\u03c02\nG\u03c01(\u2113\u2212n)G\u03c02(\u2113\u2212n\u2032)C\u03c01,\u03c02(n\u2032 \u2212n)\n+ 2\nX\n0<n<n\u2032<\u2113\nX\n\u03c01,\u03c02\n[G\u03c01(\u2113+ n) \u2212G\u03c01(n)] [G\u03c02(\u2113+ n\u2032) \u2212G\u03c02(n\u2032)] C\u03c01,\u03c02(n \u2212n\u2032)\n+ 2\nX\n0\u2264n<\u2113\nX\nn\u2032>0\nX\n\u03c01,\u03c02\nG\u03c01(\u2113\u2212n) [G\u03c02(\u2113+ n\u2032) \u2212G\u03c02(n\u2032)] C\u03c02,\u03c01(n\u2032 + n).\n(20)\nThe function D(\u2113)/\u2113, which should be constant for a strictly di\ufb00usive process, is plotted in Fig.\n9, the symbols\nindicate the empirical data, and the dashed lines correspond to Eq. (20). Note that we \ufb01t both models to each stock\nseparately, compute D(\u2113)/\u2113in each case, and then average the results. We see that the overall agreement is fair for\nsmall tick stocks, but very bad for large tick stocks. The reason will turn out to be that for large ticks, a permanent,\nnon \ufb02uctuating impact model accounts very well for the dynamics. This re\ufb02ects that the spread and the gaps behind\nthe best quotes are nearly constant in that case. But any small variation of G\u03c0 is ampli\ufb01ed through the second term\nof Eq. (20) which is an in\ufb01nite sum of positive terms. Hence it is much better to work backwards and test a model\nwhere the single event propagator is assumed to be strictly constant over time, as we will explain in the next section.\n6.\nA CONSTANT IMPACT MODEL\nIn the above section we found that the single event propagators G\u03c0 appear to have a non-trivial time dependence.\nAnother way to test this result is to invert the logic and assume \ufb01rst that the G\u03c0 are time independent and see how\nwell, or how badly, this assumption fares at accounting for the shape of the response functions R\u03c0(\u2113) and of the price\ndi\ufb00usion D(\u2113).\n\n14\n 0\n 0.005\n 0.01\n 0.015\n 0.02\n 0.025\n 0.03\n 0.035\n100\n101\n102\n103\n104\nD(l)/l (ticks2)\nl (events)\nsmall tick\nlarge tick\nFigure 9: D(\u2113)/\u2113and its approximations for the two groups of stocks. For small tick stocks the values were divided by 10 for\nclarity. Symbols correspond to the empirical result. Dashed lines correspond to the temporary impact model with all 6 events\nand they are calculated from Eq. (20). The agreement is acceptable for small tick stocks, but very poor for large tick ones.\nSolid lines correspond to the constant impact model, see Eq. (23) below; in this case the agreement with large tick stocks in\nnearly perfect, at least visually.\nLet us start from the following exact formula for the midpoint price:\npt+\u2113= pt +\nX\nt\u2264t\u2032<t+\u2113\n\u01ebt\u2032\u2206\u03c0t\u2032 ,\u01ebt\u2032 ,t\u2032.\n(21)\nHere \u2206\u03c0,\u01ebt\u2032,t\u2032 denotes the price change at time t\u2032 if an event of type \u03c0 happens. This \u2206can also depend on the sign\n\u01ebt\u2032. For example, if \u03c0 = MO\u2032 and \u01ebt\u2032 = \u22121 this means that at a sell market order executed the total volume at the\nbid. The midquote price change is \u2212\u2206MO\u2032,\u22121,t\u2032, which usually means that the second best level was bt\u2032 \u22122\u2206MO\u2032,\u22121,t\u2032,\nwhere bt\u2032 is the bid price before the event.",
    "chunk_index": 18,
    "start_char": 43196,
    "end_char": 46053,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "following exact formula for the midpoint price:\npt+\u2113= pt +\nX\nt\u2264t\u2032<t+\u2113\n\u01ebt\u2032\u2206\u03c0t\u2032 ,\u01ebt\u2032 ,t\u2032.\n(21)\nHere \u2206\u03c0,\u01ebt\u2032,t\u2032 denotes the price change at time t\u2032 if an event of type \u03c0 happens. This \u2206can also depend on the sign\n\u01ebt\u2032. For example, if \u03c0 = MO\u2032 and \u01ebt\u2032 = \u22121 this means that at a sell market order executed the total volume at the\nbid. The midquote price change is \u2212\u2206MO\u2032,\u22121,t\u2032, which usually means that the second best level was bt\u2032 \u22122\u2206MO\u2032,\u22121,t\u2032,\nwhere bt\u2032 is the bid price before the event. The factor 2 is necessary, because the ask did not change, and the impact\nis de\ufb01ned by the change of the midquote. Hence \u2206MO\u2032\u2019s (and similarly \u2206CA\u2032\u2019s) correspond to half of the gap between\nthe \ufb01rst and the second best quote just before the level was removed (see also Ref. [18]). Another example when\n\u03c0 = LO\u2032 and \u01ebt\u2032 = \u22121. This means that at t\u2032 a sell limit order was placed inside the spread. The midquote price\nchange is \u2212\u2206LO\u2032,\u22121,t\u2032, which means that the limit order was placed at at\u2032 \u22122\u2206LO\u2032,\u22121,t\u2032, where at\u2032 is the ask price.\nThus \u2206LO\u2032\u2019s correspond to half of the gap between the \ufb01rst and the second best quote right after the limit order was\nplaced. In the following we will call the \u2206\u2019s gaps. Note that the events MO0, CA0 and LO0 do not change the price,\nso their respective gaps are always zero: there are only three types of \u2206\u2019s that are non-zero.\nThe permanent impact model is de\ufb01ned by replacing the time dependent \u2206\u2019s by their average values. More precisely,\nlet us introduce the average realized gap:\n\u2206R\n\u03c0 = \u27e8\u2206\u03c0t,\u01ebt,t|\u03c0t = \u03c0\u27e9.\n(22)\nThe conditional expectation means that the gaps are sampled only when the price change corresponding to that\nparticular kind of gap is truly realized. Therefore, in general \u2206R\n\u03c0 \u0338= \u27e8\u2206\u03c0,\u01ebt,t\u27e9, see Table III where one sees that the\nrealized gap when a market order moves the price is in fact larger than the unconditional average. The logic is that\nthe opening of a large gap behind the ask is a motivation for buying rapidly (or cancelling rapidly for sellers) before\nthe price moves up.\nOur approximate constant impact model then reads:\npt+\u2113= pt +\nX\nt\u2264t\u2032<t+\u2113\n\u2206R\n\u03c0t\u2032 \u01ebt\u2032.\n(23)\nThe response functions are then, by using Eq. (12), easily given by:\nR\u03c0(\u2113) = \u27e8(pt+\u2113\u2212pt) \u00b7 \u01ebt|\u03c0t = \u03c0\u27e9=\nX\n0\u2264t\u2032<\u2113\nX\n\u03c01\n\u2206R\n\u03c01P(\u03c01)C\u03c0,\u03c01(t\u2032),\n(24)\nThe formula (24) is quite simple to interpret. We \ufb01xed that the event that happened at t was of type \u03c0. Let us now\nexpress C\u03c0,\u03c01(\u2113) as:\nP(\u03c0)P(\u03c01)C\u03c0,\u03c01(\u2113) \u221dP(\u03c0t+\u2113= \u03c01, \u01ebt+\u2113= \u01ebt|\u03c0t = \u03c0) \u2212P(\u03c0t+\u2113= \u03c01, \u01ebt+\u2113= \u2212\u01ebt|\u03c0t = \u03c0).\n(25)\n\n15\nticker\n2\u2206R\nMO\u2032 2\u2206R\nCA\u2032 2\u2206R\nLO\u2032 2 \u27e8\u2206MO\u2032\u27e9\nlarge tick\nAMAT\n1.02\n1.04\n1.02\n1.00\nCMCSA\n1.03\n1.14\n1.06\n1.00\nCSCO\n1.01\n1.02\n1.01\n1.00\nDELL\n1.01\n1.05\n1.02\n1.00\nINTC\n1.00\n1.01\n1.01\n1.00\nMSFT\n1.01\n1.02\n1.01\n1.00\nORCL\n1.01\n1.02\n1.02\n1.00\nsmall tick",
    "chunk_index": 19,
    "start_char": 45570,
    "end_char": 48278,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "that the event that happened at t was of type \u03c0. Let us now\nexpress C\u03c0,\u03c01(\u2113) as:\nP(\u03c0)P(\u03c01)C\u03c0,\u03c01(\u2113) \u221dP(\u03c0t+\u2113= \u03c01, \u01ebt+\u2113= \u01ebt|\u03c0t = \u03c0) \u2212P(\u03c0t+\u2113= \u03c01, \u01ebt+\u2113= \u2212\u01ebt|\u03c0t = \u03c0).\n(25)\n\n15\nticker\n2\u2206R\nMO\u2032 2\u2206R\nCA\u2032 2\u2206R\nLO\u2032 2 \u27e8\u2206MO\u2032\u27e9\nlarge tick\nAMAT\n1.02\n1.04\n1.02\n1.00\nCMCSA\n1.03\n1.14\n1.06\n1.00\nCSCO\n1.01\n1.02\n1.01\n1.00\nDELL\n1.01\n1.05\n1.02\n1.00\nINTC\n1.00\n1.01\n1.01\n1.00\nMSFT\n1.01\n1.02\n1.01\n1.00\nORCL\n1.01\n1.02\n1.02\n1.00\nsmall tick\nAAPL\n1.31\n1.27\n1.27\n1.14\nAMZN\n1.51\n1.22\n1.30\n1.17\nAPOL\n1.76\n1.50\n1.52\n1.42\nCOST\n1.35\n1.23\n1.24\n1.15\nESRX\n1.85\n1.54\n1.60\n1.45\nGILD\n1.11\n1.13\n1.11\n1.03\nTable III: Mean realized gaps and unconditional gaps in ticks for all stocks. All values were multiplied by 2, so that they\ncorrespond to the instantaneous change of the bid/ask and not of the midquote. Note that \u27e8\u2206MO\u2032\u27e9= \u27e8\u2206CA\u2032\u27e9, while \u27e8\u2206LO\u2032\u27e9is\nnot observable.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n100\n101\n102\n103\nR\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO0\n\u03c0=CA0\n\u03c0=LO0\n100\n101\n102\n103\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nl (events)\nFigure 10: Comparison of true and approximated normalized response functions R\u03c0(\u2113), using the constant gap model, for (left)\nlarge tick stocks and (right) small tick stocks, for events that do not change the price. Symbols correspond to the true value,\nand lines to the approximation. The data are labeled according to \u03c0 in the legend.\nThis represents the following: Given that the event at t was of type \u03c0 and the event at t + \u2113is of type \u03c01, how\nmuch more is it probable, that the direction of the second event is the same as that of the \ufb01rst event? The total\nprice response to some event can be understood as its own impact (lag zero), plus the sum of the biases in the course\nof future events, conditional to this initial event. These biases are multiplied by the average price change \u2206R that\nthese induced future events cause. Of course, correlation does not mean causality, and we cannot a priori distinguish\nbetween events that are induced by the initial event, and those that merely follow the initial event (see [19] for a\nrelated discussion). However, it seems reasonable to assume that there is a true causality chain between di\ufb00erent\ntypes of events occuring on the same side of the book (i.e. a limit order re\ufb01lling the best quote after a market order).\nLet us now take Eq. (24), and check how well the true response functions are described by the above constant impact\nmodel. Figs. 10 and 11 show that the agreement is very satisfactory for large tick stocks, except when \u03c01 = CA\u2032,\nbut these events are very rare (less than \u223c0.2%). This agreement is expected because the order book is usually so\ndense that gaps hardly \ufb02uctuate at all; the small remaining discrepancies will in fact be cured below. The quality of\nthe agreement suggests that the time dependence of the bare impact function G\u03c0 obtained in Sec.",
    "chunk_index": 20,
    "start_char": 47871,
    "end_char": 50673,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "well the true response functions are described by the above constant impact\nmodel. Figs. 10 and 11 show that the agreement is very satisfactory for large tick stocks, except when \u03c01 = CA\u2032,\nbut these events are very rare (less than \u223c0.2%). This agreement is expected because the order book is usually so\ndense that gaps hardly \ufb02uctuate at all; the small remaining discrepancies will in fact be cured below. The quality of\nthe agreement suggests that the time dependence of the bare impact function G\u03c0 obtained in Sec. 5 above is partly\na numerical artefact coming from the \u201cbrute force\u201d inversion of Eq. (18).\nFor small ticks on the other hand, noticeable deviations are observed as expected, and call for an extension of the\nmodel. This will be the focus of the next sections. One can extend the above model in yet another direction, by\nstudying the dynamics of the spread rather than the dynamics of the mid-point, see Appendix A.\n\n16\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n100\n101\n102\n103\nR\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO\u2019\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103\n 0\n 0.4\n 0.8\n 1.2\n 1.6\n 2\nl (events)\nFigure 11: Comparison of true and approximated normalized response functions R\u03c0(\u2113), using the constant gap model, for (left)\nlarge tick stocks and (right) small tick stocks, for events that change the price. Symbols correspond to the true value, and lines\nto the approximation. The data are labeled according to \u03c0 in the legend.\nOne can approximate the volatility within the same model as\nD(\u2113) =\n\n(pt+\u2113\u2212pt)2\u000b\n\u2248\nX\n0\u2264t\u2032,t\u2032\u2032<\u2113\nX\n\u03c01\nX\n\u03c02\nP(\u03c01)P(\u03c02)C\u03c01,\u03c02(t\u2032 \u2212t\u2032\u2032)\u2206R\n\u03c01\u2206R\n\u03c02.\n(26)\nAs shown in Fig.\n9, the constant gap model is very precise for large tick stocks (as again expected), but clear\ndiscrepancies are visible for small tick ones.\n7.\nTHE GAP DYNAMICS OF SMALL TICK STOCKS\n7.1.\nA linear model for gap \ufb02uctuations\nLet us now try to better understand how gap \ufb02uctuations contribute to the response function, and why replacing\nthe gap by its average realized value is not a good approximation for small tick stocks. By de\ufb01nition, without the\nconstant gap approximation, the response function contains contributions which have the form\n\u001c\n\u2206\u03c02,\u01ebt+\u2113,t+\u2113\u01ebt+\u2113\u01ebt\n\f\f\f\f\u03c0t = \u03c01, \u03c0t+\u2113= \u03c02\n\u001d\n.\nAfter using some basic properties of the event signs this quantity can be written as a sum over three contributions:\n1. Firstly, there is the term from the constant gap approximation:\n\u2206R\n\u03c02\n\u001c\n\u01ebt\u01ebt+\u2113\n\f\f\f\f\u03c0t = \u03c01, \u03c0t+\u2113= \u03c02\n\u001d\n.\nThis contains the highest order of the e\ufb00ect of event-event correlations.\n2. There is a second term that we write as:\n1\n2\n*\n[\u2206\u03c02,+,t+\u2113\u2212\u2206\u03c02,\u2212,t+\u2113]\u01ebt\n|\n{z\n}\n(a)\n\f\f\f\f\u03c0t = \u03c01, \u03c0t+\u2113= \u03c02\n+\n,\nwhich is the conditional expectation value of the quantity (a). If (a) is positive, then after an upward price move\nconsecutive upward moves are larger than downward ones, while if (a) is negative then they are smaller. This\nprocess can thus either accelerate or dampen the growth of the response function.",
    "chunk_index": 21,
    "start_char": 50157,
    "end_char": 53050,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "of the e\ufb00ect of event-event correlations.\n2. There is a second term that we write as:\n1\n2\n*\n[\u2206\u03c02,+,t+\u2113\u2212\u2206\u03c02,\u2212,t+\u2113]\u01ebt\n|\n{z\n}\n(a)\n\f\f\f\f\u03c0t = \u03c01, \u03c0t+\u2113= \u03c02\n+\n,\nwhich is the conditional expectation value of the quantity (a). If (a) is positive, then after an upward price move\nconsecutive upward moves are larger than downward ones, while if (a) is negative then they are smaller. This\nprocess can thus either accelerate or dampen the growth of the response function.\n\n17\n3. The third contribution is of the form\n1\n2\n*\n[\u2206\u03c02,+,t+\u2113+ \u2206\u03c02,\u2212,t+\u2113\u22122\u2206R\n\u03c02]\n|\n{z\n}\n(b)\n\u01ebt\u01ebt+\u2113\n| {z }\n(c)\n\f\f\f\f\u03c0t = \u03c01, \u03c0t+\u2113= \u03c02\n+\n.\nHere (b) is positive, when the average of the two gaps (up and down) is greater than the time averaged realized\nvalue. (c) is positive, when the two events move the price in the same direction. Thus the full term gives a\npositive contribution to the response function, if two \u201cparallel\u201d events are correlated with larger gaps and hence\ndecreased liquidity at the time of the second event, while opposing events correspond to increased liquidity at\nthe time of the second event. The \ufb01nal e\ufb00ect of this term agrees with the previous one: If (b) \u00d7 (c) is positive,\nthen after an upward price move the consecutive upward moves become larger than downward ones and vice\nversa.\nAt this point we need a dynamical model for the \u2206\u2019s, to quantify the above correlations, but we are faced with the\ndi\ufb03culty that \u2206\u03c0,\u01eb,t is only observed for \u03c0 = \u03c0t and \u01eb = \u01ebt. What we will do instead is to write a simple regression\nmodel directly for the observable quantity \u2206\u03c0,\u01ebt,tI(\u03c0t = \u03c0)\u01ebt, that can be evaluated from data. Then based on this\nknowledge we will revisit the in\ufb02uence of gap \ufb02uctuations on the price dynamics in Sec. 7.3.\n7.2.\nA linear model for gap \ufb02uctuations\nThe correlation between events has a dynamical origin: market orders and cancellations attract replacement limit\norders and vice versa.\nEq.\n(21) is the exact time evolution of price written as a sum of the random variables\n\u2206\u03c0,\u01ebt,tI(\u03c0t = \u03c0)\u01ebt. We will postulate that both the realized gap \u2206\u03c0,\u01ebt,t and the order \ufb02ow I(\u03c0t = \u03c0)\u01ebt are in\ufb02uenced\nby the past order \ufb02ow I(\u03c0t\u2032 = \u03c0)\u01ebt\u2032, t\u2032 < t in a linear fashion, i.e.:\n\u2206\u03c0,\u01ebt,tI(\u03c0t = \u03c0)\u01ebt =\nX\nt\u2032<t\nX\n\u03c01\nK\u03c01,\u03c0(t \u2212t\u2032)I(\u03c0t\u2032 = \u03c01)\u01ebt\u2032 + \u03b7\u03c01,t,\n(27)\nwhere all \u03b7\u2019s are independent noise variables. Similarly, we write for the three price changing events MO\u2032, LO\u2032 and\nCA\u2032:\n\u2206R\n\u03c0 I(\u03c0t = \u03c0)\u01ebt =\nX\nt\u2032<t\nX\n\u03c01\n\u02dcK\u03c01,\u03c0(t \u2212t\u2032)I(\u03c0t\u2032 = \u03c01)\u01ebt\u2032 + \u02dc\u03b7\u03c01,t,\n(28)\nwith other noise variables \u02dc\u03b7, and we introduced \u2206R\n\u03c0 for later convenience. Note the above equations are again of the\nvector autoregression type, where the kernel K and \u02dcK have a 3 \u00d7 6 matrix structure.\nBoth models (27) and (28) can be calibrated to the data by using the same trick an in Sec.",
    "chunk_index": 22,
    "start_char": 52591,
    "end_char": 55313,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "price changing events MO\u2032, LO\u2032 and\nCA\u2032:\n\u2206R\n\u03c0 I(\u03c0t = \u03c0)\u01ebt =\nX\nt\u2032<t\nX\n\u03c01\n\u02dcK\u03c01,\u03c0(t \u2212t\u2032)I(\u03c0t\u2032 = \u03c01)\u01ebt\u2032 + \u02dc\u03b7\u03c01,t,\n(28)\nwith other noise variables \u02dc\u03b7, and we introduced \u2206R\n\u03c0 for later convenience. Note the above equations are again of the\nvector autoregression type, where the kernel K and \u02dcK have a 3 \u00d7 6 matrix structure.\nBoth models (27) and (28) can be calibrated to the data by using the same trick an in Sec. 5, forming expectation\nvalues on both sides and solving a set of linear equations between correlation functions, for example for K:\n\n\u2206\u03c0,\u01ebt+\u2113,t+\u2113I(\u03c0t+\u2113= \u03c0)\u01ebt+\u2113I(\u03c0t = \u03c01)\u01ebt\n\u000b\n=\nX\nt\u2032<t+\u2113\nX\n\u03c02\nK\u03c02,\u03c0(t + \u2113\u2212t\u2032) \u27e8I(\u03c0t\u2032 = \u03c02)\u01ebt\u2032I(\u03c0t = \u03c01)\u01ebt\u27e9,\n(29)\nexcept this time we have three separate solutions for \u03c0 = MO\u2032, CA\u2032 and LO\u2032. An example of the solution kernels K\nis given in Fig. 12; the sign of these kernels is expected from what we learnt in Sec. 4. We see for example that a MO\nevent tends to make a future MO\u2032 more probable, and with an increased gap, which makes sense. The same can be\nrepeated with respect to Eq. (28) to calculate the \u02dcK\u2019s.\nAn important aspect of these VAR models is that once we have an estimate for their kernels, they can be used\nfor forecasting the future price changes caused by each component of the event \ufb02ow based on the events that oc-\ncured in the recent past [1].\nEq.\n(27) prescribes for us an estimate for conditional expectation values such as\n\n\u2206MO\u2032,\u01ebt,tI(\u03c0t = MO\u2032)\u01ebt| \u00b7 \u00b7 \u00b7\n\u000b\n, which is the expected price change due to a market order in the next event (times the\nprobability of such an outcome), and the conditioning is on past signs and indicators. We can proceed similarly for\nCA\u2032 and LO\u2032, and \ufb01nally the sum of the three components gives the expected price change in the next event.\nSuch forecasts based on Eqs. (27), (28) perform surprisingly well in practice, although liquidity is fragmented and\nsome events are unobserved. Fig. 13 shows that the expectation value of the left hand side of Eq. (27) is a monotonic\nfunction of our prediction, and the relationship on average can be \ufb01tted with a straight line with slope 1, although\nsmall higher order (cubic) corrections seem to be present as well. Similar results can be found for Eq. (28) and \u02dcK\u2019s.\nAs discussed in Sec. 2, one should interpret the kernels K as \u201cdressed\u201d objects that include the contribution of\nevents that occur on unobserved platforms. This is justi\ufb01ed as long as one is concerned with linear observables, such\nas average response functions. For non-linear quantities, such as di\ufb00usion, some discrepancies are expected.\n\n18\n-0.06\n-0.04\n-0.02\n 0\n 0.02\n 0.04\n 0.06\n 0.08\n100\n101\n102\n103\nK\u03c0, MO\u2019(l) (ticks)\nl (events)\n\u03c0=MO0\n\u03c0=MO\u2019\n\u03c0=CA0\n\u03c0=LO0\n\u03c0=CA\u2019\n\u03c0=LO\u2019\nFigure 12: Estimates of K\u03c0,MO\u2032(t) for small ticks.",
    "chunk_index": 23,
    "start_char": 54905,
    "end_char": 57621,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "events that occur on unobserved platforms. This is justi\ufb01ed as long as one is concerned with linear observables, such\nas average response functions. For non-linear quantities, such as di\ufb00usion, some discrepancies are expected.\n\n18\n-0.06\n-0.04\n-0.02\n 0\n 0.02\n 0.04\n 0.06\n 0.08\n100\n101\n102\n103\nK\u03c0, MO\u2019(l) (ticks)\nl (events)\n\u03c0=MO0\n\u03c0=MO\u2019\n\u03c0=CA0\n\u03c0=LO0\n\u03c0=CA\u2019\n\u03c0=LO\u2019\nFigure 12: Estimates of K\u03c0,MO\u2032(t) for small ticks.\n-3\n-2\n-1\n 0\n 1\n 2\n 3\n 4\n-3\n-2\n-1\n 0\n 1\n 2\n 3\nrealized value (normalized)\nlinear prediction of \u2206\u03c0, t+lI(\u03c0t+l=\u03c0)\u03b5t+l (normalized)\n\u03c0 = MO\u2019\n\u03c0 = CA\u2019\n\u03c0 = LO\u2019\ny=x\nFigure 13: Performance of Eq. (27) for small ticks. Both axes normalized by standard deviation of predictor.\n7.3.\nThe \ufb01nal model for small ticks\nThe above analysis suggests a way to build and calibrate an impact model that describes in a consistent way (a)\nall types of events and (b) the history dependence of the gaps, as we argued to be necessary in Sec. 2. The discussion\nof the previous section motivates the following model:\npt+\u2113= pt +\nX\nt\u2264t\u2032<t+\u2113\n\"\n\u2206R\n\u03c0t\u2032 +\nX\nt\u2032\u2032<t\u2032\n\u03ba\u03c0t\u2032\u2032 ,\u03c0t\u2032(t\u2032 \u2212t\u2032\u2032)\u01ebt\u2032\u01ebt\u2032\u2032\n#\n\u01ebt\u2032,\n(30)\nwhere \u03ba\u03c02,\u03c01 is a kernel that models the \ufb02uctuations of the gaps and their history dependence, which will be chosen\nsuch that the bare propagator of the model is given by Eq. (43) above.\nThe model speci\ufb01cation, Eq. (30), is the central result of this paper. It can be seen as a permanent impact model,\nbut with some history dependence, modeled as a linear regression on past events. By symmetry, this dependence\nshould only include terms containing \u01ebt\u2032\u01ebt\u2032\u2032 since the in\ufb02uence of any past string of events on the ask must be the\nsame as that of the mirror image of the string on the bid. More generally, one may expect higher order, non-linear\ncorrection terms of the form\nX\nt1,t2,t3<t\u2032\n\u03ba\u03c0t1,\u03c0t2 ,\u03c0t3;\u03c0t\u2032 (t\u2032 \u2212t1, t\u2032 \u2212t2, t\u2032 \u2212t3)\u01ebt1\u01ebt2\u01ebt3\u01ebt\u2032,\n(31)\nor with a larger (even) number of \u01eb\u2019s. We will not explore such corrections further here, although Fig. 13 suggests\nthese terms are present.\nUpon direct identi\ufb01cation of Eq. (30) with Eq. (21), and using (27) and (28) one \ufb01nds that \u03ba can be expressed in\nterms of K and \u02dcK as:\n\u03ba\u03c0,\u03c02(\u2113) = K\u03c0,\u03c02(\u2113) \u2212\u02dcK\u03c0,\u03c02(\u2113).\n(32)\n\n19\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n100\n101\n102\n103\nR\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO0\n\u03c0=CA0\n\u03c0=LO0\n100\n101\n102\n103 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nl (events)\nFigure 14: Comparison of true and approximated normalized response functions R\u03c0(\u2113) of the \ufb01nal model for (left) large tick\nstocks and (right) small tick stocks, for events that do not change the price. Symbols correspond to the true value, and lines\nto the approximation. To illustrate the goodness of \ufb01t on a stock by stock basis, we calculated the absolute di\ufb00erence between\nthe true and the approximated value, the average of this quantity across stocks is indicated by the error bars.",
    "chunk_index": 24,
    "start_char": 57213,
    "end_char": 60002,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "normalized response functions R\u03c0(\u2113) of the \ufb01nal model for (left) large tick\nstocks and (right) small tick stocks, for events that do not change the price. Symbols correspond to the true value, and lines\nto the approximation. To illustrate the goodness of \ufb01t on a stock by stock basis, we calculated the absolute di\ufb00erence between\nthe true and the approximated value, the average of this quantity across stocks is indicated by the error bars. The inaccuracy\nfor large \u2113is due to a \ufb01nite size e\ufb00ect in matrix inversion. The data are labeled according to \u03c0 in the legend.\n 0\n 0.25\n 0.5\n 0.75\n 1\n100\n101\n102\n103\nR\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO\u2019\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103 0\n 0.5\n 1\n 1.5\n 2\nl (events)\nFigure 15: Comparison of true and approximated normalized response functions R\u03c0(\u2113) of the \ufb01nal model for (left) large tick\nstocks and (right) small tick stocks, for events that change the price. The inaccuracy for large \u2113is due to a \ufb01nite size e\ufb00ect in\nmatrix inversion. Symbols correspond to the true value, and lines to the approximation. To illustrate the goodness of \ufb01t on a\nstock by stock basis, we calculated the absolute di\ufb00erence between the true and the approximated value, the average of this\nquantity across stocks is indicated by the error bars. The data are labeled according to \u03c0 in the legend.\nWe can now compute the average response functions R\u03c0(\u2113) and the di\ufb00usion curve D(\u2113) within this model, and\ncompare the results with empirical data.\nFor the response functions, the addition of the \ufb02uctuating gap term in Eq. (30) corrects the small discrepancies\nfound within the constant impact model for large tick stocks. It also allows one to capture very satisfactorily the\nresponse function for small tick stocks, see Figs. 14 and 15.6\nA much more stringent test of the model is to check the behaviour of the di\ufb00usion curve D(\u2113). The exact calculation\nin fact involves three and four-point correlation functions, for which we have no model. A closure scheme where these\n6 Note that in making these plots we neglected the \ufb01rst 30 and last 40 minutes of trading days, so they slightly di\ufb00er from those in Sec.\n6. The results of the constant gap model are essentially unchanged regardless of such an exclusion.\n\n20\nhigher correlation functions are assumed to factorize yields the following approximation:\nD(\u2113) =\n\n(pt+\u2113\u2212pt)2\u000b\n\u2248\nX\n0\u2264t\u2032,t\u2032\u2032<\u2113\nX\n\u03c01\nX\n\u03c02\nP(\u03c01)P(\u03c02)C\u03c01,\u03c02(t\u2032 \u2212t\u2032\u2032)\u2206R\n\u03c01\u2206R\n\u03c02 +\n2\nX\n\u2212\u2113<t<\u2113\nX\n\u03c02,\u03c03\nX\n\u03c4>0\n(\u2113\u2212|t|)\u2206R\n\u03c03\u03ba+\n\u03c02,\u03c03(\u03c4, t)C\u03c02,\u03c03(t + \u03c4)P(\u03c02)P(\u03c03) +\nX\n\u2212\u2113<t<\u2113\nX\n\u03c02,\u03c04\nX\n\u03c4,\u03c4 \u2032>0\n(\u2113\u2212|t|)\u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, t)C\u03c02,\u03c04(\u03c4 \u2212\u03c4 \u2032 + t)P(\u03c02)P(\u03c04),\n(33)\nwhere\n\u03ba+\n\u03c02,\u03c03(\u03c4, t) =\nX\n\u03c01\n\u03ba\u03c02,\u03c01(\u03c4)[I(t = 0)I(\u03c01 = \u03c03) + I(t \u0338= 0)P(\u03c01) + I(t = \u2212\u03c4)P(\u03c01)\u03a0\u03c02\u03c01(\u03c4)],\n(34)\nand, for t \u22650,\n\u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, t) =\nX\n\u03c01,\u03c03\n\u03ba\u03c02,\u03c01(\u03c4)\u03ba\u03c04,\u03c03(\u03c4 \u2032){I(t = \u03c4 \u2032)I(\u03c01 = \u03c04)P(\u03c03) +\nI(t \u0338= \u03c4 \u2032)P(\u03c01)P(\u03c03)[\u03a0\u03c01,\u03c03(t) + 1]},\n(35)\nwhereas for t < 0, we use \u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, \u2212t) = \u03ba++\n\u03c04,\u03c02(\u03c4 \u2032, \u03c4, t).",
    "chunk_index": 25,
    "start_char": 59561,
    "end_char": 62458,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "X\n\u03c4>0\n(\u2113\u2212|t|)\u2206R\n\u03c03\u03ba+\n\u03c02,\u03c03(\u03c4, t)C\u03c02,\u03c03(t + \u03c4)P(\u03c02)P(\u03c03) +\nX\n\u2212\u2113<t<\u2113\nX\n\u03c02,\u03c04\nX\n\u03c4,\u03c4 \u2032>0\n(\u2113\u2212|t|)\u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, t)C\u03c02,\u03c04(\u03c4 \u2212\u03c4 \u2032 + t)P(\u03c02)P(\u03c04),\n(33)\nwhere\n\u03ba+\n\u03c02,\u03c03(\u03c4, t) =\nX\n\u03c01\n\u03ba\u03c02,\u03c01(\u03c4)[I(t = 0)I(\u03c01 = \u03c03) + I(t \u0338= 0)P(\u03c01) + I(t = \u2212\u03c4)P(\u03c01)\u03a0\u03c02\u03c01(\u03c4)],\n(34)\nand, for t \u22650,\n\u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, t) =\nX\n\u03c01,\u03c03\n\u03ba\u03c02,\u03c01(\u03c4)\u03ba\u03c04,\u03c03(\u03c4 \u2032){I(t = \u03c4 \u2032)I(\u03c01 = \u03c04)P(\u03c03) +\nI(t \u0338= \u03c4 \u2032)P(\u03c01)P(\u03c03)[\u03a0\u03c01,\u03c03(t) + 1]},\n(35)\nwhereas for t < 0, we use \u03ba++\n\u03c02,\u03c04(\u03c4, \u03c4 \u2032, \u2212t) = \u03ba++\n\u03c04,\u03c02(\u03c4 \u2032, \u03c4, t). Direct numerical simulation of Eq. (30) con\ufb01rms that our\napproximation yields D(\u2113) curves which are indistinguishable from those of the true model. As Fig. 16 shows, for\nsmall tick stocks this approximation indeed shows some improvement for large \u2113when compared to the constant gap\nmodel. For small \u2113there is still a discrepancy coming from errors in the data that adds some spurious high frequency\nwhite noise. To account for these, we add an e\ufb00ective, lag-independent constant to D(\u2113), whose value was chosen as\nD0 = 0.04 ticks squared. According to Fig. 16, this substantially improves the \ufb01t for short times, while leaving the\nlong time contribution una\ufb00ected.\nThe conclusion is that our history dependent impact model reproduces the empirical average response function in\na rather accurate way, and also improves the estimation of the di\ufb00usion curve. The discrepancies are expected, since\nwe have neglected several e\ufb00ects, including (i) all volume dependence, (ii) unobserved events deeper in the book and\non other platforms and (iii) higher order, non-linear contributions to model history dependence.\n 0.1\n 0.12\n 0.14\n 0.16\n 0.18\n 0.2\n100\n101\n102\n103\n104\nD(l)/l (ticks2)\nl (events)\ndata\nconst. gap\nfluct. gap\nfluct. gap + D0\nFigure 16: D(\u2113)/\u2113and its approximations for small tick stocks. Symbols correspond to the true result excluding the beginning\nand the end of trading days, the red line corresponds to the permanent impact model with constant gaps, the green line to\nthe \ufb02uctuating gap model (both analytically and by simulation), and the blue line to the \ufb02uctuating gap model plus constant.\nNote: Unlike the small tick data in Fig. 9, the vertical axis was not rescaled here.\n7.4.\nInterpretation: direct impact vs. induced impact\nThe relevance of the history kernels K and \u02dcK can be understood through a simple argument. The sequence of\nevents is characterized by the time series {\u03c0t, \u01ebt} and together with the gaps this series de\ufb01nes the course of the price.\n\n21\nHow will the event \u03c0 at time t a\ufb00ect the price at some later time t + \u2113? This quantity, denoted by G\u2217\n\u03c0(\u2113), is de\ufb01ned\nas the average of the formal total derivative of the price with respect to the past order \ufb02ow:\nG\u2217\n\u03c0(\u2113) =\n\u001cdpt+\u2113\nd\u03be\u03c0\nt\n\u001d\n,\n\u03be\u03c0\nt \u2261I(\u03c0t = \u03c0)\u01ebt.\n(36)\nIt contains two distinct contributions:\n\u2022 A direct one: the immediate price change caused by the event, which is constant in time (zero or non-zero\ndepending on the value of the corresponding gap).",
    "chunk_index": 26,
    "start_char": 61995,
    "end_char": 64897,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "t a\ufb00ect the price at some later time t + \u2113? This quantity, denoted by G\u2217\n\u03c0(\u2113), is de\ufb01ned\nas the average of the formal total derivative of the price with respect to the past order \ufb02ow:\nG\u2217\n\u03c0(\u2113) =\n\u001cdpt+\u2113\nd\u03be\u03c0\nt\n\u001d\n,\n\u03be\u03c0\nt \u2261I(\u03c0t = \u03c0)\u01ebt.\n(36)\nIt contains two distinct contributions:\n\u2022 A direct one: the immediate price change caused by the event, which is constant in time (zero or non-zero\ndepending on the value of the corresponding gap).\nFor example, if right now a large buy market order is\nsubmitted, it will cause an immediate upward jump in the (ask) price. The average of such jumps due to events\nof type \u03c0 is represented by the mean realized gap \u2206R\n\u03c0 .\n\u2022 An induced, dynamic one: the change of the future event rates and their associated gaps. This modeled by Eq.\n(27), and quanti\ufb01ed by the kernels K. To continue the above example of a large market order, it removes the\nbest ask level, and hence we move into a denser part of the order book. The new \ufb01rst gap behind the ask is\non average smaller here. So in e\ufb00ect, our initial event makes the ask gap shrink. In addition, some time after\nwe submit our order, additional sell limit orders will arrive to compensate part of our upwards price pressure.\nThese may move the (ask) price back downwards. If we had decided not to submit our market order, these extra\nlimit orders would not arrive either.\nTo put this decomposition into precise quantitative terms, recall that as an exact identity,\npt+\u2113= pt +\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c01\n\u2206\u03c01,\u01ebt\u2032,t\u2032\u03be\u03c01\nt\u2032 .\n(37)\nThe average derivative of the price with respect to an earlier event is therefore:\nG\u2217\n\u03c0(\u2113) =\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c01\n\u001cd[\u2206\u03c01,\u01ebt\u2032 ,t\u2032\u03be\u03c01\nt\u2032 ]\nd\u03be\u03c0\nt\n\u001d\n=\nX\n\u03c01\n\u001cd[\u2206\u03c01,\u01ebt,t\u03be\u03c01\nt ]\nd\u03be\u03c0\nt\n\u001d\n|\n{z\n}\n\u2206R\n\u03c0\n+\nX\nt<t\u2032<t+\u2113\nX\n\u03c01\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u001cd[\u2206\u03c01,\u01ebt\u2032,t\u2032]\nd\u03be\u03c0\nt\n\u001d\n\u03be\u03c01\nt\u2032 + \u2206R\n\u03c01 \u27e8\u03be\u03c01\nt\u2032 \u27e9d\u03be\u03c0\nt\n|\n{z\n}\n\u02dc\nK\u03c0,\u03c01(t\u2032\u2212t)\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n|\n{z\n}\nK\u03c0,\u03c01(t\u2032\u2212t)\n(38)\nwhere we have introduced the formal de\ufb01nitions of the kernels K and \u02dcK, in a way compatible with the linear model\nspeci\ufb01ed by Eqs. (27) and (28). Therefore, the average price change until time t + \u2113attributed to an event of type \u03c0\nat time t is found to be:\nG\u2217\n\u03c0(\u2113) = \u2206R\n\u03c0 +\nX\n0<t\u2032<\u2113\nX\n\u03c01\nK\u03c0,\u03c01(t\u2032).\n(39)\nNumerically, the G\u2217\n\u03c0\u2019s are given in Fig. 17(left) for small tick stocks, very similar curves were found for large ticks\nbut we will not detail those here.\nThese G\u2217\u2019s are, however, di\ufb00erent from the bare response functions G\u03c0 which are de\ufb01ned as a partial derivative of\nthe price with respect to event \ufb02ow (see Eqs. (4), (16)), where all events except the one occuring at t are kept \ufb01xed:\nG\u03c0(\u2113) =\n\u001c\u2202pt+\u2113\n\u2202\u03be\u03c0\nt\n\u001d\n.",
    "chunk_index": 27,
    "start_char": 64465,
    "end_char": 66990,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "17(left) for small tick stocks, very similar curves were found for large ticks\nbut we will not detail those here.\nThese G\u2217\u2019s are, however, di\ufb00erent from the bare response functions G\u03c0 which are de\ufb01ned as a partial derivative of\nthe price with respect to event \ufb02ow (see Eqs. (4), (16)), where all events except the one occuring at t are kept \ufb01xed:\nG\u03c0(\u2113) =\n\u001c\u2202pt+\u2113\n\u2202\u03be\u03c0\nt\n\u001d\n.\n(40)\nFollowing the logic of the previous calculation and reindexing the terms, one \ufb01nds, within the linear model\nG\u03c0(\u2113) = \u2206R\n\u03c0 +\nX\nt<t\u2032<t+\u2113\nX\n\u03c01\n\u001c\u2202\u2206\u03c01,\u01ebt\u2032,t\u2032\n\u2202\u03be\u03c0\nt\n\u03be\u03c01\nt\u2032\n\u001d\n= \u2206R\n\u03c0 +\nX\n0<t\u2032<\u2113\nX\n\u03c01\nh\nK\u03c0,\u03c01(t\u2032) \u2212\u02dcK\u03c0,\u03c01(t\u2032)\ni\n.\n(41)\nWhat is the di\ufb00erence between Eq. (39) and Eq. (41)? In the former, we calculate the total price change until time\nt + \u2113due to the initial event, and this includes the adaptation of future event \ufb02ow and of future gaps. In the latter,\nwe only keep the possible jump due to this event and the adaptation of gaps, but not of the event \ufb02ow, which is\n\n22\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n100\n101\n102\nG*\n\u03c0 (ticks)\nl (events)\n100\n101\n102\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n 0\n 0.02\n\u03b4G*\n\u03c0 (ticks)\nl (events)\n\u03c0=MO0\n\u03c0=MO\u2019\n\u03c0=CA0\n\u03c0=LO0\n\u03c0=CA\u2019\n\u03c0=LO\u2019\nFigure 17: (left) Final estimate of the total average price change G\u2217\n\u03c0(\u2113) due to an event \u03c0, based on Eq. (39), for small tick\nstocks. (right) Contribution of gap \ufb02exibility to the price change: \u03b4G\u2217\n\u03c0(\u2113) calculated from Eq. (42) for small tick stocks. The\ncurves are labeled according to \u03c0 in the legend.\nassumed to be \ufb01xed. This omission is indeed consistent with Eq. (16), since the e\ufb00ect of event \ufb02ow adaptation is\nalready accounted for: the equation is based on the true event \ufb02ow, and hence already includes the full correlation\nstructure between events.\nWhen the tick is small, the gaps are allowed to \ufb02uctuate and adapt to the order \ufb02ow. The extra impact contribution\nis therefore captured by the above term:\n\u03b4G\u2217\n\u03c0(\u2113) =\nX\n0<t<\u2113\nX\n\u03c01\nh\nK\u03c0,\u03c01(t) \u2212\u02dcK\u03c0,\u03c01(t)\ni\n.\n(42)\nOur \ufb01nal model de\ufb01ned by Eqs. (30) and (32) amounts to adding this \ufb02uctuating gap contribution to the average\nrealized gap in the bare propagator, i.e.,\nG\u03c0(\u2113) = \u2206R\n\u03c0 + \u03b4G\u2217\n\u03c0(\u2113).\n(43)\nThe new second term describes the contribution of the gap \u201ccompressibility\u201d to the impact of an event up to a time\nlag \u2113, and it is shown in Fig. 17(right). Perhaps surprisingly, it appears that a small market order MO0 \u201csoftens\u201d the\nbook for small ticks: the gaps tend to grow on average and \u03b4G\u2217\nMO0 is positive. Price changing events on the other\nhand \u201charden\u201d the book, for all stocks the contribution is negative. Queue \ufb02uctuations (CA0 and LO0) seem less\nimportant, but for small ticks these types of events also harden the book.",
    "chunk_index": 28,
    "start_char": 66619,
    "end_char": 69243,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "17(right). Perhaps surprisingly, it appears that a small market order MO0 \u201csoftens\u201d the\nbook for small ticks: the gaps tend to grow on average and \u03b4G\u2217\nMO0 is positive. Price changing events on the other\nhand \u201charden\u201d the book, for all stocks the contribution is negative. Queue \ufb02uctuations (CA0 and LO0) seem less\nimportant, but for small ticks these types of events also harden the book. For large ticks \u03b4G\u2217\u2019s are found to be about\ntwo orders of magnitude smaller, which con\ufb01rms that gap \ufb02uctuations can be neglected to a good approximation in\nthat case.\n8.\nCONCLUSIONS\nPrevious studies have focused on the impact of market orders only and have concluded that this impact decays in\nsuch a way to o\ufb00set the correlation of the sign of the trades. The underlying mechanism is that market orders on one\nside of the book attract compensating limit orders. These limit orders do not necessarily change the best limits, but\nare such that the conditional impact of a buy trade following other buy trades is smaller than the conditional impact\nof a sell trade following buy trades. As pointed out in Gerig [14], the strength of this asymmetric liquidity e\ufb00ect is\nthe dominant e\ufb00ect that mitigates persistent trends in prices. Our study con\ufb01rms this \ufb01nding: events happening on\nthe same side of the book are long-range correlated, but the signed correlation function (that assigns an opposite sign\nto limit order and market order on the same side of the book) is short ranged, demonstrating the compensating e\ufb00ect\nalluded to above.\nThis e\ufb00ect leads to a strong \u201cdressing\u201d of the bare impact of market orders by limit orders. In fact, by including \u2013\nbesides the market orders \u2013 all limit orders and cancellations at the bid/ask, the price becomes a pure jump process.\nEvery price change, whatever its cause (news, information or noise), can be attributed to exactly one of these events.\nIn a \ufb01rst approximation, the various event types lead to a constant jump size that equals the average price change\n\n23\nthey cause. This simple picture works very well for large tick stocks, where both the average impact of all event types,\nand the volatility, are quantitatively reproduced by a constant jump model. The situation is di\ufb00erent for small tick\nstocks, where the history dependence of these otherwise permanent jumps becomes important. Note that the e\ufb00ect\ndiscussed here is related to, but di\ufb00erent from the Lillo-Farmer model that connects the temporal decay of the dressed\nmarket order impact to the history dependent conditional impact of a new trade. Here, we are speaking of the history\ndependence in a framework where the impact of all events, not only of market orders, is already accounted for.\nAnother important observation is that not only the jump sizes are history dependent, the events themselves also\nbehave in an adaptive way. An event can induce further events that amplify or dampen its own e\ufb00ect.",
    "chunk_index": 29,
    "start_char": 68855,
    "end_char": 71758,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "conditional impact of a new trade. Here, we are speaking of the history\ndependence in a framework where the impact of all events, not only of market orders, is already accounted for.\nAnother important observation is that not only the jump sizes are history dependent, the events themselves also\nbehave in an adaptive way. An event can induce further events that amplify or dampen its own e\ufb00ect. As it is well\nknown, the arrival of excess buy market orders is shortly followed by additional sell limit orders, but this is just one\nmanifestation of such adaptive dynamics. For example, the reverse process, i.e. market orders following an excess\nof limit orders are also present, albeit with some delay and a smaller intensity. Our description of these and similar\nmechanisms, also involving cancellations, is a generalization of the theory of market order price impact in the related\nliterature.\nIn sum, the dynamics of prices consists of three processes: instantaneous jumps due to events, events inducing further\nevents and thereby a\ufb00ecting the future jump probabilities (described by the correlation between events), and events\nexerting pressure on the gaps behind the best price and thereby a\ufb00ecting the future jump sizes. By approximating\nthis third e\ufb00ect with a linear regression process, we have written down an explicit model, Eq. (30), that accounts\nvery satisfactorily for most of our observations. We have shown how to calibrate such a model on empirical data\nusing some auxiliary kernels K and \u02dcK de\ufb01ned by Eqs. (27) and (28). This way of extracting the bare propagator G\u03c0,\nmotivated by the above decomposition, seems to be less prone to numerical errors than the \u201cbrute force\u201d inversion\nmethod used in Sec. 5.\nThe methods proposed in this work are rather simple and general, and can be adapted to measure the impact of\nany type of trade once a discrete categorization is adopted. One could for example subdivide the category MO0 into\nsmall volumes and large volumes, or look at the impact of di\ufb00erent option trades on the underlying, etc. Here, we\nhave established that the bare impact of market orders is clearly larger than that of limit orders and that the bare\nimpact of price-changing events shows only partial decay on the time scale that we are able to probe (1000 events\nonly corresponds to a few minutes). It would certainly be interesting to study the long time behavior of these bare\nimpact functions, as well as to understand how these impact functions behave overnight.\nWe hope to have provided here a consistent and complete framework to describe price \ufb02uctuations and impact\nat the \ufb01nest possible scale. Our approach can be seen as a \u201cmicroscopic\u201d construction of VAR-like models, with a\nclearly motivated regression structure. We believe that the interaction between market orders and limit orders, and\nthe impact of these two types of orders, are crucial to understand the dynamics of the markets, the origin of volatility\nand the incipient instabilities that can arise when these counteracting forces are not on even keel.",
    "chunk_index": 30,
    "start_char": 71364,
    "end_char": 74416,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "a consistent and complete framework to describe price \ufb02uctuations and impact\nat the \ufb01nest possible scale. Our approach can be seen as a \u201cmicroscopic\u201d construction of VAR-like models, with a\nclearly motivated regression structure. We believe that the interaction between market orders and limit orders, and\nthe impact of these two types of orders, are crucial to understand the dynamics of the markets, the origin of volatility\nand the incipient instabilities that can arise when these counteracting forces are not on even keel. The interesting next\nstep would be to analyze in detail these situations, where large liquidity \ufb02uctuations arise, and the above \u2018average\u2019\nmodel breaks down. On a longer term, a worthwhile project is to construct a coarse-grained, continuous time model\nfrom the above microstructural bricks, and justify or reject the slew of models that have been proposed to describe\n\ufb01nancial time series (L\u00b4evy processes, GARCH, multifractal random walk, etc.).\n[1] J. Hasbrouck, Empirical Market Microstructure: The Institutions, Economics, and Econometrics of Securities Trading\n(Oxford University Press, 2007).\n[2] S. Mike and J. D. Farmer, Journal of Economic Dynamics and Control 32, 200 (2008).\n[3] J.-P. Bouchaud, Y. Gefen, M. Potters, and M. Wyart, Quantitative Finance 4, 176 (2004).\n[4] J.-P. Bouchaud, J. Kockelkoren, and M. Potters, Quantitative Finance 6, 115 (2006).\n[5] R. K. Lyons, The Microstructure Approach to Exchange Rates (MIT Press, 2006).\n[6] J.-P. Bouchaud, J. D. Farmer, and F. Lillo, How markets slowly digest changes in supply and demand, in Handbook of\nFinancial Markets: Dynamics and Evolution, North-Holland, Elsevier (2009).\n[7] A. Kyle, Econometrica 53, 1315 (1985).\n[8] N. Hautsch and R. Huang, The market impact of a limit order (2009), working Paper 2009/23, Center for Financial Studies,\nFrankfurt am Main.\n[9] T. Hendershott, C. M. Jones, and A. J. Menkveld, Does algorithmic trading improve liquidity? (2008), wFA 2008 Paper,\nto be published in Journal of Finance.\n[10] B. Biais and P.-O. Weill, Liquidity shocks and order book dynamics (2009), nBER Working Papers 15009.\n[11] J.-P. Bouchaud and M. Potters, Theory of Financial Risk (Cambridge University Press, Cambridge, 2000).\n[12] J. Hasbrouck, Journal of Finance 46, 176 (1991).\n[13] J. D. Farmer, A. Gerig, F. Lillo, and S. Mike, Quantitative Finance 6, 107 (2006).\n[14] A. Gerig, A theory for market impact: How order \ufb02ow a\ufb00ects stock price, PhD thesis (2008), arXiv:0804.3818.\n[15] C. Jones, G. Kaul, and M. L. Lipson, Review of Financial Studies 7, 631 (1994).\n\n24\n[16] F. Lillo and J. D. Farmer, Studies in Nonlinear Dynamics and Econometrics 8, 1 (2004).\n[17] P. Weber and B. Rosenow, Quantitative Finance 5, 357 (2005).\n[18] J. D. Farmer, L. Gillemot, F. Lillo, S. Mike, and A. Sen, Quantitative Finance 4, 383 (2004).\n[19] J. D. Farmer and N. Zamani, Eur. Phys. J. B 55, 189 (2007).\n[20] A. Ponzi, F. Lillo, and R. N. Mantegna (2006), physics/0608032.\nAppendix A: The dependence of the spread on the event \ufb02ow\nIn this appendix we show how the framework introduced in the main text can be used to study the dynamics of\nthe bid-ask spread.",
    "chunk_index": 31,
    "start_char": 73889,
    "end_char": 77042,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "[17] P. Weber and B. Rosenow, Quantitative Finance 5, 357 (2005).\n[18] J. D. Farmer, L. Gillemot, F. Lillo, S. Mike, and A. Sen, Quantitative Finance 4, 383 (2004).\n[19] J. D. Farmer and N. Zamani, Eur. Phys. J. B 55, 189 (2007).\n[20] A. Ponzi, F. Lillo, and R. N. Mantegna (2006), physics/0608032.\nAppendix A: The dependence of the spread on the event \ufb02ow\nIn this appendix we show how the framework introduced in the main text can be used to study the dynamics of\nthe bid-ask spread. For the spread St, one can write an exact formula very similar to Eq. (21):\nSt+\u2113= St +\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c0\n\u2206\u03c0,\u01ebt\u2032,t\u2032I(\u03c0t\u2032 = \u03c0),\n(A1)\nwhere \u2206\u03c0 = \u00b12\u2206\u03c0 with the + sign for \u03c0 = MO\u2032, CA\u2032 and the \u2212sign for \u03c0 = LO\u2032. The other three \u2206\u2019s are zero, just\nas the respective \u2206\u2019s were. The above equation is accurate because our model includes all the possible events that\ncan change the best quotes, and thus all the possible events that can change the spread.\nHowever, when formulating a permanent impact model for the spread dynamics in the same spirit as we did for\nthe price, one should bear in mind that the spread is a mean-reverting quantity that oscillates around a mean value\n\u27e8S\u27e9. In other words, the average value of St+\u2113when \u2113\u2192\u221eis equal to \u27e8S\u27e9, independently of the initial value St.\nTherefore:\nlim\n\u2113\u2192\u221e\u27e8St+\u2113\u2212St|St\u27e9= lim\n\u2113\u2192\u221e\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c02\n\n\u2206\u03c02,t\u2032,\u01ebt\u2032 I(\u03c0t\u2032 = \u03c02)|St\n\u000b\n= \u27e8S\u27e9\u2212St.\n(A2)\nSince the right hand side obviously depends on St, the conditional value\n\n\u2206\u03c02,t\u2032,\u01ebt\u2032 I(\u03c0t\u2032 = \u03c02)|St\n\u000b\nalso has to. This\nmeans that the event \ufb02ow and possibly the gaps are correlated with the spread, and they adjust such that the spread\nmean reverts. If this were not true, the spread would follow an unbounded random walk. To illustrate this, the spread\ndependence of realized gaps is shown in Fig. 18.\n 0.5\n 1\n 2\n 4\n 8\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n<\u2206\u03c0\nR|S> (ticks)\nS (ticks)\n\u03c0=MO\u2019\n\u03c0=CA\u2019\n\u03c0=LO\u2019\nFigure 18: Realized gaps as a function of the spread (after removing the beginning and the end of the trading days).\nA related study by Ponzi et al. [20] based on a selection of stocks from LSE comes to a similar conclusion. They\n\ufb01nd that both realized gaps and event rates are functions of the spread. In particular limit orders are placed deeper\nin the spread when the spread is larger. In addition, they show that the rate of transactions decreases with larger\nspreads, while the rates of cancellations and incoming limit orders increase sharply.\nSuch an adaptive behavior can be quanti\ufb01ed through Eq. (A1), for \u2113= 1 this reads\nSt+1 \u2212St = \u2206MO\u2032,\u01ebt,tI(\u03c0t = MO\u2032) + \u2206CA\u2032,\u01ebt,tI(\u03c0t = CA\u2032) \u2212\n\f\f\u2206LO\u2032,\u01ebt,t\n\f\f I(\u03c0t = LO\u2032),\n(A3)\nwhere we took the negative absolute value of \u2206LO\u2032 to emphasize that it is strictly negative, while all the other\nquantities in the equation are non-negative.",
    "chunk_index": 32,
    "start_char": 76558,
    "end_char": 79293,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "of transactions decreases with larger\nspreads, while the rates of cancellations and incoming limit orders increase sharply.\nSuch an adaptive behavior can be quanti\ufb01ed through Eq. (A1), for \u2113= 1 this reads\nSt+1 \u2212St = \u2206MO\u2032,\u01ebt,tI(\u03c0t = MO\u2032) + \u2206CA\u2032,\u01ebt,tI(\u03c0t = CA\u2032) \u2212\n\f\f\u2206LO\u2032,\u01ebt,t\n\f\f I(\u03c0t = LO\u2032),\n(A3)\nwhere we took the negative absolute value of \u2206LO\u2032 to emphasize that it is strictly negative, while all the other\nquantities in the equation are non-negative. The unconditional expectation value of the left hand side is zero, since\n\u27e8St+1\u27e9= \u27e8St\u27e9. Thus on average the spread-altering e\ufb00ect of market orders, cancellations and limit orders balances\nout. If for example St > \u27e8S\u27e9, then the spread mush shrink back to its mean, so the left hand side must be negative.\n\n25\nThe only way for the right hand side to become negative as well, is if the spread-opening contribution of market\norders/cancellations decreases and/or the spread-closing e\ufb00ect of limit orders increases.\nThis is possible by the variation of both the gaps and the event rates, for our purposes it is enough to introduce a\ncombined description the two. The simplest possible model of such adaptive dynamics is to assume that the conditional\ndistribution of the random variable \u2206\u03c0,\u01ebt,tI(\u03c0t = \u03c0) given the current spread value St can be approximated by that\nof\n\u2206\nR\n\u03c0 I(\u03c0t = \u03c0)\n\"\n1 + \u03b1\n\u2206\nR\n\u03c0\n(\u27e8S\u27e9\u03c0 \u2212St)\n#\n,\nwhere I(\u03c0t = \u03c0) now follows its unconditional distribution, \u27e8S\u27e9\u03c0 is the average value of the spread at the time of\nevents of type \u03c0, and \u03b1 is a constant parameter characterizing the strength of mean-reversion.\nEven though the term in the brackets is understood to include contributions from both gap and rate adjustments,\ntechnically such a model only amounts to substituting\n\u2206\u03c0,t\u2032,\u01ebt\u2032 = \u2206\nR\n\u03c0 + \u03b1(\u27e8S\u27e9\u03c0 \u2212St\u2032),\n(A4)\nfor the gap dynamics in Eq. (A1), and this makes analytical calculations possible. One \ufb01nds that the modi\ufb01ed spread\nbehavior is such that\nSt+\u2113= St(1 \u2212\u03b1)\u2113+\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c0\n(1 \u2212\u03b1)t+\u2113\u2212t\u2032+1I(\u03c0t\u2032 = \u03c0)(\u2206\nR\n\u03c0 + \u03b1 \u27e8S\u27e9\u03c0),\n(A5)\nfrom which one deduces the spread response function:\nRS\n\u03c01(\u2113) = \u27e8(St+\u2113\u2212St)I(\u03c0t = \u03c01)\u27e9/P(\u03c01) =\n\u0002\n\u27e8S\u27e9\u2212\u27e8S\u27e9\u03c01\n\u0003\n[1 \u2212(1 \u2212\u03b1)\u2113] +\nX\nt\u2264t\u2032<t+\u2113\nX\n\u03c02\n(1 \u2212\u03b1)(t+\u2113\u22121)\u2212t\u2032\u2206\nR\n\u03c02P(\u03c02)\u03a0\u03c01,\u03c02(t\u2032 \u2212t).\n(A6)\nEq. (A6) tells us that the dynamics of the spread is related to the autocorrelation of (unsigned) event types, just\nas the response function was related to the signed event autocorrelation functions (except for the inclusion of \u03b1 to\ndescribe adjustment to the event \ufb02ow).\nTo test this model on real data, in order to remove the e\ufb00ect of intraday periodicity and overnight e\ufb00ects, we will\nneglect the \ufb01rst 30 and last 40 minutes of trading days, and in all correlation functions we will only consider times\nwhen both events are within the same day. The spread response functions and their approximations without allowing\nfor gap \ufb02uctuations (\u03b1 = 0) are shown in Figs. 19 and 20.",
    "chunk_index": 33,
    "start_char": 78841,
    "end_char": 81697,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "the event \ufb02ow).\nTo test this model on real data, in order to remove the e\ufb00ect of intraday periodicity and overnight e\ufb00ects, we will\nneglect the \ufb01rst 30 and last 40 minutes of trading days, and in all correlation functions we will only consider times\nwhen both events are within the same day. The spread response functions and their approximations without allowing\nfor gap \ufb02uctuations (\u03b1 = 0) are shown in Figs. 19 and 20. The constant gap approximation works well for large tick\nstocks, but for small tick stocks only for short times, up to l \u224810 \u221230 events. This is in line with the \ufb01ndings of Sec.\n6 for the response function of price.\nOne \ufb01nds that the discrepancy for small tick stocks has two origins. First, due to the intraday non-stationarity of\nthe spread the relationship\nlim\n\u2113\u2192\u221e\n\u2206\nR\n\u03c02 \u27e8I(\u03c0t = \u03c01)I(\u03c0t+\u2113= \u03c02)\u27e9\n\n\u2206\u03c02,\u01ebt+\u2113,t+\u2113I(\u03c0t = \u03c01)I(\u03c0t+\u2113= \u03c02)\n\u000b = 1\nno longer holds. After excluding the overnight contribution (when t and t + \u2113are in di\ufb00erent days), the gaps in the\ndenominator are no longer sampled from the \ufb01rst \u2113events of the day, where they are systematically larger than at\nthe end of the day. Even after adjusting \u03a0\u2019s to have the correct asymptotic value, one needs to introduce \u03b1 > 0 to\n\ufb01nd an approximately correct shape of the spread response functions. The results for one example stock (AAPL) are\nshown in Fig. 21.\nClearly this model is only intended as a \ufb01rst approximation, since it leads to an exponential decay of the spread\nautocorrelation function in constrast with the long memory found in the data, see Fig. 22. It is possible to give a\nmore complete description of the spread along the lines of Sec. 7.3, but we will leave this for future research.\nAppendix B: Plots of various correlation functions\nIn this appendix we show all the signed and some unsigned correlation functions, signed and unsigned, for small\nand large tick stocks separately, see Figs. 23-30.\n\n26\n-0.03\n 0\n 0.03\n 0.06\n 0.09\n 0.12\n 0.15\n100\n101\n102\n103\nRS\n\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO0\n\u03c0=CA0\n\u03c0=LO0\n100\n101\n102\n103\n-0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nl (events)\nFigure 19: Comparison of true and approximated normalized spread response function RS\n\u03c0(\u2113)/P(\u03c0) for (left) large tick stocks\nand (right) small tick stocks for events that do not change the price. Symbols correspond to the true value, and lines to the\napproximation. The curves are labeled according to \u03c0 in the legend.\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n100\n101\n102\n103\nRS\n\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO\u2019\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\nl (events)\nFigure 20: Comparison of true and approximated normalized spread response function RS\n\u03c0(\u2113)/P(\u03c0) for (left) large tick stocks and\n(right) small tick stocks for events that change the price. Symbols correspond to the true value, and lines to the approximation.\nThe curves are labeled according to \u03c0 in the legend.",
    "chunk_index": 34,
    "start_char": 81276,
    "end_char": 84151,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "in the legend.\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n100\n101\n102\n103\nRS\n\u03c0 and its approximation (ticks)\nl (events)\n\u03c0 = MO\u2019\n\u03c0=CA\u2019\n\u03c0=LO\u2019\n100\n101\n102\n103\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\nl (events)\nFigure 20: Comparison of true and approximated normalized spread response function RS\n\u03c0(\u2113)/P(\u03c0) for (left) large tick stocks and\n(right) small tick stocks for events that change the price. Symbols correspond to the true value, and lines to the approximation.\nThe curves are labeled according to \u03c0 in the legend.\n\n27\n0.00\n0.50\n1.00\n1.50\nMO0\n0.40\n0.80\n1.20\nMO\u2019\n-0.20\n0.00\n0.20\nCA0\n0.40\n0.80\n1.20\nCA\u2019\n-0.15\n-0.10\n-0.05\n0.00\n100\n101\n102\n103\nl (events)\nLO0\n100\n101\n102\n103\n-1.20\n-0.80\n-0.40\nl (events)\nLO\u2019\nFigure 21: Comparison of the spread response functions of AAPL for four di\ufb00erent cases: Eq. (A6) with \u03b1 = 0 (red), Eq. (A6)\nwith adjusted \u03a0\u2019s and \u03b1 = 0 (green), Eq. (A6) with adjusted \u03a0\u2019s and \u03b1 = 10\u22122 (blue), true response functions (black). Price\nin ticks.\n10-2\n10-1\n100\n100\n101\n102\n103\n104\nnorm. autocorrelation of spread\nl (events)\nlarge t.\nsmall t.\nexp. fit\nFigure 22: Autocorrelation function of the spread (after removing the beginning and the end of the trading days). One can see\nthat the exponential decay suggested by our simpli\ufb01ed model captures the short-time dynamics, but not the slow decay lasting\nfor thousands of events.\n\n28\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO\u2019, \u03c02(l)\nFigure 23: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for large tick stocks, (left) \u03c01 = MO0, (right) \u03c01 = MO\u2032.\nThe curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCCA0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCLO0, \u03c02(l)\nFigure 24: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for large tick stocks, (left) \u03c01 = CA0, (right) \u03c01 = LO0.\nThe curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.",
    "chunk_index": 35,
    "start_char": 83658,
    "end_char": 86148,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "= MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCLO0, \u03c02(l)\nFigure 24: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for large tick stocks, (left) \u03c01 = CA0, (right) \u03c01 = LO0.\nThe curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n\n29\n-10-3\n-10-2\n-10-1\n-100\n-101\n-102\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101\n 102 100\n101\n102\n103\n104\nCCA\u2019, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCLO\u2019, \u03c02(l)\nFigure 25: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for large tick stocks, (left) \u03c01 = CA\u2032, (right) \u03c01 = LO\u2032.\nThe curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCMO\u2019, \u03c02(l)\nFigure 26: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for small tick stocks, (left) \u03c01 = MO0, (right) \u03c01 = MO\u2032.\nThe curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n\n30\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCCA0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCLO0, \u03c02(l)\nFigure 27: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for small tick stocks, (left) \u03c01 = CA0, (right) \u03c01 = LO0.\nThe curves correspond to the six possible values of \u03c02\u2019s, see the legend of Fig. 26 for details. The bottom panels show the\nnegative values.\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCCA\u2019, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101 100\n101\n102\n103\n104\nCLO\u2019, \u03c02(l)\nFigure 28: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for small tick stocks, (left) \u03c01 = CA\u2032, (right) \u03c01 = LO\u2032.\nThe curves correspond to the six possible values of \u03c02\u2019s, see the legend of Fig. 26 for details. The bottom panels show the\nnegative values.",
    "chunk_index": 36,
    "start_char": 85801,
    "end_char": 88242,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "103\n104\nCLO\u2019, \u03c02(l)\nFigure 28: The normalized, signed event correlation functions C\u03c01,\u03c02(\u2113) for small tick stocks, (left) \u03c01 = CA\u2032, (right) \u03c01 = LO\u2032.\nThe curves correspond to the six possible values of \u03c02\u2019s, see the legend of Fig. 26 for details. The bottom panels show the\nnegative values.\n\n31\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n\u03c02 = MO0\n\u03c02 = MO\u2019\n\u03c02 = CA0\n\u03c02 = LO0\n\u03c02 = CA\u2019\n\u03c02 = LO\u2019\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO\u2019, \u03c02(l)\nFigure 29: The normalized, unsigned event correlation functions \u03a0\u03c01,\u03c02(\u2113) for large tick stocks, (left) \u03c01 = MO0, (right)\n\u03c01 = MO\u2032. The curves are labeled by their respective \u03c02\u2019s in the legend. The bottom panels show the negative values.\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO0, \u03c02(l)\n-10-3\n-10-2\n-10-1\n-100\n-101\n100\n101\n102\n103\n104\nl (events)\n 10-3\n 10-2\n 10-1\n 100\n 101\n100\n101\n102\n103\n104\n\u03a0MO\u2019, \u03c02(l)\nFigure 30: The normalized, unsigned event correlation functions \u03a0\u03c01,\u03c02(\u2113) for small tick stocks, (left) \u03c01 = MO0, (right)\n\u03c01 = MO\u2032. The curves correspond to the six possible values of \u03c02\u2019s, see the legend of Fig. 29 for details. The bottom panels\nshow the negative values.",
    "chunk_index": 37,
    "start_char": 87952,
    "end_char": 89288,
    "paper_title": "The price impact of order book events market order",
    "paper_category": "q-fin.TR",
    "paper_filename": "The_price_impact_of_order_book_events_market_order.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/The_price_impact_of_order_book_events_market_order.pdf"
  },
  {
    "text": "TRANSFORMERS VERSUS LSTMS FOR ELECTRONIC TRADING\nA PREPRINT\nPaul Bilokon\nDepartment of Computing\nImperial College London\nSouth Kensington Campus\nLondon SW7 2AZ\npaul.bilokon@imperial.ac.uk\nYitao Qiu\nDepartment of Computing\nImperial College London\nSouth Kensington Campus\nLondon SW7 2AZ\nyitao.qiu21@imperial.ac.uk\nSeptember 21, 2023\nABSTRACT\nWith the rapid development of artificial intelligence, long short term memory (LSTM), one\nkind of recurrent neural network (RNN), has been widely applied in time series prediction.\nLike RNN, Transformer is designed to handle the sequential data. As Transformer achieved\ngreat success in Natural Language Processing (NLP), researchers got interested in Trans-\nformer\u2019s performance on time series prediction, and plenty of Transformer-based solutions\non long time series forecasting have come out recently. However, when it comes to financial\ntime series prediction, LSTM is still a dominant architecture. Therefore, the question this\nstudy wants to answer is: whether the Transformer-based model can be applied in financial\ntime series prediction and beat LSTM.\nTo answer this question, various LSTM-based and Transformer-based models are compared\non multiple financial prediction tasks based on high-frequency limit order book data. A\nnew LSTM-based model called DLSTM is built and new architecture for the Transformer-\nbased model is designed to adapt for financial prediction. The experiment result reflects\nthat the Transformer-based model only has the limited advantage in absolute price sequence\nprediction. The LSTM-based models show better and more robust performance on difference\nsequence prediction, such as price difference and price movement.\narXiv:2309.11400v1 [q-fin.TR] 20 Sep 2023\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n1\nIntroduction\nFinancial time series prediction is a significant task in investing and market-making activities. The Efficient\nMarket Hypothesis proposed by Eugene [1] states that all the information of the asset\u2019s inner value has\nalready precisely and completely reflected on the asset price, and it is impossible to beat the market by\nfinancial prediction. However, whether the market is efficient is questionable because technical analysis\n[2] believes the financial market is the physical movement of price (or features derived from prices). The\nprice information can be interpreted by waves and patterns that can repeat themselves, where it is possible\nto make profitable buy or sell decisions in advance [3, 4]. During the prediction, challenging factors are\nnoise and volatile features because price information is generally non-linear and non-stationary [5]. Lots\nof models are proposed to solve the financial time series problem. A typical linear model for regression\nis Auto-Regressive Integrated Moving average (ARIMA) [6] and its variations, which requires domain\nexpertise to handcraft features.\nWith the development of machine learning, Artificial Neural Networks\n(ANN) raises great interest because of their capability to extract more abstract features from data and find a\nhidden non-linear relationship without assumptions or human expertise by adding more parameters. Long\nshort-term memory (LSTM), which is a special recurrent neural network (RNN) architecture that has been\nproven successful in the application of sequential data, is widely applied to handwriting recognition [7] and\nspeech recognition [8]. Like RNN, the Transformer [9] is also used to handle the sequential data.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3499,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "Artificial Neural Networks\n(ANN) raises great interest because of their capability to extract more abstract features from data and find a\nhidden non-linear relationship without assumptions or human expertise by adding more parameters. Long\nshort-term memory (LSTM), which is a special recurrent neural network (RNN) architecture that has been\nproven successful in the application of sequential data, is widely applied to handwriting recognition [7] and\nspeech recognition [8]. Like RNN, the Transformer [9] is also used to handle the sequential data. Compared\nto LSTM, the Transformer does not need to handle the sequence data in order, which instead confers the\nmeaning of the sequence by the Self-attention mechanism.\nApplying LSTM and Transformer for financial time series prediction is a popular trend nowadays. Depending\non the historical financial data, researchers usually make predictions for the future numerical prices, price\ndifference, return or future price movement (rise, stationary, fall). Although LSTM and Transformer are\napplied in different aspects for this problem. There are mainly two research directions:\n1) Make predictions based on high-frequency Limit Order Book (LOB) data and its derived features,\nsuch as Volume Order Imbalance (VOI) and Trade Flow Imbalance (TFI) [10\u201314].\n2) Make predictions based on OHLC (Open, High, Low, Close) data and its derived financial indices,\nsuch as Relative Strength Index (RSI) and Moving average convergence divergence (MACD) [15\u2013\n25].\nSince 2017, the Transformer has been increasingly used for Natural Language Processing (NLP) problems.\nIt produces more impressive results than RNN, such as machine translation [26] and speech applications\n[27], replacing RNN models such as LSTM in NLP tasks. Recently, a surge of Transformer-based solutions\nfor less explored long time series forecasting problem has appeared [28]. However, as for the financial time\n2\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nseries prediction, LSTM remains the dominant architecture.\nWhether Transformer-based methods can be the right solution for financial time series forecasting is a prob-\nlem worth investigating. Therefore, this paper is going to compare the performance of different Transformer-\nbased and LSTM-based methods on financial time series prediction problems based on LOB data and attempt\nto adapt new models based on Transformer and LSTM. The contributions of this study are summed up as\nfollows:\n1. Systematically compare Transformer-based and LSTM-based methods in different financial predic-\ntion tasks based on high frequency LOB data collected from Binance Exchange. Tasks include (1)\nmid-price prediction, (2) mid-price difference prediction and (3) mid-price movement prediction.\n2. For the first and second tasks, comparisons are all conducted on previous LSTM-based and\nTransformer-based methods. In the first task, the Transformer-based method has around 10% \u221225%\nprediction error less than the LSTM-based method, but the prediction result quality is insufficient\nfor trading. In the second task, the LSTM-based method performs better than the Transformer-based\nmethod, where its highest out-of-sample R2 reaches around 11.5%.\n3. The most significant contribution of this study is in the last task, mid-price movement predic-\ntion. A new LSTM-based model named DLSTM is developed for this task, which combines LSTM\nand the time series decomposition method.",
    "chunk_index": 1,
    "start_char": 2949,
    "end_char": 6389,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "In the first task, the Transformer-based method has around 10% \u221225%\nprediction error less than the LSTM-based method, but the prediction result quality is insufficient\nfor trading. In the second task, the LSTM-based method performs better than the Transformer-based\nmethod, where its highest out-of-sample R2 reaches around 11.5%.\n3. The most significant contribution of this study is in the last task, mid-price movement predic-\ntion. A new LSTM-based model named DLSTM is developed for this task, which combines LSTM\nand the time series decomposition method. This model achieves 63.73% to 73.31% accuracy and\nshows strong profitability and robustness in simulated trading, outperforming previous LSTM and\nTransformer-based methods. In addition, the architecture of previous Transformer-based methods is\nalso changed in order to adapt movement prediction task.\nThe later parts of this study are structured as follows: The background and related work are introduced\nin Section 2. Section 3 describes the formulation of three financial prediction tasks. Section 4 explains\nthe details of previous Transformer-based and LSTM-based methods used for comparison.\nSection 5 is\nthe analysis of experiment results.\nPlease note that the details of the newly developed DLSTM model\nand the architecture changes of Transformer models are explained in Section 5.3 in Section 5. The study\nis organized in this way to provide readers with a better understanding of the relationship among three\nfinancial prediction tasks.\nThe Source Code of this study is available at: https://github.com/772435284/transformers_versus_\nlstms_for_electronic_trading\nAcknowledgements\nWe would like to express our gratitude to Zhipeng Wang for constructive comments\nand suggestions.\n3\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n2\nBackground and Related work\nTime series prediction has been applied in different financial activities. For example, the trader or market\nmaker predicts the future price or the price movement of the assets so that he/she can design trading/market\nmaking strategies based on the prediction results to make profit from it. This part examines papers related\nto time series prediction using LSTM and Transformer. Most of them are related to financial time series\nprediction.\n2.1\nLSTM-based Time Series Prediction Solutions\nLSTM has been widely utilized in financial time series prediction depending on OHLC data and its derived\nfinancial indices. Many works [15\u201319] present predicting stock prices as successful by LSTM. Bidirectional\nLSTM (BiLSTM) is applied to increase the prediction performance [20]. With the rise of NLP, Sequence-to-\nSequence Model (S2S) [29] is proposed and applied in machine translation and question answering. It is\nnow also applied in financial time series prediction by combining LSTM structure and attention mechanisms\ncontributing to higher performance [21, 22].\nLSTM has successfully forecasted high-frequency data depending on the large datasets extracted from the\nlimit order book (LOB). In terms of making predictions upon order book data, Convolution Neural Network\n(CNN) and LSTM are both in favour of research and sometimes they are combined. Sirignano et al. [10]\ntrained a universal model using LSTM to predict the LOB price movement by data from all stocks, which\noutperforms linear and non-linear models trained on a specific asset.",
    "chunk_index": 2,
    "start_char": 5829,
    "end_char": 9211,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "to higher performance [21, 22].\nLSTM has successfully forecasted high-frequency data depending on the large datasets extracted from the\nlimit order book (LOB). In terms of making predictions upon order book data, Convolution Neural Network\n(CNN) and LSTM are both in favour of research and sometimes they are combined. Sirignano et al. [10]\ntrained a universal model using LSTM to predict the LOB price movement by data from all stocks, which\noutperforms linear and non-linear models trained on a specific asset. Zhang et al. [11] combine CNN and\nLSTM to form a new deep neural network architecture called DeepLOB to predict future stock price move-\nments in LOB data outperforming architecture only containing LSTM. Zhang et al. [12] also combine the\nDeepLOB architecture with Seq2Seq and Attention model to forecast multi-horizon future price movements\nin one forward procedure, which reduces the training effort and achieves better performance in long horizon\nprediction than DeepLOB. According to Tsantekidis et al. [13], the structure of CNN-LSTM architecture is\nable to outperform other models on LOB price movement prediction because of its more stable behaviour.\nKolm et al. [14] use the OFI feature derived from the LOB to predict the future min-price return, where the\nCNN-LSTM structure still achieves the best performance.\n2.2\nTransformer-based Time Series Prediction Solutions\nAs Transformer makes a great contribution to NLP [30], many advanced models are proposed, such as\nBERT and GPT-3, which now have more influence in time series prediction. As the canonical Self-attention\nmechanism has O(L2) time and memory complexity, many modifications have been made to the Transformer\nin order to adapt to the time series prediction problem to process the long sequence efficiently. There are\n4\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nmany alternative Transformer models for long time series forecasting problem have been developed recently\n[28]: LogTrans[31], Reformer [32], Informer [33], Autoformer [34], Pyraformer [35] and the recent\nFEDformer [36].\nTheses Transformer models mentioned above are mainly evaluated on non-financial datasets, such as\nelectricity consumption, traffic usage, and solar energy dataset. They achieve a considerable performance\nincrease in accuracy over the LSTM model.\nSome works start applying Transformers for financial time\nseries prediction. Hu [23] uses a Temporal Fusion Transformer with support vector regression (SVR) and\nLSTM to predict stock price. Sridhar et al. [24] predict the Dogecoin price through Transformer, which has\nsuperior performance compared to LSTM. Sonkiya et al. [25] do sentiment analysis by BERT to generate\nthe sentiment score, which is then combined with other financial indices and passed into the Generative\nAdversarial Network (GAN) for stock price prediction. These works [23\u201325] use Transformer-based methods\nto make predictions based on OHLC data and the research on applying Transformer for LOB prediction is\nlimited.\nOverall, LSTM is applied broadly in financial time series prediction and has been tested on various datasets,\nwhile the Transformer is limited. Therefore, this study wants to apply these Transformer-based models to a\nwider region in financial time series prediction to compare their performance to LSTM.",
    "chunk_index": 3,
    "start_char": 8699,
    "end_char": 12021,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "the Generative\nAdversarial Network (GAN) for stock price prediction. These works [23\u201325] use Transformer-based methods\nto make predictions based on OHLC data and the research on applying Transformer for LOB prediction is\nlimited.\nOverall, LSTM is applied broadly in financial time series prediction and has been tested on various datasets,\nwhile the Transformer is limited. Therefore, this study wants to apply these Transformer-based models to a\nwider region in financial time series prediction to compare their performance to LSTM.\n3\nFinancial Time Series Prediction Tasks Formulation\nThis study compares LSTM-based and Transformer-based methods among three financial prediction tasks\nbased on LOB data. In this section, the basic concept of LOB will be first introduced, and then the formulation\nof three financial prediction tasks will be explained in detail. Three tasks are listed below:\n\u2022 Task 1: LOB Mid-Price Prediction\n\u2022 Task 2: LOB Mid-Price Difference Prediction\n\u2022 Task 3: LOB Mid-Price Movement Prediction\n3.1\nLimit Order Book\nWith computers and the Internet, most financial products such as stocks, forex and cryptocurrency are traded\non the electronic market nowadays. Two types of orders exist in the electronic market: limit order and market\norder. According to Gould et al. [37], a limit order is the order to execute buy or sell direction at a specific\nprice, where the orders can be succeeded, overdue, or cancelled and are recorded by the LOB. There are bid\nlimit orders and ask limit orders used to buy and sell products by the trader or sell and buy products by the\n5\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nmarket marker. The highest bid price the buyers are ready to buy is referred to as the best bid price, and the\nlowest ask price the sellers are ready to sell is called the best ask price. The average of these two prices is\ncalled the mid-price, which reflects the current value of the financial product. The difference between them\nis the spread, which the market marker can usually make a profit. The illustration of the LOB is shown in Fig\n1. Another type of order is the market order, where the trader can immediately buy/sell the product at the\nFigure 1: An illustration of LOB based on Zhang et al. [11] and Kolm et al. [14].\nbest price. There exists a matching mechanism in the LOB. Most exchanges adopt the price and time priority\nmatching mechanism. The limit orders will be first executed in order with a better price. If two orders have\nthe same execution price, then the order that comes first in time will be executed first, following the first in\nfirst out (FIFO) principle.\n3.2\nTask 1: LOB Mid-Price Prediction\nThe first task is to predict the LOB Mid-Price Prediction, which is to compare the ability to predict absolute\nprice values similar to non-financial datasets in previous works [31, 33\u201336]. The definition of time series\nprediction is given below and shown in Figure 2:\nFigure 2: The illustration of time series prediction.\n6",
    "chunk_index": 4,
    "start_char": 11488,
    "end_char": 14488,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "price, then the order that comes first in time will be executed first, following the first in\nfirst out (FIFO) principle.\n3.2\nTask 1: LOB Mid-Price Prediction\nThe first task is to predict the LOB Mid-Price Prediction, which is to compare the ability to predict absolute\nprice values similar to non-financial datasets in previous works [31, 33\u201336]. The definition of time series\nprediction is given below and shown in Figure 2:\nFigure 2: The illustration of time series prediction.\n6\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFirst, define a sliding window size Lx for the past data. The input data at each time step t is defined as:\nXt = {x1, x2, . . . , xLx}t\n(1)\nThen define a prediction window size k, where the goal is to predict the information in future Lx +k steps. It\nwill be the single-step prediction when k = 1 and be multi-horizon prediction when k > 1. Then the output\nat time step t is defined as:\nYt = {y1, y2, . . . , yk}t\n(2)\nThe next step is to define the xt and yt in the input and output for mid-price prediction. Assume the market\ndepth is 10. For a limit bid order at time t, the bid price is denoted as pbid\ni,t and the volume is vbid\ni,t , where i is\nthe market depth. Same for the limit ask order, ask price is pask\ni,t and volume is vask\ni,t . Then the LOB data at\ntime t is defined as:\nxt =\n\u0002\npask\ni,t , vask\ni,t , pbid\ni,t , vbid\ni,t\n\u0003n=10\ni=1\n\u2208R40\n(3)\nThe past mid-price will be added to LOB data as input, and the mid-price is represented as:\npmid\nt\n= pask\n1,t + pbid\n1,t\n2\n(4)\nFinally, the xt will be:\nxt =\nh\npask\ni,t , vask\ni,t , pbid\ni,t , vbid\ni,t , pmid\nt\nin=10\ni=1\n\u2208R41\n(5)\nThe target is to predict the future mid-price, so yt = pmid\nt\n.\n3.3\nTask 2: LOB Mid-Price Difference Prediction\nThe second task is to predict the mid-price change, which is the the difference of two mid-prices in different\ntime step. Trading strategies can be designed if the price change becomes negative or positive. The input\nof this task is the same as the mid-price prediction, as described in Equation 3. The target is to regress the\nfuture difference between current mid-price pmid\nt\nand the future mid-price pmid\nt+\u03c4:\ndt+\u03c4 = pmid\nt+\u03c4 \u2212pmid\nt\n(6)\nLike the mid-price prediction, a prediction window size is defined as k, then the output of this task in each\ntimestamp t is represented as:\nYt = {dt+1, dt+2, . . . , dt+k}t\n(7)\n3.4\nTask 3: LOB Mid-Price Movement Prediction\nAccording to Ruppert [5], the absolute price information is generally non-stationary, while the price change\ninformation, such as price difference and return and approximately stationary. The the mid-price difference\nis a difficult target for deep learning methods to predict because it is hard to extract meaningful pattern\n7",
    "chunk_index": 5,
    "start_char": 14006,
    "end_char": 16739,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "dt+\u03c4 = pmid\nt+\u03c4 \u2212pmid\nt\n(6)\nLike the mid-price prediction, a prediction window size is defined as k, then the output of this task in each\ntimestamp t is represented as:\nYt = {dt+1, dt+2, . . . , dt+k}t\n(7)\n3.4\nTask 3: LOB Mid-Price Movement Prediction\nAccording to Ruppert [5], the absolute price information is generally non-stationary, while the price change\ninformation, such as price difference and return and approximately stationary. The the mid-price difference\nis a difficult target for deep learning methods to predict because it is hard to extract meaningful pattern\n7\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nfrom it, although it helps design trading strategies. An example of non-stationary and stationary sequence is\nshown in Figure 3.\nFigure 3: An example of non-stationary sequence vs stationary sequence.\nFor this reason, an easier classification task for predicting mid-price movement is introduced here. To train a\nmodel to predict mid-price movement, the first step is to create price movement labels for each timestamp.\nThis study follows the smoothing labelling method from Tsantekidis et al. [38] and Zhang et al. [11]: Use\nm\u2212to represent the average of the last k mid-price and m+ to represent the average of the next k mid-price:\nm\u2212(t) = 1\nk\nk\nX\ni=0\npmid\nt\u2212k\n(8)\nm+(t) = 1\nk\nk\nX\ni=1\npmid\nt+k\n(9)\nk is set to 20, 30, 50, 100 in this study following previous work of Zhang et al. [11].\nAnd then, define a percentage change lt to decide the price change direction.\nlt = m+(t) \u2212m\u2212(t)\nm\u2212(t)\n(10)\nThe label is dependent on the value of lt. A threshold \u03b4 is set to decide the corresponding label. There are\nthree labels for the price movement:\nlabel =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0( fall ), when lt > \u03b4\n1( stationary ), when \u2212\u03b4 \u2264lt \u2264\u03b4\n2( rise ), when lt < \u2212\u03b4\n(11)\nAn example of labelling for horizon 100 is shown in Figure 4. Assume there is an input in Equation 3 at\nFigure 4: An example of labelling on horizon 100 on ETH-USDT dataset with fixed threshold \u03b4. The green\ncolour represents the rise signal. Purple represents the price is stationary and red colour means the price\nfall.\n8\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\ntimestamp t, predicting mid-price movement is a one-step ahead prediction, which is to predict the mid-price\nmovement in timestamp t + 1.\n4\nMethodology\n4.1\nLSTM\nLSTM was introduced by Hochreiter et al. [39] is one of the RNNs with structural adaptability in time series\ndata input. Although the traditional RNN has the capacity to store data, but it suffers from the exploding\ngradient problem and vanishing gradient problem. Exploding/vanishing gradient means the gradient that\nis used to update the neural networks increases/decreases exponentially, which makes the neural network\nuntrainable [40]. Therefore, RNN is not successful in studying long-time series relations [41].",
    "chunk_index": 6,
    "start_char": 16161,
    "end_char": 19014,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "by Hochreiter et al. [39] is one of the RNNs with structural adaptability in time series\ndata input. Although the traditional RNN has the capacity to store data, but it suffers from the exploding\ngradient problem and vanishing gradient problem. Exploding/vanishing gradient means the gradient that\nis used to update the neural networks increases/decreases exponentially, which makes the neural network\nuntrainable [40]. Therefore, RNN is not successful in studying long-time series relations [41]. LSTM neural\nnetwork utilizes the coordination of three gates to keep long-term dependency and short-term memory.\nAccording to Gers et al. [42], the three gates that LSTM utilizes are 1) forget gate, 2) input gate 3) output\ngate. The structure of an LSTM cell is shown in Figure 5. The calculations of the LSTM are as follows [39]:\nFigure 5: LSTM Structure based on Graves [43] and Fisher [19].\nFirst, the LSTM need to decide to forget some information from the cell state, which is done by the forget\ngate. The forget gate has its sigmoid function \u03c3. Then the function of forget gate is:\n\u0393\u27e8t\u27e9\nf\n= \u03c3\n\u0010\nWf\nh\nh\u27e8t\u22121\u27e9, x\u27e8t\u27e9i\n+ bf\n\u0011\n(12)\nWhere the Wf is the weight of the last hidden state and input, bf is the bias of the hidden state.\nThe next step is to design what information the neural cell should remember. To update the information,\nthe input gate should coordinate with a tanh layer containing a vector of new candidate values \u02dcc\u27e8t\u27e9. The\ncalculation is as follows:\n\u0393\u27e8t\u27e9\ni\n= \u03c3\n\u0010\nWi\nh\nh\u27e8t\u22121\u27e9, x\u27e8t\u27e9i\n+ bi\n\u0011\n(13)\n\u02dcc\u27e8t\u27e9= tanh\n\u0010\nWc\nh\nh\u27e8t\u22121\u27e9, x\u27e8t\u27e9i\n+ bc\n\u0011\n(14)\n9\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nAnd then, with the previous steps of calculations, the cell state can be updated:\nc\u27e8t\u27e9= \u0393\u27e8t\u27e9\nf\n\u25e6c\u27e8t\u22121\u27e9+ \u0393\u27e8t\u27e9\ni\n\u25e6\u02dcc\u27e8t\u27e9\n(15)\nLastly, calculate the result from the output gate to get the new hidden state:\n\u0393\u27e8t\u27e9\no\n= \u03c3\n\u0010\nWo\nh\nh\u27e8t\u22121\u27e9, x\u27e8t\u27e9i\n+ bo\n\u0011\n(16)\nh\u27e8t\u22121\u27e9= \u0393\u27e8t\u27e9\no\n\u25e6tanh\n\u0010\n\u02dcc\u27e8t\u27e9\u0011\n(17)\nThe output of LSTM can be every hidden state or the final hidden state, which depends on the application.\nIn the implementation, this hidden state will be fed into a multi-layer perceptron (MLP), also known as\nfeedforward-backwards propagation neural network (FFBPN). The output of this layer will pass through an\nactivation function to generate the final output. Usually, the final hidden state will be utilized in financial\ntime series prediction, which produces absolute price prediction or price movement prediction.\n4.1.1\nAlternative LSTM-based Models\nBesides the canonical LSTM, three more LSTM based-models are chosen for comparison to Transformer-\nbased models. They are DeepLOB [11], DeepLOB-Seq2Seq [12] and DeepLOB-Attention [12] created by\nZhang et al. The architecture of these three models are shown in Figure 6 and 7. Here the structures of\nthese three models are briefly explained:\nDeepLOB [11] DeepLOB\u2019s architecture consists of three main components: Convolutional Blocks, an Incep-\ntion Module and an LSTM layer.\nA.",
    "chunk_index": 7,
    "start_char": 18517,
    "end_char": 21450,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "Alternative LSTM-based Models\nBesides the canonical LSTM, three more LSTM based-models are chosen for comparison to Transformer-\nbased models. They are DeepLOB [11], DeepLOB-Seq2Seq [12] and DeepLOB-Attention [12] created by\nZhang et al. The architecture of these three models are shown in Figure 6 and 7. Here the structures of\nthese three models are briefly explained:\nDeepLOB [11] DeepLOB\u2019s architecture consists of three main components: Convolutional Blocks, an Incep-\ntion Module and an LSTM layer.\nA. Convolutional Blocks The LOB inputs mentioned in Equation 3 are fed into the convolutional blocks that\ncontain multiple convolutional layers, where the first and second convolutional blocks are more important\nthan the third one. The first convolutional block has a layer with filter size of (1 \u00d7 2) and stride of (1 \u00d7 2).\nAt each order book level, this layer summarise the price and volume information\n\b\np(i), v(i) \n. For the second\nconvolutional block, it has a layer with the same filter size and stride as the first one, but it is a feature\nmapping for the micro-price defined by [44]:\npmicro = Ipask\ni\n+ (1 \u2212I)pbid\ni\n(18)\nI =\nvbid\ni\nvask\ni\n+ vbid\ni\n(19)\nI is called the imbalance. Then the last convolutional block integrates the feature information from the\nprevious two layers. The whole convolutional blocks work as a feature extractor.\nB. Inception Module the Inception Module employs the time series decomposition method. The input is\ndecomposed by two 1 \u00d7 1 convolutions and one max-pooling layer into three lower-dimensional representa-\ntions. Then these representations pass through convolution layers with 32 channels to be merged together.\n10\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 6: DeepLOB architecture sourced from Zhang et al. [11]. \u201dConv 1 \u00d7 2 @16\u201d means there is 16 filters\nof size 1 \u00d7 2 in this convolutional layer.\nThis decomposition method improves the prediction accuracy.\nC. LSTM Layer Finally, the extracted features are inputted into one LSTM layer to capture the underlying\npattern and dependencies. The last output layer can be a SoftMax layer or a linear layer, which depends on\nthe specific tasks.\nDeepLOB-Seq2Seq [12] To generate multi-horizon predictions, Zhang et al. developed DeepLOB-Seq2Seq.\nThe main idea is to feed the output of the Inception Module into a Seq2Seq architecture to do iterated multi-\nstep (IMS) prediction. Seq2Seq [45] architecture contains encoder and decoder constructed by recurrent\nneural network (RNN). Assume the sequence input is XT = (x1, x2, \u00b7 \u00b7 \u00b7 , xT ), the encoder will output a\n11\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 7: DeepLOB-Seq2Seq and DeepLOB-Attention architecture sourced from Zhang et al. [12].\nhidden state at each timestamp t:\nht = f (ht\u22121, xt)\n(20)\nAfter obtaining the hidden states from the encoder, a context vector c has to be constructed from these hidden\nstates. The last hidden state or the mean of all hidden states can be taken as a context vector.",
    "chunk_index": 8,
    "start_char": 20943,
    "end_char": 23949,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "output a\n11\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 7: DeepLOB-Seq2Seq and DeepLOB-Attention architecture sourced from Zhang et al. [12].\nhidden state at each timestamp t:\nht = f (ht\u22121, xt)\n(20)\nAfter obtaining the hidden states from the encoder, a context vector c has to be constructed from these hidden\nstates. The last hidden state or the mean of all hidden states can be taken as a context vector. Context vector\nwork as a \u201dbridge\u201d between the encoder and decoder, where the context vector is utilized to initialize the\ndecoder, and the hidden state output of the decoder at each timestamp is:\ndt = f (dt\u22121, yt\u22121, c)\n(21)\nThen the distribution of the output yt is:\nP (yt | y<t, c) = g (dt, c)\n(22)\nwhere the output of the decoder not only depends on the previous true value as input but is also conditioned\non the context vector.\nDeepLOB-Attention [12] With the same idea as the DeepLOB-Seq2Seq model, the difference of the\nDeepLOB-Attention model is changing the Seq2Seq architecture into Attention. The attention model [46]\nconstructs the context vector differently instead of using the last hidden state or the mean of all hidden\nstates. Same as Seq2Seq model, the encoder outputs hidden state ht and decoder outputs hidden state dt.\nThe first step is to compute a similarity score between the hidden state dt of the decoder and each encoder\nstate hi, where the similarity score is usually calculated by dot product:\nsi = hT\ni dt\n(23)\nAnd then, normalize the similarity scores to obtain a weight distribution by softmax:\n{\u03b11, \u03b12, . . . , \u03b1S} = softmax ({s1, s2, . . . , sS})\n(24)\n12\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFinally, generate the context vector from the attention weights:\nct =\nS\nX\ni=1\n\u03b1ihi\n(25)\nAfter that, the process of producing the output yt is the same as the Seq2Seq model.\n4.2\nTransformer\nThe Transformer [9] has an encoder-decoder structure that relies on the Self-attention mechanism without\nrelying on CNN and RNN. This architecture allows the Transformer to process the long sequence and has no\nproblem with the vanishing gradient, which means it can model the dependency regardless of the length of\nthe input/output. A series of components forms the Transformer: Multi-head Self-attention, positional-wise\nfeed-forward neural network, layer-normalization, and residual connection. According to Vaswani et al. [9]\nand Farsani et al. [47], the architecture of the Transformer for financial time series prediction is shown in\nFig.8. There is a slight difference between this transformer and the vanilla one used for NLP tasks. The word\nFigure 8: Transformer Structure based on Vaswani et al. [9] and Farsani et al. [47].\nembedding process is omitted, and the financial time series is fed into the transformer using time-stamp\nencoding. The details of time-stamp encoding will be explained in Section 4.2.2.\n13\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n4.2.1\nMulti-head Self-attention Mechanism\nSelf-attention is a mechanism for finding the relevant vector in a sequence.",
    "chunk_index": 9,
    "start_char": 23519,
    "end_char": 26590,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "Fig.8. There is a slight difference between this transformer and the vanilla one used for NLP tasks. The word\nFigure 8: Transformer Structure based on Vaswani et al. [9] and Farsani et al. [47].\nembedding process is omitted, and the financial time series is fed into the transformer using time-stamp\nencoding. The details of time-stamp encoding will be explained in Section 4.2.2.\n13\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n4.2.1\nMulti-head Self-attention Mechanism\nSelf-attention is a mechanism for finding the relevant vector in a sequence. The target of Self-attention is to\ncompute the attention score and then extract information based on the score. According to Vaswani et al.\n[9], the calculations of scaled dot-production Self-attention are as follows: First, to compute the attention\nscore, multiply the input vector I with different learnable weight matrices W q and W k to obtain the query\nand key:\nQ = W qI\n(26)\nK = W kI\n(27)\nThe next step is to calculate the attention matrix, where each element in it is an attention score:\nA = softmax\n\u0012QKT\n\u221adk\n\u0013\n(28)\nWhere dk is the dimension of query and key.\nLastly, multiply the attention matrix with the values, and the final output can be obtained:\nAttention (Q, K, V ) = softmax\n\u0012QKT\n\u221adk\n\u0013\nV\n(29)\nThe output is the weighted sum of the relevance of different vectors in the sequence. In the implementa-\ntion of the transformer, Multi-head Self-attention is used, which is beneficial for finding different types of\nrelevance.\n4.2.2\nLearnable Time-stamp Encoding\nDifferent from the fixed sinusoid encoding used in the vanilla transformer [9], timestamp encoding from\nthe Informer [33] is more informative for time series data and a similar method is applied in Autoformer\n[34] and FEDformer [36]. The timestamp encoding method uses learnable embedding layers to produce\npositional information to add to the sequence, where timestamp information like a minute, hour, week, year\nand extra time stamps like event or holiday can be incorporated. To obtain the time-step encoding, the\nfirst step is to calculate fixed sinusoid encoding. Assume the input is Xt =\n\b\nx1, x2, . . . , xLx | xi \u2208Rdx \nt at\ntimestamp t , where Lx is the sliding window size and dx is the model dimensionality, then the encoding is\ncalculated as follows:\nPEpos,2i = sin\n \npos\n(2Lx)\n2i\nd\n!\n(30)\nPEpos,2i+1 = cos\n \npos\n(2Lx)\n2i\nd\n!\n(31)\nAnd then, project the original input xt\ni into the model dimensionality vector ut\ni using convolutional filters.\nThe next step is to use a learnable embedding layer SEpos to incorporate the timestamp information. The\nstructure of the timestamp embedding is shown in Fig.9.\n14\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 9: Time-stamp embeddings based on Zhou et al. [33].\nFinally, add up all the calculation results above to get the final encoding:\nXt\nfeed[i] = \u03b1ut\ni + PE(Lx\u00d7(t\u22121)+i) +\nX\np\n\u0002\nSELx(t\u22121)+i\n\u0003\np\n(32)\nWhere i \u2208{1, .",
    "chunk_index": 10,
    "start_char": 26027,
    "end_char": 28957,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "using convolutional filters.\nThe next step is to use a learnable embedding layer SEpos to incorporate the timestamp information. The\nstructure of the timestamp embedding is shown in Fig.9.\n14\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 9: Time-stamp embeddings based on Zhou et al. [33].\nFinally, add up all the calculation results above to get the final encoding:\nXt\nfeed[i] = \u03b1ut\ni + PE(Lx\u00d7(t\u22121)+i) +\nX\np\n\u0002\nSELx(t\u22121)+i\n\u0003\np\n(32)\nWhere i \u2208{1, . . . , Lx} and \u03b1 is the parameter balancing the ratio of scalar projection and embeddings.\n4.2.3\nAlternative Transformer-based Models\nAs mentioned in Section 2, several new Transformer-based models [31\u201334, 36] are dedicated for long time\nseries forecasting. However, they have not been tested on financial time series data. In this study, they are\nchosen as the alternative models to compare with the vanilla Transformer and LSTM. These models and\ntheir relationships are briefly summarized as follows (see Figure 10):\nFigure 10: Architecture designs for alternative Transformer models sourced from Zeng et al.[48]. In step\n(1) and (2), the operations inside the solid box are required, while those in the dashed box can be applied\noptionally.\nLogTrans Li et al. [31] put forward LogTrans with convolutional Self-attention generating queries and keys\nin the Self-attention layer. This work makes the convolutional layer widely used in the attention module\nin the later studies [33, 34, 36]. It uses a Logsparse mask to reduce the time complexity from O(L2) to\nO(L log(L)).\n15\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nReformer Reformer [32] changes the time complexity of the Transformer to O(L log(L)) as well by using\nsensitive hashing instead of dot-product in the calculate of attention. It also replaces the residual connection\nin the vanilla Transformer with a reversible residual connection, which makes the Transformer more efficient.\nInformer Although Reformer and LogTrans reduced the time complexity to O(L log(L)), the memory\nconsumption is still the same, so the efficiency gain is not high. Also, LogTrans and Reformer use iterated\nmulti-step prediction (IMS), generating a single prediction in each timestamp and using that prediction\niteratively to obtain multi-step prediction [49], which suffers from error accumulation.\nInformer [33]\nproposed by Zhou et al. employs a ProbSparse Self-attention mechanism to achieve O(L log(L)) time and\nmemory complexity. They propose an innovative generative style decoder to make direct multi-step(DMS)\nprediction, which is to generate the multi-step prediction at once in one forward procedure [50]. This\nmethod speed up the long-term forecast compared to the LogTrans and Reformer. The DMS forecast method\nand learnable timestamp encoding are applied in the Autoformer [34] and FEDformer [36].\nAutoformer The optimization of Transformer on time series prediction is a trade-off between efficiency\nand information utilization. Depending on the structure of Informer, Autoformer [34] reduced the time\ncomplexity with an Auto-Correlation mechanism rather than making Self-attention sparse in LogTrans and\nInformer, which can preserve the information well and measure the sub-series dependency.",
    "chunk_index": 11,
    "start_char": 28489,
    "end_char": 31722,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "LogTrans and Reformer. The DMS forecast method\nand learnable timestamp encoding are applied in the Autoformer [34] and FEDformer [36].\nAutoformer The optimization of Transformer on time series prediction is a trade-off between efficiency\nand information utilization. Depending on the structure of Informer, Autoformer [34] reduced the time\ncomplexity with an Auto-Correlation mechanism rather than making Self-attention sparse in LogTrans and\nInformer, which can preserve the information well and measure the sub-series dependency. Time series\ndecomposition is a method commonly used in time series analysis to deconstruct the time series into several\ncomponents [51, 52]. The underlying temporal pattern can be revealed from these components to make\nthe time series more predictable [53]. Autoformer first embeds the time series decomposition as an inner\nneural block to derive the trend-cyclical component from the input sequence and the seasonal component\nfrom the difference between the trend-cyclical component and the input sequence. This new decomposition\narchitecture can deconstruct time series to use the series periodicity to update attention.\nFEDformer Based on the decomposition architecture of Autoformer, Zhou et al. [36] builds the FEDformer\nto use Fourier transform and Wavelet transform, which are in the frequency domain as a new decomposition\nmethod to reach linear complexity O(L). .\n16\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n5\nExperimentation Result and Evaluation\n5.1\nComparison of LOB Mid-Price Prediction\nThe first task to compare transformer versus LSTM is the mid-price prediction. In this task, predicting the\nabsolute value of the mid-price is similar to the previous work\u2019s [31, 33\u201336] experiment on non-financial\ndatasets.\n5.1.1\nExperiment Setting for LOB Mid-Price Prediction\nDataset All the experiments are based on cryptocurrency LOB data, which are collected in real-time from\nBinance Exchange using cryptofeed [54] WebSocket API and saved to the database using kdb+tick triplet\n[55]. In this experiment, one-day LOB data of product BTC-USDT (Bitcoin-U.S. dollar tether) on 2022.07.15.\ncontaining 863397 ticks is utilized. The time interval between each ticks is not evenly spaced. The time\ninterval is 0.1 second on average. The first 70% data is used to construct the training set, and the rest 10%\nand 20% of data are used for validation and testing. The reason why only using one day of LOB data is that\nit is enough to train for a task prediction for absolute value without overfitting according to previous works\u2019\nexperiments on non-financial datasets.\nModels For the comparison purpose, I choose canonical LSTM and vanilla Transformers along with four\nTransformer-based models: FEDformer [36], Autoformer [34], Informer [33] and Reformer [32]. For the\nimplementation of Transformer-based models, they are taken from open-source code repositories [56, 57].\nThe implementation of vanilla Transformer [9], Reformer [32], Informer [33] and Autoformer [34] are from\nthe Autoformer repository [56]. The implementation of FEDformer [36] is from its own repository [57].\nTraining setting The dataset is normalized by the z-score normalization method. The validation set and the\ntest set are normalized by the mean and standard deviation of the training set.",
    "chunk_index": 12,
    "start_char": 31191,
    "end_char": 34504,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "the\nimplementation of Transformer-based models, they are taken from open-source code repositories [56, 57].\nThe implementation of vanilla Transformer [9], Reformer [32], Informer [33] and Autoformer [34] are from\nthe Autoformer repository [56]. The implementation of FEDformer [36] is from its own repository [57].\nTraining setting The dataset is normalized by the z-score normalization method. The validation set and the\ntest set are normalized by the mean and standard deviation of the training set. All the models are trained for\n10 epochs using the Adaptive Momentum Estimation optimizer and L2 loss with early stopping. The batch\nsize is 32, and the initial learning rate is 1e-4. All models are implemented by Pytorch [58] and trained on\na single NVIDIA RTX A5000 GPU with 24 GB memory.\n5.1.2\nResult and Analysis for LOB Mid-Price Prediction\nModels\nFEDformer\nAutoformer\nInformer\nReformer\nTransformer\nLSTM\nMetrics\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\nMSE\nMAE\n96\n0.0793\n0.179\n0.0926\n0.201\n1.411\n0.543\n2.186\n0.619\n2.836\n0.696\n0.104\n0.204\n192\n0.155\n0.257\n0.176\n0.279\n1.782\n0.749\n1.842\n0.824\n2.799\n0.832\n0.195\n0.287\n336\n0.274\n0.348\n0.319\n0.376\n2.080\n0.830\n9.218\n1.947\n1.456\n0.665\n0.315\n0.369\n720\n0.608\n0.514\n0.643\n0.539\n2.808\n1.093\n72.57\n6.824\n4.306\n1.297\n0.771\n0.587\nTable 1: Mid price prediction result with different prediction lengths k \u2208{96, 192, 336, 720} in test set. The\ninput window size is set to 96 (MSE\u2019s unit is in 10\u22122 and MAE\u2019s unit is in 10\u22121).\n17\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nQuantitative result To evaluate the performance of different models, following the previous works [31,\n33\u201336], the performance metrics consist of Mean Square Error (MSE) and Mean Absolute Error (MAE),\nrepresenting the prediction error. Lower MSE or MAE indicates the model has less prediction error. MSE and\nMAE are calculated by:\nMSE = 1\nn\nn\nX\ni=1\n\u0010\nYi \u2212bYi\n\u00112\n(33)\nMAE = 1\nn\nn\nX\ni=1\n\f\f\fYi \u2212\u02c6Yi\n\f\f\f\n(34)\nwhere Yi is the true value and bYi is the predicted value. n is the number of ticks.\nThe results of all models are shown in Table 1. From the table, these outcomes can be summarized:\n1. Both FEDformer and Autoformer outperform LSTM and FEDformer has the best performance in all the\nprediction lengths. FEDformer and Autoformer give a large increase in performance in terms of MSE and\nMAE compared to LSTM. For FEDformer, it gives 24%(0.104 \u21920.0793) MSE reduction on 96 prediction\nlength and 21%(0.771 \u21920.608) on 336 prediction length. For Autoformer, it gives 11%(0.104 \u21920.0926)\nMSE reduction on 96 prediction length and 16%(0.771 \u21920.643) MSE reduction on 336 prediction length.\nThese results show that both Autoformer and FEDformer perform well in terms of MSE and MAE because of\ntheir low error and long-term robustness.\n2.\nAlthough FEDformer and Autoformer\u2019s MSE and MAE are low on this task, LSTM is relatively not\nbad on mid-price prediction. LSTM outperforms the other three models: Informer, Reformer, and vanilla\nTransformer, which indicates that LSTM is robust in handling LOB data, while transformer-based models\nrequire lots of modification to perform well.",
    "chunk_index": 13,
    "start_char": 34003,
    "end_char": 37105,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "\u21920.0926)\nMSE reduction on 96 prediction length and 16%(0.771 \u21920.643) MSE reduction on 336 prediction length.\nThese results show that both Autoformer and FEDformer perform well in terms of MSE and MAE because of\ntheir low error and long-term robustness.\n2.\nAlthough FEDformer and Autoformer\u2019s MSE and MAE are low on this task, LSTM is relatively not\nbad on mid-price prediction. LSTM outperforms the other three models: Informer, Reformer, and vanilla\nTransformer, which indicates that LSTM is robust in handling LOB data, while transformer-based models\nrequire lots of modification to perform well.\n3. Vanilla Transformer model has worse performance on prediction lengths 96 and 192 and Reformer has\nworse performance on prediction lengths 336 and 720 because they suffered from error accumulation during\nthe IMS prediction process. Informer\u2019s worse performance than LSTM is mainly due to its sparse version of\nattention, leading to information loss on the time series.\nQualitative results The prediction results of compared models on all the prediction horizons are shown in\nFigure 11. When the prediction horizon is 96, Autoformer and Reformer are able to generate a proper trend\nfor the future mid-price, while other models generate almost a flat line as predictions. On the prediction\nhorizon of 192, almost all the models\u2019 predictions plateau except the Reformer, but Reformer\u2019s result\nbecomes more stochastic than the prediction horizon of 96. For larger prediction horizons 336 and 720, all\nthe models can hardly predict a proper trend and Reformer\u2019s result in not plotted because it becomes too\nstochastic.\nBased on the qualitative results above, although Autoformer and FEDformer outperform LSTM in terms of\nMSE and MAE, their actual prediction performance is far more inadequate for high-frequency trading. The\nanalysis from Figure 11 is just \u201ceyeballing\u201d whether the model is good. In this case, another formal metric\n18\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 11: Illustration of normalized forecasting outputs with 96 input window size and {96, 192, 336, 720}\nprediction lengths. Each timestamp is one tick. Reformer\u2019s results are not plotted in the lower two panels\nfor better visualisation.\nModels\nAutoformer\nFEDformer\nInformer\nReformer\nLSTM\nTransformer\n96\n-0.753\n-0.237\n-43.811\n-69.080\n-0.946\n-87.899\n192\n-0.596\n-0.205\n-25.281\n-26.792\n-0.644\n-43.368\n336\n-1.032\n-0.364\n-20.123\n-63.252\n-0.414\n-13.035\n720\n-0.521\n-0.189\n-7.760\n-137.322\n-0.589\n-16.314\nTable 2: Average of out of sample R2 result with different prediction lengths k \u2208{96, 192, 336, 720}.\nout of sample R2 is added here to judge the prediction quality. According to Lewis-Beck [59], R2 determine\nhow well the prediction result Y can be explained by the input X, and higher R2 indicates the model has a\nbetter fit for the predicted value. Out of sample R2 is defined by:\nR2 = 1 \u2212\nPn\ni=1\n\u0010\nYi \u2212bYi\n\u00112\nPn\ni=1\n\u0000Yi \u2212\u00afY\n\u0001\n(35)\nwhere \u00afY = 1\nn\nPn\ni=1 Yi; bYi is the predicted value and Yi is the ground truth.\nHere the out-of-sample R2 is calculated based on the price difference.",
    "chunk_index": 14,
    "start_char": 36507,
    "end_char": 39584,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "result Y can be explained by the input X, and higher R2 indicates the model has a\nbetter fit for the predicted value. Out of sample R2 is defined by:\nR2 = 1 \u2212\nPn\ni=1\n\u0010\nYi \u2212bYi\n\u00112\nPn\ni=1\n\u0000Yi \u2212\u00afY\n\u0001\n(35)\nwhere \u00afY = 1\nn\nPn\ni=1 Yi; bYi is the predicted value and Yi is the ground truth.\nHere the out-of-sample R2 is calculated based on the price difference. Please note that the price difference\nhere differs from the one mentioned in Section 3.3. The price difference here is calculated from the absolute\nprice prediction result, while the one in Section 3.3 is the direct prediction target. The result of R2 is shown\nin Table 2.\nFrom the table, all the out-of-sample R2 values are negative for all models.\nHowever, according to\nLewis-Beck [59], R2 at least needs to be larger than zero to indicate that input X can explain output Y .\nThis indicates that predictions for absolute mid-price are useless for trading purposes. The metrics of MSE\nand MAE obscure the real quality of prediction results, highlighting the importance of R2 calculated based\n19\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\non the price difference.\nTo sum up, although Autoformer and FEDformer have lower MSE and MAE than LSTM, their prediction\nresult is not helpful and practical for trading. The more sensible way is to directly use the price difference as\nthe prediction target.\n5.2\nComparison of LOB Mid-Price Diff Prediction\nBoth transformer-based models and LSTM are unable to generate the satisfactory result on the mid-price\nprediction, so this task turns to the mid-price difference prediction. The mid-price difference is a useful\nalpha-term structure in trading for traders and market makers [14].\n5.2.1\nExperiment Setting for LOB Mid-Price Diff Prediction\nDataset The dataset for this experiment is collected the same way as the last experiment, but the dataset size\nbecomes larger to avoid overfitting. In this experiment, four days of LOB data for the product BTC-USDT\nfrom 2022.07.03 (inclusive) to 2022.07.06 (inclusive) is used, containing 3432211 ticks. The first 80% of\ndata is used as a training set, and the rest 20% is split in half for validation and testing.\nModels and Limitations Five models are being compared in this experiment: canonical LSTM [39], vanilla\ntransformer [9], CNN-LSTM (DeepLOB [11] model used for regression), Informer [33] and Reformer [32].\nState-of-the-art FEDformer and Autoformer are not compared in this task because their time decomposition\nstructure is limited in handling price difference series. As mentioned in Section 3.4, the price difference is\napproximately stationary. The time decomposition method is useful for the non-stationary time series, such\nas the mid-price series, where it can extract meaningful patterns. In contrast, the price difference series is\napproximately stationary, and little meaningful information can be extracted from it. Therefore, FEDformer\nand Autoformer can only produce a poor result in this task and will not be compared below.\nTraining settings the training setting is the same as the last experiment.",
    "chunk_index": 15,
    "start_char": 39232,
    "end_char": 42311,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "series. As mentioned in Section 3.4, the price difference is\napproximately stationary. The time decomposition method is useful for the non-stationary time series, such\nas the mid-price series, where it can extract meaningful patterns. In contrast, the price difference series is\napproximately stationary, and little meaningful information can be extracted from it. Therefore, FEDformer\nand Autoformer can only produce a poor result in this task and will not be compared below.\nTraining settings the training setting is the same as the last experiment.\n5.2.2\nResult and analysis for LOB Mid-Price Diff Prediction\nFollowing the previous works [14], out of sample R2 is the evaluation metric for this task. The performance\nof all the models is shown in Figure 12. The canonical LSTM achieves the best performance among all\nmodels, which reaches the highest R2 around 11.5% in forecast length 5 to 15.\nFor CNN-LSTM, it has\ncomparable performance to LSTM. On the other hand, Informer, Reformer and Transformer have worse R2\nthan LSTM, but their R2 trend is similar. In short, for the price difference prediction task, LSTM-based\nmodels is more stable and more robust than Transformer-based models. This result is in expectation because\nReformer, Informer and Transformer already have worse performance than LSTM in mid-price prediction\ntask because of their shortcomings. At the same time, the state-of-art FEDformer and Autoformer cannot be\n20\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 12: Performance of price difference prediction with input window size 100 and prediction length 100.\nNegative data points are not plotted for ease of visualization.\napplied because of their limitation. In order to let these state-of-the-art transformer-based models make a\nmeaningful prediction, a new structure is designed in the next part, and it is applied to the price movement\nprediction task.\n5.3\nComparison of LOB Mid-Price Movement Prediction\n5.3.1\nInnovative Architecture on Transformer-based Methods\nThe alternative Transformer-based models mentioned in Section 4.2.3 mainly focus on the long time series\nforecasting problem, which is a regression task. For the LOB data, it is easy to adapt these models for the mid-\nprice prediction and mid-price difference prediction because both are regression problems. For the mid-price\nmovement prediction task, the model needs to produce a classification result for the future, and there are few\nexisting Transformer models specialized available for this task. In contrast, most of the Transformer-based\nmodels are designed for classification tasks without forecasting, such as sentiment analysis, spam detection\nand pos-tagging in NLP. In this case, adapting the existing Transformer-based models to do price movement\nforecasting is necessary. I first adapt Transformer-based models in price movement forecasting and want\nto facilitate transformer development in this specific task. The new architecture of the transformer-based\n21\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nmodel is shown in Figure 13. The details are explained as follows:\nPredicting the next mid-price movement based on the past price and volume information is an one step ahead\nof prediction.",
    "chunk_index": 16,
    "start_char": 41760,
    "end_char": 45002,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "is necessary. I first adapt Transformer-based models in price movement forecasting and want\nto facilitate transformer development in this specific task. The new architecture of the transformer-based\n21\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nmodel is shown in Figure 13. The details are explained as follows:\nPredicting the next mid-price movement based on the past price and volume information is an one step ahead\nof prediction. A straightforward method to adapt the transformer-based model is to pass the next predicted\nFigure 13: New architecture of transformer-based model for LOB mid-price movement prediction.\nmid-price into a softmax activation. However, this method performs poorly because it only considers the past\nmid-price information and ignores future ones. It is worth noting that in the labelling process in Section 3.4,\nprevious and next k mid-price information are utilized. In this case, I adapt the existing transformer-based\nmodels to feed the whole predicted mid-price sequence into a linear layer and finally pass through a softmax\nactivation function to generate price movement output. This adaptation will benefit those transformer-based\nmodels using the DMS forecasting method because they have fewer errors in the long-time series prediction\nprocess.\n5.3.2\nDLSTM: Innovation on LSTM-based Methods\nInspired by the Dlinear model [48] and Autoformer, combining the merits of time decomposition with the\nLSTM, a new model named DLSTM is designed. DLSTM is designed based on these three observations:\nFirstly, the time series decomposition method is capable of increasing the performance, especially embedding\nthis process by neural blocks in previous works [11, 34, 36]. Secondly, LSTM is a robust and simple model for\nmultiple forecasting tasks. Thirdly, Dlinear beats other Transformer-based models in some long time series\nforecasting tasks thanks to the time series decomposition method and DMS prediction. However, predicting\nprice movement is one step ahead prediction, where the model will not suffer from the error accumulation\neffect. In this case, it is sensible to replace linear with LSTM, because LSTM is a model well-known better\nthan linear in handing time series.\nFigure 14: Architecture of DLSTM\n22\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nThe architecture of DLSTM is shown in Figure 14. The main difference between Dlinear and DLSTM is\nthat the LSTM layers replace the Linear layer. According to the time decomposition method introduced\nin Autoformer [34], in the prediction process, assume there is a time series XT = (x1, x2, . . . , xT ), first\ndecompose it into Trend series by the moving average:\nXt = AvgPool(Padding(XT ))\n(36)\nwhere AvgPool(\u00b7) is the average pooling operation and the Padding(\u00b7) is to fix the input length.\nAnd then the Remainder series is calculated by Xr = XT \u2212Xt. After that, these two series are inputted into\ntwo LSTM layers. Finally, the hidden states Ht and Hr produced by two LSTM layers will be added together\nand then pass through a linear and softmax activation to generate the final price movement result.",
    "chunk_index": 17,
    "start_char": 44551,
    "end_char": 47676,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "is the average pooling operation and the Padding(\u00b7) is to fix the input length.\nAnd then the Remainder series is calculated by Xr = XT \u2212Xt. After that, these two series are inputted into\ntwo LSTM layers. Finally, the hidden states Ht and Hr produced by two LSTM layers will be added together\nand then pass through a linear and softmax activation to generate the final price movement result.\n5.3.3\nSetting for LOB Mid-Price Movement Prediction\nDataset In this experiment, the largest dataset among three tasks is utilized to avoid over-fitting and test\nthe model\u2019s robustness. The whole dataset contains 12 days of LOB data of product ETH-USDT (Ethereum-\nU.S. dollar tether) from 2022.07.03 (inclusive) to 2022.07.14 (inclusive), containing 10255144 ticks. The\ntraining and testing data are taken from the first six days and the last three days, and the left data are used\nfor validation. The test set is also used for the simple trading simulation.\nModels Thanks to the innovative structure mentioned in Section 5.3.1, most of the transformer-based models\ncan be adapted and applied in this task for comparison, which are: Vanilla Transformer [9], Reformer\n[32], Informer [33], Autoformer [34], FEDformer [36]. On the other hand, all the LSTM-based models\nare compared in this task as well, which are: canonical LSTM [39], DLSTM, DeepLOB [11], DeepLOB-\nSeq2Seq [12], DeepLOB-Attention [12]. Besides these models, a simple MLP model is built as a baseline.\nThe implementation of the Transformer-based models are based on the code repository mentioned above.\nThe implementation of DeepLOB [11], DeepLOB-Seq2Seq [12], DeepLOB-Attention [12] are based on two\nrepositories [60, 61]. For the DLSTM, it is inspired by code of Dlinear[48] model from its repository [62].\nTraining settings The batch size for training is set to 64 and the loss function is changed to Crossentropy\nloss. Other training settings are the same as the last experiment.\n5.3.4\nResult and analysis for LOB Mid-Price Movement Prediction\nThe performance of models is evaluated by classification metrics: accuracy and the mean of precision, recall\nand F1-score. Result are shown in Table 3 and Table 4.\nA few outcomes can be observed from the result:\n1. DLSTM outperforms all the previous LSTM-based and Transformer-based models. It achieves the highest\naccuracy, precision, recall and F1 score in all the prediction horizons.\nThis result shows that the time\nseries decomposition structure originating from Autoformer can effectively handle time series, especially\nwhen combined with a simple LSTM model. DLSTM is making one step ahead prediction for the mid-price\n23\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nModel\nAccuracy\nPrecision\nRecall\nF1\nPrediction Horizon k = 20\nMLP\n61.58\n61.70\n61.58\n61.47\nLSTM\n62.77\n62.91\n62.77\n62.78\nDeepLOB\n70.29\n70.58\n70.30\n70.24\nDeepLOB-Seq2Seq\n70.40\n70.79\n70.42\n70.37\nDeepLOB-Attention\n70.04\n70.26\n70.03\n70.01\nAutoformer\n68.89\n68.99\n68.89\n68.91\nFEDformer\n65.37\n65.70\n65.37\n65.20\nInformer\n68.71\n68.82\n68.72\n68.71\nReformer\n68.01\n68.26\n68.00\n67.95\nTransformer\n67.80\n67.99\n67.81\n67.77\nDLSTM\n73.10\n74.01\n73.11\n73.11\nPrediction Horizon k = 30\nMLP\n59.19\n59.30\n58.70\n58.48\nLSTM",
    "chunk_index": 18,
    "start_char": 47286,
    "end_char": 50469,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "electronic trading\nA PREPRINT\nModel\nAccuracy\nPrecision\nRecall\nF1\nPrediction Horizon k = 20\nMLP\n61.58\n61.70\n61.58\n61.47\nLSTM\n62.77\n62.91\n62.77\n62.78\nDeepLOB\n70.29\n70.58\n70.30\n70.24\nDeepLOB-Seq2Seq\n70.40\n70.79\n70.42\n70.37\nDeepLOB-Attention\n70.04\n70.26\n70.03\n70.01\nAutoformer\n68.89\n68.99\n68.89\n68.91\nFEDformer\n65.37\n65.70\n65.37\n65.20\nInformer\n68.71\n68.82\n68.72\n68.71\nReformer\n68.01\n68.26\n68.00\n67.95\nTransformer\n67.80\n67.99\n67.81\n67.77\nDLSTM\n73.10\n74.01\n73.11\n73.11\nPrediction Horizon k = 30\nMLP\n59.19\n59.30\n58.70\n58.48\nLSTM\n60.64\n60.47\n60.45\n60.45\nDeepLOB\n67.23\n67.26\n67.17\n67.15\nDeepLOB-Seq2Seq\n67.56\n67.73\n67.53\n67.49\nDeepLOB-Attention\n67.21\n67.39\n66.98\n66.96\nAutoformer\n67.93\n67.86\n67.77\n67.77\nFEDformer\n66.57\n66.44\n66.05\n65.83\nInformer\n65.41\n65.33\n65.14\n65.13\nReformer\n64.28\n64.31\n64.08\n64.06\nTransformer\n64.25\n64.16\n64.13\n64.13\nDLSTM\n70.61\n70.83\n70.63\n70.59\nTable 3: Experiment results of Mid Price Movement for prediction horizons 20 and 30. Red Bold represents\nthe best result and blue underline represents the second best result.\nmovement, so it will not suffer from error accumulation from the DMS prediction process.\n2.\nDeepLOB-Attention model has the second best result in horizon 50 and 100 (excluding accuracy).\nDeepLOB-Seq2Seq has the second best result for prediction horizon 20.\nThis result indicates that the\nencode-decoder structure and attention mechanism can contribute to the prediction performance because\nthe autoregressive process can correlate the mid-price movement from different prediction horizons.\n3. The DeepLOB-Attention and DeepLOB-Seq2Seq performance is comparable to DeepLOB but better than\nDeepLOB, especially in the long prediction horizon. This result accords with the result in the previous paper\n[12], which proves the correctness of the result.\n4. The Autoformer gets the second-best result in prediction horizon 30. Although it is not the best model, it\nstill means that Autoformer is usable for the time series prediction, and its time decomposition structure is\nadequate. The shortcoming of Autoformer and the latest FEDformer is that they are huge models compared\nto LSTM, and they need to be fine-tuned to work well in a specific task. In contrast, LSTM-based models\u2019\nsizes are much smaller and do not need much hyper-parameters tuning. More analysis of the efficiency will\nbe discussed in the Section 5.3.7.\n24\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nModel\nAccuracy\nPrecision\nRecall\nF1\nPrediction Horizon k = 50\nMLP\n55.65\n55.71\n55.62\n54.98\nLSTM\n62.77\n62.91\n62.77\n62.78\nDeepLOB\n63.32\n63.69\n63.32\n63.37\nDeepLOB-Seq2Seq\n63.62\n64.04\n63.61\n63.59\nDeepLOB-Attention\n64.05\n64.19\n64.04\n63.94\nAutoformer\n60.17\n60.64\n60.12\n58.40\nFEDformer\n63.46\n63.44\n63.42\n62.52\nInformer\n61.76\n61.64\n61.74\n61.55\nReformer\n60.43\n60.79\n60.42\n60.37\nTransformer\n59.51\n59.78\n59.51\n59.46\nDLSTM\n67.45\n67.96\n67.45\n67.59\nPrediction Horizon k = 100\nMLP\n57.03\n56.03\n56.36\n56.01\nLSTM\n53.49\n52.83\n52.82\n52.36\nDeepLOB\n58.12\n58.50\n57.92\n57.86\nDeepLOB-Seq2Seq\n58.30\n58.43\n57.93\n57.77\nDeepLOB-Attention\n59.16\n58.59\n58.65\n58.50\nAutoformer\n59.18\n58.34\n58.40\n57.83\nFEDformer\n57.97\n56.97\n56.62\n54.14\nInformer\n56.11\n56.15\n55.85\n55.81\nReformer\n54.92\n54.47\n54.53\n54.47\nTransformer\n55.42\n55.04\n54.92\n54.72\nDLSTM\n63.73\n63.02\n63.18\n63.05\nTable 4: Experiment results of Mid Price Movement for",
    "chunk_index": 19,
    "start_char": 49948,
    "end_char": 53250,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "60.37\nTransformer\n59.51\n59.78\n59.51\n59.46\nDLSTM\n67.45\n67.96\n67.45\n67.59\nPrediction Horizon k = 100\nMLP\n57.03\n56.03\n56.36\n56.01\nLSTM\n53.49\n52.83\n52.82\n52.36\nDeepLOB\n58.12\n58.50\n57.92\n57.86\nDeepLOB-Seq2Seq\n58.30\n58.43\n57.93\n57.77\nDeepLOB-Attention\n59.16\n58.59\n58.65\n58.50\nAutoformer\n59.18\n58.34\n58.40\n57.83\nFEDformer\n57.97\n56.97\n56.62\n54.14\nInformer\n56.11\n56.15\n55.85\n55.81\nReformer\n54.92\n54.47\n54.53\n54.47\nTransformer\n55.42\n55.04\n54.92\n54.72\nDLSTM\n63.73\n63.02\n63.18\n63.05\nTable 4: Experiment results of Mid Price Movement for prediction horizons 50 and 100.Red Bold represents\nthe best result and blue underline represents the second best result.\nTo summarize, combing the results of this task and previous tasks, LSTM-based models generally have their\nadvantage in financial time series for their robustness and good compatibility. Although the Transformer-\nbased model is large and complicated to tune and requires a long training time, they are still usable for\nthe forecasting task. Furthermore, the research of Transformer-based method in time series prediction is\nmeaningful because the time decomposition method from Autoformer contributes back to the original LSTM\nmodel.\nFigure 15: Confusion matrix of DLSTM model with prediction horizon 20, 30, 50, 100.\n25\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n5.3.5\nSimple Trading Simulation without transaction cost\nIn order to show the models and their predictions are practical and useful in trading, a simple trading\nsimulation (backtesting) is designed. Three models with good classification metrics performance are chosen\nfor comparison: DLSTM, DeepLOB [11], Autoformer [34]. Canonical LSTM [39] and Vanilla Transformer\n[9] are used as baselines. The three-day test set is used for this trading simulation. To make a fair comparison\namong models, the trading simulation follows the simple setup in the previous work [11]: The number of\nshares pre-trade \u00b5 (volume) is set to one. At each timestamp, the model will predict the price movement\n(0: fall, 1: stationary, 2: rise) as a trading signal. When the prediction is 2, enter the long position, and the\nposition is held until it encounters 0. The same rule is applied to the short position when the prediction is 0,\nand only one direction of position can exist in this simulation trading. A delay is set between the prediction\nand the order execution to simulate the high-frequency trading latency. For example, assume the model\ngenerates a prediction 2 at time t, \u00b5 shares will be bought at time t + 5.\nForecast Horizon\nPrediction Horizon = 20\nPrediction Horizon = 30\nPrediction Horizon =50\nPrediction Horizon=100\nModel\nCPR\nSR\nCPR\nSR\nCPR\nSR\nCPR\nSR\nLSTM\n15.396\n51.489\n12.458\n41.411\n8.484\n28.817\n4.914\n20.941\nDLSTM\n14.966\n46.949\n12.634\n37.432\n6.194\n22.027\n3.215\n16.346\nDeepLOB\n13.859\n56.094\n12.789\n42.567\n5.726\n21.014\n2.646\n14.992\nTransformer\n14.553\n59.995\n12.737\n41.044\n6.896\n28.147\n2.859\n16.981\nAutoformer\n9.942\n32.688\n8.617\n30.576\n8.214\n25.882\n3.620\n17.765\nTable 5: Cumulative price returns and annualized sharpe ratio of different models.\nSeveral assumptions are made for the simulation trading:\n1) Since the trading product is in cryptocurrency exchange, the trading volume is considerable sufficient in\nthe market, which means the simulated trades will not have a market impact.",
    "chunk_index": 20,
    "start_char": 52726,
    "end_char": 56033,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "CPR\nSR\nLSTM\n15.396\n51.489\n12.458\n41.411\n8.484\n28.817\n4.914\n20.941\nDLSTM\n14.966\n46.949\n12.634\n37.432\n6.194\n22.027\n3.215\n16.346\nDeepLOB\n13.859\n56.094\n12.789\n42.567\n5.726\n21.014\n2.646\n14.992\nTransformer\n14.553\n59.995\n12.737\n41.044\n6.896\n28.147\n2.859\n16.981\nAutoformer\n9.942\n32.688\n8.617\n30.576\n8.214\n25.882\n3.620\n17.765\nTable 5: Cumulative price returns and annualized sharpe ratio of different models.\nSeveral assumptions are made for the simulation trading:\n1) Since the trading product is in cryptocurrency exchange, the trading volume is considerable sufficient in\nthe market, which means the simulated trades will not have a market impact.\n2) The focus of this part of the experiment is to show the practicality of the prediction result and make\nrelative comparisons among models instead of inventing a fully developed high-frequency trading strategy.\nIndustrial HFT trading strategies usually require the combination of different prediction signals and precise\nentry exit rules [11]. For simplicity, the order is assumed to be executed at the mid-price without transaction\ncost.\nAs displayed in Table 5 and Figure 16, each model\u2019s profitability is presented. The performance of simulated\ntrading is evaluated by cumulative price return (CPR) and the Annualized Sharpe Ratio (SR). The CPR is\nformulated by:\nCPR =\nt\nX\n1\ns \u2217\u00b5 \u2217\n\u0010\npholding ,t\nmid\n\u2212psettlement ,t\nmid\n\u0011\n(37)\nwhere s is the trading position, which is 1 for long position and \u22121 for short position. \u00b5 is the number of\nshares.\n26\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 16: Cumulative return curve for different models in trading simulation.\nAnd the Sharpe Ratio is calculated by:\nSR =\n\u221a\n365 \u00d7\nAverage ( daily CPR)\nStandardDeviation ( daily CPR)\n(38)\nThe value of annualized SR is enormous because the assumptions mentioned above are not realistic for prac-\ntical trading.\nBased on the results, LSTM based-model\u2019s performance in simulated trading is generally better than\nTransformer-based model. The canonical LSTM model achieves highest CPR and SR in prediction horizon\n20 and 30 and DeepLOB has the best performance in prediction horizon 50. For DLSTM, it has compa-\nrable performance to canonical LSTM and DeepLOB model. This result shows that the prediction result\nfrom LSTM-based models are robust and practical for trading. Autoformer\u2019s CPR is the lowest in predic-\ntion horizon 20 and 30. The state-of-the-art Autoformer sometimes has even worse performance than the\nvanilla Transformer in simulated trading, although it obtains a better classification metrics. To summarize,\nLSTM-based models are relatively the better models for electronic trading.\n5.3.6\nSimple Trading Simulation with transaction cost\nIn the real-world market, all operations, including buying or selling, need a commission fee, and sometimes\nthe transaction cost might outweight the return. This section will introduce a hypothetical transaction cost\n27\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nof 0.002% to further compare the robustness among different models. The results are shown in Table 6 and\nFigure 17.\nForecast Horizon\nPrediction Horizon = 20\nPrediction Horizon = 30\nPrediction Horizon =50\nPrediction Horizon=100\nModel\nCPR\nSR\nCPR",
    "chunk_index": 21,
    "start_char": 55392,
    "end_char": 58619,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "cost\nIn the real-world market, all operations, including buying or selling, need a commission fee, and sometimes\nthe transaction cost might outweight the return. This section will introduce a hypothetical transaction cost\n27\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nof 0.002% to further compare the robustness among different models. The results are shown in Table 6 and\nFigure 17.\nForecast Horizon\nPrediction Horizon = 20\nPrediction Horizon = 30\nPrediction Horizon =50\nPrediction Horizon=100\nModel\nCPR\nSR\nCPR\nSR\nCPR\nSR\nCPR\nSR\nLSTM\n2.102\n15.160\n1.767\n12.429\n1.596\n11.536\n0.778\n6.014\nDLSTM\n3.039\n19.962\n2.716\n16.523\n1.957\n12.359\n1.180\n9.811\nDeepLOB\n1.964\n15.082\n1.924\n13.128\n1.450\n10.273\n0.823\n7.993\nTransformer\n1.860\n13.894\n1.561\n10.917\n1.047\n6.612\n0.118\n-23.496\nAutoformer\n0.189\n-8.704\n0.873\n5.118\n-0.225\n-9.193\n-0.061\n-14.835\nTable 6: Cumulative price returns and annualized sharpe ratio of different models under 0.002% transaction\ncost.\nFrom the table, DLSTM has the highest CPRs and SRs for all the prediction horizons, outperforming all other\nmodels. This shows DLSTM\u2019s strong profitability and robustness against the risk brought by the transaction\ncost. LSTM-based methods\u2019 performance is generally better than Transformer-based methods. Canonical\nLSTM and DeepLOB achieve the second-best CPRs and SRs in different prediction horizons. This indicates\nthat the LSTM-based model\u2019s prediction results are more practical and effective in electronic trading.\nInterestingly, Transformer-based models\u2019 performance drops significantly under the transaction cost. The\nstate-of-the-art Autoformer produces even less profit than vanilla Transformer, yielding negative CPRs and\nSRs in prediction horizon 50 and 100, although its prediction classification metrics is better than Transformer.\nFigure 17: Cumulative return curve under 0.002% transaction cost.\n28\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nTo further investigate the impact of transaction cost on simulated trading. An experiment is extended to see\nhow the CPR and SR change as the transaction cost increase. The experiment on CPR is done on the three-day\ntest set from 2022.07.12 (inclusive) to 2022.07.14 (inclusive) as mentioned in Section 5.3.3. The experiment\non SR has a longer backtesting period ranging from 2022.07.13 (inclusive) to 2022.07.24 (inclusive) because\nannualized Sharpe Ratio is calculated based on daily CPR, so using a longer backtesting period can produce\na more accurate SR. The experiment results are shown in Figure 18 and Figure 19.\nFigure 18: Cumulative price return change with increasing transaction cost (back testing period: 2022.07.12\n(inclusive) to 2022.07.14 (inclusive)).\nIn terms of the CPR, all models\u2019 CPR decreases as the transaction cost increases. For the prediction horizon 20\nand 30, DLSTM still outperforms other models when the transaction cost increases. All the models generate\ncomparable CPR except the Autoformer in the prediction horizon 50 and 100. Autoformer\u2019s CPR is the lowest\nin all prediction horizons for most transaction cost settings. Regarding the SR, all models\u2019 SR decrease as the\ntransaction cost increases. However, DLSTM maintains a higher SR than other models as the transaction cost\nincreases. At the same time, for the Autoformer, its SR drops significantly and even becomes the lowest as\nthe transaction cost increases.",
    "chunk_index": 22,
    "start_char": 58090,
    "end_char": 61477,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "when the transaction cost increases. All the models generate\ncomparable CPR except the Autoformer in the prediction horizon 50 and 100. Autoformer\u2019s CPR is the lowest\nin all prediction horizons for most transaction cost settings. Regarding the SR, all models\u2019 SR decrease as the\ntransaction cost increases. However, DLSTM maintains a higher SR than other models as the transaction cost\nincreases. At the same time, for the Autoformer, its SR drops significantly and even becomes the lowest as\nthe transaction cost increases. Overall, DLSTM keeps its profitability and robustness for different transaction\ncosts, while the Transformer-based method\u2019s performance can be largely affected by the transaction cost.\nThis result further indicates that the LSTM-based method is superior for electronic trading.\n5.3.7\nEfficiency Comparison on Transformers versus LSTM\nThe efficiency comparison of Transformer-based and LSTM-based models on price movement prediction\nis shown in Table 7. The comparison is separated into two parts, the left panel is the practical efficiency,\n29\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 19: Sharpe ratio change with increasing transaction cost (back testings period: 2022.07.13 (inclusive)\nto 2022.07.24 (inclusive)).\nMethod\nMACs\nParameter\nTime\nMemory\nTime\nMemory\nTest Step\nDLSTM\n6.72 M\n193.9 k\n4.4ms\n1404MiB\nO(L)\nO(L)\n1\nDeepLOB\n36.42 M\n143.91 k\n6.3ms\n2250MiB\nO(L)\nO(L)\n1\nTransformer\n1.25 G\n10.64 M\n17.7ms\n3534MiB\nO(L2)\nO(L2)\n1\nReformer\n1.17 G\n5.84 M\n23.6ms\n4966MiB\nO(LlogL)\nO(L2)\n1\nInformer\n1.15 G\n11.43 M\n22.1ms\n4361MiB\nO(LlogL)\nO(LlogL)\n1\nAutoformer\n1.25 G\n10.64M\n75.2ms\n5394MiB\nO(L)\nO(L)\n1\nFEDformer\n1.25 G\n16.47 M\n38.3ms\n3556MiB\nO(L)\nO(L)\n1\nTable 7: Efficiency comparison of Transformer-based and LSTM-based models on price movement prediction.\nMACs represent the number of Multiply-accumulate operations.\nand the right is the theoretical efficiency. The latest transformer-based models have a focus on lowering\nthe time and memory complexity. Autoformer and FEDformer claim to achieve O(L) time and memory\ncomplexity in theory. However, their actual inference time and memory consumption are higher than the\nvanilla transformer models because of their complex design. For the training process, it usually takes more\nthan 12 hours to train an Autoformer and FEDformer model, even with the cutting-edge GPU device (e.g.,\n24GB NVIDIA RTX 3090 GPU is used here), which is not efficient to retrain the model on new data. The\nresearchers should reconsider the focus of the Transformer-based model on time series application. The\ntime and memory complexity is not a big threshold for the vanilla Transformer, where its inference speed\nand memory consumption is acceptable depending on today\u2019s computing power.\n30\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nThe LSTM-based model has higher efficiency than the Transformer-based model for its low inference time\nand small model size, where its theoretical efficiency corresponds to its practical efficiency. This gives the\nLSTM-based model advantage in high-frequency trading, which requires fast execution speed. This also\nagain emphasizes that LSTM-based models are the better model in electronic trading.\n6\nConclusion and Future work\nThis study systematically compares LSTM-based and Transformer-based models among three financial time\nseries prediction tasks based on cryptocurrency LOB data.",
    "chunk_index": 23,
    "start_char": 60953,
    "end_char": 64363,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "power.\n30\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nThe LSTM-based model has higher efficiency than the Transformer-based model for its low inference time\nand small model size, where its theoretical efficiency corresponds to its practical efficiency. This gives the\nLSTM-based model advantage in high-frequency trading, which requires fast execution speed. This also\nagain emphasizes that LSTM-based models are the better model in electronic trading.\n6\nConclusion and Future work\nThis study systematically compares LSTM-based and Transformer-based models among three financial time\nseries prediction tasks based on cryptocurrency LOB data. The first task is to predict the LOB mid-price.\nFEDformer and Autoformer have less error than other models, and LSTM is still a strong model that\nsurpasses Informer, Reformer and vanilla Transformer. Although the mid-price prediction error is low, the\nquality of the mid-price prediction result is far from sufficient for practical use in high-frequency trading.\nThe second task is to predict LOB mid-price difference. LSTM-based methods show their robustness in time\nseries prediction and perform better than Transformer-based models, which reach the highest 11.5% R2 in\naround 10 prediction steps. State-of-the-art Autoformer and FEDformer are limited in this task because their\ntime decomposition architecture can not handle the difference sequence. However, in a separate study [63],\nit was shown that custom transformer configurations can outperform the standard transformers.\nThe last task is to predict the LOB mid-price movement. New architecture for the Transformer-based model\nis designed for adapting the classification task. A new DLSTM model is proposed combining the merits\nof LSTM and time decomposition architecture from Autoformer. DLSTM outperforms all other models in\nclassification metrics, and Autoformer shows comparable performance to LSTM-based models. A simple\ntrading simulation is done to verify the practicality of the prediction. LSTM-based models have overall\nbetter performance than Transformer-based models and DLSTM model beats all other models under the\ntransaction cost.\nIn conclusion, based on all the experiments on three different tasks, the Transformer-based model can only\noutperform LSTM-based models by a large margin in terms of the limited metrics for mid-price prediction.\nIn comparison, the LSTM-based model is still dominant in the later two tasks, so LSTM-based models are\ngenerally the better model in financial time series prediction for electronic trading.\nFor future research, applying LSTM-based and Transformer-based models in Deep Reinforcement Learning\n(DRL) can be a proper direction. A complete high-frequency trading strategy usually requires the combina-\ntion of different prediction signals and needs an experienced trader to control the take-profit and stop-loss.\n31\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nIn this case, using DRL to generate the optimal trading strategy directly can get us one step closer to the\nactual trading.\n32\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nReferences\n[1] Eugene F. Fama. Efficient capital markets: A review of theory and empirical work. The Journal of\nfinance (New York), 25(2):383\u2013, 1970. ISSN 0022-1082.\n[2] John J. Murphy. Study guide for Technical analysis of the financial markets : a comprehensive guide to\ntrading methods and applications.",
    "chunk_index": 24,
    "start_char": 63705,
    "end_char": 67148,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "A PREPRINT\nIn this case, using DRL to generate the optimal trading strategy directly can get us one step closer to the\nactual trading.\n32\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nReferences\n[1] Eugene F. Fama. Efficient capital markets: A review of theory and empirical work. The Journal of\nfinance (New York), 25(2):383\u2013, 1970. ISSN 0022-1082.\n[2] John J. Murphy. Study guide for Technical analysis of the financial markets : a comprehensive guide to\ntrading methods and applications. New York Institute of Finance, New York, 1999. ISBN 0735200653.\n[3] Constance M. Brown. Mastering elliott wave principle elementary concepts, wave patterns, and practice\nexercises. Bloomberg financial series. Wiley, Hoboken, N.J, 1st edition edition, 2012. ISBN 1-280-\n67304-4.\n[4] Carolyn Boroden. Fibonacci trading: how to master the time and price advantage. Mcgraw-hill New\nYork, NY, 2008.\n[5] David. Ruppert. Statistics and Data Analysis for Financial Engineering with R examples. Springer Texts\nin Statistics. Springer New York, New York, NY, 2nd ed. 2015. edition, 2015. ISBN 1-4939-2614-4.\n[6] Adebiyi A. Ariyo, Adewumi O. Adewumi, and Charles K. Ayo. Stock price prediction using the arima\nmodel. In 2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation,\npages 106\u2013112, 2014. doi: 10.1109/UKSim.2014.67.\n[7] Victor Carbune, Pedro Gonnet, Thomas Deselaers, Henry A. Rowley, Alexander N. Daryin, Marcos\nCalvo, Li-Lun Wang, Daniel Keysers, Sandro Feuz, and Philippe Gervais. Fast multi-language lstm-\nbased online handwriting recognition. CoRR, abs/1902.10525, 2019. URL http://arxiv.org/abs/\n1902.10525.\n[8] Hagen Soltau, Hank Liao, and Hasim Sak. Neural speech recognizer: Acoustic-to-word lstm model for\nlarge vocabulary speech recognition, 2016. URL https://arxiv.org/abs/1610.09975.\n[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http:\n//arxiv.org/abs/1706.03762.\n[10] Justin Sirignano and Rama Cont. Universal features of price formation in financial markets: perspec-\ntives from deep learning, 2018. URL https://arxiv.org/abs/1803.06917.\n[11] Zihao Zhang, Stefan Zohren, and Stephen Roberts. DeepLOB: Deep convolutional neural networks\nfor limit order books.\nIEEE Transactions on Signal Processing, 67(11):3001\u20133012, jun 2019.\ndoi:\n10.1109/tsp.2019.2907260. URL https://doi.org/10.1109%2Ftsp.2019.2907260.\n[12] Zihao Zhang and Stefan Zohren. Multi-horizon forecasting for limit order books: Novel deep learn-\ning approaches and hardware acceleration using intelligent processing units. CoRR, abs/2105.10430,\n2021. URL https://arxiv.org/abs/2105.10430.\n33\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[13] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and\nAlexandros Iosifidis. Using deep learning for price prediction by exploiting stationary limit order book\nfeatures, 2018. URL https://arxiv.org/abs/1810.09965.\n[14] Petter N. Kolm, Jeremy D. Turiel, and Nicholas Westray. Deep order flow imbalance: Extracting alpha at\nmultiple horizons from the limit order book. Econometric Modeling: Capital Markets - Portfolio Theory\neJournal, 2021.\n[15] Murtaza Roondiwala, Harshal Patel, and Shraddha Varma. Predicting stock prices using lstm. Interna-\ntional Journal of Science and Research (IJSR), 6, 04 2017. doi: 10.21275/ART20172755.\n[16] Jian Cao, Zhi Li, and Jian Li. Financial time series forecasting model based on ceemdan and lstm.\nPhysica A: Statistical Mechanics and its Applications, 519:127\u2013139, 2019. ISSN 0378-4371. doi: https:\n//doi.org/10.1016/j.physa.2018.11.061.",
    "chunk_index": 25,
    "start_char": 66643,
    "end_char": 70367,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "flow imbalance: Extracting alpha at\nmultiple horizons from the limit order book. Econometric Modeling: Capital Markets - Portfolio Theory\neJournal, 2021.\n[15] Murtaza Roondiwala, Harshal Patel, and Shraddha Varma. Predicting stock prices using lstm. Interna-\ntional Journal of Science and Research (IJSR), 6, 04 2017. doi: 10.21275/ART20172755.\n[16] Jian Cao, Zhi Li, and Jian Li. Financial time series forecasting model based on ceemdan and lstm.\nPhysica A: Statistical Mechanics and its Applications, 519:127\u2013139, 2019. ISSN 0378-4371. doi: https:\n//doi.org/10.1016/j.physa.2018.11.061. URL https://www.sciencedirect.com/science/article/\npii/S0378437118314985.\n[17] Wei Bao, Jun Yue, and Yulei Rao. A deep learning framework for financial time series using stacked\nautoencoders and long-short term memory. PLOS ONE, 12(7):1\u201324, 07 2017. doi: 10.1371/journal.\npone.0180944. URL https://doi.org/10.1371/journal.pone.0180944.\n[18] Sreelekshmy Selvin, R Vinayakumar, E. A Gopalakrishnan, Vijay Krishna Menon, and K. P. Soman.\nStock price prediction using lstm, rnn and cnn-sliding window model. In 2017 International Conference\non Advances in Computing, Communications and Informatics (ICACCI), pages 1643\u20131647, 2017. doi:\n10.1109/ICACCI.2017.8126078.\n[19] Thomas Fischer and Christopher Krauss. Deep learning with long short-term memory networks for\nfinancial market predictions. European Journal of Operational Research, 270(2):654\u2013669, 2018. ISSN\n0377-2217.\ndoi: https://doi.org/10.1016/j.ejor.2017.11.054.\nURL https://www.sciencedirect.\ncom/science/article/pii/S0377221717310652.\n[20] Sima Siami-Namini, Neda Tavakoli, and Akbar Siami Namin. A comparative analysis of forecasting\nfinancial time series using arima, lstm, and bilstm. CoRR, abs/1911.09512, 2019. URL http://arxiv.\norg/abs/1911.09512.\n[21] Sangyeon Kim and Myungjoo Kang. Financial series prediction using attention lstm, 2019. URL https:\n//arxiv.org/abs/1902.10877.\n[22] Xuan Zhang, Xun Liang, Aakas Zhiyuli, Shusen Zhang, Rui Xu, and Bo Wu. AT-LSTM: An attention-\nbased LSTM model for financial time series prediction. IOP Conference Series: Materials Science and\nEngineering, 569(5):052037, jul 2019. doi: 10.1088/1757-899x/569/5/052037. URL https://doi.\norg/10.1088/1757-899x/569/5/052037.\n34\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[23] Xiaokang Hu. Stock price prediction based on temporal fusion transformer. In 2021 3rd International\nConference on Machine Learning, Big Data and Business Intelligence (MLBDBI), pages 60\u201366, 2021. doi:\n10.1109/MLBDBI54094.2021.00019.\n[24] Sashank Sridhar and Sowmya Sanagavarapu. Multi-head self-attention transformer for dogecoin price\nprediction. In 2021 14th International Conference on Human System Interaction (HSI), pages 1\u20136, 2021.\ndoi: 10.1109/HSI52170.2021.9538640.\n[25] Priyank Sonkiya, Vikas Bajpai, and Anukriti Bansal. Stock price prediction using bert and gan, 2021.\nURL https://arxiv.org/abs/2107.09055.\n[26] Surafel M. Lakew, Mauro Cettolo, and Marcello Federico. A comparison of transformer and recurrent\nneural networks on multilingual neural machine translation, 2018. URL https://arxiv.org/abs/\n1806.06957.\n[27] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao\nSomeki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, Shinji Watanabe, Takenori\nYoshimura, and Wangyou Zhang. A comparative study on transformer vs RNN in speech applications.\nIn 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, dec 2019. doi:\n10.1109/asru46091.2019.9003750. URL https://doi.org/10.1109%2Fasru46091.2019.9003750.\n[28] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Trans-\nformers in time series: A survey, 2022. URL https://arxiv.org/abs/2202.07125.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks,\n2014. URL https://arxiv.org/abs/1409.3215.\n[30] Tom B.",
    "chunk_index": 26,
    "start_char": 69779,
    "end_char": 73747,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "in speech applications.\nIn 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, dec 2019. doi:\n10.1109/asru46091.2019.9003750. URL https://doi.org/10.1109%2Fasru46091.2019.9003750.\n[28] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Trans-\nformers in time series: A survey, 2022. URL https://arxiv.org/abs/2202.07125.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks,\n2014. URL https://arxiv.org/abs/1409.3215.\n[30] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\n[31] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\nhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\n2019. URL https://arxiv.org/abs/1907.00235.\n[32] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer, 2020. URL\nhttps://arxiv.org/abs/2001.04451.\n[33] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\nInformer: Beyond efficient transformer for long sequence time-series forecasting, 2020. URL https:\n//arxiv.org/abs/2012.07436.\n35\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[34] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers\nwith auto-correlation for long-term series forecasting, 2021.\nURL https://arxiv.org/abs/2106.\n13008.\n[35] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, and Schahram Dustdar.\nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecast-\ning. In International Conference on Learning Representations, 2022. URL https://openreview.net/\nforum?id=0EXmFzUn5I.\n[36] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency\nenhanced decomposed transformer for long-term series forecasting, 2022. URL https://arxiv.org/\nabs/2201.12740.\n[37] Martin D. Gould, Mason A. Porter, Stacy Williams, Mark McDonald, Daniel J. Fenn, and Sam D. Howi-\nson. Limit order books, 2010. URL https://arxiv.org/abs/1012.0349.\n[38] Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and\nAlexandros Iosifidis. Forecasting stock prices from the limit order book using convolutional neural\nnetworks. In 2017 IEEE 19th Conference on Business Informatics (CBI), volume 01, pages 7\u201312, 2017.\ndoi: 10.1109/CBI.2017.23.\n[39] Sepp Hochreiter and J\u00a8urgen Schmidhuber.\nLong Short-Term Memory.\nNeural Computation, 9(8):\n1735\u20131780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/\n10.1162/neco.1997.9.8.1735.\n[40] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.\ndeeplearningbook.org.\n[41] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-\npropagating errors. Nature, 323:533\u2013536, 1986.\n[42] F.A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: continual prediction with lstm. In 1999\nNinth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume 2,\npages 850\u2013855 vol.2, 1999. doi: 10.1049/cp:19991218.\n[43] Alex Graves. Generating sequences with recurrent neural networks, 2013. URL https://arxiv.org/\nabs/1308.0850.\n[44] Roel Oomen and Jim Gatheral. Zero-intelligence realized variance estimation.",
    "chunk_index": 27,
    "start_char": 73202,
    "end_char": 77139,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-\npropagating errors. Nature, 323:533\u2013536, 1986.\n[42] F.A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: continual prediction with lstm. In 1999\nNinth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume 2,\npages 850\u2013855 vol.2, 1999. doi: 10.1049/cp:19991218.\n[43] Alex Graves. Generating sequences with recurrent neural networks, 2013. URL https://arxiv.org/\nabs/1308.0850.\n[44] Roel Oomen and Jim Gatheral. Zero-intelligence realized variance estimation. Finance and Stochastics,\n14:249\u2013283, 04 2010. doi: 10.1007/s00780-009-0120-1.\n[45] Kyunghyun Cho, Bart van Merrienboer, C\u00b8aglar G\u00a8ulc\u00b8ehre, Fethi Bougares, Holger Schwenk, and Yoshua\nBengio. Learning phrase representations using RNN encoder-decoder for statistical machine transla-\ntion. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.\n36\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[46] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based\nneural machine translation. CoRR, abs/1508.04025, 2015. URL http://arxiv.org/abs/1508.04025.\n[47] R Farsani, Ehsan Pazouki, and Jecei Jecei. A transformer self-attention model for time series forecasting.\nJournal of Electrical and Computer Engineering Innovations, 9:1\u201310, 01 2021. doi: 10.22061/JECEI.\n2020.7426.391.\n[48] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecast-\ning?, 2022. URL https://arxiv.org/abs/2205.13504.\n[49] Souhaib Ben Taieb and Rob J Hyndman.\nRecursive and direct multi-step forecasting: the best of\nboth worlds. Monash Econometrics and Business Statistics Working Papers 19/12, Monash University,\nDepartment of Econometrics and Business Statistics, 2012. URL https://ideas.repec.org/p/msh/\nebswps/2012-19.html.\n[50] Guillaume Chevillon.\nDirect multi-step estimation and forecasting.\nJournal of Economic Surveys,\n21(4):746\u2013785, 2007.\ndoi:\nhttps://doi.org/10.1111/j.1467-6419.2007.00518.x.\nURL https://\nonlinelibrary.wiley.com/doi/abs/10.1111/j.1467-6419.2007.00518.x.\n[51] Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. Stl: A seasonal-trend\ndecomposition procedure based on loess. Journal of Official Statistics, 6:3\u201373, 1990.\n[52] James Douglas Hamilton. Time Series Analysis. Princeton University Press, Princeton, 1994. ISBN\n0691042896.\n[53] Rob J. Hyndman. Forecasting : principles and practice. OTexts, Melbourne, third edition. edition, 2021\n- 2021. ISBN 9780987507136.\n[54] Bryant Moscon. cryptofeed. https://github.com/bmoscon/cryptofeed, 2022.\n[55] Jan Novotny, Paul A Bilokon, Aris Galiotos, and Fr\u00b4ed\u00b4eric D\u00b4el`eze. Machine Learning and Big Data with\nkdb+/q. John Wiley & Sons, 2019.\n[56] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer. https://github.com/thuml/\nAutoformer, 2022.\n[57] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.\nFedformer.\nhttps:\n//github.com/MAZiqing/FEDformer, 2022.\n[58] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00a8opf, Edward Z. Yang,\nZach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. CoRR,\nabs/1912.01703, 2019. URL http://arxiv.org/abs/1912.01703.\n37\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[59] Michael S. Lewis-Beck. Applied regression : an introduction. Sage university papers series. Quantitative\napplications in the social sciences ; no. 07-022. Sage Publications, Beverly Hills, Calif, 1980. ISBN\n0803914946.\n[60] Zihao Zhang. Deeplob-deep-convolutional-neural-networks-for-limit-order-books. https://github.\ncom/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books, 2021.\n[61] Zihao Zhang.\nMulti-horizon-forecasting-for-limit-order-books.\nhttps://github.com/zcakhaa/\nMulti-Horizon-Forecasting-for-Limit-Order-Books, 2021.\n[62] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.\nLtsf-linear.\nhttps://github.com/cure-lab/\nLTSF-Linear, 2022.",
    "chunk_index": 28,
    "start_char": 76533,
    "end_char": 80802,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. CoRR,\nabs/1912.01703, 2019. URL http://arxiv.org/abs/1912.01703.\n37\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\n[59] Michael S. Lewis-Beck. Applied regression : an introduction. Sage university papers series. Quantitative\napplications in the social sciences ; no. 07-022. Sage Publications, Beverly Hills, Calif, 1980. ISBN\n0803914946.\n[60] Zihao Zhang. Deeplob-deep-convolutional-neural-networks-for-limit-order-books. https://github.\ncom/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books, 2021.\n[61] Zihao Zhang.\nMulti-horizon-forecasting-for-limit-order-books.\nhttps://github.com/zcakhaa/\nMulti-Horizon-Forecasting-for-Limit-Order-Books, 2021.\n[62] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.\nLtsf-linear.\nhttps://github.com/cure-lab/\nLTSF-Linear, 2022.\n[63] Fazl Barez, Paul Bilokon, Arthur Gervais, and Nikita Lisitsyn. Exploring the advantages of transformers\nfor high-frequency trading. SSRN Electronic Journal, 2023. doi: 10.2139/ssrn.4364833.\nA\nLabelling Details\nAs mentioned in Section 3.4, a threshold \u03b4 needs to be set to decide the corresponding labels. The choice\nof \u03b4 follows a simple rule, which is to make the labelling roughly balanced. The choice of \u03b4 for different\nprediction horizon on ETH-USDT dataset is shown in Table 8 and the distribution of labelling is shown in\nFigure 20.\nHorizon\n20\n30\n50\n100\n\u03b4\n0.17\n0.3\n0.6\n0.92\nTable 8: \u03b4 for different prediction horizons for ETH-USDT dataset.(units in 10\u22124)\n38\n\nTransformers versus LSTMs for electronic trading\nA PREPRINT\nFigure 20: Labelling distribution for different prediction horizon in ETH-USDT dataset\n39",
    "chunk_index": 29,
    "start_char": 79913,
    "end_char": 81623,
    "paper_title": "Transformers versus LSTMs for electronic trading",
    "paper_category": "q-fin.TR",
    "paper_filename": "Transformers_versus_LSTMs_for_electronic_trading.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Transformers_versus_LSTMs_for_electronic_trading.pdf"
  },
  {
    "text": "arXiv:1705.08022v1 [q-fin.TR] 22 May 2017\nUsing Macroeconomic Forecasts to Improve Mean Reverting\nTrading Strategies\nYash Sharma\nAbstract\nA large class of trading strategies focus on opportunities o\ufb00ered by the yield curve. In partic-\nular, a set of yield curve trading strategies are based on the view that the yield curve mean-\nreverts. Based on these strategies\u2019 positive performance, a multiple pairs trading strategy on ma-\njor currency pairs was implemented. To improve the algorithm\u2019s performance, machine learning\nforecasts of a series of pertinent macroeconomic variables were factored in, by optimizing the\nweights of the trading signals. This resulted in a clear improvement in the APR over the evalua-\ntion period, demonstrating that macroeconomic indicators, not only technical indicators, should\nbe considered in trading strategies.\n1. Yield Curve\nThe Yield Curve is a line that plots the interest rate, at a set point in time, of bonds having\nequal credit quality but di\ufb00ering maturity dates. The U.S. Treasury Yield Curve is used as a\nbenchmark for other debt in the market, and can also be used to predict changes in economic\noutput and growth. The shape of the yield curve gives an idea of future interest rate changes\nand economic activity. An inverted yield curve is often a harbinger of recession. A positively\nsloped yield curve is often a harbinger of in\ufb02ationary growth. These predictive properties make\nthe yield curve quite applicable in devising successful trading strategies.\nMany yield curve trading strategies are based on the conventional view that the yield curve\nmean-reverts to some historical norm. This market view is consistent with historical experience.\nFor instance, U.S. Treasury bill rates, spreads and curvature all trade within tight, \ufb01nite bounds.\nThe interest rate term structures in other countries also exhibit similar patterns. This suggests\nthat some form of mean-reversion mechanism is at work that prevents the yield curve from drift-\ning to extreme levels or shapes over time.\nIn [1], three classes of mean-reverting trading strategies were considered, focusing on three\naspects of the yield curve: level, spread, and curvature. For each strategy, the holding period\nof a trade was \ufb01xed at one month, after which a new trade was initiated. The condition of cash\nneutrality was imposed, so that any excess cash was deposited at the 1-month tenor. Similarly,\nif additional funding was required, that was also carried out at the 1-month tenor. A 102-month\ntraining period was allowed in the construction of the unconditional yield curve, so that the cal-\nculation of the average payo\ufb00of each yield curve strategy starts from January 1973 to December\n\n2000. 1\nThese mean-reverting trading strategies were compared to two benchmarks, investment in\nthe Lehman Brothers U.S. Government Intermediate Bond Index, and cash-neutral investment in\nthe S&P index. 2\nThe performance analysis indicated that a yield spread mean-reverting trading strategy per-\nforms remarkably well compared to the benchmarks. The monthly payo\ufb00is about 5.1 times that\nof the monthly payo\ufb00of the equity benchmark, hence outperforming an equity investment strat-\negy, on a risk-adjusted basis.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3211,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "starts from January 1973 to December\n\n2000. 1\nThese mean-reverting trading strategies were compared to two benchmarks, investment in\nthe Lehman Brothers U.S. Government Intermediate Bond Index, and cash-neutral investment in\nthe S&P index. 2\nThe performance analysis indicated that a yield spread mean-reverting trading strategy per-\nforms remarkably well compared to the benchmarks. The monthly payo\ufb00is about 5.1 times that\nof the monthly payo\ufb00of the equity benchmark, hence outperforming an equity investment strat-\negy, on a risk-adjusted basis. Furthermore, it outperformed the bond benchmark, with an average\nmonthly payo\ufb00of about 5.9 times that of the benchmark. 3\nA paired-t test and the Diebold-Mariano statistical test (D-M test) [2] was conducted to test\nwhether the strategy signi\ufb01cantly outperforms the benchmarks. The tests were successful; even\nwhen transaction costs were accounted for, the yield spread mean-reverting strategy was still\nsigni\ufb01cantly more pro\ufb01table than both of the benchmarks under all measures. 4\nAs the yield curve is highly correlated with changes in economic activity, the yield spread\nmean reverting is a strong justi\ufb01cation for trading upon the mean reversion of the spread, in-\ndependent of the \ufb01nancial instrument used. Therefore, a Multiple Pairs trading strategy was\nimplemented.\n2. Multiple Pairs Trading\nA Pairs Trade is a strategy based on securities involved in the pair having a mean-reverting\nnature. The goal is to match two trading vehicles, trading one long and the other short when the\npair\u2019s price ratio diverges. The number of standard deviations the pair\u2019s price ratio diverges in\norder to initiate a trade is determined through historical data analysis. If the pair reverts to its\nmean trend, a pro\ufb01t is made.\nIn our implementation, currency pairs were used. Daily data from January 2008 to June 2014\nfor each of the major currency pairs was retrieved through Yahoo Finance.\nThe currency pair price series were plotted, shown in Figure 1. Clearly, the series do not look\nstationary. In order to perform a pairs trade, we desire a stationary pair, as the spread is \ufb01xed,\nand hence statistically the pair is mean reverting.\nConsider a pair of non-stationary time series. If a particular linear combination of these series\nwill lead to a stationary series, the pair of series are termed cointegrated. In order to \ufb01nd that\nparticular combination, one needs to utilize tests for unit roots.\n1Pro\ufb01ting from Mean Reverting Yield Curve Trading Strategies, 9\n2Pro\ufb01ting from Mean Reverting Yield Curve Trading Strategies, 14\n3Pro\ufb01ting from Mean Reverting Yield Curve Trading Strategies, 16\n4Pro\ufb01ting from Mean Reverting Yield Curve Trading Strategies, 17-19\n2\n\n2008\n2010\n2012\n2014\n0.6\n0.8\n1.0\nAUDUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n1.2\n1.4\n1.6\nEURUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n1.4\n1.7\n2.0\nGBPUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.5\n0.7\nNZDUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.80\n0.95\nCADUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.8\n1.0\n1.2\n1.4\nCHFUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.009\n0.012\nJPYUSD\nTime\nPrices\nFigure 1: Major Currency Pair Price Series\n2.1.",
    "chunk_index": 1,
    "start_char": 2663,
    "end_char": 5773,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "2\n\n2008\n2010\n2012\n2014\n0.6\n0.8\n1.0\nAUDUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n1.2\n1.4\n1.6\nEURUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n1.4\n1.7\n2.0\nGBPUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.5\n0.7\nNZDUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.80\n0.95\nCADUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.8\n1.0\n1.2\n1.4\nCHFUSD\nTime\nPrices\n2008\n2010\n2012\n2014\n0.009\n0.012\nJPYUSD\nTime\nPrices\nFigure 1: Major Currency Pair Price Series\n2.1. Cointegration\nOne would normally test for cointegration using the Augmented Dickey-Fuller test (ADF)\n[3]. However, a big drawback of that test is that it is only capable of being applied to two sep-\narate time series. However, one can imagine a set of three or more \ufb01nancial assets that might\nshare an underlying cointegrated relationship. Hence the Johansen test is used, which can be\nseen as a multivariate generalization of the ADF test. [4]The maximum number of currency\npairs that can be combined in one relationship was set to 4. This gives the algorithm more \ufb02exi-\nbility, as all combinations of two, three, or four separate time series were tested for cointegration.\nBefore applying the Johansen test, or any unit root test for that matter, one must ensure that\nall the series are integrated of order 1. Technically, if two or more series are individually inte-\ngrated, but some linear combination of them has a lower order of integration, then the series are\nsaid to be cointegrated. So, to guarantee that the cointegrated series is stationary, the time series\nneed to be I(1). A price series is I(1) if the levels contain a unit root, meaning the price series is\nat least I(1), and the di\ufb00erenced prices are stationary, I(0), meaning the price series is not I(2).\nThe ADF test with the general regression equation was used, assuming the series has drift\nbut no linear trend, in order to test for the existence of a unit root. Lags are included in the\nADF formulation, allowing for higher-order autoregressive processes. The BIC information cri-\nterion is used to select the optimal lag length. [5] This criterion is used for consistency reasons\ndue to the large sample size. Though ine\ufb03cient, the criterion delivers asymptotically correct\n3\n\nresults. Finally, the maximum lag length is chosen by the rule of thumb formula, proposed by\nSchwert, in [6]. The ADF test is performed on the levels and the di\ufb00erenced levels, and the test\nvalues were compared with the critical value at the 95% con\ufb01dence level to generate conclusions.\nEach of the major currency pair price series were conclusively found to be I(1), and hence\nwere used in the Johansen test. For the Johansen test, the VAR optimal solution lag length needs\nto be found. Again, the series was assumed to have drift but no linear trend, and the lag length\nwas found by minimizing the SC information criterion for, again, the large sample size. Again,\nthough ine\ufb03cient, it provides asymptotically correct results.",
    "chunk_index": 2,
    "start_char": 5368,
    "end_char": 8254,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "Each of the major currency pair price series were conclusively found to be I(1), and hence\nwere used in the Johansen test. For the Johansen test, the VAR optimal solution lag length needs\nto be found. Again, the series was assumed to have drift but no linear trend, and the lag length\nwas found by minimizing the SC information criterion for, again, the large sample size. Again,\nthough ine\ufb03cient, it provides asymptotically correct results. A lagged VAR is used in the Jo-\nhansen procedure, so 1 is subtracted from the optimal VAR lag length.\nThe Johansen procedure was then initiated, again with the assumption of drift but no linear\ntrend. The trace statistic was used, where the null hypothesis is that the number of cointegration\nvectors is less than or equal to the total number of vectors tested. Lastly, the longrun speci\ufb01cation\nfor error correction was used.\n91 combinations of time series were tested for cointegration, and 16 were found to be coin-\ntegrated. The spreads of the cointegrated portfolios were plotted, shown in Figure 2.\nAs can be seen, the half life for each of the spreads was computed. Ignoring the drift\nand lagged di\ufb00erence terms, the di\ufb00erential form of the spread is equivalent to the Ornstein-\nUhlenbeck stochastic process [7]. The di\ufb00erential form leads to an analytical solution for the\nexpected value of the portfolio spread which, for a mean-reverting process with negative \u03bb, tells\nus that the expected value of the price decays exponentially at a half-life of \u2212log(2)/\u03bb. \u03bb, which\nmeasures the speed of an O-U process returning to its mean level, can be estimated from a linear\nregression of the daily change of the spread versus the spread itself. In order to perform this\ncalculation, the spread and transaction costs are needed for the portfolio, and computing these\ncosts will be discussed subsequently when the mean reversion strategy execution is described.\n2.2. Execution\nFinally, a mean reversion strategy was executed on each of the cointegrated spreads. Firstly,\nthe spread and transaction costs that are incurred when buying/selling the cointegrated portfolio\nat the corresponding timestamp were computed. This computation depends on the individual\nprice series of the instruments within the portfolio, and the optimal eigenvector associated with\nthe results of the cointegration test. The eigenvectors generated from the Johansen test can be\nused as hedge ratios. The optimal eigenvector is the one that has the maximum eigenvalue, as it\nhas the shortest half life.\nThe spread is calculated by multiplying the hedge ratio values with their associated price\nvalues and a summation is made. If the portfolios are cointegrated, the resulting spread will be\nstationary.\nThe absolute value of the hedge ratio represents the number of units of each currency pair\nthat we buy or sell. Hence, the total transaction costs can be computed with a summation on\nthese amounts multiplied by the transaction costs per unit for each currency pair.\n4",
    "chunk_index": 3,
    "start_char": 7813,
    "end_char": 10796,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "associated price\nvalues and a summation is made. If the portfolios are cointegrated, the resulting spread will be\nstationary.\nThe absolute value of the hedge ratio represents the number of units of each currency pair\nthat we buy or sell. Hence, the total transaction costs can be computed with a summation on\nthese amounts multiplied by the transaction costs per unit for each currency pair.\n4\n\n2009\n2011\n2013\n\u22127.5\n\u22126.0\n(AUDUSD,GBPUSD)\nTime (HalfLife is 118 days)\nSpread\n2009\n2011\n2013\n\u22121.10\n\u22120.95\n(AUDUSD,CADUSD)\nTime (HalfLife is 43 days)\nSpread\n2008\n2010\n2012\n2014\n0.05\n0.25\n(EURUSD,GBPUSD)\nTime (HalfLife is 76 days)\nSpread\n2009\n2011\n2013\n\u22120.5\n0.5\n(AUDUSD,EURUSD,GBPUSD)\nTime (HalfLife is 84 days)\nSpread\n2009\n2011\n2013\n\u22121.40\n\u22121.20\n(AUDUSD,GBPUSD,CADUSD)\nTime (HalfLife is 49 days)\nSpread\n2009\n2011\n2013\n\u22121.5\n\u22121.2\n(AUDUSD,GBPUSD,JPYUSD)\nTime (HalfLife is 111 days)\nSpread\n2009\n2011\n2013\n\u22120.95\n\u22120.80\n(AUDUSD,CADUSD,CHFUSD)\nTime (HalfLife is 30 days)\nSpread\n2009\n2011\n2013\n\u22121.05\n\u22120.85\n(AUDUSD,EURUSD,GBPUSD,CADUSD)\nTime (HalfLife is 59 days)\nSpread\n2009\n2011\n2013\n\u22120.85\n\u22120.70\n(AUDUSD,EURUSD,CADUSD,CHFUSD)\nTime (HalfLife is 30 days)\nSpread\n2009\n2011\n2013\n\u22120.80\n\u22120.70\n(AUDUSD,GBPUSD,NZDUSD,CADUSD)\nTime (HalfLife is 35 days)\nSpread\n2009\n2011\n2013\n\u22121.00\n\u22120.85\n(AUDUSD,GBPUSD,CADUSD,CHFUSD)\nTime (HalfLife is 30 days)\nSpread\n2009\n2011\n2013\n\u22121.1\n\u22120.8\n(AUDUSD,GBPUSD,CADUSD,JPYUSD)\nTime (HalfLife is 51 days)\nSpread\n2009\n2011\n2013\n\u22120.90\n\u22120.80\n(AUDUSD,NZDUSD,CADUSD,CHFUSD)\nTime (HalfLife is 30 days)\nSpread\n2009\n2011\n2013\n\u22120.75\n\u22120.65\n(AUDUSD,NZDUSD,CADUSD,JPYUSD)\nTime (HalfLife is 30 days)\nSpread\n2009\n2011\n2013\n\u22120.95\n\u22120.80\n(AUDUSD,CADUSD,CHFUSD,JPYUSD)\nTime (HalfLife is 30 days)\nSpread\n2008\n2010\n2012\n2014\n\u22120.10\n0.10\n(EURUSD,GBPUSD,NZDUSD,CHFUSD)\nTime (HalfLife is 40 days)\nSpread\nFigure 2: Cointegrated Spreads\nThe mean and standard deviation of the spread were computed in order to calculate the z\nscore, which simply represents the number of standard deviations separating the current price\nfrom the mean. If the z score is positive, the current price of the security is above the mean. If\nthe z score is negative, the current price of the security is below the mean. Hence, the z score is\nused to generate the mean reversion trading signals.\nThe entry z score was set to 1 and the exit z score was set to 0. These values set the extreme\nvalues, or thresholds, which when crossed by the signal, trigger trading orders. Entry into a long\nposition is made if the z score is less than the negated entry score. Exit from a long position is\nmade if the z score is greater than the negated exit score. Entry into a short position is made if\nthe z score is greater than the entry score. Exit from a short position is made if the z score is less\nthan the exit score.\nWith this, the long/short positions were set, yielding the number of units of the portfolio\nbought or sold at each timestamp. The USD capital allocation to buy the portfolio, speci\ufb01cally\nthe USD capital invested in each currency pair at each timestamp, was found by computing the\nproduct between the hedge ratio matrix, which represents the",
    "chunk_index": 4,
    "start_char": 10403,
    "end_char": 13505,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "z score is greater than the entry score. Exit from a short position is made if the z score is less\nthan the exit score.\nWith this, the long/short positions were set, yielding the number of units of the portfolio\nbought or sold at each timestamp. The USD capital allocation to buy the portfolio, speci\ufb01cally\nthe USD capital invested in each currency pair at each timestamp, was found by computing the\nproduct between the hedge ratio matrix, which represents the hedge ratio of each currency pair\nat each timestamp, and the price matrix, which represents the price of each currency pair at each\ntimestamp.\nUsing these computations, the P&L of the strategy at each timestamp was computed. Trans-\n5\n\naction costs were subtracted from this value, and Return was found by dividing the resulting\nP&L by the gross market value of the portfolio. From the return, the APR, Sharpe Ratio, and\nMaximum Drawdown of the strategy was computed. The spread (w/ half life), the standardized\nspread with color-coding indicating the long and short positions, the daily returns, and the cu-\nmulative returns were all plotted. Furthermore, the average and standard deviation of the daily\nreturns, the APR, Sharpe Ratio, and Maximum Drawdown results were all reported.\nFor the forthcoming analysis, a cointegrated portfolio was randomly selected. The chosen\nportfolio consists of a combination of AUDUSD, CADUSD, NZDUSD, and JPYUSD currency\npairs. The results generated after backtesting the strategy is shown in Figure 3\n2009\n2010\n2011\n2012\n2013\n2014\n\u22120.75\n\u22120.65\n1*AUDUSD\u22120.52*NZDUSD\u22121.1*CADUSD\u221213.66*JPYUS\nTime (HalfLife is 30 days)\nSpread\n2009\n2010\n2011\n2012\n2013\n2014\n\u22124\n\u22122\n0\n1\n2\nZscore entry 1 std, Zscore exit 0 std\nTime\nStandardized Spread\n2009\n2010\n2011\n2012\n2013\n2014\n\u22121.0\n0.0\n1.0\nDaily Returns (average: 0.0156%, std: 0.1591%)\nTime\nDaily Returns (%)\n2009\n2010\n2011\n2012\n2013\n2014\n0\n5\n15\n25\nAPR: 3.983%, Sharpe Ratio: 1.559, maxDD: 3.0018%\nTime\nCumulative Returns (%)\nFigure 3: Mean Reversion Results\n3. Macroeconomic Indicators\nOur goal was to use macroeconomic indicators to improve upon the implemented Multiple\nPairs trading strategy. By generating buy/sell/hold signals based upon the forecasts of the cho-\nsen variables, and combining these signals with the mean reverting signal, we hoped to obtain\nimproved performance.\nThere is much history with improving a Pairs Trading Strategy by factoring in additional\nvariables, however the indicators chosen have typically been technical, and not macroeconomic.\n6\n\nIn [8], the authors\u2019 implemented Pairs Trading model was extended to take into account\ntechnical indicators. Rather than only considering the spread of the price, technical indicator\nmovements were also considered. Technical indicators which were selected exhibited similar be-\nhaviors for both securities: Simple moving average (SMA), Weighted moving average (WMA),\nMoney \ufb02ow index (MFI), and Relative strength index (RSI). T-scores were computed for the\nprice and the selected indicators, and a trained SVM was applied to the test dataset to make the\n\ufb01nal trading decisions.\nThis decision is typical, to not only use the price spread but other indicators intrinsic to\nthe currency pair being traded.",
    "chunk_index": 5,
    "start_char": 13045,
    "end_char": 16246,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "spread of the price, technical indicator\nmovements were also considered. Technical indicators which were selected exhibited similar be-\nhaviors for both securities: Simple moving average (SMA), Weighted moving average (WMA),\nMoney \ufb02ow index (MFI), and Relative strength index (RSI). T-scores were computed for the\nprice and the selected indicators, and a trained SVM was applied to the test dataset to make the\n\ufb01nal trading decisions.\nThis decision is typical, to not only use the price spread but other indicators intrinsic to\nthe currency pair being traded. However, little exploration has been done for considering how\nmacroeconomic movements re\ufb02ecting the overall economy can improve trading performance. As\ncurrency pairs are representative of foreign exchange rates, with a downturn/upturn in the U.S.\neconomy, a increase/decrease in the prices of currency pairs relative to the U.S. Dollar should\ntake place.\nTherefore, macroeconomic indicators which exhibit movements that are correlated with the\nstrengthening/weakening of the U.S. Dollar were considered.\n3.1. S&P 500\nThe Standard & Poor\u2019s 500 Composite Stock Price Index was used. The S&P Index Com-\nmittee chooses the indexed stocks based upon market size, liquidity and industry group repre-\nsentation. Component companies are periodically replaced. Companies are most often removed\nbecause of a merger with another company, \ufb01nancial operating failure or restructuring. Prospec-\ntive companies are placed in an index \u201dreplacement pool\u201d and vacancies are \ufb01lled from that pool.\nThe index is designed to measure changes in the stock prices of component companies. It\nis used as a measure of the nation\u2019s stock of capital, as well as a gauge of future business and\nconsumer con\ufb01dence levels. With that growth, the U.S. Dollar should strengthen.\n3.2. Federal Funds\nThe e\ufb00ective Federal Funds Rate was also used. It is the interest rate at which a deposi-\ntory institution lends funds maintained at the Federal Reserve to another depository institution\novernight. The higher the federal funds rate, the more expensive it is to borrow money. Since\nit is only applicable to very creditworthy institutions for extremely short-term (overnight) loans,\nthe federal funds rate can be viewed as the base rate that determines the level of all other interest\nrates in the U.S. economy.\nThe Federal Open Market Committee (FOMC), which is the Federal Reserves primary mon-\netary policymaking body, telegraphs its desired target for the federal funds rate through open\nmarket operations. A rise in the Federal Funds Rate indicates the FOMC attempting to curb bur-\ngeoning economic growth to prevent an in\ufb02ationary period, and therefore the U.S. Dollar should\nstrengthen.\n3.3. 10-Year Treasury\nThe 10-Year Treasury Note Yield at Constant Maturity was also used. The Treasury Yield is\nthe return on investment, expressed as a percentage, on the U.S. government\u2019s debt obligations\n7\n\n(bonds, notes and bills). From another perspective, it is the interest rate the U.S. government\npays to borrow money for di\ufb00erent lengths of time.\nThe 10-Year Treasury in particular tends to signal investor con\ufb01dence. When con\ufb01dence is\nhigh, the ten-year treasury bond\u2019s price drops and yields rise because investors feel they can \ufb01nd\nhigher returning investments and do not feel the need to play it safe.",
    "chunk_index": 6,
    "start_char": 15687,
    "end_char": 19017,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "The 10-Year Treasury Note Yield at Constant Maturity was also used. The Treasury Yield is\nthe return on investment, expressed as a percentage, on the U.S. government\u2019s debt obligations\n7\n\n(bonds, notes and bills). From another perspective, it is the interest rate the U.S. government\npays to borrow money for di\ufb00erent lengths of time.\nThe 10-Year Treasury in particular tends to signal investor con\ufb01dence. When con\ufb01dence is\nhigh, the ten-year treasury bond\u2019s price drops and yields rise because investors feel they can \ufb01nd\nhigher returning investments and do not feel the need to play it safe. But when con\ufb01dence is low,\nthe price increases and yields fall as there is more demand for safe investment. Therefore, the\nhigher the yield on long-term bonds like the 10-Year Treasury, the better the economic outlook,\nand the stronger the U.S. dollar.\n4. Macroeconomic Forecasts\nEach of the indicators were forecast in order to generate a trading signal. A Support Vec-\ntor Machine was utilized for this purpose. Support Vector Machines, or SVMs, are supervised\nlearning models with associated learning algorithms that analyze data used for classi\ufb01cation and\nregression analysis. Given a set of training examples, each marked as belonging to one of mul-\ntiple categories, an SVM training algorithm builds a model that assigns new examples to one\nof the categories, making it a multiclass non-probabilistic linear classi\ufb01er. An SVM model is a\nrepresentation of the examples as points in space, mapped so that the examples of the separate\ncategories are divided by a clear gap that is as wide as possible. New examples are then mapped\ninto that same space and predicted to belong to a category based on which side of the gap they fall.\nSupport Vector Machines have been found to particularly show excellent performance in the\n\ufb01eld of time series prediction. [9] SVMs can accurately forecast time series data when the un-\nderlying system processes are typically nonlinear, nonstationary and not de\ufb01ned apriori.\nHowever, forecasting the exact value that the macroeconomic indicators would hold on a\nmonth-to-month basis is a infeasible problem, even for SVMs. A vast amount of feature engi-\nneering would need to be done, which is both di\ufb03cult and expensive. Variables would need to be\ncreated using domain knowledge of the data to give the SVM the adequate data needed to make\naccurate forecasts. But, for our purposes, forecasting the direction in which the macroeconomic\nindicators move would be su\ufb03cient, and this is certainly a easier problem to solve.\nAn SVM was trained on monthly data, acquired through the Haver Analytics Database, from\n1995-2008, in order to successfully forecast whether the indicator of note increased, decreased,\nor stayed constant in the evaluation period, 2008-2014, on a month-by-month basis. With little\nfeature engineering, a classi\ufb01cation accuracy of 70% was yielded.\nWith these monthly forecasts, a trading signal can be generated. If the forecast details that the\nindicator will stay constant, a \u201dhold\u201d position was taken.",
    "chunk_index": 7,
    "start_char": 18424,
    "end_char": 21475,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "acquired through the Haver Analytics Database, from\n1995-2008, in order to successfully forecast whether the indicator of note increased, decreased,\nor stayed constant in the evaluation period, 2008-2014, on a month-by-month basis. With little\nfeature engineering, a classi\ufb01cation accuracy of 70% was yielded.\nWith these monthly forecasts, a trading signal can be generated. If the forecast details that the\nindicator will stay constant, a \u201dhold\u201d position was taken. If instead it predicts that the indicator\nwill increase, then a \u201dsell/short\u201d position was taken, and if a decrease is predicted, a \u201dbuy/long\u201d\nposition was taken. This is because if, for example, the indicator were to increase, the value of\nthe major currencies relative to the U.S. Dollar would decrease, meaning a \u201dsell/short\u201d position\nshould be taken.\nThese monthly signals were normalized to daily signals, in order to be compared with the\nmean reverting signal.\n8\n\n5. Signal Weighting\nIn order to combine the 4 signals into one, the signals were one-hot encoded. If each of the\npossible signals were encoded with nominal values, the ordinal property would cause greater\nvalues to have greater weight. Instead, a boolean vector is generated for each possible position,\nand only one of these vectors can take on the value \u201d1\u201d for each sample.\nWith the signals one-hot encoded, each possible signal now mathematically is equal in value.\nWith that, a weight vector was generated which signi\ufb01es how much weight will be given to each\nindicator\u2019s signal. The weights were bounded on the range 0 to 1, and these weights were then\noptimized using Sequential Least-Squares Programming (SLSQP), an iterative method for non-\nlinear optimization. The objective function here was the APR, and the near negligible transaction\ncosts were not considered for the optimization.\nThe results are summarized in the following table:\n10-Year\nS&P 500\nFederal Funds\nMean Reversion\nAPR\n0\n0\n0\n1\n4.084%\n0.5\n0.5\n0.75\n1\n4.112%\nTable 1: Optimization Results\nIf only the mean reverting signal was considered, an APR of 4.084% was yielded. After op-\ntimization, giving non-zero weight to the macroeconomic indicators was found to be ideal, with\na increased APR of 4.112% obtained. With an evaluation period spanning 6 years and only 3\nmacroeconomic signals stemming from satisfactory forecasts considered, this is certainly a sig-\nni\ufb01cant improvement.\nThe full backtested results can be seen in Figure 4, and can be compared to the results gener-\nated using solely the mean reverting signal, summarized in Figure 3. The lessening in the APR is\ndue to the transaction costs factored in. Qualitatively evaluating the positions taken by examining\nthe plot on the upper-right reveals the bene\ufb01t to giving non-zero weight to the macroeconomic\nsignals.\nComparing the historical positions taken reveals a similar pattern, except for the 2008-2010\nperiod. The recession in the United States spanned the majority of that time, beginning Decem-\nber 2007 and ending June 2009. During this time, rather than continuing to rapidly transition\nbetween taking short and long positions based solely on the level of the spread,",
    "chunk_index": 8,
    "start_char": 21009,
    "end_char": 24155,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "Qualitatively evaluating the positions taken by examining\nthe plot on the upper-right reveals the bene\ufb01t to giving non-zero weight to the macroeconomic\nsignals.\nComparing the historical positions taken reveals a similar pattern, except for the 2008-2010\nperiod. The recession in the United States spanned the majority of that time, beginning Decem-\nber 2007 and ending June 2009. During this time, rather than continuing to rapidly transition\nbetween taking short and long positions based solely on the level of the spread, the macroeco-\nnomic indicators in\ufb02uenced the strategy to simply hold a short position throughout that time,\nbetting that though the U.S. dollar was weak during this recessionary phase, it should revert\nback to the mean. That decision was the signi\ufb01cant di\ufb00erence between the strategies, and what\nprimarily net the improvement in performance.\n9\n\n2009\n2010\n2011\n2012\n2013\n2014\n\u22120.75\n\u22120.65\n1*AUDUSD\u22120.52*NZDUSD\u22121.1*CADUSD\u221213.66*JPYUS\nTime (HalfLife is 30 days)\nSpread\n2009\n2010\n2011\n2012\n2013\n2014\n\u22124\n\u22122\n0\n1\n2\nZscore entry 1 std, Zscore exit 0 std\nTime\nStandardized Spread\n2009\n2010\n2011\n2012\n2013\n2014\n\u22121.0\n0.0\n1.0\nDaily Returns (average: 0.016%, std: 0.1955%)\nTime\nDaily Returns (%)\n2009\n2010\n2011\n2012\n2013\n2014\n0\n5\n15\n25\n35\nAPR: 4.0573%, Sharpe Ratio: 1.297, maxDD: 3.7254%\nTime\nCumulative Returns (%)\nFigure 4: Optimal Signal Weighting Results\n6. Conclusion\nThe Yield Curve is a strong economic indicator, and studies suggest that trading upon the\nmean-reverting nature of its spread yields statistically signi\ufb01cant positive performance results.\nDue to that, a Multiple Pairs Trading Strategy was implemented using major currency pairs. Us-\ning the Johansen test, stationary cointegrated portfolios were yielded from the major currency\npairs considered. With that, a mean reversion strategy was executed, by computing z scores\nand comparing them to the entry and exit thresholds. The representative cointegrated portfolio\nyielded a APR of 4.084%. By using a SVM to generate monthly forecasts on a set of macroe-\nconomic indicators, trading signals for each of the indicators considered were obtained. After\noptimizing upon the weights for each of the indicators, it was found that giving the macroeco-\nnomic signals nonzero weights yielded an APR of 4.112%, an improvement upon the original\nstrategy. With a large evaluation period and a small indicator set, this improvement is signi\ufb01cant\nand demonstrates that macroeconomic indicators can certainly improve trading strategies.\nReferences\n[1] C. T. Chua, W. T. H. Koh, K. Ramawamy, Pro\ufb01ting from Mean Reverting Yield Curve Strategies (2004).\n[2] F. X. Diebold, R. S. Mariano, Comparing Predictive Accuracy (1995).\n[3] S. E. Said, D. A. Dickey, Testing For Unit Roots in Auto-Regressive Moving Average Models of Unknown Order\n(1984).\n10\n\n[4] S. Johansen, Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models\n(1991).\n[5] G. E. Schwarz, Estimating the Dimension of a Model (1978).\n[6] G. W. Schwert, Tests for Unit Roots: A Monte Carlo Investigation (1989).\n[7] G. Uhlenbeck, L. Ornstein, On the Theory of Brownian Motion (1930).\n[8] J. Wu, A Pairs Trading Strategy for GOOG/GOOGL Using Machine Learning (2015).",
    "chunk_index": 9,
    "start_char": 23632,
    "end_char": 26864,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "D. A. Dickey, Testing For Unit Roots in Auto-Regressive Moving Average Models of Unknown Order\n(1984).\n10\n\n[4] S. Johansen, Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models\n(1991).\n[5] G. E. Schwarz, Estimating the Dimension of a Model (1978).\n[6] G. W. Schwert, Tests for Unit Roots: A Monte Carlo Investigation (1989).\n[7] G. Uhlenbeck, L. Ornstein, On the Theory of Brownian Motion (1930).\n[8] J. Wu, A Pairs Trading Strategy for GOOG/GOOGL Using Machine Learning (2015).\n[9] K. Miller, A. Smola, G. Ratsch, B. Schollopf, J. Kohlmorgen, V. Vapnik, Using Support Vector Machines for Time\nSeries Prediction (2000).\n11",
    "chunk_index": 10,
    "start_char": 26339,
    "end_char": 27008,
    "paper_title": "Using Macroeconomic Forecasts to Improve Mean Reve",
    "paper_category": "q-fin.TR",
    "paper_filename": "Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/q-fin.TR/Using_Macroeconomic_Forecasts_to_Improve_Mean_Reve.pdf"
  },
  {
    "text": "Hawkes processes for credit indices time series\nanalysis: How random are trades arrival times?\nAchraf Bahamou, Maud Doumergue, and Philippe Donnat\nHellebore Capital Ltd\nMichelin House, London\nAbstract. Targeting a better understanding of credit market dynam-\nics, the authors have studied a stochastic model named Hawkes process.\nDescribing trades arrival times, this kind of model allows for the cap-\nture of self-excitement and mutual interactions phenomena. The authors\npropose here a simple yet conclusive method for \ufb01tting multidimensional\nHawkes processes with exponential kernels, based on a maximum likeli-\nhood non-convex optimization. The method was successfully tested on\nsimulated data, then used on new publicly available real trading data\nfor three European credit indices, thus enabling quanti\ufb01cation of self-\nexcitement as well as volume impacts or cross indices in\ufb02uences.\nKeywords: Point and counting processes, multidimensional Hawkes pro-\ncess, \ufb01nancial time series analysis, non-convex optimization, maximum\nlikelihood optimization, credit indices.\n1\nIntroduction\n1.1\nFrom credit derivative indices to Hawkes processes\nCredit indices are \ufb01nancial instruments comprised of a set of credit securities,\nmainly used to hedge credit default risk. Each new index series is issued every\n6 months and expires after a de\ufb01ned maturity. Though liquid, those indices are\ntraded at a rather \u201dmid-frequency\u201d rate, with trades occurring at a minute scale.\nFor European indices, the market is continuously open from 7:00 to 17:00, with\na total of \ufb01ve to ten billion euros reported each day on average. Fig 1 gives an\nidea of the activity on a sampled day of trading. If recent regulations have led\nto a greater public reporting of trading activities, thus releasing more amount of\ntraded data, this data is by essence quite sparse. Picturing such market behaviour\nover time represents a challenging opportunity: it originally motivated the work\npresented in this article.\nThis study focuses on three principal European credit indices, of the most\ntraded 5-year maturity1: the Main Index, noted here itxeb, a combination of 125\nequally weighted investment grade entities; the Crossover Index, noted itxex,\nwith 75 sub-investment grade names; and the Senior Financial, noted itxes, a\nsubset of 30 \ufb01nancial entities from Main Index, referencing senior debt.\n1 Data provided by otcstreaming.com, and available on demand.\narXiv:1902.03714v1 [stat.AP] 11 Feb 2019\n\nBahamou, A., Doumergue, M., Donnat, P.\nFig. 1. Publicly reported trading activity on 22/05/2018 (times, volumes) for European\ncredit indices itxeb, itxes & itxex.\nWhen trying to capture the underlying dynamics of this market, one of the\n\ufb01rst questions that comes to mind is: how random is the timing of trades? A very\nnaive approach is to model trades arrival times as purely random and uncorre-\nlated processes, for example a Poisson process by trying to \ufb01t an exponential\nlaw over the distribution of inter-arrival times. Yet, such a model fails to \ufb01t the\ndata: from Fig 2, we can notice a high density of very short inter-arrival times,\nsuggesting that some self-excitation phenomenon occurs, with trades triggering\nmore trades; an intuition that every practitioner of the \ufb01eld would con\ufb01rm.",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3258,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "very\nnaive approach is to model trades arrival times as purely random and uncorre-\nlated processes, for example a Poisson process by trying to \ufb01t an exponential\nlaw over the distribution of inter-arrival times. Yet, such a model fails to \ufb01t the\ndata: from Fig 2, we can notice a high density of very short inter-arrival times,\nsuggesting that some self-excitation phenomenon occurs, with trades triggering\nmore trades; an intuition that every practitioner of the \ufb01eld would con\ufb01rm.\nE(\u2206t) \u03c3(\u2206t) Q1(\u2206t) Q2(\u2206t) Q3(\u2206t) Number of trades\nitxeb 7 min 10 min 1 min\n3 min\n8 min\n22 227\nitxex 8 min 12 min 2 min\n4 min 10 min\n18 152\nitxes 17 min 24 min 3 min\n9 min 21 min\n7 194\nTable 1. Statistics on inter-arrival times \u2206t (period: 03/01/2017 to 14/12/2017)\nFig. 2. itxeb, from 15/01/2017 to 15/05/2018: right: log-scaled distribution of inter-\narrival times; left: Q-Q plot. The red lines represent the best exponential law \ufb01tting.\nWith a p-value of 0.0, this model is clearly inappropriate.\nLiterature review reveals that one model has recently driven considerable\nattention and a growing interest from both the scienti\ufb01c and quantitative com-\nmunities: Hawkes processes. This model, introduced in 1971 by Hawkes ([6], [7])\n\nHawkes processes for credit indices time series analysis\nto characterize earthquake tremors, can be used to describe timing of trades and\ntheir cross in\ufb02uences from a point process perspective.\nAfter summarizing related work for Hawkes processes, we will recall the the-\noretical framework around this stochastic model. We will then focus on opti-\nmization methods for \ufb01tting such a process and describe Two Stage Hawkes\nLikelihood Optimization (2SHLO), a maximum likelihood based algorithm to \ufb01t\nmultidimensional Hawkes processes with a parametric exponential kernel. Lastly,\nwe will present our results, \ufb01rstly on simulated data, then on credit derivative\nindices mentioned above. Hawkes processes in \ufb01nance have mainly been applied\nto high frequency data. Is such a model appropriate for \u201dmid-frequency\u201d credit\ndata? Can we specify the impact of volumes in trades or describe mutual in\ufb02u-\nences between di\ufb00erent indices?\n1.2\nRelated work\nHawkes processes were originally designed for seismology analysis and deeply\nstudied for that purpose by Ogota [12], [10], [11]. The concept has been utilized\nto model other e\ufb00ects where self and cross ignitions happen, such as social media\ntweets cascading [14], crime occurrences due to gang retaliations [9], or \ufb01nancial\nmarket events. As regards to \ufb01nancial applications, Bowsher [4] proposed in 2002\na generalized Hawkes process model taking into account night gaps, inter-day\ndependencies as well as intraday seasonality (with a piece-wise baseline inten-\nsity), which was used to model interactions between trades and price changes\nand also estimate of the price volatility based on mid-quote intensity for NYSE\nstock. Market price microstructure & market impact (in\ufb02uence of market or-\nders on forthcoming prices) was modelled in [3], price impact was also studied\nin [1]. [13] focused on the impact of volume and order types on the limit order\nbook.",
    "chunk_index": 1,
    "start_char": 2777,
    "end_char": 5892,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "dependencies as well as intraday seasonality (with a piece-wise baseline inten-\nsity), which was used to model interactions between trades and price changes\nand also estimate of the price volatility based on mid-quote intensity for NYSE\nstock. Market price microstructure & market impact (in\ufb02uence of market or-\nders on forthcoming prices) was modelled in [3], price impact was also studied\nin [1]. [13] focused on the impact of volume and order types on the limit order\nbook. [2] (2015) exposes a full review of Hawkes process applications to \ufb01nance,\nsuch as price & volatility modeling, market re\ufb02exivity measurement, order book\nmodeling or risk contagion modeling.\n2\nModeling Self and Cross Excitement with Hawkes\nProcesses\nThis section re-frames the required formalism, as very clearly stated in [2], [8] or\n[15], starting from point and counting process de\ufb01nitions, through the key con-\ncept of intensity function. Hawkes model formulation is detailed for the reader,\nwith a peculiar focus on exponential kernel structure.\n2.1\nCore concepts: from counting & point processes to intensity\nfunctions\nDe\ufb01nition 1 (Point Process). Let (\u2126, F, P) be a probability space.\nLet (tk)k\u2208N\u2217be a sequence of non-negative random variables such that \u2200k \u2208N\u2217,\ntk < tk+1. (tk)k\u2208N\u2217is called a (simple) point process on R+.\n\nBahamou, A., Doumergue, M., Donnat, P.\nDe\ufb01nition 2 (Counting Process). Let (tk)k\u2208N be a point process. The right-\ncontinuous stochastic process de\ufb01ned for all t \u2208R+ as N(t) = P\nk\u2208N\u22171tk\u2264t is\ncalled the counting process associated with (tk)k\u2208N\u2217.\nThe study of point processes goes through a single mathematical object,\nthe intensity function, which is de\ufb01ned as the conditional probability density of\noccurrence of an event in the immediate future.\nDe\ufb01nition 3 (Intensity Function). Let N be a counting process adapted to\na \ufb01ltration Ft. The left-continuous intensity is heuristically de\ufb01ned as\n\u03bb(t|Ft) = lim\nh\u21920 E\n\u0014N(t + h) \u2212N(t)\nh\n\f\f\f\fFt\n\u0015\n(1)\nThe intensity function depends on the choice of \ufb01ltration Ft, which represents\nthe amount of information available until time t (See [5] for a rigorous de\ufb01nition\nof the intensity function). In the context of Hawkes processes, we will simply use\nthe natural \ufb01ltration, with all previous information being available, and consider\n\u03bb(t).\nHomogeneous Poisson process\nOne of the simplest point processes is the homogeneous Poisson process, for\nwhich the intensity function is constant over time: \u2200t \u22650, \u03bb(t) = \u03bb. In that\ncase, durations (or inter-event waiting times) are independent and identically\ndistributed (following an exponential distribution of hazard rate \u03bb). As presented\nin the introduction, credit trades cannot be modeled by this memory-less model.\nLet\u2019s introduce Hawkes processes, a peculiar kind of non-homogeneous Poisson\nprocesses with linear dependencies over functions of past events.\n2.2\nSelf-excitement: one-dimensional Hawkes processes\nDe\ufb01nition 4 (One-dimensional Hawkes Processes).\nLet (tk)k\u2208N be a point process and N the associated counting process, such\nthat its intensity function \u03bb is de\ufb01ned for each time t \u22650 as\n\u03bb(t) = \u00b5(t) +\nZ t\n\u2212\u221e\n\u03c6(t \u2212\u03c4) dN(\u03c4) = \u00b5(t) +\nX\nti<t\n\u03c6(t \u2212ti)\n(2)\nwhere",
    "chunk_index": 2,
    "start_char": 5416,
    "end_char": 8575,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "be modeled by this memory-less model.\nLet\u2019s introduce Hawkes processes, a peculiar kind of non-homogeneous Poisson\nprocesses with linear dependencies over functions of past events.\n2.2\nSelf-excitement: one-dimensional Hawkes processes\nDe\ufb01nition 4 (One-dimensional Hawkes Processes).\nLet (tk)k\u2208N be a point process and N the associated counting process, such\nthat its intensity function \u03bb is de\ufb01ned for each time t \u22650 as\n\u03bb(t) = \u00b5(t) +\nZ t\n\u2212\u221e\n\u03c6(t \u2212\u03c4) dN(\u03c4) = \u00b5(t) +\nX\nti<t\n\u03c6(t \u2212ti)\n(2)\nwhere \u00b5 : R 7\u2192R+ is an exogenous base intensity and \u03c6 : R+ 7\u2192R+ is a\nnon-negative, measurable function such that ||\u03c6||1 =\nR \u221e\n0\n\u03c6(s)ds < 1.\nN is called a Hawkes process with baseline \u00b5 and kernel \u03c6.\nThe kernel \u03c6 expresses the positive in\ufb02uence of past events on the current\nvalue of the intensity. Each jump dN(\u03c4) \u0338= 0 increases the probability of future\nevents through the kernel \u03c6. Clustering e\ufb00ects and branching structure are well\ndepicted by Hawkes processes, with the baseline activity generating immigrant\nevents and descendant events enhancing the intensity.\n\nHawkes processes for credit indices time series analysis\nFor this study, we focus on exponential kernels, de\ufb01ned as \u03c6(t) = \u03b1\u03b2 e\u2212\u03b2t 1t\u22650,\nwith parameters \u03b1, \u03b2 \u22650, which allows easy interpretation: \u03b1 or adjacency rep-\nresents the weight of previous events while \u03b2 is the decay / typical duration of\nin\ufb02uence for a past event.\nThe branching ratio n = ||\u03c6||1 =\nR \u221e\n0\n\u03c6(s)ds represents the average number\nof descendants for any event. For exponential kernel, n = \u03b1.\n2.3\nIncluding mutual-excitement with multidimensional Hawkes\nprocesses\nDe\ufb01nition 5 (Multidimensional Hawkes Processes).\nLet M \u2208N\u2217and {(ti\nk)k}i=1,...,M be a M-dimensional point process.\nWe denote by Nt = (N 1\nt , ..., N M\nt ) the associated counting process such that its\nvector intensity function \u03bb : R+ 7\u2192RM\n+ , is de\ufb01ned as, for all t \u22650, i \u22081, ..., M:\n\u03bbi(t) = \u00b5i +\nM\nX\nj=1\nZ t\n0\n\u03c6i,j(t \u2212\u03c4)dNj(\u03c4) = \u00b5i +\nM\nX\nj=1\nNj(t)\nX\nn=1\n\u03c6i,j(t \u2212tj,n),\n(3)\nwith \u00b5 = (\u00b5i)i=1,...,M an exogenous base intensity vector\nand \u03c6(t) = (\u03c6i,j)i,j=1,...,M a matrix-valued kernel that is component-wise\npositive and causal (null values when t < 0) and with each component belonging\nto the space of L1-integrable functions.\nNt is called a multidimensional (or multivariate) Hawkes process with base-\nline \u00b5 and kernel \u03c6.\nThe choice of an exponential kernel can be generalized for multivariate Hawkes\nprocesses with kernel components being expressed as \u03c6i,j(t) = \u03b1i,j\u03b2i,je\u2212\u03b2i,jt1t\u22650.\nThis article focuses on Hawkes processes with exponential kernel and constant\nbaselines, to which we propose a maximum likelihood based \ufb01tting method.\n3\nHawkes Processes: Model Calibration\nThe most commonly used technique for parametric inference of Hawkes processes\nis a direct numerical optimization of the Maximum Likelihood, which was \ufb01rst\nintroduced in the work of Ogata (1978) [10]. He proved the asymptotic consis-\ntency and e\ufb03ciency of the Maximum Likelihood Estimator (MLE) under the\nassumption of stationary condition of the underlying point process.",
    "chunk_index": 3,
    "start_char": 8086,
    "end_char": 11117,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "\u03c6i,j(t) = \u03b1i,j\u03b2i,je\u2212\u03b2i,jt1t\u22650.\nThis article focuses on Hawkes processes with exponential kernel and constant\nbaselines, to which we propose a maximum likelihood based \ufb01tting method.\n3\nHawkes Processes: Model Calibration\nThe most commonly used technique for parametric inference of Hawkes processes\nis a direct numerical optimization of the Maximum Likelihood, which was \ufb01rst\nintroduced in the work of Ogata (1978) [10]. He proved the asymptotic consis-\ntency and e\ufb03ciency of the Maximum Likelihood Estimator (MLE) under the\nassumption of stationary condition of the underlying point process. The nega-\ntive log-likelihood function of a multidimensional Hawkes process over the time\ninterval [0, t] is given in Daley & Vere-Jones [5] Proposition 13.1.VI by :\nLt(\u03bb) := \u2212\nM\nX\ni=1\n\u0012Z t\n0\nlog \u03bbi(\u03c4)dNi(\u03c4) \u2212\nZ t\n0\n\u03bbi(\u03c4)d\u03c4\n\u0013\n.\n(4)\n\nBahamou, A., Doumergue, M., Donnat, P.\nDepending on the shape of kernels, Lt is generally non-convex. Classic non-\nconvex numerical optimization pitfalls are to be feared: a direct numerical opti-\nmization could converge to a merely local minimum. On the other hand, it may\ntake too many iterations to converge as the negative log-likelihood function can\nbe \ufb02at on some regions of the space of parameters. This problem is also faced\nwhen using an Expectation Maximization (EM) algorithm to \ufb01nd the MLE.\nOn the computational side, the repeated evaluation of the negative log-\nlikelihood can be highly time consuming, essentially due to the nested sum in\nthe \ufb01rst part of 4 where the conditional intensity is also expressed as a sum over\nthe history.\n3.1\nMaximum Likelihood Estimation (MLE) for exponential\nmultidimensional Hawkes processes\nLet\u2019s consider a M-dimensional multivariate Hawkes model with constant base-\nlines \u00b5i and kernel functions \u03c6i,j as de\ufb01ned in 2.3. The choice of an exponential\nkernel has proved to be very interesting as it allows intuitive and meaningful\ninterpretations of its parameters and also reduces the computational cost of the\nevaluation of the negative log-likelihood function as noted by Ogata (1981) [11]\nwho exhibited a recursive formula that eliminated the nested sum evaluation\nproblem.\nThe existing methods to \ufb01t exponential parameters through MLE directly\nuse non-linear optimization algorithms such as Nelder-Mead (also called down-\nhill simplex method) or BFGS, with performance decreasing as the number of\nparameters increases. This represents quite an issue as M(2M + 1) parameters\ndescribe a M-dimensional Hawkes process with exponential kernel.\nIn addition, other existing methods often make the strong assumption that\nthe decays \u03b2i,j of the exponential kernel are given and \ufb01xed a priori. Conse-\nquently, the estimation of each kernel function is equivalent to the estimation\nof its adjacency coe\ufb03cient \u03b1i,j and since the conditional intensity \u03bbi(t) is linear\nwith respect to kernel functions \u03c6i,1(t), . . . , \u03c6i,p(t), the negative log-likelihood\nfunction Lt is convex with respect to all parameters. Therefore we can use the\nwidely available convex optimization machinery to \ufb01nd the global minimum ef-\n\ufb01ciently. The main drawback of this method is the possible inaccuracy of the\ndecays parameters \u03b2i,j which can lead to model mismatch.",
    "chunk_index": 4,
    "start_char": 10526,
    "end_char": 13733,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "of each kernel function is equivalent to the estimation\nof its adjacency coe\ufb03cient \u03b1i,j and since the conditional intensity \u03bbi(t) is linear\nwith respect to kernel functions \u03c6i,1(t), . . . , \u03c6i,p(t), the negative log-likelihood\nfunction Lt is convex with respect to all parameters. Therefore we can use the\nwidely available convex optimization machinery to \ufb01nd the global minimum ef-\n\ufb01ciently. The main drawback of this method is the possible inaccuracy of the\ndecays parameters \u03b2i,j which can lead to model mismatch.\n3.2\nIntroducing Two Stage Hawkes Likelihood Optimization\n(2SHLO):\nIn order to combine the bene\ufb01ts of both approaches described above, we propose\nthe following estimation method to \ufb01t exponential Hawkes processes.\nThe negative log-likelihood function for exponential multivariate Hawkes pro-\ncesses is given by (see Ogata[11] for the recursive formulation):\n\nHawkes processes for credit indices time series analysis\nLt(\u00b5, \u03b1, \u03b2) =\nM\nX\nm=1\n\u0012\n\u00b5mT +\nM\nX\nn=1\n\u03b1mn\nX\n{k:tn\nk <T }\n[1 \u2212e\u2212\u03b2mn(T \u2212tn\nk )]\n\u2212\nX\n{k:tn\nk <T }\nlog[\u00b5m +\nM\nX\nn=1\n\u03b1mn\u03b2mn\nX\n{k:tn\nk <tm\ni }\ne\u2212\u03b2mn(tm\ni \u2212tn\nk )]\n\u0013\n(5)\nAs we mentioned earlier, if \u03b2 is \ufb01xed, then Lt(\u00b7, \u00b7, \u03b2) is convex with respect to\nparameters (\u00b5, \u03b1). As a result it can be minimized using any convex optimization\nalgorithm, for example an Accelerated Gradient Descent (AGD) which converges\nto a global minimum (\u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2)) such that:\nL\u2217\nt (\u03b2) = Lt(\u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2), \u03b2) = min(\u00b5,\u03b1)Lt(\u00b5, \u03b1, \u03b2)\n(6)\nL\u2217\nt (\u03b2) is still a non-convex function but de\ufb01ned on a space with a lower\ndimension than the initial parameter space. Any non-linear heuristic algorithms\ncan now be used to minimize L\u2217\nt (\u03b2) leading to the MLE as2:\nmin(\u00b5,\u03b1,\u03b2)Lt(\u00b5, \u03b1, \u03b2) = min(\u03b2)min(\u00b5,\u03b1)Lt(\u00b5, \u03b1, \u03b2)\n(7)\nWe summarize the \ufb01tting method as follow :\nAlgorithm 1 Two Stage Hawkes Likelihood Optimization (2SHLO)\nStart from mean of inter arrival times \u03b20\nfor each step of Nelder-Mead method until convergence do\nfor each needed computation of L\u2217\nt (\u03b2i) do\nStart from a random (\u00b50, \u03b10)\nUse Accelerated Gradient Descent to optimize Lt(\u00b5, \u03b1, \u03b2i)\nRetrieve resulting L\u2217\nt (\u03b2i)\nend for\nend for\nreturn last (\u03b2, \u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2))\n3.3\nGoodness of \ufb01t\nThe compensator function \u039b of a point process with intensity \u03bb is de\ufb01ned as\n\u2200t \u22650, \u039b(t) =\nR t\n0 \u03bb(s|Ft)ds. To assess the goodness of \ufb01t of our estimation, we\nuse the residual point process analysis theorem, as stated in [5]:\n2 if we call m = min(\u00b5,\u03b1,\u03b2)Lt(\u00b5, \u03b1, \u03b2), the following inequality holds for all (\u00b5, \u03b1, \u03b2):\nm \u2264Lt(\u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2), \u03b2) \u2264Lt(\u00b5, \u03b1, \u03b2) 3.2 follows immediately by taking the mini-\nmum over (\u00b5, \u03b1, \u03b2) in each side of the inequality.\n\nBahamou, A., Doumergue, M., Donnat, P.\nThe transformed sequence {t\u2217\nk} = {\u039b(tk)} is a realization of a unit rate Pois-\nson process if and only if the original sequence {tk} is a realization from the\npoint process de\ufb01ned by \u03bb.",
    "chunk_index": 5,
    "start_char": 13217,
    "end_char": 16025,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "m = min(\u00b5,\u03b1,\u03b2)Lt(\u00b5, \u03b1, \u03b2), the following inequality holds for all (\u00b5, \u03b1, \u03b2):\nm \u2264Lt(\u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2), \u03b2) \u2264Lt(\u00b5, \u03b1, \u03b2) 3.2 follows immediately by taking the mini-\nmum over (\u00b5, \u03b1, \u03b2) in each side of the inequality.\n\nBahamou, A., Doumergue, M., Donnat, P.\nThe transformed sequence {t\u2217\nk} = {\u039b(tk)} is a realization of a unit rate Pois-\nson process if and only if the original sequence {tk} is a realization from the\npoint process de\ufb01ned by \u03bb.\nThe goodness of \ufb01t can be tested by comparing the transformed estimated\ntimes to a standard Poisson process using QQ plots and comparison of density\nfunctions.\n4\nResults - 2SHLO in practice\nWe have been running our experiments in Python 3, relying on tick[16] library\nfor Hawkes process simulations, AGD optimization and analytic tools. 2SHLO\nis also bene\ufb01ting of well-known scipy scienti\ufb01c package.\nTo provide an idea of the computational performance of the \ufb01tting method,\nwe mention that, on average, it takes 4.69 seconds to \ufb01t 2 dimensional exponen-\ntial hawkes process with training length of T = 10000.\n4.1\nValidating 2SHLO performances over simulated data\nTo generate Hawkes processes, a few methods are available (as summarized in\n[2]), either based on thinning, time-change or cluster algorithm. We have here\nused tick functions for simulations which are based on the thinning algorithm\ndescribed in (Ogata, 1981, p.25, Algorithm 2) [11].\nTo validate the performance of the \ufb01tting algorithm, we ran a series of 100\nsimulation and \ufb01tting procedure on the simulated 2D Hawkes time-stamps using\ndi\ufb00erent end times T to have di\ufb00erent training sizes and using the following\nsimulation parameters :\n\u00b5 =\n\u0000 0.1\n0.2\n\u0001\n\u03b1 =\n\u0000 0.5 0.00\n0.4 0.3\n\u0001\n\u03b2 =\n\u0000 0.3 0.00\n0.2 0.2\n\u0001\nFig. 3 con\ufb01rms that the estimated parameters converge to their true opti-\nmal values for increasing N number of ticks in the training set. This reassures\nthat when the Hawkes process is well speci\ufb01ed and indeed recovers the optimal\nparameters asymptotically.\n4.2\nCalibrating univariate Hawkes processes on \u201dmid-frequency\u201d\ncredit derivative trades\nWe have \ufb01tted a modi\ufb01ed one-dimensional Hawkes process on the period from\n15/01/2017 to 15/12/2017 of reported trades data3 on 3 indices ITXEB, ITXES\nand ITXEX. The modi\ufb01cation consists of imposing a null value of the intensity\nin the gap between two days and the standard exponential kernel format during\ntrading days so as to account for the absence of trading activity overnight. The\nmodel is thus slightly modi\ufb01ed and described in the appendix section in a more\n3 The trading data have been cleaned by discarding transactions that do not re\ufb02ect\nmarket signals such as roll and switch trades\n\nHawkes processes for credit indices time series analysis\nFig. 3. Estimated parameters of a 2D exponential Hawkes process trained with 2SHLO\non simulated datasets with increasing lengths\ndetailed way. Before making this modeling choice, we experimented with a more\nelaborate model that accounts for the overnight spillover e\ufb00ect as described in\nthe work of C.",
    "chunk_index": 6,
    "start_char": 15591,
    "end_char": 18595,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "a more\n3 The trading data have been cleaned by discarding transactions that do not re\ufb02ect\nmarket signals such as roll and switch trades\n\nHawkes processes for credit indices time series analysis\nFig. 3. Estimated parameters of a 2D exponential Hawkes process trained with 2SHLO\non simulated datasets with increasing lengths\ndetailed way. Before making this modeling choice, we experimented with a more\nelaborate model that accounts for the overnight spillover e\ufb00ect as described in\nthe work of C. G.Bowsher [4] where the author de\ufb01nes the intensity as recursively\ndepending on the level of the stochastic parts of the intensity at the end of the\nprevious trading day and thus extending the e\ufb00ect of trading days over time.\nThe \ufb01tting procedure resulted into a null parameter estimation of the spillover\ne\ufb00ect which motivated the choice of the model modi\ufb01cation described above.\nTo validate the \u2019universality\u2019 of our estimates, we tested the goodness of \ufb01t\nof the Hawkes model with the estimated parameters on out of sample period\nfrom 15/01/2018-15/07/2018. The model \ufb01tted well for the 3 indices time se-\nries (see Fig. 4) suggesting that each series has stable intrinsic parameters that\ncharacterize the pattern of trades arrival times.\nEstimated \u00b5\u22121 Estimated \u03b1 Estimated \u03b2\u22121\nITXEB 22 min\n0.62\n20 min\nITXEX 24 min\n0.60\n23 min\nITXES 58 min\n0.65\n36 min\nTable 2. Estimated parameters for ITXEB, ITXES and ITXEX over the period\n(2017-01-15 to 2017-12-15)\n4.3\nMeasuring the in\ufb02uence of traded volumes\nWhen considering the distribution of trade volumes for credit indices, volume\nclusters are clearly outlined. For example, for index itxeb, three bins can be\ndistinguished: \u201dsmall\u201d trades, \u201dmedium\u201d trades and \u201dbig\u201d trades (See Fig. 5).\n\nBahamou, A., Doumergue, M., Donnat, P.\nFig. 4. Hawkes process \ufb01tted on 2017 data and tested on 2018 data for each index\nUsing multivariate Hawkes processes model, volume impacts have been modeled\nby considering 3 counting processes, splitting trades per volume size.\nBy using the same model as the last section to take overnight gaps into\nconsideration, the \ufb01tting algorithm trained on 2018 data resulted in the following\nparameters estimates (\u00b5 and \u03b2 expressed in minute\u22121) :\n1\n\u00b5 =\n\u0000 40\n42\n38\n\u0001\n\u03b1 =\n\u0000 0.40 0.07 0.57\n0.00 0.53 0.21\n0.00 0.16 0.59\n\u0001\n1\n\u03b2 =\n\u0000 12 21 53\n71 18 12\n53 14 34\n\u0001\nVisualizing in Fig. 5 the branching ratio which corresponds the the values\nof \u03b1 allows some intuitive interpretation of the \ufb01tted model as it shows that\nlarge trades are purely self exciting process and they have a major in\ufb02uence\nin triggering small trades, we also notice that all trade categories have a non\nnegligible self excitation component.\n4.4\nMeasuring cross interactions between traded indices\nWe have adopted again a multivariate Hawkes processes with null intensity\novernight model to study the cross interaction between indices trades times.\nThe \ufb01tting algorithm trained on 2017 data resulted in the following parameters\nestimates (\u00b5 and \u03b2 expressed in minute\u22121) :\n1\n\u00b5 =\n\u0000 29\n30\n125\n\u0001\n\u03b1 =\n\u0000 0.44 0.25 0.20\n0.23 0.37 0.18\n0.07 0.08 0.45\n\u0001\n1\n\u03b2 =\n\u0000 23 9 21\n14 13 31\n39 17 25\n\u0001",
    "chunk_index": 7,
    "start_char": 18100,
    "end_char": 21206,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "are purely self exciting process and they have a major in\ufb02uence\nin triggering small trades, we also notice that all trade categories have a non\nnegligible self excitation component.\n4.4\nMeasuring cross interactions between traded indices\nWe have adopted again a multivariate Hawkes processes with null intensity\novernight model to study the cross interaction between indices trades times.\nThe \ufb01tting algorithm trained on 2017 data resulted in the following parameters\nestimates (\u00b5 and \u03b2 expressed in minute\u22121) :\n1\n\u00b5 =\n\u0000 29\n30\n125\n\u0001\n\u03b1 =\n\u0000 0.44 0.25 0.20\n0.23 0.37 0.18\n0.07 0.08 0.45\n\u0001\n1\n\u03b2 =\n\u0000 23 9 21\n14 13 31\n39 17 25\n\u0001\n\nHawkes processes for credit indices time series analysis\nFig. 5. Distribution of trade sizes over one week trading period 2018-05-21:2018-05-25\n(left; adjacency matrix (\u03b1i,j) of 3D exponential Hawkes \ufb01tted on 3 volume bin series\n(for index itxeb).\nFrom Fig. 6 we can notice that itxes index is the least in\ufb02uenced by the other\nindices with the very low cross excitation components and a high self excitation\ncomponent which was expected due the its low liquidity. We also notice that\nthe estimated baselines of all indices are greater for this multivariate model\ncompared to the uni-dimensional model which can be explained by the fact that\nwe decreased the \u2019Poissonnian\u2019 behaviour of the arrival times by including more\ninformation about the in\ufb02uence of other indices.\nFig. 6. adjacency matrix (\u03b1i,j) of 3D exponential Hawkes \ufb01tted on 3 indices series\n(itxeb, itxes, itxex) for the period 2017-01-15 - 2017-12-15.\n5\nConclusion\nThrough point process analysis, Hawkes processes allows a relatively simple and\ninterpretable representation of time dependant events with self and mutual ex-\ncitements. With 2SHLO; a maximum likelihood based algorithm mixing convex\nand non-convex optimization, we have been able to \ufb01t fastly and properly ex-\nponential kernel multivariate Hawkes processes. The algorithm has been applied\nto credit derivative trading data, an unexplored \u201dmid-frequency\u201d market for\nsuch processes. We have been able to emphasize self excitement for three index\n\nBahamou, A., Doumergue, M., Donnat, P.\ntrades and to measure impacts of traded volumes. With the same framework,\nwe were also able to quantify cross in\ufb02uences between indices. Next prospects\nturns towards testing 2SHLO performances on high dimensional datasets, for in-\nstance studying both price & volume trade in\ufb02uences as well as market bid/ask\ndynamics on credit derivatives in a forecasting perspective.\nReferences\n1. Amaral, L., Papanicolaou, A.: Price Impact of Large Orders Using Hawkes Pro-\ncesses. In: NYU Tandon Research Paper No. 2874042 (2017)\n2. Bacry, E., Mastromatteo, I., Muzy, J.F.: Hawkes Processes in Finance. In: Market\nMicrostructure and Liquidity, vol. 01 (2015)\n3. Bacry, E., Muzy, J.F.: Hawkes models for price and trades high-frequency dynamics.\nIn: Quantitative Finance, vol. 14, pp. 1147\u20131166 (2013)\n4. Bowsher, C. G.: Modelling Security Market Events in Continuous Time: Intensity\nBased, Multivariate Point Process Models. In: Nu\ufb03eld Economics Working Paper\n(2003)\n5. Daley, D.J., Vere-Jones, D.: An introduction to the theory of point processes, vol.",
    "chunk_index": 8,
    "start_char": 20586,
    "end_char": 23761,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "Paper No. 2874042 (2017)\n2. Bacry, E., Mastromatteo, I., Muzy, J.F.: Hawkes Processes in Finance. In: Market\nMicrostructure and Liquidity, vol. 01 (2015)\n3. Bacry, E., Muzy, J.F.: Hawkes models for price and trades high-frequency dynamics.\nIn: Quantitative Finance, vol. 14, pp. 1147\u20131166 (2013)\n4. Bowsher, C. G.: Modelling Security Market Events in Continuous Time: Intensity\nBased, Multivariate Point Process Models. In: Nu\ufb03eld Economics Working Paper\n(2003)\n5. Daley, D.J., Vere-Jones, D.: An introduction to the theory of point processes, vol.\n2. Springer (1988)\n6. Hawkes, A.G.: Spectra of Some Self-Exciting and Mutually Exciting Point Processes.\nIn: Biometrika, vol. 58, pp. 83\u201390 (1971)\n7. Hawkes, A.G.: Point Spectra of Some Mutually Exciting Point Processes. In: Journal\nof the Royal Statistical Society. Series B (Methodological), vol. 33, pp. 438\u2013443\n(1971)\n8. Laub,\nP.,\nTaimre,\nT.,\nPollett,\nP.:\nHawkes\nProcesses.\nIn:\narXiv\npreprint\narXiv:1507.02822 (2015)\n9. Mohler, G. O., Short, M. B., Brantingham, P. J., Schoenberg, F. P., Tita, G. E.: Self-\nExciting Point Process Modeling of Crime. In: Journal of the American Statistical\nAssociation, vol. 106, pp. 100\u2013108 (2011)\n10. Ogata, Y., The asymptotic behaviour of maximum likelihood estimators for sta-\ntionary point processes. Annals of the Institute of Statistical Mathematics, vol. 30,\npp. 243-261 (1978)\n11. Ogata, Y., On Lewis simulation method for point processes, IEEE Transactions on\nInformation Theory, vol. 27, pp. 23-31 (1981)\n12. Ogata, Y.: Statistical Models for Earthquake Occurrences and Residual Analysis\nfor Point Processes. In: Journal of the American Statistical Association, vol. 83, pp.\n9\u201327 (1988)\n13. Rambaldi, M., Bacry, E., Lillo, F.: The role of volume in order book dynamics: a\nmultivariate Hawkes process analysis. In: Quantitative Finance (2016)\n14. Rizoiu, M., Lee, Y., Mishra, S., Xie, L.: A Tutorial on Hawkes Processes for Events\nin Social Media. In: Frontiers of Multimedia Research, pp. 191\u2013218 (2017)\n15. Toke, I. M., An Introduction to Hawkes Processes with Applications to Finance\n(2011) http://lamp.ecp.fr/MAS/fiQuant/ioane_files/HawkesCourseSlides.pdf\n16. Bacry E., Bompaire M., Ga\ufb00as S., Poulsen S., Tick: a Python library for sta-\ntistical learning, with a particular emphasis on time-dependent modelling. https:\n//x-datainitiative.github.io/tick/#\n\nHawkes processes for credit indices time series analysis\nAppendix: Hawkes Processes Formulas\n5.1\nThe Univariate Bowsher Hawkes process model\nDe\ufb01nition: As described in [4], the intensity of the univariate Bowsher process\nis de\ufb01ned recursively depending on the level of the stochastic parts of the in-\ntensity function at the end of the (d \u22121)th trading day and the contributions\nof the events occurring on day d. Thus it is necessary to perform the following\ndata transformation: the real half-line is partitioned into intervals of length l\ncorresponding to the di\ufb00erent trading days. This partition is written as :\n(0, \u221e) = (0, \u03c41] \u222a(\u03c41, \u03c42] \u222a\u00b7 \u00b7 \u00b7 \u222a(\u03c4d1, \u03c4d] . . .\nThe model is thus de\ufb01ned by the (scalar) stochastic intensity :\n\u2200t \u22650,\n\u03bb(t) = \u00b5 + e\u03bb(t)\nsuch that :\n\u2200t \u2208]\u03c4d\u22121, \u03c4d]\ne\u03bb(t) = \u03c0e\u03bb(\u03c4d\u22121)e\u2212\u03c1(t\u2212\u03c4d\u22121) +\nZ\n[\u03c4d\u22121,t)\n\u03b1e\u2212\u03b2(t\u2212u)dN(u)\nwhere the parameters used are :\n\u2013 ]\u03c4d\u22121, \u03c4d] is the interval de\ufb01ning",
    "chunk_index": 9,
    "start_char": 23213,
    "end_char": 26475,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "real half-line is partitioned into intervals of length l\ncorresponding to the di\ufb00erent trading days. This partition is written as :\n(0, \u221e) = (0, \u03c41] \u222a(\u03c41, \u03c42] \u222a\u00b7 \u00b7 \u00b7 \u222a(\u03c4d1, \u03c4d] . . .\nThe model is thus de\ufb01ned by the (scalar) stochastic intensity :\n\u2200t \u22650,\n\u03bb(t) = \u00b5 + e\u03bb(t)\nsuch that :\n\u2200t \u2208]\u03c4d\u22121, \u03c4d]\ne\u03bb(t) = \u03c0e\u03bb(\u03c4d\u22121)e\u2212\u03c1(t\u2212\u03c4d\u22121) +\nZ\n[\u03c4d\u22121,t)\n\u03b1e\u2212\u03b2(t\u2212u)dN(u)\nwhere the parameters used are :\n\u2013 ]\u03c4d\u22121, \u03c4d] is the interval de\ufb01ning the day number d\n\u2013 \u00b5 is the baseline intensity\n\u2013 \u03c0 is the spillover adjacency coe\ufb03cient\n\u2013 \u03c1 is the spillover decay\n\u2013 \u03b1 is the self excitement adjacency coe\ufb03cient\n\u2013 \u03b2 is the self excitement decay\n\u2013 dN is the process di\ufb00erentiate\nThe Loglikelihood: The minus loglikelihood of a univariate point process is\nde\ufb01ned as :\nLT (\u03bb) = \u2212\n Z T\n0\nlog \u03bb(t)dNi(t) \u2212\nZ T\n0\n\u03bb(t)dt\n!\nComputing the minus loglikelihood in the Hawkes Bowsher model results into\nthe following closed form :\nLT (\u03bb) = \u00b5T +\nX\nd\u2208days\n\u03c0e\u03bb(\u03c4d\u22121)(1 \u2212e\u2212\u03c1(\u03c4d\u2212\u03c4d\u22121))\n+\u03b1\nX\nt\u2208]\u03c4d\u22121,\u03c4d]\n(1 \u2212e\u2212\u03c1(t\u2212\u03c4d\u22121)) \u2212\nX\nti\nlog \u03bb(ti)\n\nBahamou, A., Doumergue, M., Donnat, P.\nThe Fitting procedure: As the task of computing the gradient of each pa-\nrameter of the loglikelihood is a fastidious one because of the recursive de\ufb01nition\nof the intensity, and also because we have only 5 parameters to \ufb01t (\u00b5, \u03c0, \u03c1, \u03b1, \u03b2),\nwe choose to use the L-BFGS-B algorithm to perform the minimization of LT (\u03bb)\nwhich had delivered satisfying performance on \ufb01tting simulated data.\nData simulation: We use the thinning algorithm to simulate arti\ufb01cial data\ndescribed by the following algorithm :\nAlgorithm 2 Simulation by thinning\n1. Given Bowsher Hawkes process described as above\n2. Set current time T = 0 and event counter i = 1\n3. While i \u2264N\n(a) Set the upper bound of Poisson intensity \u03bb\u2217= \u03bb(T) .\n(b) Sample inter-arrival time: draw u \u223cU(0, 1) and let \u03c4 = \u2212ln(u)\n\u03bb\u2217\n(as described\nin).\n(c) Update current time: T = T + \u03c4.\n(d) Draw s \u223cU(0, 1).\n(e) If s \u2264\u03bb(T )\n\u03bb\u2217, accept the current sample: let Ti = T and i = i + 1.\nOtherwise reject the sample, return to step (a).\n5.2\nThe Day Gaps Multivariate Exponential Hawkes process model\nDe\ufb01nition: The intensity of the Day Gaps Multivariate Exponential Hawkes\nprocess is de\ufb01ned just like the standard multivariate exponential Hawkes process\nmodel described in [6] given by :\n\u2200i \u2208[1 . . . D],\n\u03bbi(t) = \u00b5i +\nD\nX\nj=1\nZ\n\u03c6ij(t \u2212s)dNj(s)\nWhere :\n\u2013 D is the number of nodes\n\u2013 \u00b5i are the baseline intensities\n\u2013 \u03c6ij are the kernels\n\u2013 dNj are the processes di\ufb00erentiates\nwith an exponential parametrisation of the kernels:\n\u03c6ij(t) = \u03b1ij\u03b2ij exp(\u2212\u03b2ijt)1t>0\n\u2013 \u03b1ij are the adjacency of the kernel\n\nHawkes processes for credit indices time series analysis\n\u2013 \u03b2ij are the decays of the kernel\nWith the modi\ufb01cation consisting on considering the intensity",
    "chunk_index": 10,
    "start_char": 26052,
    "end_char": 28770,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "D],\n\u03bbi(t) = \u00b5i +\nD\nX\nj=1\nZ\n\u03c6ij(t \u2212s)dNj(s)\nWhere :\n\u2013 D is the number of nodes\n\u2013 \u00b5i are the baseline intensities\n\u2013 \u03c6ij are the kernels\n\u2013 dNj are the processes di\ufb00erentiates\nwith an exponential parametrisation of the kernels:\n\u03c6ij(t) = \u03b1ij\u03b2ij exp(\u2212\u03b2ijt)1t>0\n\u2013 \u03b1ij are the adjacency of the kernel\n\nHawkes processes for credit indices time series analysis\n\u2013 \u03b2ij are the decays of the kernel\nWith the modi\ufb01cation consisting on considering the intensity as a null func-\ntion between two consecutive days to take the day gaps into consideration :\n\u2200t between two consecutive days,\n\u03bbi(t) = 0\nThe Loglikelihood: The minus loglikelihood of a Multivariate Point Process\nis de\ufb01ned as :\nLt(\u03bb) := \u2212\nM\nX\ni=1\n\u0012Z t\n0\nlog \u03bbi(\u03c4)dNi(\u03c4) \u2212\nZ t\n0\n\u03bbi(\u03c4)d\u03c4\n\u0013\n.\nComputing the minus loglikelihood in The Day Gaps Multivariate Exponen-\ntial Hawkes process model results into the following closed form :\nLt(\u03bb) := D \u2217\u03b4\nM\nX\nm=1\n\u00b5m +\nM\nX\nm=1\nM\nX\nn=1\n\u03b1mn\nX\nd\u2208days\nX\ntn\ni \u2208day d\n(1 \u2212e\u2212\u03b2mn(\u03c4d\u2212tn\ni ))\n\u2212\nM\nX\nm=1\nX\ntm\ni\nlog(\u00b5m +\nM\nX\nn=1\n\u03b1mn\u03b2mnRmn(i))\nwith Rmn(i) are de\ufb01ned using the Ogoata [11] recursive formula :\nRmn(i) =\nX\n{k:tn\nk <tm\ni }\ne\u2212\u03b2mn(tm\ni \u2212tn\nk )\n= e\u2212\u03b2mn(tm\ni \u2212tm\ni\u22121)Rmn(i \u22121) +\nX\n{k:tm\ni\u22121\u2264tn\nk <tm\ni }\ne\u2212\u03b2mn(tm\ni \u2212tn\nk )\nWhere :\n\u2013 D is the number of days in the data\n\u2013 \u03b4 is the length of one day\n\u2013 \u00b5i are the baseline intensities\n\u2013 \u03b1ij are the adjacency coe\ufb03cients\n\u2013 \u03b2ij are the decays coe\ufb03cients\n\u2013 \u03c4d is the time de\ufb01ning the last time of the day number d\n\nBahamou, A., Doumergue, M., Donnat, P.\nThe Fitting procedure: To \ufb01t this model, we use the algorithm described\nin this paper, using the projected Newton Descent for the convex optimization\npart and Nelder Mead for the non convex optimization step. The choice of the\nnewton descent is justi\ufb01ed by the easy computation of the gradient and hessian\nmatrix which is sparse and because this algorithm has a quadratic convergence\nrate compared to gradient descent methods.\nThe gradient of Lt(\u03bb) is thus given by the following close formulas :\n\u2202Lt(\u03bb)\n\u2202\u00b5m\n= D \u2217\u03b4 \u2212\nX\ntm\ni\n1\n\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i)\n\u2202Lt(\u03bb)\n\u2202\u03b1mn\n=\nX\nd\u2208days\nX\ntn\ni \u2208day d\n(1 \u2212e\u2212\u03b2mn(\u03c4d\u2212tn\ni )) \u2212\nX\ntm\ni\n\u03b2mnRmn(i)\n\u00b5m + PM\nl=1 \u03b1ml\u03b2mlRml(i)\nThe hessian of Lt(\u03bb) is given by the following close formulas :\n\u22022Lt(\u03bb)\n\u2202\u00b5m\u2202\u00b5m\n=\nX\ntm\ni\n1\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u00b5m\u2202\u03b1ml\n=\nX\ntm\ni\n\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u03b1ml\u2202\u00b5m\n=\nX\ntm\ni\n\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u03b1mk\u2202\u03b1ml\n=\nX\ntm\ni\n\u03b2mkRmk(i)\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\nand all other coe\ufb03cients are null.",
    "chunk_index": 11,
    "start_char": 28324,
    "end_char": 30803,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "The hessian of Lt(\u03bb) is given by the following close formulas :\n\u22022Lt(\u03bb)\n\u2202\u00b5m\u2202\u00b5m\n=\nX\ntm\ni\n1\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u00b5m\u2202\u03b1ml\n=\nX\ntm\ni\n\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u03b1ml\u2202\u00b5m\n=\nX\ntm\ni\n\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\n\u22022Lt(\u03bb)\n\u2202\u03b1mk\u2202\u03b1ml\n=\nX\ntm\ni\n\u03b2mkRmk(i)\u03b2mlRml(i)\n(\u00b5m + PM\nn=1 \u03b1mn\u03b2mnRmn(i))2\nand all other coe\ufb03cients are null. The \ufb01tting algorithm is described as follow:\nAlgorithm 3 Two Stage Hawkes Likelihood Optimization using Projected New-\nton Descent\nStart from mean of inter arrival times \u03b20\nfor each step of Nelder-Mead method until convergence do\nfor each needed computation of L\u2217\nt (\u03b2i) do\nStart from a well chosen (\u00b50, \u03b10)\nUse Projected Newton Descent to optimize Lt(\u00b5, \u03b1, \u03b2i) :\nRetrieve resulting L\u2217\nt (\u03b2i)\nend for\nend for\nreturn last (\u03b2, \u00b5\u2217(\u03b2), \u03b1\u2217(\u03b2))\nAnd the projected newton descent is described as follow :\n\nHawkes processes for credit indices time series analysis\nAlgorithm 4 Projected Newton Descent\nStart from a well chosen x0\nfor each step until convergence or iter max do\nSet prev x = x, prev obj = obj\nSet step = 1\nSet shrink = 0.5 (user choice of a shrinx in [0, 1])\nwhile True do\nnext x = project(x - step*(\u22072f(x))\u22121\u2207f(x))\nnext obj = f(next x)\nif next obj > prev obj then\nstep *= shrink\nelse\nx = next x\nend if\nend while\ntest convergence abs(next obj - prev obj) / abs(prev obj) < tolerance\nend for\nreturn x\n5.3\nSome empirical results:\nLoglikelihood landscape: Visualizing the minus loglikelihood function for 2D\nexponential Hawkes process with regards to two non convex variables (\u03b20,0, \u03b21,1)\nresulted into the following \ufb01gure, emphasizing the \ufb02atness of the function to\nminimize and the existence of valleys that pose problem to gradient based opti-\nmization :\nFig. 7. Lt plot for a 2D exponential Hawkes process loglikelihood with respect to\ndecays variables (\u03b20,0, \u03b21,1), other parameters being \ufb01xed: (\u03b20,1, \u03b21,0) = (0.1, 0.1), \u00b5 =\n(0.02, 0.25), \u03b1 = ((0.3, 0.15), (0.01, 0.35))\n\nBahamou, A., Doumergue, M., Donnat, P.\nGoodness of \ufb01t size bins study: Assessing the goodness of \ufb01t of the mul-\ntivariate Hawkes process \ufb01tted on 2017 itxeb reported trades split into three\nseries with regards to the size of trades:\nFig. 8. QQPlots of goodness of \ufb01t of the multivariate Hawkes process \ufb01tted on 2017\nitxeb by size bin reported trades\n\nHawkes processes for credit indices time series analysis\nGoodness of \ufb01t Multi index study: Assessing the goodness of \ufb01t of the\nmultivariate Hawkes process \ufb01tted on 2017 itxeb, itxes and itxex reported\ntrades\nFig. 9. QQPlots of goodness of \ufb01t of the multivariate Hawkes process \ufb01tted on 2017\nitxeb, itxes and itxex",
    "chunk_index": 12,
    "start_char": 30452,
    "end_char": 33048,
    "paper_title": "Hawkes processes for credit indices time series an",
    "paper_category": "stat.AP",
    "paper_filename": "Hawkes_processes_for_credit_indices_time_series_an.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.AP/Hawkes_processes_for_credit_indices_time_series_an.pdf"
  },
  {
    "text": "High-Dimensional Mean-Variance Spanning Tests\nPreliminary\nDavid Ardiaa, S\u00e9bastien Laurentb, Rosnel Sessinoua,\u2217\naGERAD & Department of Decision Sciences, HEC Montr\u00e9al, Montr\u00e9al, Canada\nbAix-Marseille School of Economics, CNRS & EHESS, Aix-Marseille Graduate School of Management \u2013 IAE, France\nAbstract\nWe introduce a new framework for the mean-variance spanning (MVS) hypothesis testing. The\nprocedure can be applied to any test-asset dimension and only requires stationary asset returns\nand the number of benchmark assets to be smaller than the number of time periods. It involves\nindividually testing moment conditions using a robust Student-t statistic based on the batch-mean\nmethod and combining the p-values using the Cauchy combination test. Simulations demonstrate\nthe superior performance of the test compared to state-of-the-art approaches. For the empirical\napplication, we look at the problem of domestic versus international diversification in equities. We\nfind that the advantages of diversification are influenced by economic conditions and exhibit cross-\ncountry variation. We also highlight that the rejection of the MVS hypothesis originates from the\npotential to reduce variance within the domestic global minimum-variance portfolio.\nKeywords: Spanning test, Mean-variance, Model validation, Diversification testing, Batch-mean\nJEL codes: B26, C12, C52\n\u2217Corresponding author. HEC Montr\u00e9al, 3000 Chemin de la C\u00f4te-Sainte-Catherine, Montreal, QC H3T 2A7.\nEmail addresses: david.ardia@hec.ca (David Ardia), sebastien.laurent@univ-amu.fr\n(S\u00e9bastien Laurent), rosnel.sessinou@hec.ca (Rosnel Sessinou)\nPreprint submitted to arXiv\nMarch 27, 2024\narXiv:2403.17127v1 [stat.ME] 25 Mar 2024\n\n1. Introduction\nMean-variance spanning (MVS) tests aim to determine if adding test assets to a set of bench-\nmark assets improves the mean-variance efficient frontier. The two-fund separation theorem sug-\ngests that MVS testing is equivalent to testing if test assets have zero weights in the maximum\nSharpe ratio and global minimum-variance portfolios of all assets. These tests are called \u201cspan-\nning tests,\u201d and rejecting any of them leads to the rejection of the MVS hypothesis. These tests can\nbe performed individually. When the focus is on the maximum Sharpe ratio portfolio of the asset\nexcess returns only, and the market factors are the benchmark assets, the term \u201cmean-variance ef-\nficiency\u201d test is used instead of the spanning test. Such spanning tests are relevant for validating\nlinear factor asset pricing models, such as the CAPM (Sharpe, 1964; Lintner, 1965) or the three-\nfactor model by Fama and French (1993). For simplicity, we use the generic term MVS to refer to\nall these tests.\nSeveral MVS tests exist in the literature, but they all have limitations; see DeRoon and Nijman\n(2001) and Fran\u00e7ois and H\u00fcbner (2024, Chapter 16) for a review. Some of them (e.g., Huberman\nand Kandel, 1987; Gibbons et al., 1989; Britten-Jones, 1999; Kempf and Memmel, 2006) require\nthe estimation of a precision matrix, which means that the number of test assets must be much\nsmaller than the number of time periods T. This problem can be avoided by testing with ad-hoc\nportfolios instead of individual assets, but this introduces an aggregation bias (e.g., Roll, 1977)\nand does not ensure asset-level results. Some others (e.g., Beaulieu et al., 2007, 2010;",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 3361,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "Gibbons et al., 1989; Britten-Jones, 1999; Kempf and Memmel, 2006) require\nthe estimation of a precision matrix, which means that the number of test assets must be much\nsmaller than the number of time periods T. This problem can be avoided by testing with ad-hoc\nportfolios instead of individual assets, but this introduces an aggregation bias (e.g., Roll, 1977)\nand does not ensure asset-level results. Some others (e.g., Beaulieu et al., 2007, 2010; Gungor\nand Luger, 2009) use simulations to relax the normality assumption in previous studies and deal\nwith serial dependence. Kan and Zhou (2012) also introduced a score test to cope with the same\nproblem. However, these approaches require a small test set to be used. Pesaran and Yamagata\n(2012) then developed an MVS test that can handle large test sets, but only under weak and sparse\ncorrelation in the disturbances. Gungor and Luger (2016) show that their test loses power as the\ncorrelations between the assets increase. We also find that that test is oversized in the realistic\nsetting where asset returns follow AR-GARCH processes. Gungor and Luger (2016) finally in-\ntroduced a simulation-based method for MVS with a large test set. However, we find their test\nbecomes uninformative when the number of benchmark assets is large despite being lower than\nthe number of time periods.\nIn this paper, we propose a new framework for MVS testing that can be applied to any test-\n2\n\nasset dimension and only requires stationary asset returns. We use new moment conditions for\nspanning and test them in two steps. First, we test each component of the moment vector using ro-\nbust Student-t tests based on the batch-mean method. We improve the computational efficiency of\nthe method by avoiding refitting the same model many times. Second, we combine the individual\np-values using the CCT of Liu and Xie (2020), which accounts for the cross-sectional dependence\nbetween the test statistics and is valid under weak assumptions. We prove that the CCT can com-\nbine Student-t test statistics p-values, and we show that it has good finite sample properties. Monte\nCarlo simulations shows that our MVS tests have correct size and high power in most setups. They\nalso work well on heteroscedastic, skewed, and fat-tailed data. However, we find that when we rely\non a standard batch-mean method, our test suffers from size distortion when the number of bench-\nmark assets is large. We solve this problem by using a randomly-weighted batch-mean procedure\non our new moment conditions.\nFor the empirical illustration, we apply our test to determine whether combining blue-chip\nstocks traded in the U.S., Canada, and Europe can improve each country\u2019s domestic mean-variance\nefficient frontier. Previous studies used portfolio-based MVS tests to deal with the high-dimensional\nnature of the problem, but results are subject to aggregation bias (Huberman and Kandel, 1987;\nBritten-Jones, 1999; Bekaert and Harvey, 1995; Kempf and Memmel, 2006). We use our test at\nthe asset level and find that the benefits of international diversification depend on economic condi-\ntions and vary across countries, in line with the literature.",
    "chunk_index": 1,
    "start_char": 2910,
    "end_char": 6077,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "apply our test to determine whether combining blue-chip\nstocks traded in the U.S., Canada, and Europe can improve each country\u2019s domestic mean-variance\nefficient frontier. Previous studies used portfolio-based MVS tests to deal with the high-dimensional\nnature of the problem, but results are subject to aggregation bias (Huberman and Kandel, 1987;\nBritten-Jones, 1999; Bekaert and Harvey, 1995; Kempf and Memmel, 2006). We use our test at\nthe asset level and find that the benefits of international diversification depend on economic condi-\ntions and vary across countries, in line with the literature. The MVS hypothesis is rejected by the\nvariance reduction potential in the domestic global minimum-variance portfolio.\nThe paper is structured as follows. Section 2 introduces our new testing procedure. Section 3\npresents an overview of MVS tests and studies the finite sample performance of these tests and our\nnewly proposed tests using Monte Carlo simulations. Section 4 presents the empirical application,\nand Section 5 concludes.\n2. Testing MVS Assumption With Many Test Assets and Serial Dependence\n2.1. The Limits of the Existing Framework\nDefine rt \u2261(r\u2032\n1,t, r\u2032\n2,t)\u2032 \u2261(r1,t, . . . , rK,t, rK+1,t, . . . , rK+N,t)\u2032 as the vector of returns of the\nK + N risky assets at time t, where r1,t is the vector of returns of the K benchmark assets and r2,t\nis the vector of returns of the N test assets. Define the expected value and covariance matrix of rt\n3\n\nas \u00b5 \u2261E[rt] and V \u2261V[rt], respectively. We assume V to be nonsingular. Let Q \u2261[0N\u00d7K\n... IN]\nbe a selection matrix where 0N\u00d7K is a N \u00d7 K matrix of zeros, IN an identity matrix of size N,\nand iN a column vector of ones of length N.\nSeveral MVS tests have been proposed in the literature and we consider, below, the three most\ncommon hypotheses.\n\u2022 By the two-fund separation theorem, the global null hypothesis of MVS can be expressed as\nH\u03b1,\u03b4\n0\n: Q[w\u03b1 ... w\u03b4] = 0N\u00d72 ,\n(1)\nwhere w\u03b1 \u2261\nV\u22121\u00b5\ni\u2032\nN+KV\u22121\u00b5 and w\u03b4 \u2261\nV\u22121iN+K\ni\u2032\nN+KV\u22121iN+K denote respectively the vectors of weights\nof the maximum Sharpe and the global minimum-variance portfolios.\n\u2022 The null hypothesis of maximum Sharpe portfolio spanning can be expressed as\nH\u03b1\n0 : Qw\u03b1 = 0N\u00d71 .\n(2)\n\u2022 The global minimum-variance portfolio spanning hypothesis is\nH\u03b4\n0 : Qw\u03b4 = 0N\u00d71 .\n(3)\nThe test of H\u03b1\n0 is referred to as a mean-efficiency test in the literature when excess returns\n(over the risk-free rate) of the test assets are used and the benchmark assets are factors (such as the\nmarket, size, and value factors in the Fama and French (1993) model). Importantly, Huberman and\nKandel (1987), Britten-Jones (1999), and Kempf and Memmel (2006) show that testing the above\nthree spanning hypotheses can be done using tests of linear restrictions on the parameters of linear\nregression models.\nTable 1 shows, for the three hypotheses, the corresponding restrictions on the weights, the\nlinear models used to test",
    "chunk_index": 2,
    "start_char": 5474,
    "end_char": 8391,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "the test assets are used and the benchmark assets are factors (such as the\nmarket, size, and value factors in the Fama and French (1993) model). Importantly, Huberman and\nKandel (1987), Britten-Jones (1999), and Kempf and Memmel (2006) show that testing the above\nthree spanning hypotheses can be done using tests of linear restrictions on the parameters of linear\nregression models.\nTable 1 shows, for the three hypotheses, the corresponding restrictions on the weights, the\nlinear models used to test these hypotheses, the assumptions made on the linear model, the linear\nrestrictions corresponding to these hypotheses, and the key references linking the restrictions on\nthe weights and the linear restrictions.\n4\n\nTable 1: Testing Spanning Hypotheses via Tests of Linear Restrictions on Linear Regression Models\nFor any d-dimensional vector v \u2261(v1, v2, . . . , vd)\u2032, v\u22121 \u2261(v2, . . . , vd)\u2032. Recall that i\u2032\nNw\u03b1 = i\u2032\nNw\u03b4 = 1.\nWe let Q \u2261[0N\u00d7K\n... IN] and Q\u22121 \u2261[0N\u00d7(K\u22121)\n... IN]. Note that Qw\u03b4\u2032\n= Q\u22121w\u03b4\u2032\n\u22121 is a column vector\nwith elements w\u03b4\u2032\nj for j = K + 1, . . . , N + K.\nHypothesis\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b4\n0\nRestrictions on weights\nQ[w\u03b1 ... w\u03b4] = 0N\u00d72\nQw\u03b1 = 0N\u00d71\nQw\u03b4 = 0N\u00d71\nLinear model\nr2,t = \u03b1 + \u03b2r1,t + \u03b7t\n1 = b\u2032\n1r1,t + b\u2032\n2r2,t + ut\nr1,1,t = c + w\u03b4\u2032\n\u22121(iN+K\u22121r1,1,t \u2212r\u22121,t) + et\nAssumptions\nE[\u03b7t] = 0N\u00d71, E[r1,t\u03b7\u2032\nt] = 0K\u00d7N\nE((1, r\u2032\nt)\u2032ut) = 0(K+N)\u00d71\nE[(1, (iN+K\u22121r1,1,t \u2212r\u22121,t)\u2032)\u2032et] = 0\nLinear restrictions\n\u03b1 = 0N\u00d71; \u03b4 \u2261iN \u2212\u03b2iK = 0N\u00d71\nb2 = 0N\u00d71\nQ\u22121w\u03b4\n\u22121 = 0N\u00d71\nReferences\nHuberman and Kandel (1987)\nBritten-Jones (1999)\nKempf and Memmel (2006)\nParameter \u03b2 is a N \u00d7 K matrix with elements (\u03b2i,j) while \u03b2\u2022,\u22121 denotes the N \u00d7 (K \u22121)\nsubmatrix of \u03b2 whose first column has been removed and let \u03b4 \u2261iN \u2212\u03b2iK. The model r2,t =\n\u03b1 + \u03b2r1,t + \u03b7t can be rewritten as\nr2,t \u2212iNr1,1,t = \u03b1 + \u03b4r1,1,t + \u03b2\u2022,\u22121(r1,\u22121,t \u2212iK\u22121r1,1,t) + \u03b7t\n(4)\nso that \u03b4 appears in the model. Testing H\u03b1\n0 , H\u03b4\n0 and H\u03b1,\u03b4\n0\ncan therefore be done by testing respec-\ntively H0 : \u03b1 = 0N, H0 : \u03b4 = 0N and H0 : \u03b1 = 0N; \u03b4 = 0N on Model (4).\nThe most natural way to test these hypotheses would be to assume that \u03b7t is Gaussian and\nto use F-tests as advocated by Huberman and Kandel (1987), Britten-Jones (1999), and Kempf\nand Memmel (2006). F-tests for the above hypotheses have been shown to have good finite sample\nproperties when the number of test assets N and the number of benchmark assets K are much lower\nthan the number of time periods T and when residuals are i.i.d. However, attention must be paid\nto the presence of serial correlation and heteroscedasticity in the residuals. Lazarus et al. (2018,\n2019) and Pedersen (2020) have reported that standard HAC procedures can be size distorted in\nfinite samples.",
    "chunk_index": 3,
    "start_char": 7889,
    "end_char": 10562,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "and Memmel (2006). F-tests for the above hypotheses have been shown to have good finite sample\nproperties when the number of test assets N and the number of benchmark assets K are much lower\nthan the number of time periods T and when residuals are i.i.d. However, attention must be paid\nto the presence of serial correlation and heteroscedasticity in the residuals. Lazarus et al. (2018,\n2019) and Pedersen (2020) have reported that standard HAC procedures can be size distorted in\nfinite samples. Bootstrapping is an alternative, but it is computationally very demanding, especially\nwhen N is large.\nTo keep N small, spanning tests are generally applied at the portfolio level, not at the asset\nlevel. We will show that this strongly impacts the conclusions and recommend carrying out the\ntest at the asset level. In this case, N will most likely be large (even larger than T), which inval-\nidates standard methods such as F-tests and score tests (see Kan and Zhou, 2012 and subsequent\nreferences for details about those tests).\nOur contribution is twofold.\n5\n\nFirst, we propose to rely on the batch-mean method instead of HAC estimates of standard errors\nto robustify the statistical inference to the possible presence of serial correlation and heteroscedas-\nticity in the residuals. According to Pedersen (2020), the batch-mean method enjoys better finite\nsample properties than standard HAC procedures in the presence of serial correlation. This method\nrequires estimating the model over sub-samples and performing a t-test on the collection of esti-\nmated coefficients. It also imposes the restriction that K must be smaller than the size of each\nsub-sample, which must be of order T 2/3 according to Sherman (1997) and Flegal et al. (2010).\nThis prohibits using standard batch-mean when K is of that order. We also show how to over-\ncome this restriction by reformulating the null hypotheses of spanning as residual-based moment\nconditions. This allows one to avoid refitting the model when using the batch-mean method. Fur-\nthermore, a weighted batch-mean method is presented to cope with the size distortion appearing\nfor the standard batch-mean when K is large but smaller than T.\nSecond, we propose to apply the tests to each test asset independently, that is, by testing for\nj = 1, . . . , N the null hypotheses H\n\u03b1j\n0\n: \u03b1j = 0, H\n\u03b4j\n0\n: \u03b4j = 0 and/or H\n\u03b1j,\u03b4j\n0\n: \u03b1j = 0; \u03b4j = 0\nand collecting the corresponding p-values. Then, to test the join hypotheses on the N assets, we\npropose to aggregate transformed individual p-values into a global p-value using the Cauchy Com-\nbination Test (CCT) of Liu and Xie (2020). This avoids having to estimate the variance-covariance\nmatrix of the N estimates of \u03b1j and/or \u03b4j, which would be the case for a classic F-test and which\nwould prove inefficient when N is large. Unlike traditional p-value merging procedures such as\nthe Bonferroni method or Benjamini and Hochberg (1995) and Benjamini and Yekutieli (2001)\u2019s",
    "chunk_index": 4,
    "start_char": 10065,
    "end_char": 13031,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "the N assets, we\npropose to aggregate transformed individual p-values into a global p-value using the Cauchy Com-\nbination Test (CCT) of Liu and Xie (2020). This avoids having to estimate the variance-covariance\nmatrix of the N estimates of \u03b1j and/or \u03b4j, which would be the case for a classic F-test and which\nwould prove inefficient when N is large. Unlike traditional p-value merging procedures such as\nthe Bonferroni method or Benjamini and Hochberg (1995) and Benjamini and Yekutieli (2001)\u2019s\nmethods, the CCT features the nice property that when all hypotheses are true, the empirical size\nconverges to the nominal size as the significance level tends to zero (e.g., for a nominal size of\n5%), for arbitrary dependency structures among the test statistics.\n2.2. A New Framework for MVS Test\nProposition 1 provides a rewriting of the null hypotheses H\u03b1\n0 and H\u03b4\n0 using moment conditions\non the residuals of regressions models. An equivalent rewriting of H\u03b1,\u03b4\n0\nis deduced as a by-product.\nProposition 1. Let us define \u03b5(yt, xt, \u03b3) \u2261yt \u2212\u03b3xt, where yt \u2208R, xt \u2208Rq and \u03b3 \u2208Rq such that\nE\n\u0002\nxt(yt \u2212\u03b3\u2032xt)\n\u0003\n= 0q\u00d71. Let xj\nt \u2261(xj\n1,t, . . . , xj\nK+2,t) \u2261(r2,j,t\u2212r1,1,t, 1, r1,1,t, r\u2032\n1,\u22121,t\u2212i\u2032\nK\u22121r1,1,t)\u2032,\nand denote xj\n\u2212i,t the vector xj\nt from which xj\ni,t is removed. For j = 1, . . . , N, one has\n6\n\n(a) E[g\u03b1j(rt, \u03b8j)] \u2261E\n\u0002\n\u03b5(xj\n1,t, xj\n\u22121,t , \u03b8j,1)\u03b5(xj\n2,t, xj\n\u22122,t, \u03b8j,2)\n\u0003\n\u221d\u03b1j ,\n(b) E[g\u03b4j(rt, \u03b8j)] \u2261E\n\u0002\n\u03b5(xj\n1,t, xj\n\u22121,t, \u03b8j,1)\u03b5(xj\n3,t, xj\n\u22123,t, \u03b8j,3)\n\u0003\n\u221d\u03b4j ,\nwith \u03b8j,i \u2208RK+1 for i = 1, 2, 3 and \u03b8j \u2261(\u03b8\u2032\nj,1, \u03b8\u2032\nj,2, \u03b8\u2032\nj,3)\u2032 \u2208R3(K+1).\nLet us also define \u03b8 \u2261(\u03b8\u2032\n1, . . . , \u03b8\u2032\nN)\u2032 \u2208R3N(K+1). Proposition 1 suggest that the null hypothe-\nsis of MVS can be written using moment conditions, that is:\n\u2022 H\u03b1\n0 : \u03b1 = 0N\u00d71 \u21d4m\u03b1(\u03b8) = E[g\u03b1(rt, \u03b8)] \u2261E\nh\u0000g\u03b11(rt, \u03b81), . . . , g\u03b1N(rt, \u03b8N)\n\u0001\u2032i\n= 0N\u00d71,\n\u2022 H\u03b4\n0 : \u03b4 = 0N\u00d71 \u21d4m\u03b4(\u03b8) = E[g\u03b4(rt, \u03b8)] \u2261E\nh\u0000g\u03b41(rt, \u03b81), . . . , g\u03b4N(rt, \u03b8N)\n\u0001\u2032i\n= 0N\u00d71,\n\u2022 H\u03b1,\u03b4\n0\n:\n\u03b1 = \u03b4 = 0N\u00d71 \u21d4m\u03b1,\u03b4(\u03b8) = E[g\u03b1,\u03b4(rt, \u03b8)] \u2261E\nh\u0000g\u03b1(rt, \u03b8)\u2032, g\u03b4(rt, \u03b8)\u2032\u0001\u2032i\n=\n02N\u00d71.\nThe proof of Proposition 1 is given in the online appendix. But for the sake of illustration,\nconsider a universe of K = 2 benchmark assets and N = 1 test asset. As shown in column 2 of\nTable 1, when N = 1, testing H\u03b11,\u03b41\n0\nimplies testing H0 : \u03b11 = 0, \u03b41 \u22611 \u2212\u03b21,1 \u2212\u03b21,2 = 0 using\nthe regression model r2,1,t = \u03b11 + \u03b21,1r1,1,t + \u03b21,2r1,2,t + \u03b71,t or the auxiliary regression\nr2,1,t \u2212r1,1,t = \u03b11 \u2212\u03b41r1,1,t + \u03b21,2(r1,2,t \u2212r1,1,t) + \u03b71,t .\n(5)\nTesting H\u03b11\n0\nand H\u03b41\n0 can also be done using the same regression model.",
    "chunk_index": 5,
    "start_char": 12535,
    "end_char": 15020,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "column 2 of\nTable 1, when N = 1, testing H\u03b11,\u03b41\n0\nimplies testing H0 : \u03b11 = 0, \u03b41 \u22611 \u2212\u03b21,1 \u2212\u03b21,2 = 0 using\nthe regression model r2,1,t = \u03b11 + \u03b21,1r1,1,t + \u03b21,2r1,2,t + \u03b71,t or the auxiliary regression\nr2,1,t \u2212r1,1,t = \u03b11 \u2212\u03b41r1,1,t + \u03b21,2(r1,2,t \u2212r1,1,t) + \u03b71,t .\n(5)\nTesting H\u03b11\n0\nand H\u03b41\n0 can also be done using the same regression model.\nConsider\nthe\nvector\nx1\nt\ncontaining\nthe\nfour\nvariables\ninvolved\nin\n(5),\nthat\nis,\nx1\nt = (r2,1,t \u2212r1,1,t, 1, r1,1,t, r1,2,t \u2212r1,1,t)\u2032 with V[x1\nt] = \u03a31. Hereafter, we omit the superscript on\n\u03a31 for simplicity.\nConsider also the following system of nodewize regressions on xt, where each variable in the\nsystem is regressed on the three other variables (only the first three equations matter for what\nfollows):\n(S) :\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nr2,1,t \u2212r1,1,t = \u03b81,2 \u00d7 1 + \u03b81,3 \u00d7 r1,1,t + \u03b81,4 \u00d7 (r1,2,t \u2212r1,1,t) + v1,t\n1 = \u03b82,1 \u00d7 (r2,1,t \u2212r1,1,t) + \u03b82,3 \u00d7 r1,1,t + \u03b82,4 \u00d7 (r1,2,t \u2212r1,1,t) + v2,t\nr1,1,t = \u03b83,1 \u00d7 (r2,1,t \u2212r1,1,t) + \u03b83,2 \u00d7 1 + \u03b83,4 \u00d7 (r1,2,t \u2212r1,1,t) + v3,t\nr1,2,t \u2212r1,1,t = \u03b84,1 \u00d7 (r2,1,t \u2212r1,1,t) + \u03b84,2 \u00d7 1 + \u03b84,3 \u00d7 r1,1,t + v4,t,\n(6)\n7\n\nwith V[vi,t] = g2\ni > 0 for i = 1, . . . , 4, G \u2261Diag(g2\n1, . . . , g2\n4) and E[xi,tvj,t] = 0 for i \u0338= j or\nmore compactly x1\nt = \u03981x1\nt + vt, where \u03981 \u2261\n\uf8eb\n\uf8ed\n0\n\u03b81,2\n\u03b81,3\n\u03b81,4\n\u03b82,1\n0\n\u03b82,3\n\u03b82,4\n\u03b83,1\n\u03b83,2\n0\n\u03b83,4\n\u03b84,1\n\u03b84,2\n\u03b84,3\n0\n\uf8f6\n\uf8f8and vt \u2261(v1,t, . . . , v4,t)\u2032.\nTherefore, (I4 \u2212\u03981)E[x1\ntx1\u2032\nt ] = G so that \u03a3\u22121 = E[x1\ntx1\u2032\nt ]\u22121 = G\u22121(I4 \u2212\u03981) if E[x1\ntx1\u2032\nt ] is\ninvertible.\nComparing the first three equations in (6) with (5) and the linear models in columns 3 and 4\nin Table 1 for K = 2 and N = 1, that is, 1 = b1,1r1,1,t + b1,2r1,2,t + b2,1r2,1,t + ut and r1,1,t =\nc + w\u03b4\n2(r1,1,t \u2212r1,2,t) + w\u03b4\n3(r1,1,t \u2212r2,1,t) + et, we can deduce by identification that\n\u03a3\u22121 =\n\uf8eb\n\uf8ed\n1/g2\n1\n\u2212\u03b81,2/g2\n1\n\u2212\u03b81,3/g2\n1\n\u2212\u03b81,4/g2\n1\n\u2212\u03b82,1/g2\n2\n1/g2\n2\n\u2212\u03b82,3/g2\n2\n\u2212\u03b82,4/g2\n2\n\u2212\u03b83,1/g2\n3\n\u2212\u03b83,2/g2\n3\n1/g2\n3\n\u2212\u03b83,4/g2\n3\n\u2212\u03b84,1/g2\n4\n\u2212\u03b84,2/g2\n4\n\u2212\u03b84,3/g2\n4\n1/g2\n4\n\uf8f6\n\uf8f8=\n\uf8eb\n\uf8ed\n1/g2\n1\n\u2212\u03b11/g2\n1\n\u03b41/g2\n1\n\u2212\u03b21,2/g2\n1\n\u2212b2,1/g2\n2\n1/g2\n2\n\u2212(b1,1 \u2212b2,1 \u2212b1,2)/g2\n2\n\u2212b1,2/g2\n2\nw\u03b4\n3/g2\n3\n\u2212c/g2\n3\n1/g2\n3\nw\u03b4\n2/g2\n3\n\u2212\u03b84,1/g2\n4\n\u2212\u03b84,2/g2\n4\n\u2212\u03b84,3/g2\n4\n1/g2\n4\n\uf8f6\n\uf8f8.\nSimilarly, since vt = (I4 \u2212\u03981)x1\nt, it holds that E[vtv\u2032\nt] = (I4 \u2212\u03981)G and that\nE[vtv\u2032\nt] =\n\uf8eb\n\uf8ed\ng2\n1\n\u2212\u03b11g2\n1\n\u03b41g2\n1\n\u2212\u03b21,2g2\n1\n\u2212b2,1g2\n2\ng2\n2\n\u2212(b1,1 \u2212b2,1 \u2212b1,2)g2\n2\n\u2212b1,2g2\n2\nw\u03b4\n3g2\n3\n\u2212cg2\n3\ng2\n3\nw\u03b4\n2g2\n3\n\u2212\u03b84,1g2\n4\n\u2212\u03b84,2g2\n4\n\u2212\u03b84,3g2\n4\ng2\n4\n\uf8f6\n\uf8f8.",
    "chunk_index": 6,
    "start_char": 14680,
    "end_char": 17044,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u2212\u03b84,1/g2\n4\n\u2212\u03b84,2/g2\n4\n\u2212\u03b84,3/g2\n4\n1/g2\n4\n\uf8f6\n\uf8f8=\n\uf8eb\n\uf8ed\n1/g2\n1\n\u2212\u03b11/g2\n1\n\u03b41/g2\n1\n\u2212\u03b21,2/g2\n1\n\u2212b2,1/g2\n2\n1/g2\n2\n\u2212(b1,1 \u2212b2,1 \u2212b1,2)/g2\n2\n\u2212b1,2/g2\n2\nw\u03b4\n3/g2\n3\n\u2212c/g2\n3\n1/g2\n3\nw\u03b4\n2/g2\n3\n\u2212\u03b84,1/g2\n4\n\u2212\u03b84,2/g2\n4\n\u2212\u03b84,3/g2\n4\n1/g2\n4\n\uf8f6\n\uf8f8.\nSimilarly, since vt = (I4 \u2212\u03981)x1\nt, it holds that E[vtv\u2032\nt] = (I4 \u2212\u03981)G and that\nE[vtv\u2032\nt] =\n\uf8eb\n\uf8ed\ng2\n1\n\u2212\u03b11g2\n1\n\u03b41g2\n1\n\u2212\u03b21,2g2\n1\n\u2212b2,1g2\n2\ng2\n2\n\u2212(b1,1 \u2212b2,1 \u2212b1,2)g2\n2\n\u2212b1,2g2\n2\nw\u03b4\n3g2\n3\n\u2212cg2\n3\ng2\n3\nw\u03b4\n2g2\n3\n\u2212\u03b84,1g2\n4\n\u2212\u03b84,2g2\n4\n\u2212\u03b84,3g2\n4\ng2\n4\n\uf8f6\n\uf8f8.\nWe deduce that E[v1,tv2,t] = \u2212\u03b11g2\n1, and that E[v1,tv3,t] = \u03b41g2\n1, where v1,t = \u03b5(x1\n1,t, x1\n\u22121,t, \u03b81,1),\nv2,t = \u03b5(x1\n2,t, x1\n\u22122,t, \u03b81,2) and v3,t = \u03b5(x1\n3,t, x1\n\u22123,t, \u03b81,3) with \u03b81,1 = (\u03b81,2, \u03b81,3, \u03b81,4), \u03b81,2 = (\u03b82,1, \u03b82,3, \u03b82,4)\nand \u03b81,3 = (\u03b83,1, \u03b83,2, \u03b83,4) using the notation of Proposition 1 so that assumptions H\u03b11\n0\nand H\u03b41\n0\ncan be tested using moment conditions, that is, H\u03b11\n0\n: E[v1,tv2,t] = 0, H\u03b41\n0 : E[v1,tv3,t] = 0.\nWe summarize and generalize the above results in Lemma 1 in the online appendix, which\nimplies that Proposition 1 is valid for any N and K < T. Let \u02c6\u03b8 denote the least square estimator\nof \u03b8. Section 2.3 introduces our batch-mean Cauchy combination spanning (BCS) tests using the\nempirical counterpart of moment conditions defined above. Those empirical moment conditions\nare\n\u2022 for H\u03b1\n0 , \u02c6m\u03b1(\u02c6\u03b8) \u22611\nT\nPT\nt=1 \u02c6g\u03b1(rt, \u02c6\u03b8), where\n\u02c6g\u03b1(rt, \u02c6\u03b8) \u2261\n\u0010\n\u02c6\u03b5(x1\n1,t, x1\n\u22121,t, \u02c6\u03b81,1)\u02c6\u03b5(x1\n2,t, x1\n\u22122,t, \u02c6\u03b81,2), . . . , \u02c6\u03b5(xN\n1,t, xN\n\u22121,t, \u02c6\u03b8N,1)\u02c6\u03b5(xN\n2,t, xN\n\u22122,t, \u02c6\u03b8N,2)\n\u0011\u2032\n,\n\u02c6\u03b5(xj\n1,t, xj\n\u22121,t, \u02c6\u03b8j,1) \u2261xj\n1,t \u2212\u02c6\u03b8j,1xj\n\u22121,t with \u02c6\u03b8j,1 \u2261\n\u0010PT\nt=1 xj\n\u22121,txj\u2032\n\u22121,t\n\u0011\u22121 PT\nt=1 xj\n\u22121,txj\n1,t \u2208\nRK+1,\nfor\nj\n=\n1, . . . , N\nand\n\u02c6\u03b5(xj\n2,t, xj\n\u22122,t, \u02c6\u03b8j,2)\n\u2261\nxj\n2,t \u2212\u02c6\u03b8j,2xj\n\u22122,t\nwith\n\u02c6\u03b8j,2 \u2261\n\u0010PT\nt=1 xj\n\u22122,txj\u2032\n\u22122,t\n\u0011\u22121 PT\nt=1 xj\n\u22122,txj\n2,t \u2208RK+1 for j = 1, . . . , N.\n8\n\n\u2022 for H\u03b4\n0 , \u02c6m\u03b4(\u02c6\u03b8) \u22611\nT\nPT\nt=1 \u02c6g\u03b4(rt, \u02c6\u03b8) where\n\u02c6g\u03b4(rt, \u02c6\u03b8) \u2261\n\u0010\n\u02c6\u03b5(x1\n1,t, x1\n\u22121,t, \u02c6\u03b81,1)\u02c6\u03b5(x1\n3,t, x1\n\u22123,t, \u02c6\u03b81,3), . . . , \u02c6\u03b5(xN\n1,t, xN\n\u22121,t, \u02c6\u03b8N,1)\u02c6\u03b5(xN\n3,t, xN\n\u22123,t, \u02c6\u03b8N,3)\n\u0011\u2032\n,\n\u02c6\u03b5(xj\n3,t, xj\n\u22123,t, \u02c6\u03b8j,3) \u2261xj\n3,t \u2212\u02c6\u03b8j,3xj\n\u22123,t with \u02c6\u03b8j,3 \u2261\n\u0010PT\nt=1 xj\n\u22123,txj\u2032\n\u22123,t\n\u0011\u22121 PT\nt=1 xj\n\u22123,txj\n3,t \u2208\nRK+1 for j = 1, . . . , N.\n\u2022 for H\u03b1,\u03b4\n0\n, \u02c6m\u03b1,\u03b4(\u02c6\u03b8) \u2261\n\u0010\n\u02c6m\u03b1(\u02c6\u03b8), \u02c6m\u03b4(\u02c6\u03b8)\n\u0011\u2032\n= 1\nT\nPT\nt=1\n\u0010\n\u02c6g\u03b1(rt, \u02c6\u03b8)\u2032, \u02c6g\u03b4(rt, \u02c6\u03b8)\u2032\u0011\u2032\n.\nTo avoid heavy notations hereafter, we will use the convention \u03b8N+j = \u03b8j and \u02c6\u03b8N+1 = \u02c6\u03b8j for\nj = 1, . . . , N.",
    "chunk_index": 7,
    "start_char": 16581,
    "end_char": 18861,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u02c6\u03b8j,3 \u2261\n\u0010PT\nt=1 xj\n\u22123,txj\u2032\n\u22123,t\n\u0011\u22121 PT\nt=1 xj\n\u22123,txj\n3,t \u2208\nRK+1 for j = 1, . . . , N.\n\u2022 for H\u03b1,\u03b4\n0\n, \u02c6m\u03b1,\u03b4(\u02c6\u03b8) \u2261\n\u0010\n\u02c6m\u03b1(\u02c6\u03b8), \u02c6m\u03b4(\u02c6\u03b8)\n\u0011\u2032\n= 1\nT\nPT\nt=1\n\u0010\n\u02c6g\u03b1(rt, \u02c6\u03b8)\u2032, \u02c6g\u03b4(rt, \u02c6\u03b8)\u2032\u0011\u2032\n.\nTo avoid heavy notations hereafter, we will use the convention \u03b8N+j = \u03b8j and \u02c6\u03b8N+1 = \u02c6\u03b8j for\nj = 1, . . . , N. We will also let m(\u03b8) \u2261m\u03b1,\u03b4(\u03b8) and\n\u02c6m\u03b1,\u03b4(\u02c6\u03b8) \u2261\u02c6m(\u02c6\u03b8)\n(7)\n= 1\nT\nT\nX\nt=1\n\u0010\n\u02c6g\u03b11(rt, \u02c6\u03b81), . . . , \u02c6g\u03b1N(rt, \u02c6\u03b8N), \u02c6g\u03b41(rt, \u02c6\u03b81), . . . , \u02c6g\u03b4N(rt, \u02c6\u03b8N)\n\u0011\u2032\n,\n(8)\n= 1\nT\nT\nX\nt=1\n\u0010\n\u02c6g1(rt, \u02c6\u03b81), . . . , \u02c6gN(rt, \u02c6\u03b8N), \u02c6gN+1(rt, \u02c6\u03b8N+1), . . . , \u02c6g2N(rt, \u02c6\u03b82N)\n\u0011\u2032\n,\n(9)\n= 1\nT\nT\nX\nt=1\n\u02c6g(rt, \u02c6\u03b8) .\n(10)\nSince the returns are assumed to be covariance-stationary with positive definite covariance\nmatrix, least squares theory ensures that \u02c6\u03b8\np\u2192\u03b8 and the continuous mapping theorem suggests\nthat m(\u02c6\u03b8)\np\u2192E\nh\u0000g\u03b1(rt, \u03b8)\u2032, g\u03b4(rt, \u03b8)\u2032\u0001\u2032i\n. To be unbiased, the BCS tests requires the following\nassumptions to be satisfied.\nAssumption 1. \u02c6\u03b8 \u2261(\u02c6\u03b81, . . . , \u02c6\u03b8d)\u2032 is a\n\u221a\nT-consistent estimator of \u03b8, the unique vector satisfying\nE[ \u02c6m(\u03b8)] = 02N\u00d71.\nAssumption 2. g(rt, \u03b8) is the t-th observation of a stationary and ergodic process.\nAssumption 1 ensures the identifiability of H0 and is satisfied as \u02c6\u03b8 is a least square estimator\nobtained on stationary data with positive definite covariance matrix. The stationarity and ergod-\nicity conditions in Assumption 2 are also satisfied since \u02c6g(rt, \u03b8) is defined in Proposition 1 as a\nstationary linear combination of rt coordinates.\nWe now introduce the BCS tests. Wlog we focus on testing H\u03b1,\u03b4\n0\n: E\n\u0002\u0000g\u03b1(rt, \u03b8)\u2032, g\u03b4(rt, \u03b8)\u2032\u0001\u0003\n=\n02N\u00d71.\n9\n\n2.3. The Batch-Mean Cauchy Combination Spanning (BCS) Tests\nHereafter, we present the BCS tests. Unlike score and Fisher (or Wald) tests, they are reliable\nand feasible even when 2N \u226bT. As such, the BCS tests can (i) replace the traditional test statistics\nin high dimensions, or (ii) deal with time-series dependence. We illustrate these properties via an\nextensive Monte Carlo simulation in Section 3.\nIn the first step of the testing procedure, we perform individual tests Hj\n0 : E[mj(\u03b8j)] = 0\nfor j = 1, . . . , 2N using a non-overlapping batch-mean method. We recall that by convention\n\u03b8N+j = \u03b8j and \u02c6\u03b8N+1 = \u02c6\u03b8j for j = 1, . . . , N.\nTo do so, we first estimate \u03b8 over the entire sample of T observations and denote it \u02c6\u03b8. Next,\ngiven the latter, we consider a partition of the observations {rt}T\nt=1 into B non-overlapping and\nconsecutive blocks of equivalent sizes Tb (b = 1, . . . , B) such that Tb/T \u21920. According to Flegal\net al. (2010), a fixed-width rule B \u2261[T \u03b6], where [T \u03b6] denotes the integer part of T \u03b6, can be used to\nchoose the number of blocks B.",
    "chunk_index": 8,
    "start_char": 18570,
    "end_char": 21191,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": ", N.\nTo do so, we first estimate \u03b8 over the entire sample of T observations and denote it \u02c6\u03b8. Next,\ngiven the latter, we consider a partition of the observations {rt}T\nt=1 into B non-overlapping and\nconsecutive blocks of equivalent sizes Tb (b = 1, . . . , B) such that Tb/T \u21920. According to Flegal\net al. (2010), a fixed-width rule B \u2261[T \u03b6], where [T \u03b6] denotes the integer part of T \u03b6, can be used to\nchoose the number of blocks B. Their analysis suggests that \u03b6 could be set to 2/3, 1/2, or 1/3 in\npractice. Denote Ib the set of indices of the observations belonging to the b-th block. We compute\n\u02c6mj(\u02c6\u03b8j)b \u22611\nTb\nX\nt\u2208Ib\n\u02c6gj(rt, \u02c6\u03b8j) ,\n(11)\nfor b = 1, . . . , B, and\n\u02c6vj(\u02c6\u03b8j)B \u2261\n1\nB \u22121\nB\nX\nb=1\n\u0010\n\u02c6mj(\u02c6\u03b8j)b \u2212\u00afmj(\u02c6\u03b8j)B\n\u00112\n,\n(12)\nwhere \u00afmj(\u02c6\u03b8j)B \u22611\nB\nPB\nb=1 \u02c6mj(\u02c6\u03b8j)b. \u02c6vj(\u02c6\u03b8j)B is called a batch-mean based estimator of V[ \u02c6mj(\u02c6\u03b8j)].\nTheorem 1. Assume that Assumptions 1 and 2 hold. Under Hj\n0 : E[mj(\u03b8j)] = 0, one has\ntj,B \u2261\n\u221a\nB\n\uf8eb\n\uf8ed\u00afmj(\u02c6\u03b8j)B\nq\n\u02c6vj(\u02c6\u03b8j)B\n\uf8f6\n\uf8f8\nL\n\u2212\u2212\u2212\u2192\nT\u2192\u221eStB\u22121 ,\n(13)\nwhere \u02c6vj(\u02c6\u03b8j)B \u2212V[mj(\u03b8j)] = op(1) and St\u03c5 denotes a Student-t distribution with \u03c5 degrees of\nfreedom. The p-value associated to Hj\n0 vs. the alternative hypothesis Hj\n1 : E[mj(\u03b8j)] \u0338= 0 is\nptj,B = 2{1 \u2212\u03a6B\u22121(|tj,B|)}, where \u03a6\u03c5(.) is the cumulative distribution fonction of the Student-t\ndistribution with \u03c5 degrees of freedom.\n10\n\nRemark 1. The standard batch-mean method (Carlstein et al., 1986; Sherman, 1997; Flegal et al.,\n2010; Ibragimov and M\u00fcller, 2010; Pedersen, 2020) requires refitting the regression above on\neach batch, that is, the blocks in the batch-mean method. In other words, it requires K + 1 <\nmaxb=1,...B Tb with maxb=1,...B Tb \u2248T/B if the batches have the same length. However, this is not\nfeasible in many empirical applications. For example, when N = 306, K = 102, and T = 250\n(see Section 4), B can be at least 6 when the fixed-width rule of Flegal et al. (2010) is used. The\nstandard batch-mean approach fails as K + 1 = 103 \u226bmaxb=1,...B Tb \u224840. In contrast, our\ntest is equivalent to computing over each subsample b = 1, . . . , B, the covariances among those\nmodels\u2019 residuals \u2013 estimated using \u02c6\u03b8 \u2013 as we are using the moment conditions in Proposition 1.\nHence, we never require refitting models and make the batch-mean method applicable even in\nsettings where K + 1 > maxb=1,...,B Tb. Therefore, we reduce the computational burden of the\nbatch-mean method as we only require fitting N + 2 models of size K instead of BN of size K \u2013\nwhen using the standard batch-mean method.\nLet D \u2286{1, . . . , 2N} with D having cardinality d > 0. One let D = {1, . . . , N} (or D =\n{N + 1, .",
    "chunk_index": 9,
    "start_char": 20758,
    "end_char": 23345,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "batch-mean method applicable even in\nsettings where K + 1 > maxb=1,...,B Tb. Therefore, we reduce the computational burden of the\nbatch-mean method as we only require fitting N + 2 models of size K instead of BN of size K \u2013\nwhen using the standard batch-mean method.\nLet D \u2286{1, . . . , 2N} with D having cardinality d > 0. One let D = {1, . . . , N} (or D =\n{N + 1, . . . , 2N}) to test H\u03b1\n0 (or H\u03b4\n0 ). To test H\u03b1,\u03b4\n0\n, one let D = {1, . . . , 2N}. In any case, to\ntest the global null hypothesis H0 = T\nj\u2208D Hj\n0 without having to compute the covariances between\nthe 2N elements \u00afmj(\u02c6\u03b8j)B, and to control for false discoveries, we use the Cauchy combination\ntest (CCT) introduced by Liu and Xie (2020). The CCT combines the individual p-values of the\ntest statistics of the d null hypotheses Hj\n0 and provides a joint test for the null hypothesis H0\nthat accounts for various kinds of dependence between the test statistics (including cross-sectional\ndependence and serial correlation). The validity of the CCT has been derived under very weak\nassumptions such as the bivariate normality of each pair of test statistics (Liu and Xie, 2020) or\ncopula arguments (Long et al., 2023). We extend this result and prove that the CCT effectively\ncombines student test statistics.\nLet us collect the individual p-values associated with each null hypothesis Hj\n0 defined above.\nTheorem 2 introduces a new test of the joint null hypothesis H0 = T\nj\u2208D Hj\n0.\nTheorem 2. Let p \u2261(p1, . . . , pd)\u2032 be a vector of d p-values, where pj is the p-value corresponding\n11\n\nto the null hypothesis Hj\n0 for j \u2208D and\nCCT(p) \u22610.5 \u2212\u03c0\u22121 arctan\n\"\nd\nX\nj=1\n\u03c9j tan((0.5 \u2212pj)\u03c0)\n#\n,\n(14)\nbe the Cauchy combination test p-value of the joint null hypothesis H0 = T\nj\u2208D Hj\n0 with \u03c9 \u2261\n(\u03c91, . . . , \u03c9d)\u2032 a vector of weights independent of p with Pd\nj=1 \u03c9j = 1. Under Assumptions 1 and\n2, for any i \u0338= j, (pi, pj)\u2032 forms a bi-variate Student-t copula with B \u22121 degrees of freedom. Then,\nfor small \u03b1\u2217, it holds that, under the global null hypothesis H0, P[CCT(p) \u2264\u03b1\u2217] = \u03b1\u2217as T \u2192\u221e.\nTheorem 2 follows from the fact that the distribution of each pair (pi, pj) for i \u0338= j can be\nrepresented using a Student-t copula with B \u22121 degrees of freedom. In fact, we deduce from\nDemarta and McNeil (2005, Equation 13) that pi and pj are asymptotically tail independent when\nT \u2192\u221eas B = [T \u03b6] \u2192\u221efor \u03b6 > 0. That is, we also deduce from Chen and Yuen (2009) that\nlim\n\u03c4\u2192+\u221e\nP\nhPd\nj=1 \u03c9j tan((0.5 \u2212pj)\u03c0) > \u03c4\ni\nP[C > \u03c4]\n= 1 ,\n(15)\nwhere C follows a standard Cauchy random variable with \u03c9 independent of p.",
    "chunk_index": 10,
    "start_char": 22978,
    "end_char": 25522,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u0338= j can be\nrepresented using a Student-t copula with B \u22121 degrees of freedom. In fact, we deduce from\nDemarta and McNeil (2005, Equation 13) that pi and pj are asymptotically tail independent when\nT \u2192\u221eas B = [T \u03b6] \u2192\u221efor \u03b6 > 0. That is, we also deduce from Chen and Yuen (2009) that\nlim\n\u03c4\u2192+\u221e\nP\nhPd\nj=1 \u03c9j tan((0.5 \u2212pj)\u03c0) > \u03c4\ni\nP[C > \u03c4]\n= 1 ,\n(15)\nwhere C follows a standard Cauchy random variable with \u03c9 independent of p.\nRemark 2. When B \u2192\u221e, these p-values behave asymptotically as if drawn from a Gaussian\ncopula. In that case, we obtain the same result as in Liu and Xie (2020), and we also deduce from\nLong et al. (2023, Theorem 5) that (spanning tests based on) the CCT has power no less than that\nof the supremum test when T \u2192\u221e. Importantly, the tail independence between pi and pj for i \u0338= j\nalso holds when p-values are correlated and when B is relatively small (see, e.g., Demarta and\nMcNeil, 2005, Table 1). This is very important because in finite samples, B = [T \u03b6] can be pretty\nsmall for some \u03b6 > 0. Liu and Xie (2020) also show via Monte Carlo simulations that the CCT has\ngood finite sample properties when the test statistics follow a multivariate Student-t distribution\nwith four degrees of freedom. According to Ling (2023, Theorem 3.2), whenever pi and pj are tail\nindependent for all i \u0338= j, the CCT has asymptotic optimal power for large d.\nIt is worth mentioning that the CCT is an alternative to popular multiple testing corrections,\nsuch as statistical inequalities, including the so-called Bonferroni correction and its subsequent\nimprovements (Holm, 1979; Hommel, 1988; Hochberg, 1988) that are known to be overly conser-\nvative or those based on the extreme value theory, and in particular, the Gumbel distribution (e.g.,\n12\n\nLee and Mykland, 2008) that assumes the test-statistics are i.i.d. under the null hypothesis, which\nis overly restrictive.\n2.4. Practical Implementation\nThe idea of the batch-mean method is to eliminate the dependence between the subseries values\n(the statistics computed on the blocks) by calculating them on sufficiently large subsamples. The\nmore dependence there is, the more observations we need to take. But the smaller the number of\nblocks, the less powerful and size-distorted the test is because there is not enough variability to\nperform a good t-test\u2014indeed, a decent number of subseries values shall be used to calculate the\nvariance of their mean. This trade-off is limiting as the sample size is finite. Therefore, we propose\nto use a randomly-weighted batch-mean method. This method allows us to break the dependence\nbetween the subseries values artificially and between the observations that generate these subseries\nvalues. By breaking the covariation between the observations over time, we can take a reasonable\nnumber of blocks without being stressed by the strength of the dependence between the variables.",
    "chunk_index": 11,
    "start_char": 25101,
    "end_char": 27981,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "mean. This trade-off is limiting as the sample size is finite. Therefore, we propose\nto use a randomly-weighted batch-mean method. This method allows us to break the dependence\nbetween the subseries values artificially and between the observations that generate these subseries\nvalues. By breaking the covariation between the observations over time, we can take a reasonable\nnumber of blocks without being stressed by the strength of the dependence between the variables.\nMore specifically, we apply the batch-mean method on randomly-weighted empirical moments,\nthat is:\n\u02dcmj(\u02c6\u03b8j)b \u22611\nTb\nX\nt\u2208Ib\n\u02c6gj(rt, \u02c6\u03b8j)\u03bat, \u2200b = 1, . . . , B ,\n(16)\ninstead of (11), with \u03bat \u2261QL\nl=1 \u03bal,t and \u03bal,t \u223cN(1, 1) and where we set \u03bat \u22611 if L = 0. Again,\nthe intuition behind this approach is that using \u03bat in (16) kills the dependence in \u02c6gj(rt, \u02c6\u03b8j). In\nfact, under Assumption 2 and the global null hypothesis H0, \u02dcmj(\u02c6\u03b8j)b has mean 0 and variance\nV[ \u02dcmj(\u02c6\u03b8j)b] = V[\u02c6gj(rt, \u02c6\u03b8j)]/Tb as Cov[\u02c6gj(rt, \u02c6\u03b8j)\u03bat, \u02c6gj(rt\u2032, \u02c6\u03b8j)\u03bat\u2032] = 0 whenever t \u0338= t\u2032. Moreover,\nif Assumptions 1 and 2 hold for gj(rt, \u02c6\u03b8j)\u03bat, Theorem 1 holds when \u02c6gj(rt, \u02c6\u03b8j)\u03bat replaces \u02c6gj(rt, \u02c6\u03b8j).\nThe procedure is equivalent to a mixture of the multiplier bootstrap (Zhang et al., 2017) and\nthe batch-mean method (Carlstein, 1986). Our combination of both methods reduces the compu-\ntational cost of the bootstrap as it requires only one resampling. However, it preserves the intrinsic\nfeatures of the batch-mean procedure introduced above, as the latter is then used in a step to ap-\nproximate the distribution of the test statistic.\nWe now denote our test as BCS\u03bb\nL when testing H\u03bb\n0 , for \u03bb \u2208{\u03b1, \u03b4, {\u03b1, \u03b4}}. When L = 0, the\np-value of H\u03bb\n0 is obtained by using (14), where the individual p-values ptj,B depend on the statistics\ntj,B as described in (13). When L > 0, the statistics tj,B are obtained using the randomly-weighted\n13\n\nversions of (11) and (12). In the Monte Carlo simulation, we will consider three choices of L, that\nis, L = 0 and 2. Following Liu and Xie (2020), we set \u03c9j = 1/d in (14).\n3. Simulation Study\nThis section benchmarks our newly proposed MVS tests with the state-of-the-art methods pro-\nposed in the literature. We first present the alternative tests and then introduce the data-generating\nprocesses used in the simulation study. We finally present the simulation results.\n3.1. Competing Spanning Tests\nRecall Huberman and Kandel (1987)\u2019s linear regression model introduced in Table 1. Let\nY = XB + E, where Y is a T \u00d7 N matrix whose t-th row is r2,t, X is a T \u00d7 (K + 1) matrix\nwhose t-th row is (1, r\u2032\n1,t), B \u2261[\u03b1, \u03b2]\u2032, and E is a T \u00d7 N matrix whose t-th row is \u03b7\u2032\nt. Assume\nthat T \u2265N +K +1, X\u2032X is nonsingular, and that conditional on r1,t, the disturbances \u03b7t are i.i.d.\nmultivariate normal with zero mean and positive definite covariance matrix.\n3.1.1.",
    "chunk_index": 12,
    "start_char": 27510,
    "end_char": 30341,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "= XB + E, where Y is a T \u00d7 N matrix whose t-th row is r2,t, X is a T \u00d7 (K + 1) matrix\nwhose t-th row is (1, r\u2032\n1,t), B \u2261[\u03b1, \u03b2]\u2032, and E is a T \u00d7 N matrix whose t-th row is \u03b7\u2032\nt. Assume\nthat T \u2265N +K +1, X\u2032X is nonsingular, and that conditional on r1,t, the disturbances \u03b7t are i.i.d.\nmultivariate normal with zero mean and positive definite covariance matrix.\n3.1.1. Testing H\u03b1,\u03b4\n0\nHK Test. Define \u02c6a \u2261\u02c6\u00b5\u2032 \u02c6V\u22121 \u02c6\u00b5,\u02c6b \u2261\u02c6\u00b5\u2032 \u02c6V\u22121iN+K, \u02c6c \u2261i\u2032\nN+K \u02c6V\u22121iN+K, \u02c6d \u2261\u02c6a\u02c6c \u2212\u02c6b2, a1 \u2261\n\u02c6\u00b5\u2032\n1 \u02c6V\u22121\n11 \u02c6\u00b51, b1 \u2261\u02c6\u00b5\u2032\n1 \u02c6V\u22121\n11 iK, c1 \u2261i\u2032\nK \u02c6V\u22121\n11 iK, and \u02c6d1 \u2261\u02c6a1\u02c6c1 \u2212\u02c6b2\n1, where \u02c6\u00b5 \u2261\n1\nT\nPT\nt=1 rt,\n\u02c6V \u22611\nT\nPT\nt=1 (rt \u2212\u02c6\u00b5) (rt \u2212\u02c6\u00b5)\u2032, \u02c6\u00b51 \u22611\nT\nPT\nt=1 r1,t, and \u02c6V11 \u22611\nT\nPT\nt=1\n\u0000r1,t \u2212\u02c6\u00b51\n\u0001 \u0000r1,t \u2212\u02c6\u00b51\n\u0001\u2032.\nUnder the normality assumption, Huberman and Kandel (1987) define the following F-test statistic\nto test the MVS hypothesis H\u03b1,\u03b4\n0 ,\nHK \u2261\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u0010\n1\nU\n1\n2 \u22121\n\u0011 \u0000 T\u2212K\u2212N\nN\n\u0001\n\u223cF2N,2(T\u2212K\u2212N)\nif\nN \u22652\n\u0000 1\nU \u22121\n\u0001 \u0000 T\u2212K\u22121\n2\n\u0001\n\u223cF2,T\u2212K\u22121\nif\nN = 1,\n(17)\nwhere U \u2261\u02c6c1+ \u02c6d1\n\u02c6c+ \u02c6d . Note that HK is not applicable when T \u2212K \u2212N < 1.\nGL Test. Gungor and Luger (2016) show that, for K < T and N > 1, Fmax \u2261maxi=1,...N Fi\nwhere Fi denotes the HK test statistic applied to the ith equation in (4). The distribution of Fmax\nis obtained via simulations assuming that the data is stationary and that conditional on X, \u03b7t has\nthe same distribution as \u2212\u03b7t, that is, the reflexive symmetric assumption; see Gungor and Luger\n(2016, Section 3.1) for details. The authors argue that their testing procedure can be used to test\n14\n\nlinear restrictions in any multivariate linear regression model when K < T. However, we find that\ntheir testing procedure can be non-informative when K is moderately large but still lower than T.\nBCS\u03b1,\u03b4 Tests. We also consider two BCS tests, namely BCS\u03b1,\u03b4\n0\nand BCS\u03b1,\u03b4\n2\n.\n3.1.2. Testing H\u03b1\n0\nGRS. Recall Model (4). Denote \u02c6\u03b2\nc the constrained OLS estimator of \u03b2 when \u03b1 = 0N is imposed.\nDenote (\u02c6\u03b1\u2032, \u02c6\u03b2\n\u2032)\u2032 the standard OLS estimator of (\u03b1, \u03b2)\u2032. The GRS test statistic for H\u03b1\n0 is\nGRS \u2261(T \u2212N \u2212K)\nN\n \n|\u02c6\u03931|\n|\u02c6\u03932|\n\u22121\n!\n,\n(18)\nwhere\n\u02c6\u03931 \u22611\nT\nT\nX\nt=1\n\u0012\nr2,t \u2212\u02c6\u03b2\nc\u2032\nr1,t\n\u0013 \u0012\nr2,t \u2212\u02c6\u03b2\nc\u2032\nr1,t\n\u0013\u2032\n,\nand\n\u02c6\u03932 \u22611\nT\nT\nX\nt=1\n\u0010\nr2,t \u2212\u02c6\u03b1 \u2212\u02c6\u03b2\n\u2032r1,t\n\u0011 \u0010\nr2,t \u2212\u02c6\u03b1 \u2212\u02c6\u03b2\n\u2032r1,t\n\u0011\u2032\n.\nGibbons et al. (1989) show that GRS follows a FN,T\u2212N\u2212K under H\u03b1\n0 .\nF1 Test. To test H\u03b1\n0 , Kan and Zhou (2012) introduce the following F-statistic (that is similar to\nGibbons et al., 1989\u2019s statistic)\nF1 \u2261\n\u0012T \u2212K \u2212N\nN\n\u0013 \u0012\u02c6a \u2212\u02c6a1\n1 + \u02c6a1\n\u0013\n\u223cFN,T\u2212K\u2212N .",
    "chunk_index": 13,
    "start_char": 29977,
    "end_char": 32388,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u22121\n!\n,\n(18)\nwhere\n\u02c6\u03931 \u22611\nT\nT\nX\nt=1\n\u0012\nr2,t \u2212\u02c6\u03b2\nc\u2032\nr1,t\n\u0013 \u0012\nr2,t \u2212\u02c6\u03b2\nc\u2032\nr1,t\n\u0013\u2032\n,\nand\n\u02c6\u03932 \u22611\nT\nT\nX\nt=1\n\u0010\nr2,t \u2212\u02c6\u03b1 \u2212\u02c6\u03b2\n\u2032r1,t\n\u0011 \u0010\nr2,t \u2212\u02c6\u03b1 \u2212\u02c6\u03b2\n\u2032r1,t\n\u0011\u2032\n.\nGibbons et al. (1989) show that GRS follows a FN,T\u2212N\u2212K under H\u03b1\n0 .\nF1 Test. To test H\u03b1\n0 , Kan and Zhou (2012) introduce the following F-statistic (that is similar to\nGibbons et al., 1989\u2019s statistic)\nF1 \u2261\n\u0012T \u2212K \u2212N\nN\n\u0013 \u0012\u02c6a \u2212\u02c6a1\n1 + \u02c6a1\n\u0013\n\u223cFN,T\u2212K\u2212N .\n(19)\nBJ Test. Recall Britten-Jones (1999) model from Table 1. These authors also show that\nBJ \u2261T \u2212N \u2212K\nN\n(SSRR \u2212SSRu)\nSSRu\n\u223cFN,T\u2212N\u2212K\n(20)\nunder H\u03b1\n0 , where SSRu is the sum of squared residuals of the complete model and SSRR is the\nsum of squared residuals of the model under the restriction that b1 = 0K\u00d7N.\n15\n\nPY Test. Pesaran and Yamagata (2012) show, under some regularity conditions, that for Gaussian\nand non-Gaussian disturbances \u03b7t, if N grows at a sufficiently slower rate than T,\nPY \u2261\nN \u22121/2 PN\ni=1\n\u0000t2\ni \u2212\nv\nv\u22122\n\u0001\n\u0000v\nv\u22122\n\u0001 q\n2(v\u22121)\nv\u22124\n\u0002\n1 + (N \u22121)\u02c6\u03c12\u0003\nL\n\u2212\u2212\u2212\u2192\nT\u2192\u221eN(0, 1) ,\n(21)\nwhere ti is the Student-t statistic of the significance of \u03b1i in (4), v \u2261T \u2212K \u22121, and \u02c6\u03c12\nis a threshold estimator of the average squares of pairwise disturbance correlations given by\n\u02c6\u03c12 \u2261\n2\nN(N\u22121)\nPN\ni=2\nPi\u22121\nj=1 \u02c6\u03c12\nijI\n\u0002\nv\u02c6\u03c12\nij \u2265\u03b8N\n\u0003\n, where I denotes the indicator function, \u02c6\u03c1ij \u2261\u02c6gi/p\u02c6gi\u02c6gj\nand \u02c6gi is the i-th element on the diagonal of \u02c6G \u2261PT\nt=1 \u02c6\u03b7t\u02c6\u03b7\u2032\nt; recall that \u02c6\u03b7t are the OLS resid-\nuals from (4). Pesaran and Yamagata (2012) suggest selecting the threshold value as \u221a\u03b8N \u2261\n\u03a6\u22121 \u0010\n1 \u2212\n\u03b1\n2(N\u22121)\n\u0011\n, where \u03a6\u22121(\u00b7) is the standard normal quantile function. However, Gungor and\nLuger (2016) found that the test is misleading for general correlation structures. In this paper, we\nshow that the test is not robust to serial dependence in general.\nBCS\u03b1 Tests. We also consider two BCS tests, namely BCS\u03b1\n0 and BCS\u03b1\n2 .\n3.1.3. Testing H\u03b4\n0\nKM Test. Let SSR denote the sum of the squared residuals in the unrestricted model r1,1,t =\nc + PN+K\nj=2\nwj(r1,1,t \u2212rj,t) + et and SSRr be the sum of the squared residuals in the restricted\nregression r1,1,t = cr + PK\nj=2 wr\nj(r1,1,t \u2212rj,t) + er\nt. Let m \u2264N \u22121 be the number of linear\nindependent restrictions. Kempf and Memmel (2006) show that for Gaussian disturbance et\nF \u2261T \u2212N \u2212K\nN\n\u0012SSRr\nSSR \u22121\n\u0013\n\u223cFN,T\u2212N\u2212K .\nF2 Test. Kan and Zhou (2012) shows that for Gaussian disturbance \u03b7t\nF2 \u2261\n\u0012T \u2212K \u2212N + 1\nN\n\u0013 \" \n\u02c6c + \u02c6d\n\u02c6c1 + \u02c6d1\n! \u00121 + \u02c6a1\n1 + \u02c6a\n\u0013\n\u22121\n#\n\u223cFN,T\u2212K\u2212N+1.\nBCS\u03b4 Tests. We also consider two BCS tests, namely BCS\u03b4\n0 and BCS\u03b4\n2.\n16",
    "chunk_index": 14,
    "start_char": 31988,
    "end_char": 34460,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u2212rj,t) + er\nt. Let m \u2264N \u22121 be the number of linear\nindependent restrictions. Kempf and Memmel (2006) show that for Gaussian disturbance et\nF \u2261T \u2212N \u2212K\nN\n\u0012SSRr\nSSR \u22121\n\u0013\n\u223cFN,T\u2212N\u2212K .\nF2 Test. Kan and Zhou (2012) shows that for Gaussian disturbance \u03b7t\nF2 \u2261\n\u0012T \u2212K \u2212N + 1\nN\n\u0013 \" \n\u02c6c + \u02c6d\n\u02c6c1 + \u02c6d1\n! \u00121 + \u02c6a1\n1 + \u02c6a\n\u0013\n\u22121\n#\n\u223cFN,T\u2212K\u2212N+1.\nBCS\u03b4 Tests. We also consider two BCS tests, namely BCS\u03b4\n0 and BCS\u03b4\n2.\n16\n\n3.2. The Setup\nLet us assume that the benchmark and test assets are generated by a DGP belonging to the class\nof Darolles et al. (2018)\u2019s Cholesky-GARCH process augmented with an AR component. Inspired\nby Pedersen (2020), we use univariate AR-GARCH processes to reproduce well-known stylized\nfacts on stock returns. Namely, we let\nr1,t = \u03d5r1,t\u22121 + R1/2\n\u00af\u03c11,1 D1/2\n1,t \u03bd1,t\n|\n{z\n}\nG1,t\n(22)\nand\nr2,t = \u03b1 + \u03b2r1,t + \u03b7t,\n(23)\n\u03b7t = \u03d5\u03b7t\u22121 + R1/2\n\u00af\u03c12,2 D1/2\n2,t \u03bd2,t\n|\n{z\n}\nG2,t\n,\n(24)\nwhere \u03d5 \u2208{0, 0.2}, \u03b2i,j = 1 for j = 2, . . . , K, G1,t \u2261(g1,j,t), G2,t \u2261(g2,i,t), D1/2\n1,t \u2261(d1,j,t) and\nD1/2\n2,t \u2261(d2,i,t) are two diagonal matrices of size K and N respectively. R1/2\n\u00af\u03c1l denotes the Cholesky\nfactor of R\u00af\u03c1l \u2261\n\u0010\n\u00af\u03c1|i\u2212j|\nl\n\u0011\n, a Toeplitz correlation matrix with coefficient \u00af\u03c1l = 0.8 for l = 1 and \u00af\u03c1l =\n0.5 for l = 2. For the specifications with GARCH effects, d1,j,t \u2261(0.1+0.1g2\n1,j,t\u22121+0.8d2\n1,j,t\u22121)1/2\nfor j = 1, . . . , K and d2,i,t \u2261(0.1 + 0.1g2\n2,i,t\u22121 + 0.8d2\n2,i,t\u22121)1/2 for i = 1, . . . , N so that the\nelements of G1,t and G2,t follow GARCH(1,1) specifications. In absence of GARCH effects,\nd1,j,t = d2,i,t = 1 for any i and j so that the elements of G1,t and G2,t are homoscedastic. The\ninnovations \u03bd1,t \u2261(\u03bd1,t, . . . , \u03bdK,t)\u2032 and \u03bd2,t \u2261(\u03bdK+1,t, . . . , \u03bdK+N,t)\u2032 are i.i.d. random variables\nwith mean 0 and will be defined below. Thus, both benchmark and test assets can display serial\ncorrelation, correlated innovations, and GARCH effects.\nWe use this general DGP to compare the finite sample properties of the F, PY, and GL tests to\nthe BCS tests and later present the benefit of using the randomly-weighted batch-mean method. To\nidentify the possible cause of the underperformance of these tests, we simulate nine specific DGPs\ncorresponding to small variations of the general DGP (22)-(24).\n\u2022 DGP1 (i.i.d. N). \u03d5 = 0, d1,j,t = d2,i,t = 1, \u03bd1,t\ni.i.d.\n\u223cN(0, IK) and \u03bd2,t\ni.i.d.\n\u223cN(0, IN).\n\u2022 DGP2 (i.i.d ST). Same as DGP1, but the elements of the innovations \u03bd1,t and \u03bd2,t follow\n17\n\nindependent standardized Student-t distributions with 5 degrees of freedom.\n\u2022 DGP3 (i.i.d SKST). Same as DGP1, but the elements of the innovations \u03bd1,t and \u03bd2,t follow\nindependent standardized skewed Student-t distribution originally proposed",
    "chunk_index": 15,
    "start_char": 34060,
    "end_char": 36698,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "to small variations of the general DGP (22)-(24).\n\u2022 DGP1 (i.i.d. N). \u03d5 = 0, d1,j,t = d2,i,t = 1, \u03bd1,t\ni.i.d.\n\u223cN(0, IK) and \u03bd2,t\ni.i.d.\n\u223cN(0, IN).\n\u2022 DGP2 (i.i.d ST). Same as DGP1, but the elements of the innovations \u03bd1,t and \u03bd2,t follow\n17\n\nindependent standardized Student-t distributions with 5 degrees of freedom.\n\u2022 DGP3 (i.i.d SKST). Same as DGP1, but the elements of the innovations \u03bd1,t and \u03bd2,t follow\nindependent standardized skewed Student-t distribution originally proposed by Fern\u00e1ndez\nand Steel (1998) and modified by Giot and Laurent (2003) to have mean zero and unit vari-\nance, with asymmetry parameter 0.9 and 4 degrees of freedom.\n\u2022 DGP4 (GARCH N), DGP5 (GARCH ST) and DGP6 (GARCH SKST). Respectively the\nsame as DGP1, DGP2 and DGP3 but G1,t and G2,t have GARCH effects.\n\u2022 DGP7 (AR-N), DGP8 (AR-ST) and DGP9 (AR-SKST). Respectively the same as DGP1,\nDGP2 and DGP3 but \u03d5 = 0.2 so that r1,t and r2,t have serial correlation.\n\u2022 DGP10 (AR-GARCH N), DGP11 (AR-GARCH ST), DGP12 (AR-GARCH SKST). Re-\nspectively the same as DGP4, DGP5 and DGP6 but \u03d5 = 0.2 so that r1,t and r2,t have serial\ncorrelation and have GARCH effects.\nIn the next sections, we study the size and power of various tests for the null hypotheses H\u03b1,\u03b4\n0\n,\nH\u03b1\n0 , and H\u03b4\n0 . The simulation is performed 500 times for a sample of T = 250 observations and\nvarious values of N and K.\n3.3. Size Results\nTo study the size of the tests, we simulate data under H\u03b1,\u03b4\n0\nby setting first \u03b1 = 0N in (24).\nRecall that \u03b4 \u2261iN \u2212\u03b2iK and note that rK+i,t = r1,t + \u03b1i + \u03b4ir1,t + PK\nj=2 \u03b2i,j(rj,t \u2212r1,t) + \u03b7i,t,\n\u2200i = 1, . . . , N. Therefore, we have \u03b2i,1 = 1 \u2212\u03b4i \u2212PK\nj=2 \u03b2i,j. To set \u03b4 = 0N, we set \u03b2i,1 =\n1 \u2212PK\nj=2 \u03b2i,j for i = 1, . . . , N, where \u03b2i,j = 1 for j > 1.\n3.3.1. Size Results for H\u03b1,\u03b4\n0\nThe empirical size of the HZ, GL and our BCS\u03b1,\u03b4\nL\ntest with L = 0 and 2 for the null hypothesis\nH\u03b1,\u03b4\n0\nis reported in Table 2 for DGP1\u2013DGP12. The nominal size of each test is 5%. For the\nreadability of the results, empirical sizes between 3% and 7% are highlighted in bold. Several\ncomments are in order.\n\u2022 HK is robust to fat-tailed and even skewed innovations but is largely oversized in the pres-\nence of serial correlation, with rejection frequencies sometimes approaching 70% when N\n18\n\nis 100 and K is small. Furthermore, the test is not applicable when N = 400, for a sample\nsize of T = 250.\n\u2022 GL is largely undersized for most configurations, even in the i.i.d. Gaussian case.\n\u2022 When N and K are relatively small, our proposed test BCS\u03b1,\u03b4\n0\nhas a decent empirical size\nbut is oversized when N and K are greater than 10 for most DGPs.",
    "chunk_index": 16,
    "start_char": 36216,
    "end_char": 38815,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "in the pres-\nence of serial correlation, with rejection frequencies sometimes approaching 70% when N\n18\n\nis 100 and K is small. Furthermore, the test is not applicable when N = 400, for a sample\nsize of T = 250.\n\u2022 GL is largely undersized for most configurations, even in the i.i.d. Gaussian case.\n\u2022 When N and K are relatively small, our proposed test BCS\u03b1,\u03b4\n0\nhas a decent empirical size\nbut is oversized when N and K are greater than 10 for most DGPs. This is especially true in\nthe presence of serial correlation.\n\u2022 Interestingly, the BCS\u03b1,\u03b4\n2\ntest improves considerably. Indeed, the empirical sizes are close to\n5% for most DGPs, even for very large values of N and K.\nOverall, the test with the best empirical size is BCS\u03b1,\u03b4\n2\n.\n19\n\nTable 2: Empirical Size Results for H\u03b1,\u03b4\n0\nThe table reports the empirical size (over 500 replications) for five tests for the global null hypothesis of\nMVS under twelve DGPs and for four values of K and N (i.e., the number of benchmark and test assets,\nrespectively). Values in bold are between 3% and 7%. A dash indicates that the test cannot be applied as\nN > T \u2212K \u22121.\nHK\nGL\nBCS\u03b1,\u03b4\n0\nBCS\u03b1,\u03b4\n2\nK\u2193N\u2192\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\nDGP1\ni.i.d\nN\n2\n5.2\n4.8\n5.2\n6.2\n-\n1.8\n1.4\n0.6\n0.6\n1.0\n6.0\n5.0\n6.8\n5.8\n5.4\n4.8\n6.0\n5.0\n2.6\n4.2\n10\n6.2\n5.6\n3.8\n5.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n5.4\n5.2\n6.2\n6.6\n7.4\n5.8\n4.4\n3.6\n5.4\n3.0\n50\n6.2\n6.8\n5.6\n6.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n6.4\n10.0\n8.0\n10.0\n10.8\n5.2\n3.0\n4.2\n4.2\n3.2\n100\n5.0\n4.0\n5.2\n6.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n16.0\n18.6\n21.4\n25.8\n5.8\n4.4\n3.8\n6.4\n6.4\nDGP2\ni.i.d\nST\n2\n4.2\n3.4\n5.4\n5.0\n-\n2.6\n1.2\n2.6\n2.0\n2.2\n5.4\n7.2\n7.2\n5.2\n5.2\n4.0\n4.4\n3.6\n2.6\n3.2\n10\n4.0\n5.0\n3.4\n4.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n4.6\n6.2\n6.6\n6.8\n7.0\n5.2\n3.6\n4.0\n4.6\n4.0\n50\n4.0\n5.8\n5.0\n5.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.8\n11.2\n10.4\n9.8\n11.4\n5.2\n3.8\n5.2\n4.6\n3.4\n100\n5.6\n5.6\n4.6\n6.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.4\n19.8\n17.8\n21.6\n27.0\n5.0\n6.2\n5.6\n5.6\n5.4\nDGP3\ni.i.d\nSKST\n2\n5.6\n5.8\n3.4\n5.8\n-\n1.6\n2.2\n0.8\n1.2\n1.0\n6.8\n5.4\n4.0\n4.8\n7.4\n4.0\n2.6\n4.0\n3.4\n2.8\n10\n5.2\n4.6\n6.4\n5.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n7.4\n6.6\n5.0\n5.8\n5.8\n5.8\n3.6\n2.4\n2.0\n3.4\n50\n5.6\n7.0\n6.0\n3.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n11.4\n12.8\n13.4\n12.0\n5.0\n3.8\n5.6\n4.2\n4.2\n100\n4.6\n3.4\n4.0\n4.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.2\n16.0\n23.2\n24.0\n31.2\n7.2\n4.2\n5.0\n5.0\n5.2\nDGP4\nGARCH\nN\n2\n5.2",
    "chunk_index": 17,
    "start_char": 38361,
    "end_char": 40622,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "1.2\n1.0\n6.8\n5.4\n4.0\n4.8\n7.4\n4.0\n2.6\n4.0\n3.4\n2.8\n10\n5.2\n4.6\n6.4\n5.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n7.4\n6.6\n5.0\n5.8\n5.8\n5.8\n3.6\n2.4\n2.0\n3.4\n50\n5.6\n7.0\n6.0\n3.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n11.4\n12.8\n13.4\n12.0\n5.0\n3.8\n5.6\n4.2\n4.2\n100\n4.6\n3.4\n4.0\n4.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.2\n16.0\n23.2\n24.0\n31.2\n7.2\n4.2\n5.0\n5.0\n5.2\nDGP4\nGARCH\nN\n2\n5.2\n5.0\n4.8\n5.0\n-\n2.2\n1.0\n1.2\n1.4\n1.2\n4.8\n3.8\n5.0\n5.0\n5.6\n4.4\n4.4\n3.6\n3.6\n2.8\n10\n4.4\n3.6\n4.0\n5.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n4.8\n4.8\n6.0\n4.4\n4.8\n4.0\n3.0\n3.0\n4.8\n4.8\n50\n4.8\n5.6\n3.8\n7.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n6.8\n8.6\n10.0\n9.0\n8.6\n4.2\n6.6\n5.8\n4.8\n4.6\n100\n5.4\n4.8\n6.8\n7.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n14.2\n17.0\n24.2\n24.0\n34.2\n5.0\n6.0\n5.2\n4.0\n7.0\nDGP5\nGARCH\nST\n2\n3.6\n4.8\n5.0\n6.4\n-\n1.6\n1.6\n1.6\n2.0\n1.4\n9.4\n8.0\n8.6\n8.4\n6.4\n4.0\n4.4\n5.0\n3.4\n2.8\n10\n5.2\n4.4\n5.0\n5.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n6.8\n8.4\n8.2\n9.2\n5.8\n4.0\n5.2\n2.6\n5.4\n2.4\n50\n3.8\n4.2\n5.2\n6.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n7.6\n12.6\n14.4\n13.2\n12.2\n5.4\n4.2\n5.6\n4.0\n4.2\n100\n5.0\n4.6\n4.4\n6.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.4\n19.2\n23.6\n28.2\n31.0\n8.4\n7.2\n5.4\n5.2\n5.8\nDGP6\nGARCH\nSKST\n2\n5.0\n4.8\n5.0\n3.6\n-\n2.4\n1.0\n0.8\n0.6\n1.8\n4.6\n5.6\n7.6\n6.4\n8.6\n3.4\n3.4\n4.6\n1.4\n2.6\n10\n5.4\n3.4\n3.0\n3.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n6.2\n6.6\n8.2\n7.6\n8.8\n5.6\n3.2\n4.2\n3.4\n2.6\n50\n3.2\n4.6\n4.6\n3.6\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n10.2\n10.2\n11.2\n12.2\n13.6\n4.4\n5.0\n3.6\n2.8\n4.2\n100\n3.8\n4.4\n4.2\n4.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.0\n20.8\n25.0\n28.0\n33.8\n6.6\n6.4\n4.4\n6.8\n6.0\nDGP7\nAR\nN\n2\n13.0\n21.6\n49.4\n64.8\n-\n7.4\n7.4\n8.8\n9.2\n15.2\n6.2\n5.8\n5.8\n4.8\n5.0\n5.0\n3.8\n3.2\n2.4\n3.6\n10\n11.0\n17.4\n45.0\n59.2\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n4.4\n2.8\n4.4\n6.0\n4.0\n4.2\n4.0\n4.0\n3.4\n4.0\n50\n8.0\n15.6\n36.0\n42.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.8\n12.0\n14.6\n14.2\n16.2\n5.4\n3.2\n5.4\n4.8\n4.4\n100\n10.8\n11.2\n24.0\n22.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n16.6\n18.2\n30.2\n34.2\n43.2\n7.0\n7.6\n8.2\n8.8\n4.8\nDGP8\nAR\nST\n2\n11.8\n20.2\n52.0\n62.4\n-\n5.6\n5.8\n8.6\n10.8\n12.8\n5.6\n5.6\n7.2\n7.2\n8.4\n5.0\n3.0\n4.4\n3.8\n1.2\n10\n11.6\n20.0\n51.2\n61.4\n-\n0.2\n0.2\n0.0\n0.0\n0.0\n6.2\n5.2\n6.8\n4.8\n7.8\n4.6\n3.2\n4.6\n4.6\n3.0\n50\n10.8\n17.2\n35.8\n44.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n9.6\n10.6\n13.0\n14.2\n14.4\n5.0\n4.6\n5.0\n4.2\n3.6\n100\n7.4\n12.0\n23.2\n25.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0",
    "chunk_index": 18,
    "start_char": 40303,
    "end_char": 42318,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "8.2\n8.8\n4.8\nDGP8\nAR\nST\n2\n11.8\n20.2\n52.0\n62.4\n-\n5.6\n5.8\n8.6\n10.8\n12.8\n5.6\n5.6\n7.2\n7.2\n8.4\n5.0\n3.0\n4.4\n3.8\n1.2\n10\n11.6\n20.0\n51.2\n61.4\n-\n0.2\n0.2\n0.0\n0.0\n0.0\n6.2\n5.2\n6.8\n4.8\n7.8\n4.6\n3.2\n4.6\n4.6\n3.0\n50\n10.8\n17.2\n35.8\n44.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n9.6\n10.6\n13.0\n14.2\n14.4\n5.0\n4.6\n5.0\n4.2\n3.6\n100\n7.4\n12.0\n23.2\n25.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n19.0\n22.6\n33.4\n37.2\n49.0\n5.4\n7.0\n8.8\n6.2\n6.6\nDGP9\nAR\nSKST\n2\n13.8\n26.0\n52.6\n66.2\n-\n6.4\n6.6\n8.8\n11.4\n10.8\n7.0\n6.6\n5.4\n5.2\n5.4\n5.4\n4.0\n2.4\n2.8\n3.8\n10\n8.4\n17.2\n48.4\n64.2\n-\n0.2\n0.0\n0.0\n0.0\n0.0\n7.2\n5.8\n5.8\n7.2\n8.4\n3.6\n3.0\n4.2\n2.4\n3.6\n50\n9.2\n18.8\n39.0\n43.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n9.2\n11.4\n13.6\n12.8\n17.6\n5.2\n5.4\n6.0\n4.0\n3.4\n100\n8.6\n14.4\n24.8\n22.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n17.4\n23.2\n31.6\n37.8\n49.6\n7.6\n8.2\n8.4\n7.8\n6.6\nDGP10\nAR-GARCH\nN\n2\n10.6\n21.6\n49.6\n66.0\n-\n5.8\n7.0\n8.2\n10.8\n13.4\n4.4\n5.2\n5.6\n6.0\n7.2\n3.2\n3.4\n4.8\n2.8\n3.2\n10\n10.8\n18.8\n48.2\n62.0\n-\n0.2\n0.0\n0.2\n0.0\n0.2\n6.2\n5.6\n6.2\n7.6\n6.2\n3.2\n2.8\n2.8\n4.2\n4.2\n50\n10.8\n18.8\n39.0\n47.0\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n9.0\n11.2\n13.8\n13.0\n14.6\n3.6\n5.2\n5.8\n5.0\n5.4\n100\n8.0\n10.8\n26.6\n24.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n15.8\n20.8\n33.2\n35.2\n50.0\n6.4\n6.8\n6.8\n5.0\n5.6\nDGP11\nAR-GARCH\nST\n2\n10.8\n23.2\n49.0\n67.8\n-\n6.2\n7.8\n10.4\n9.2\n9.4\n9.2\n9.4\n7.0\n8.6\n7.2\n5.6\n3.4\n3.2\n3.2\n6.2\n10\n11.8\n21.6\n47.6\n65.0\n-\n0.2\n0.0\n0.0\n0.0\n0.0\n7.8\n8.6\n7.4\n11.2\n10.0\n5.0\n4.4\n2.6\n3.2\n2.2\n50\n9.0\n17.2\n42.2\n45.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n8.6\n13.4\n14.8\n18.2\n19.8\n5.4\n5.0\n5.4\n6.6\n3.0\n100\n8.0\n14.0\n23.8\n20.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n19.4\n24.4\n34.0\n41.2\n52.8\n7.6\n6.6\n5.8\n5.4\n4.8\nDGP12\nAR-GARCH\nSKST\n2\n10.0\n21.0\n47.6\n63.2\n-\n5.0\n6.4\n10.0\n9.2\n10.4\n4.6\n5.6\n7.4\n8.4\n9.4\n2.4\n3.4\n3.4\n2.8\n4.2\n10\n11.4\n22.0\n47.8\n60.8\n-\n0.0\n0.0\n0.0\n0.0\n0.2\n6.0\n6.0\n7.4\n8.4\n9.8\n4.6\n3.8\n4.4\n4.0\n4.4\n50\n9.6\n15.8\n35.8\n42.4\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n12.8\n14.0\n16.2\n16.0\n18.8\n3.8\n5.2\n7.0\n5.6\n6.2\n100\n7.8\n16.2\n20.0\n21.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n18.4\n26.2\n38.8\n42.8\n52.6\n6.0\n5.2\n5.8\n6.0\n6.0\n3.3.2. Size Results for H\u03b1\n0\nSimulation results for the maximum Sharpe portfolio spanning H\u03b1\n0 are reported in Table 3.\nWhile GRS, F1, BJ, and PY have an empirical size close to the nominal size of 5% for the first\nsix DGPs, they are all largely oversized in the presence of serial correlation, with empirical sizes\nsometimes approaching 80%. Note also that the first three tests are not applicable when N >\n20",
    "chunk_index": 19,
    "start_char": 41991,
    "end_char": 44240,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "20.0\n21.8\n-\n0.0\n0.0\n0.0\n0.0\n0.0\n18.4\n26.2\n38.8\n42.8\n52.6\n6.0\n5.2\n5.8\n6.0\n6.0\n3.3.2. Size Results for H\u03b1\n0\nSimulation results for the maximum Sharpe portfolio spanning H\u03b1\n0 are reported in Table 3.\nWhile GRS, F1, BJ, and PY have an empirical size close to the nominal size of 5% for the first\nsix DGPs, they are all largely oversized in the presence of serial correlation, with empirical sizes\nsometimes approaching 80%. Note also that the first three tests are not applicable when N >\n20\n\nT \u2212K \u22121, e.g., when N = 400 and T = 250. As for the joint test H\u03b1,\u03b4\n0\n, GL is largely undersized\nfor most configurations, even in the i.i.d. Gaussian case. The results of our proposed tests are\nsatisfactory for all configurations and DGPs, especially BCS\u03b1\n2 .\nTable 3: Size Results for H\u03b1\n0\nThe table reports the empirical size (over 500 replications) for eight tests for the maximum Sharpe portfolio\nspanning under twelve DGPs and for four values of K and N (i.e., the number of benchmark and test assets,\nrespectively). Values in bold are between 3% and 7%. A dash indicates that the test cannot be applied as\nN > T \u2212K \u22121.\nGRS\nF1\nBJ\nPY\nGL\nBCS\u03b1,\u03b4\n0\nBCS\u03b1,\u03b4\n2\nK\u2193N\u2192\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\nDGP1\ni.i.d.\nN\n2\n5.2\n4.4\n5.4\n5.4\n-\n5.2\n4.2\n5.2\n5.4\n-\n5.0\n3.8\n4.8\n5.6\n-\n5.8\n6.6\n6.2\n5.8\n6.0\n1.0\n0.2\n0.0\n0.0\n0.2\n5.2\n4.8\n6.4\n6.6\n4.8\n2.8\n4.0\n2.4\n4.4\n4.0\n10\n5.2\n4.2\n4.6\n5.0\n-\n5.2\n4.2\n4.6\n5.0\n-\n5.2\n4.4\n4.6\n5.0\n-\n6.8\n7.0\n6.4\n5.4\n6.2\n0.0\n0.0\n0.0\n0.0\n0.0\n5.2\n5.4\n8.2\n7.0\n5.8\n4.0\n4.2\n4.2\n3.2\n2.4\n50\n5.8\n6.2\n5.4\n4.2\n-\n5.8\n5.6\n5.0\n4.2\n-\n6.0\n6.2\n5.0\n4.0\n-\n6.4\n7.4\n6.4\n4.2\n4.8\n0.0\n0.0\n0.0\n0.0\n0.0\n8.8\n8.8\n9.8\n10.0\n10.2\n4.6\n4.0\n4.6\n3.0\n4.0\n100\n4.6\n4.0\n4.2\n4.2\n-\n4.6\n4.0\n4.2\n4.2\n-\n4.4\n4.4\n4.2\n4.4\n-\n5.6\n7.6\n6.2\n5.8\n3.8\n0.0\n0.0\n0.0\n0.0\n0.0\n8.8\n14.4\n20.0\n19.0\n21.8\n6.0\n5.8\n6.8\n7.6\n4.4\nDGP2\ni.i.d.\nST\n2\n4.2\n4.2\n5.8\n5.6\n-\n4.2\n4.0\n5.4\n4.8\n-\n4.4\n4.0\n5.8\n5.8\n-\n6.8\n6.8\n7.0\n5.6\n6.2\n0.8\n0.4\n0.6\n0.2\n0.2\n6.2\n8.2\n8.0\n6.0\n6.4\n4.0\n3.8\n2.8\n3.0\n3.4\n10\n4.6\n4.4\n5.6\n5.6\n-\n4.4\n4.2\n5.6\n5.0\n-\n4.2\n4.8\n6.2\n5.8\n-\n6.8\n7.6\n8.4\n6.6\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.4\n6.0\n7.2\n6.0\n5.2\n4.2\n4.4\n4.2\n4.6\n4.0\n50\n3.2\n5.6\n4.4\n7.4\n-\n3.2\n5.6\n4.4\n7.2\n-\n3.4\n5.8\n5.0\n7.4\n-\n5.8\n7.0\n4.0\n5.6\n5.6\n0.0\n0.0",
    "chunk_index": 20,
    "start_char": 43753,
    "end_char": 45958,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "-\n6.8\n6.8\n7.0\n5.6\n6.2\n0.8\n0.4\n0.6\n0.2\n0.2\n6.2\n8.2\n8.0\n6.0\n6.4\n4.0\n3.8\n2.8\n3.0\n3.4\n10\n4.6\n4.4\n5.6\n5.6\n-\n4.4\n4.2\n5.6\n5.0\n-\n4.2\n4.8\n6.2\n5.8\n-\n6.8\n7.6\n8.4\n6.6\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.4\n6.0\n7.2\n6.0\n5.2\n4.2\n4.4\n4.2\n4.6\n4.0\n50\n3.2\n5.6\n4.4\n7.4\n-\n3.2\n5.6\n4.4\n7.2\n-\n3.4\n5.8\n5.0\n7.4\n-\n5.8\n7.0\n4.0\n5.6\n5.6\n0.0\n0.0\n0.0\n0.0\n0.0\n7.0\n9.4\n9.0\n9.8\n9.6\n5.6\n4.6\n4.0\n3.8\n3.4\n100\n3.4\n4.8\n5.6\n5.8\n-\n3.4\n4.8\n5.4\n5.4\n-\n3.6\n4.4\n6.0\n5.6\n-\n6.2\n7.4\n9.0\n6.0\n5.6\n0.0\n0.0\n0.0\n0.0\n0.0\n13.4\n17.8\n20.8\n19.0\n22.4\n5.8\n5.8\n4.4\n6.2\n6.2\nDGP3\ni.i.d.\nSKST\n2\n4.6\n5.6\n4.6\n5.6\n-\n4.6\n5.4\n4.6\n5.2\n-\n4.4\n5.6\n4.4\n5.8\n-\n7.0\n6.4\n3.8\n4.8\n4.6\n0.2\n0.0\n0.2\n0.2\n0.4\n5.2\n5.4\n5.4\n5.4\n7.6\n4.4\n4.4\n3.8\n4.6\n3.2\n10\n4.8\n5.8\n4.4\n5.4\n-\n4.8\n5.8\n4.4\n5.4\n-\n4.8\n6.2\n4.4\n5.4\n-\n7.6\n7.4\n5.0\n5.2\n3.2\n0.0\n0.0\n0.0\n0.0\n0.0\n6.4\n7.8\n6.0\n3.8\n6.2\n6.2\n4.0\n3.4\n2.4\n4.0\n50\n4.6\n5.4\n4.6\n6.2\n-\n4.6\n5.4\n4.6\n6.2\n-\n4.4\n5.8\n4.4\n6.2\n-\n6.0\n8.4\n6.4\n6.8\n5.4\n0.0\n0.0\n0.0\n0.0\n0.0\n7.2\n10.0\n11.2\n12.0\n9.4\n4.4\n3.2\n4.2\n5.4\n3.6\n100\n4.4\n6.2\n4.4\n3.4\n-\n4.4\n6.2\n4.4\n3.0\n-\n3.8\n5.4\n5.0\n4.4\n-\n6.0\n6.8\n6.0\n5.4\n5.4\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n15.4\n19.8\n20.8\n27.4\n5.0\n3.6\n6.4\n5.4\n7.0\nDGP4\nGARCH\nN\n2\n5.0\n5.6\n4.0\n4.0\n-\n5.0\n5.2\n4.0\n4.0\n-\n5.6\n5.4\n4.0\n4.0\n-\n6.2\n5.4\n4.8\n4.0\n4.8\n0.4\n0.4\n0.4\n0.4\n0.6\n3.6\n3.4\n4.8\n4.4\n5.0\n6.4\n3.0\n3.4\n3.2\n3.2\n10\n5.2\n4.0\n5.0\n3.0\n-\n5.2\n3.8\n5.0\n3.0\n-\n5.0\n4.4\n4.6\n3.2\n-\n6.0\n6.0\n6.4\n5.8\n4.8\n0.0\n0.0\n0.0\n0.0\n0.0\n4.4\n5.8\n6.2\n4.6\n5.0\n5.2\n3.2\n3.6\n2.4\n5.0\n50\n5.2\n6.4\n3.8\n5.2\n-\n4.8\n6.2\n3.8\n5.0\n-\n5.0\n5.2\n4.2\n5.6\n-\n7.0\n8.0\n5.2\n7.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n8.8\n7.0\n8.0\n8.2\n4.6\n4.2\n2.6\n4.8\n3.2\n100\n4.8\n4.6\n8.6\n6.4\n-\n4.8\n4.6\n8.0\n6.4\n-\n5.0\n4.8\n8.2\n6.4\n-\n8.0\n6.8\n8.4\n8.0\n6.6\n0.0\n0.0\n0.0\n0.0\n0.0\n12.2\n13.0\n18.0\n21.0\n27.2\n7.4\n5.8\n5.4\n6.6\n4.4\nDGP5\nGARCH\nST\n2\n5.0\n3.6\n6.0\n5.2\n-\n5.0\n3.4\n5.8\n5.2\n-\n4.8\n4.4\n5.6\n5.0\n-\n7.4\n7.0\n8.0\n7.4\n5.4\n1.2\n0.0\n0.2\n0.2\n0.2\n7.8\n5.4\n6.8\n6.0\n6.6\n4.4\n2.6\n4.4\n3.0\n4.0\n10\n4.2\n4.8\n5.2\n5.6\n-\n4.2\n4.8\n4.8\n5.6\n-\n4.6\n4.8\n4.8\n5.4\n-\n6.0\n5.8\n7.0\n6.2\n5.2\n0.0\n0.0\n0.0\n0.0\n0.0\n5.8\n9.6\n6.6\n6.6\n6.0\n5.0\n3.8\n4.6\n4.6\n4.2\n50\n3.4",
    "chunk_index": 21,
    "start_char": 45655,
    "end_char": 47586,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "5.4\n6.6\n4.4\nDGP5\nGARCH\nST\n2\n5.0\n3.6\n6.0\n5.2\n-\n5.0\n3.4\n5.8\n5.2\n-\n4.8\n4.4\n5.6\n5.0\n-\n7.4\n7.0\n8.0\n7.4\n5.4\n1.2\n0.0\n0.2\n0.2\n0.2\n7.8\n5.4\n6.8\n6.0\n6.6\n4.4\n2.6\n4.4\n3.0\n4.0\n10\n4.2\n4.8\n5.2\n5.6\n-\n4.2\n4.8\n4.8\n5.6\n-\n4.6\n4.8\n4.8\n5.4\n-\n6.0\n5.8\n7.0\n6.2\n5.2\n0.0\n0.0\n0.0\n0.0\n0.0\n5.8\n9.6\n6.6\n6.6\n6.0\n5.0\n3.8\n4.6\n4.6\n4.2\n50\n3.4\n4.2\n5.8\n3.8\n-\n3.2\n4.2\n5.6\n3.8\n-\n3.6\n4.0\n5.4\n4.2\n-\n6.6\n5.4\n6.8\n5.6\n5.4\n0.0\n0.0\n0.0\n0.0\n0.0\n8.2\n9.8\n12.6\n10.8\n11.4\n5.4\n7.6\n3.6\n3.6\n4.0\n100\n5.2\n5.6\n4.0\n3.8\n-\n5.2\n5.6\n4.0\n3.8\n-\n5.6\n5.4\n4.4\n4.0\n-\n6.6\n5.8\n5.4\n5.2\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12.6\n16.2\n21.6\n24.2\n28.4\n3.8\n5.2\n5.8\n4.6\n6.0\nDGP6\nGARCH\nSKST\n2\n4.0\n4.2\n5.6\n4.0\n-\n4.0\n4.2\n5.6\n3.8\n-\n4.2\n4.0\n5.4\n3.8\n-\n7.2\n7.2\n8.2\n5.4\n4.8\n0.8\n0.2\n0.2\n0.0\n0.2\n5.8\n6.4\n6.0\n7.4\n10.2\n5.2\n4.2\n4.6\n3.2\n5.0\n10\n6.0\n5.6\n5.4\n3.2\n-\n6.0\n5.4\n5.4\n3.2\n-\n6.2\n5.6\n5.4\n4.0\n-\n5.8\n8.2\n6.4\n5.0\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.4\n6.6\n7.2\n6.4\n8.4\n4.0\n3.0\n3.0\n4.8\n2.8\n50\n3.8\n4.6\n4.4\n1.0\n-\n3.8\n4.6\n4.2\n0.8\n-\n4.0\n4.8\n4.2\n1.0\n-\n6.6\n6.6\n4.0\n5.4\n6.2\n0.0\n0.0\n0.0\n0.0\n0.0\n8.0\n10.2\n10.2\n10.0\n11.8\n4.0\n4.2\n3.8\n4.2\n3.2\n100\n5.0\n5.4\n3.2\n3.8\n-\n5.0\n5.2\n3.2\n3.8\n-\n4.6\n5.0\n3.6\n3.6\n-\n5.2\n7.0\n5.4\n5.8\n6.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.8\n20.8\n22.8\n29.6\n5.8\n4.6\n5.2\n4.8\n6.0\nDGP7\nAR\nN\n2\n15.4\n27.0\n60.6\n76.4\n-\n15.4\n26.8\n59.6\n76.0\n-\n16.0\n25.6\n60.2\n76.4\n-\n16.0\n24.2\n52.0\n73.8\n99.8\n3.8\n3.0\n4.8\n5.0\n6.8\n5.6\n5.4\n4.6\n5.0\n3.8\n3.8\n3.6\n4.4\n3.0\n3.8\n10\n13.8\n24.0\n57.2\n70.8\n-\n13.8\n24.0\n57.0\n70.2\n-\n14.0\n24.2\n57.4\n70.6\n-\n13.0\n20.8\n50.4\n72.8\n99.6\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n5.2\n5.4\n7.2\n6.2\n4.6\n3.2\n2.6\n3.0\n3.2\n50\n11.2\n19.6\n43.2\n54.6\n-\n11.2\n19.2\n43.0\n53.6\n-\n11.8\n19.4\n44.0\n55.0\n-\n14.0\n18.6\n41.8\n60.6\n96.6\n0.0\n0.0\n0.0\n0.0\n0.0\n9.2\n12.6\n12.4\n13.4\n17.4\n4.4\n5.8\n5.0\n5.2\n5.2\n100\n9.6\n14.0\n28.8\n29.4\n-\n9.6\n14.0\n28.4\n29.2\n-\n9.8\n14.0\n29.8\n29.8\n-\n13.2\n17.4\n32.4\n48.2\n87.6\n0.0\n0.0\n0.0\n0.0\n0.0\n15.8\n21.0\n32.4\n37.6\n49.0\n6.0\n6.6\n8.6\n8.6\n6.6\nDGP8\nAR\nST\n2\n14.6\n24.0\n62.0\n72.8\n-\n14.6\n23.8\n60.8\n72.6\n-\n13.8\n24.4\n61.6\n74.2\n-\n14.6\n22.8\n54.8\n73.0\n99.6\n3.2\n2.2\n4.2\n4.8\n5.6\n5.8\n4.4\n6.4\n7.0\n6.4\n5.4\n3.0\n3.8\n2.4\n3.0\n10\n14.4\n26.4\n60.8\n71.2\n-\n14.2\n26.2\n60.4\n71.0\n-\n14.8\n26.2\n62.0\n72.4\n-\n16.8",
    "chunk_index": 22,
    "start_char": 47281,
    "end_char": 49306,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "14.0\n29.8\n29.8\n-\n13.2\n17.4\n32.4\n48.2\n87.6\n0.0\n0.0\n0.0\n0.0\n0.0\n15.8\n21.0\n32.4\n37.6\n49.0\n6.0\n6.6\n8.6\n8.6\n6.6\nDGP8\nAR\nST\n2\n14.6\n24.0\n62.0\n72.8\n-\n14.6\n23.8\n60.8\n72.6\n-\n13.8\n24.4\n61.6\n74.2\n-\n14.6\n22.8\n54.8\n73.0\n99.6\n3.2\n2.2\n4.2\n4.8\n5.6\n5.8\n4.4\n6.4\n7.0\n6.4\n5.4\n3.0\n3.8\n2.4\n3.0\n10\n14.4\n26.4\n60.8\n71.2\n-\n14.2\n26.2\n60.4\n71.0\n-\n14.8\n26.2\n62.0\n72.4\n-\n16.8\n26.0\n55.8\n71.6\n99.4\n0.2\n0.0\n0.0\n0.0\n0.0\n5.2\n5.2\n7.2\n6.0\n6.2\n3.6\n4.2\n4.6\n3.0\n3.0\n50\n11.4\n18.2\n42.4\n51.8\n-\n11.4\n18.0\n41.0\n51.6\n-\n11.4\n18.4\n42.6\n53.6\n-\n12.0\n17.8\n39.0\n58.8\n97.8\n0.0\n0.0\n0.0\n0.0\n0.0\n10.2\n13.2\n12.8\n13.0\n14.0\n4.6\n5.0\n5.4\n5.2\n3.6\n100\n9.6\n12.6\n29.2\n27.8\n-\n9.6\n12.6\n29.2\n27.8\n-\n9.6\n13.2\n30.2\n29.4\n-\n9.8\n14.4\n31.8\n46.6\n89.6\n0.0\n0.0\n0.0\n0.0\n0.0\n16.2\n23.4\n35.4\n41.2\n54.4\n8.0\n5.4\n4.8\n5.6\n8.4\nDGP9\nAR\nSKST\n2\n15.6\n28.4\n62.2\n75.4\n-\n15.6\n28.2\n61.2\n75.0\n-\n15.2\n27.4\n61.4\n75.8\n-\n15.8\n25.8\n58.6\n76.4\n99.4\n3.6\n3.0\n4.8\n5.2\n5.4\n6.8\n6.4\n6.2\n6.0\n5.0\n3.8\n4.8\n4.4\n3.8\n1.4\n10\n12.2\n22.8\n55.6\n72.0\n-\n12.2\n22.6\n55.2\n71.8\n-\n12.2\n23.2\n57.2\n72.0\n-\n14.2\n20.8\n52.8\n73.0\n99.4\n0.2\n0.0\n0.0\n0.0\n0.0\n4.8\n5.6\n6.2\n6.4\n6.4\n4.8\n3.6\n4.6\n3.6\n2.2\n50\n8.6\n18.8\n45.0\n54.6\n-\n8.6\n18.8\n44.6\n54.4\n-\n8.6\n19.4\n47.2\n55.8\n-\n9.8\n18.8\n40.6\n60.4\n97.4\n0.0\n0.0\n0.0\n0.0\n0.0\n8.4\n11.4\n15.0\n15.8\n18.8\n5.8\n5.6\n3.0\n4.0\n4.0\n100\n10.2\n16.8\n32.8\n29.0\n-\n10.2\n16.8\n32.6\n29.0\n-\n9.8\n16.8\n33.0\n28.6\n-\n11.4\n16.4\n30.0\n47.4\n90.8\n0.0\n0.0\n0.0\n0.0\n0.0\n17.8\n22.4\n30.4\n40.6\n51.6\n4.8\n7.8\n7.0\n8.0\n7.6\nDGP10\nAR-GARCH\nN\n2\n13.2\n25.0\n58.0\n76.4\n-\n13.2\n24.6\n57.6\n76.4\n-\n13.0\n24.6\n57.6\n78.0\n-\n13.6\n25.8\n52.0\n73.0\n99.8\n2.8\n2.6\n2.8\n5.0\n5.8\n4.4\n4.0\n5.0\n4.8\n5.4\n7.0\n3.2\n4.0\n3.2\n2.4\n10\n10.0\n23.4\n57.4\n75.6\n-\n9.8\n23.2\n56.8\n73.8\n-\n10.6\n23.4\n56.6\n74.0\n-\n13.4\n23.6\n52.0\n72.2\n99.6\n0.2\n0.0\n0.0\n0.0\n0.0\n4.4\n5.0\n5.2\n4.8\n6.6\n4.4\n4.8\n3.4\n3.0\n4.2\n50\n11.8\n25.4\n49.6\n58.0\n-\n11.8\n25.2\n48.8\n57.2\n-\n11.8\n26.2\n49.8\n59.4\n-\n14.0\n22.8\n44.0\n63.0\n97.0\n0.0\n0.0\n0.0\n0.0\n0.0\n8.4\n11.8\n12.6\n13.2\n14.2\n4.4\n4.4\n3.6\n4.2\n3.6\n100\n9.4\n14.6\n32.0\n31.0\n-\n9.4\n14.4\n31.2\n31.0\n-\n9.2\n14.6\n30.8\n30.4\n-\n11.0\n16.0\n33.8\n51.6\n91.0\n0.0\n0.0\n0.0\n0.0\n0.0\n14.8\n20.0\n34.2\n40.8\n55.8\n7.6\n7.0\n6.2\n8.4\n5.6\nDGP11\nAR-GARCH\nST\n2\n14.0\n24.8\n60.6\n75.8\n-\n14.0\n24.2\n59.4\n75.0\n-\n14.2\n25.4\n61.0\n76.8\n-\n15.4\n22.0\n51.8\n75.0\n99.6\n3.0\n2.8\n6.2\n3.6\n4.6\n7.0\n5.6\n6.4\n6.2\n7.8\n3.4",
    "chunk_index": 23,
    "start_char": 48962,
    "end_char": 51127,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "11.8\n12.6\n13.2\n14.2\n4.4\n4.4\n3.6\n4.2\n3.6\n100\n9.4\n14.6\n32.0\n31.0\n-\n9.4\n14.4\n31.2\n31.0\n-\n9.2\n14.6\n30.8\n30.4\n-\n11.0\n16.0\n33.8\n51.6\n91.0\n0.0\n0.0\n0.0\n0.0\n0.0\n14.8\n20.0\n34.2\n40.8\n55.8\n7.6\n7.0\n6.2\n8.4\n5.6\nDGP11\nAR-GARCH\nST\n2\n14.0\n24.8\n60.6\n75.8\n-\n14.0\n24.2\n59.4\n75.0\n-\n14.2\n25.4\n61.0\n76.8\n-\n15.4\n22.0\n51.8\n75.0\n99.6\n3.0\n2.8\n6.2\n3.6\n4.6\n7.0\n5.6\n6.4\n6.2\n7.8\n3.4\n3.2\n4.0\n3.8\n3.2\n10\n11.6\n24.0\n58.4\n72.4\n-\n11.6\n23.6\n57.2\n72.0\n-\n11.4\n25.8\n58.6\n73.6\n-\n13.2\n24.6\n50.8\n72.8\n99.2\n0.0\n0.0\n0.0\n0.0\n0.0\n6.6\n11.0\n9.2\n7.2\n7.6\n4.0\n2.6\n4.6\n5.0\n3.2\n50\n10.6\n20.2\n41.8\n55.6\n-\n10.6\n19.6\n41.4\n55.2\n-\n11.4\n19.6\n42.6\n56.2\n-\n13.4\n21.2\n42.0\n60.0\n98.0\n0.0\n0.0\n0.0\n0.0\n0.0\n9.8\n12.6\n17.6\n16.0\n20.0\n5.6\n7.4\n4.6\n4.8\n5.4\n100\n10.8\n17.4\n28.4\n25.4\n-\n10.8\n17.2\n28.2\n25.2\n-\n11.4\n16.8\n30.4\n27.2\n-\n12.4\n16.8\n31.0\n48.0\n89.4\n0.0\n0.0\n0.0\n0.0\n0.0\n16.6\n23.0\n35.2\n39.6\n54.2\n4.4\n5.0\n7.0\n5.6\n7.6\nDGP12\nAR-GARCH\nSKST\n2\n11.4\n29.4\n61.2\n74.8\n-\n11.2\n29.2\n60.6\n74.4\n-\n11.2\n28.6\n61.6\n74.8\n-\n14.2\n25.0\n53.2\n72.8\n99.6\n2.4\n3.0\n5.0\n4.8\n5.2\n5.4\n7.2\n6.6\n8.0\n10.0\n4.4\n3.2\n2.8\n3.8\n4.4\n10\n14.4\n26.4\n59.6\n72.2\n-\n14.4\n26.2\n59.2\n71.6\n-\n14.0\n26.8\n59.8\n71.2\n-\n12.6\n23.4\n50.8\n68.6\n99.0\n0.0\n0.0\n0.0\n0.0\n0.2\n7.6\n6.6\n6.6\n7.0\n9.0\n5.2\n5.2\n3.2\n4.8\n2.6\n50\n12.0\n21.4\n41.4\n52.8\n-\n12.0\n21.2\n41.0\n52.0\n-\n12.2\n20.2\n41.8\n53.2\n-\n14.2\n20.0\n38.6\n59.6\n97.2\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n14.6\n15.4\n17.2\n19.4\n4.8\n3.8\n4.4\n6.4\n5.0\n100\n7.8\n15.0\n27.6\n27.0\n-\n7.8\n14.8\n26.6\n27.0\n-\n7.8\n15.0\n27.0\n27.6\n-\n11.2\n16.2\n33.2\n46.6\n87.8\n0.0\n0.0\n0.0\n0.0\n0.0\n15.2\n23.8\n35.4\n41.0\n55.6\n8.8\n5.8\n6.6\n8.6\n9.2\n3.3.3. Size Results for H\u03b4\n0\nResults for the global minimum-variance portfolio hypothesis H\u03b4\n0 are reported in Table 4.\nWhile KM and F2 are not applicable for N = 400, they have an empirical size close to the nominal\n21\n\nsize of 5% for the four considered numbers of test assets N when K is small (i.e., K = 2 or 10).\nFor K = 50 and 100, both tests are slightly oversized in the presence of serial correlation. Like\nfor the other two simulations, our proposed BCS\u03b4\n2 test has, overall, the closest empirical size to the\nnominal size of 5%, even when N is much larger than the sample size.\nTable 4: Size Results for H\u03b4\n0\nThe table reports the empirical size (over 500 replications) for five tests for the global minimum-variance\nspanning under twelve DGPs and four values of K and N (i.e., the number of benchmark and test assets,\nrespectively). Values in bold are between 3% and 7%.",
    "chunk_index": 24,
    "start_char": 50776,
    "end_char": 53143,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "for the other two simulations, our proposed BCS\u03b4\n2 test has, overall, the closest empirical size to the\nnominal size of 5%, even when N is much larger than the sample size.\nTable 4: Size Results for H\u03b4\n0\nThe table reports the empirical size (over 500 replications) for five tests for the global minimum-variance\nspanning under twelve DGPs and four values of K and N (i.e., the number of benchmark and test assets,\nrespectively). Values in bold are between 3% and 7%. A dash indicates that the test cannot be applied as\nN > T \u2212K \u22121.\nKM\nF2\nBCS\u03b4\n0\nBCS\u03b4\n2\nK\u2193N\u2192\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\n2\n10\n50\n100\n400\nDGP1\ni.i.d.\nN\n2\n5.4\n5.4\n5.6\n6.2\n-\n4.8\n5.6\n5.2\n6.2\n-\n5.4\n7.0\n6.2\n6.4\n5.2\n4.6\n4.0\n3.8\n4.4\n3.4\n10\n5.6\n4.8\n5.4\n6.0\n-\n5.6\n4.8\n5.8\n5.2\n-\n5.2\n5.0\n5.2\n5.4\n6.0\n5.0\n2.4\n4.4\n4.4\n3.2\n50\n5.8\n5.2\n6.0\n6.0\n-\n5.8\n5.4\n5.4\n5.6\n-\n5.6\n8.4\n9.0\n9.6\n10.0\n4.0\n6.4\n2.8\n2.8\n4.8\n100\n4.0\n4.6\n5.0\n6.2\n-\n4.4\n4.4\n4.6\n6.6\n-\n7.6\n13.6\n15.2\n20.2\n28.6\n4.0\n6.0\n4.0\n7.0\n6.6\nDGP2\ni.i.d.\nST\n2\n4.6\n5.6\n5.8\n5.0\n-\n4.4\n5.2\n5.8\n5.6\n-\n3.0\n3.8\n6.6\n6.2\n5.0\n3.6\n2.8\n4.2\n4.6\n3.2\n10\n5.0\n4.4\n5.2\n4.2\n-\n4.8\n4.0\n4.4\n4.4\n-\n4.2\n5.0\n6.6\n7.4\n6.8\n4.0\n5.0\n4.0\n3.4\n2.8\n50\n5.4\n7.2\n5.0\n5.8\n-\n5.4\n6.8\n5.0\n5.6\n-\n7.4\n9.6\n11.6\n11.2\n13.2\n4.4\n3.0\n3.4\n4.0\n5.2\n100\n4.2\n6.0\n5.6\n6.4\n-\n4.4\n6.4\n5.2\n6.2\n-\n11.2\n14.8\n15.8\n17.0\n24.8\n7.2\n6.2\n6.6\n6.8\n6.2\nDGP2\ni.i.d.\nSKST\n2\n6.0\n4.4\n4.0\n6.0\n-\n6.2\n4.2\n4.2\n5.4\n-\n5.8\n7.2\n4.0\n5.8\n6.0\n4.4\n4.6\n4.4\n3.8\n4.2\n10\n5.0\n5.0\n5.2\n4.4\n-\n5.2\n4.8\n5.4\n4.8\n-\n5.8\n6.4\n6.0\n6.4\n6.4\n4.4\n5.8\n4.0\n2.8\n3.2\n50\n7.2\n6.8\n5.2\n4.4\n-\n6.6\n6.2\n5.0\n4.8\n-\n8.2\n10.4\n12.6\n14.2\n13.2\n6.6\n6.0\n4.4\n3.6\n4.6\n100\n5.4\n4.0\n3.8\n6.2\n-\n4.8\n4.0\n3.8\n6.4\n-\n12.0\n18.0\n20.2\n21.6\n26.2\n5.6\n5.8\n5.8\n5.0\n7.8\nDGP4\nGARCH\nN\n2\n4.8\n5.4\n4.6\n5.4\n-\n4.8\n5.6\n4.6\n5.2\n-\n6.8\n6.0\n5.6\n5.2\n5.0\n3.8\n2.8\n1.8\n4.0\n4.4\n10\n6.2\n3.2\n4.0\n4.2\n-\n5.8\n3.6\n3.6\n4.6\n-\n6.4\n4.4\n6.4\n5.0\n6.2\n3.6\n3.2\n3.4\n3.4\n3.6\n50\n5.2\n4.0\n5.6\n6.0\n-\n5.0\n3.8\n5.8\n5.6\n-\n5.6\n8.2\n9.4\n9.2\n9.4\n4.0\n4.0\n4.0\n5.6\n2.8\n100\n4.6\n5.8\n5.8\n6.0\n-\n4.4\n5.6\n5.4\n5.8\n-\n12.0\n16.8\n22.0\n22.8\n31.0\n6.0\n5.8\n5.2\n5.4\n5.8\nDGP5\nGARCH\nST\n2\n4.0\n4.6\n5.0\n6.4\n-\n4.4\n4.4\n4.4\n6.4\n-\n7.4\n8.8\n9.4\n10.2\n10.2\n3.8\n4.4\n3.6\n4.2\n3.8\n10\n5.4\n3.0",
    "chunk_index": 25,
    "start_char": 52677,
    "end_char": 54752,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "-\n6.4\n4.4\n6.4\n5.0\n6.2\n3.6\n3.2\n3.4\n3.4\n3.6\n50\n5.2\n4.0\n5.6\n6.0\n-\n5.0\n3.8\n5.8\n5.6\n-\n5.6\n8.2\n9.4\n9.2\n9.4\n4.0\n4.0\n4.0\n5.6\n2.8\n100\n4.6\n5.8\n5.8\n6.0\n-\n4.4\n5.6\n5.4\n5.8\n-\n12.0\n16.8\n22.0\n22.8\n31.0\n6.0\n5.8\n5.2\n5.4\n5.8\nDGP5\nGARCH\nST\n2\n4.0\n4.6\n5.0\n6.4\n-\n4.4\n4.4\n4.4\n6.4\n-\n7.4\n8.8\n9.4\n10.2\n10.2\n3.8\n4.4\n3.6\n4.2\n3.8\n10\n5.4\n3.0\n3.6\n6.2\n-\n5.6\n2.8\n3.8\n6.2\n-\n6.6\n5.8\n8.4\n9.2\n8.4\n6.0\n3.0\n4.0\n3.8\n3.0\n50\n5.8\n6.2\n6.2\n8.4\n-\n6.0\n6.4\n6.8\n7.8\n-\n7.8\n12.4\n12.2\n13.8\n12.6\n4.8\n5.6\n5.0\n3.2\n4.8\n100\n3.8\n4.2\n5.6\n6.0\n-\n4.0\n4.0\n6.0\n6.4\n-\n15.0\n17.8\n19.2\n24.2\n29.4\n6.4\n6.6\n6.4\n5.8\n6.2\nDGP6\nGARCH\nSKST\n2\n3.8\n5.0\n3.2\n4.6\n-\n4.0\n4.6\n2.8\n4.2\n-\n4.4\n6.0\n7.8\n6.8\n8.0\n4.6\n5.0\n2.8\n3.2\n2.8\n10\n4.6\n4.6\n3.6\n5.2\n-\n5.4\n4.4\n3.8\n4.2\n-\n6.8\n8.2\n8.6\n8.0\n11.2\n5.0\n5.0\n6.6\n4.6\n3.4\n50\n4.6\n5.2\n6.2\n7.4\n-\n4.2\n4.8\n6.4\n7.0\n-\n10.0\n9.4\n10.8\n13.8\n12.8\n5.4\n5.0\n4.8\n4.6\n4.4\n100\n4.4\n5.4\n5.4\n3.8\n-\n4.8\n5.0\n6.2\n5.0\n-\n14.4\n17.6\n22.8\n25.4\n29.6\n6.4\n4.0\n5.8\n5.6\n5.8\nDGP7\nAR\nN\n2\n5.0\n8.8\n8.4\n11.8\n-\n6.0\n9.0\n9.0\n11.0\n-\n6.4\n6.2\n5.4\n6.2\n5.4\n5.6\n3.4\n5.0\n3.0\n4.8\n10\n4.6\n5.0\n8.4\n10.0\n-\n4.6\n4.4\n8.2\n9.8\n-\n4.8\n3.0\n4.8\n5.2\n4.6\n4.6\n3.2\n2.8\n4.6\n4.0\n50\n4.8\n7.4\n9.4\n8.0\n-\n4.8\n7.8\n9.2\n8.6\n-\n7.6\n11.2\n12.0\n12.2\n12.0\n3.8\n5.8\n3.6\n5.2\n5.4\n100\n6.4\n5.8\n6.4\n7.2\n-\n6.6\n5.8\n6.4\n6.2\n-\n14.0\n12.2\n22.2\n24.0\n28.4\n5.4\n4.6\n6.6\n5.6\n5.4\nDGP8\nAR\nST\n2\n6.0\n7.4\n7.8\n12.4\n-\n6.0\n6.6\n7.4\n11.6\n-\n6.2\n6.8\n6.0\n6.6\n5.8\n5.2\n3.2\n4.2\n4.0\n4.0\n10\n5.6\n6.0\n9.0\n11.8\n-\n5.2\n5.2\n9.4\n11.4\n-\n5.2\n5.8\n7.0\n5.0\n8.2\n4.0\n4.2\n2.6\n3.8\n3.8\n50\n6.4\n7.4\n10.8\n10.0\n-\n6.4\n7.2\n11.4\n9.4\n-\n8.0\n9.0\n11.0\n13.0\n11.4\n4.2\n4.6\n4.4\n4.8\n4.4\n100\n4.8\n7.0\n8.8\n7.0\n-\n4.8\n6.4\n8.4\n6.8\n-\n13.0\n15.4\n20.0\n24.2\n26.8\n5.2\n4.4\n6.0\n5.8\n5.8\nDGP9\nAR\nSKST\n2\n6.2\n8.2\n9.8\n12.0\n-\n6.0\n7.6\n11.4\n12.0\n-\n5.2\n7.0\n5.2\n5.4\n4.8\n4.6\n3.6\n3.2\n5.2\n3.0\n10\n6.6\n7.2\n12.4\n13.2\n-\n6.4\n7.0\n11.6\n13.0\n-\n8.0\n5.8\n6.6\n7.0\n9.2\n5.0\n6.4\n4.4\n3.6\n3.0\n50\n8.0\n8.2\n11.2\n11.0\n-\n8.2\n7.8\n11.8\n10.2\n-\n7.2\n8.4\n7.8\n8.8\n11.8\n4.2\n5.0\n3.0\n4.8\n3.2\n100\n6.2\n6.0\n6.6\n8.0\n-\n5.8\n5.6\n7.0\n8.2\n-\n13.0\n16.8\n23.2\n28.2\n32.2\n7.2\n7.0\n5.6\n7.0\n4.4\nDGP10\nAR-GARCH\nN\n2\n5.8\n8.0\n10.2\n11.2\n-\n5.8\n8.2\n11.2",
    "chunk_index": 26,
    "start_char": 54442,
    "end_char": 56408,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "4.6\n3.6\n3.2\n5.2\n3.0\n10\n6.6\n7.2\n12.4\n13.2\n-\n6.4\n7.0\n11.6\n13.0\n-\n8.0\n5.8\n6.6\n7.0\n9.2\n5.0\n6.4\n4.4\n3.6\n3.0\n50\n8.0\n8.2\n11.2\n11.0\n-\n8.2\n7.8\n11.8\n10.2\n-\n7.2\n8.4\n7.8\n8.8\n11.8\n4.2\n5.0\n3.0\n4.8\n3.2\n100\n6.2\n6.0\n6.6\n8.0\n-\n5.8\n5.6\n7.0\n8.2\n-\n13.0\n16.8\n23.2\n28.2\n32.2\n7.2\n7.0\n5.6\n7.0\n4.4\nDGP10\nAR-GARCH\nN\n2\n5.8\n8.0\n10.2\n11.2\n-\n5.8\n8.2\n11.2\n11.8\n-\n6.2\n6.2\n6.6\n6.8\n7.6\n4.4\n4.4\n2.6\n3.8\n3.6\n10\n6.4\n7.0\n9.2\n10.6\n-\n5.8\n6.6\n8.8\n11.2\n-\n7.8\n6.0\n8.0\n7.0\n7.8\n5.2\n3.8\n4.8\n2.8\n4.0\n50\n6.0\n5.4\n10.0\n9.2\n-\n6.0\n5.4\n10.0\n9.4\n-\n7.8\n10.2\n14.4\n13.0\n12.8\n3.8\n5.2\n2.8\n4.0\n3.2\n100\n4.8\n6.6\n7.6\n8.2\n-\n5.2\n6.6\n6.8\n7.8\n-\n13.2\n17.6\n24.4\n23.6\n34.0\n5.2\n5.8\n4.8\n4.6\n4.8\nDGP11\nAR-GARCH\nST\n2\n4.8\n7.6\n8.6\n13.6\n-\n4.8\n8.2\n8.8\n13.8\n-\n7.0\n9.8\n8.4\n9.2\n9.6\n3.2\n6.0\n3.2\n1.8\n3.4\n10\n7.4\n6.2\n8.0\n12.4\n-\n7.0\n6.6\n8.0\n13.0\n-\n6.6\n7.0\n9.6\n10.8\n11.4\n5.4\n4.0\n3.0\n3.8\n2.6\n50\n6.4\n7.2\n12.4\n12.6\n-\n6.4\n7.8\n12.2\n11.8\n-\n9.4\n12.0\n14.6\n16.0\n16.0\n4.8\n4.6\n4.2\n3.0\n5.2\n100\n5.2\n5.4\n8.8\n8.6\n-\n5.4\n6.0\n9.2\n9.2\n-\n13.6\n18.8\n27.0\n32.0\n38.2\n7.2\n6.6\n6.8\n7.4\n5.2\nDGP12\nAR-GARCH\nSKST\n2\n6.4\n7.6\n10.0\n12.8\n-\n6.8\n7.8\n10.8\n13.2\n-\n4.2\n5.6\n6.4\n6.4\n8.0\n3.4\n5.0\n3.6\n4.2\n3.0\n10\n5.6\n6.4\n8.6\n11.2\n-\n4.8\n6.6\n8.6\n10.6\n-\n5.4\n8.0\n8.0\n8.4\n10.2\n4.4\n6.8\n4.2\n5.4\n5.2\n50\n4.8\n7.6\n12.8\n11.2\n-\n5.2\n7.0\n12.8\n11.2\n-\n10.6\n10.4\n13.0\n15.4\n16.4\n5.8\n5.2\n4.2\n4.4\n5.8\n100\n4.6\n6.0\n7.4\n6.4\n-\n4.8\n6.2\n7.4\n7.0\n-\n15.2\n18.6\n27.2\n30.0\n35.2\n4.8\n4.4\n5.0\n7.4\n5.0\n22\n\n3.4. Power Results\nIn this section, we study the power of the tests considered in the previous section. To sim-\nulate data under the alternative, we consider a sparse setting where we set \u03b1i = \u03b4i = a for\ni = 1, . . . , [N/2] with a \u2208[\u22120.4, 0.4] and a = 0 for i > [N/2], where [x] denotes the integer\npart of x.\nFor the three tested null hypotheses, we focus on the most complete DGP, namely DGP12,\nfor which returns have serial correlation, GARCH effects, and innovations follow an asymmetric\nStudent distribution.\n3.4.1. Power Results for H\u03b1,\u03b4\n0\nThe empirical power functions for H\u03b1,\u03b4\n0\n(i.e., HK, GL, and BCS\u03b1,\u03b4\n2\n) are plotted in Figure 1. It\nis important to note that the rejection frequencies are not adjusted for the possible size distortion.\nRecall that we have seen in Table 2 that HK is largely oversized in this case while GL is largely\nundersized except for K = 2.",
    "chunk_index": 27,
    "start_char": 56085,
    "end_char": 58296,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "and innovations follow an asymmetric\nStudent distribution.\n3.4.1. Power Results for H\u03b1,\u03b4\n0\nThe empirical power functions for H\u03b1,\u03b4\n0\n(i.e., HK, GL, and BCS\u03b1,\u03b4\n2\n) are plotted in Figure 1. It\nis important to note that the rejection frequencies are not adjusted for the possible size distortion.\nRecall that we have seen in Table 2 that HK is largely oversized in this case while GL is largely\nundersized except for K = 2.\nThis figure demonstrates the contribution of the randomly-weighted batch-mean CCT with\nL = 2. In the unlikely case K = N = 2, this test is dominated by both HK and GL, although\nHK was found to be oversized in this case. HK displays the highest rejection frequencies, but\nthose rejections are mostly due to the large size distortion of this test when K and N are large.\nWhile GL has slightly more power than BCS\u03b1,\u03b4\n2\nwhen K is small, it has no power when K is large\n(i.e., K = 100). Importantly, BCS\u03b1,\u03b4\n2\nhas similar power empirical curves, whatever the number\nof benchmark assets, for a given number of test assets and is found to have decent power in all\nconfigurations.\n23\n\nFigure 1: Empirical Power of the MVS Tests\nThe plots display the empirical power (over 500 replications) for the MVS tests H\u03b1,\u03b4\n0\nunder the AR-\nGARCH(1,1) model with skewed Student-t errors for various K benchmark and N test assets. We consider\nK \u2208{2, 10, 100} and N \u2208{2, 50, 400}. For N = 400, the test HK is not feasible. The solid horizontal\nlines correspond to the nominal level of 5%.\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=400\nBCS2\u03b1,\u03b4\nHK\nGL\n3.4.2. Power Results for H\u03b1\n0\nThe power curves of the maximum Sharpe portfolio spanning tests H\u03b1\n0 are plotted in Figure 2.\nAs for the joint test, the contribution of BCS\u03b1\n2 is visible when the number of test assets is large.\nIndeed, unlike the GRS, F1, and BJ, this test is still applicable when N > T and in this case, it has\na decent size (unlike PY) and it has power against the null hypothesis H\u03b1\n0 (unlike GL).\n24\n\nFigure 2: Empirical Power of the Maximum Sharpe Portfolio Spanning Tests\nThe plots display the empirical power (over 500 replications) of the maximum Sharpe portfolio spanning\ntests H\u03b1\n0 under the AR-GARCH(1,1) model with skewed Student-t errors for K benchmark and N test\nassets.",
    "chunk_index": 28,
    "start_char": 57877,
    "end_char": 60591,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "GRS, F1, and BJ, this test is still applicable when N > T and in this case, it has\na decent size (unlike PY) and it has power against the null hypothesis H\u03b1\n0 (unlike GL).\n24\n\nFigure 2: Empirical Power of the Maximum Sharpe Portfolio Spanning Tests\nThe plots display the empirical power (over 500 replications) of the maximum Sharpe portfolio spanning\ntests H\u03b1\n0 under the AR-GARCH(1,1) model with skewed Student-t errors for K benchmark and N test\nassets. We consider K \u2208{2, 10, 100} and N \u2208{2, 50, 400}. For N = 400, the GRS, F1, and BJ tests are\nnot feasible. The solid horizontal lines correspond to the nominal level of 5%.\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=400\nBCS2\u03b1\nGRS\nF1\nBJ\nPY\nGL\n3.4.3. Power Results for H\u03b4\n0\nFinally, the power curves of the minimum-variance portfolio spanning tests H\u03b4\n0 are plotted in\nFigure 3. When N = 2, KM and F2 have much higher power than BCS\u03b4\n2. When N is moderately\nlarge (i.e., N = 50), rejection frequencies of KM and F2 are higher than BCS\u03b4\n2 but these two tests\nare slightly oversized unlike BCS\u03b4\n2. When N = 400, the only applicable test is BCS\u03b4\n2, and the\npower curves are again insensitive to the number of benchmark assets.\nTo sum up, the BCS\u03b1,\u03b4\n2\n, BCS\u03b1\n2 and BCS\u03b4\n2 tests are the ones retaining good properties in the\npresence of serial correlation and GARCH effects, even when the number of test and benchmark\nassets is large.\n25\n\nFigure 3: Empirical Power of the Global Minimum-Variance Portfolio Spanning Tests\nThe plots display the empirical power (over 500 replications) of the global minimum-variance portfolio\nspanning tests H\u03b4\n0 under the AR-GARCH(1,1) model with skewed Student-t errors for various K benchmark\nand N test assets. We consider K \u2208{2, 10, 100} and N \u2208{2, 50, 400}. For N = 400, the KM and F2 tests\nare not feasible. The solid horizontal lines correspond to the nominal level of 5%.\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=50\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=400\n0.2\n0.4\n0.6\n0.8",
    "chunk_index": 29,
    "start_char": 60135,
    "end_char": 62811,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "The solid horizontal lines correspond to the nominal level of 5%.\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=50\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=2;N=400\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=2\n0.00\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=50\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=10;N=400\n0.2\n0.4\n0.6\n0.8\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=2\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=50\n0.25\n0.50\n0.75\n1.00\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nK=100;N=400\nBCS2\u03b4\nKM\nF2\n4. Empirical Application\nThis section tests to which extent domestic equity investors could benefit from international\ndiversification. Our empirical illustration uses daily returns from 2007 to 2022 on blue-chip stocks\ntraded in North America and Europe. Specifically, at the begining of each year, we use stock returns\nof the historical constituents in the following equity indices: (i) S&P 100 (U.S.), (ii) Euronext 100\n(Eurozone), and (iii) SMI (Switzerland). We apply the three spanning tests H\u03b1,\u03b4\n0\n, H\u03b1\n0 , and H\u03b4\n0\nevery year to determine whether it is relevant to include assets from the other two countries. The\nnumber of benchmark and test assets varies slightly over time but remains a large-dimensional\nproblem, and the sample size for each year is slightly less than 250 days; see the online appendix\nfor details.\nFor BCS\u03bb\nL, we set L = 2 as the empirical size was found to be satisfactory in the Monte\nCarlo study for T = 250. We consider a fixed-width rule \u03b6 = 1/3, but results are robust to\nalternative choices. We compare our results for H\u03b1,\u03b4\n0\nand H\u03b1\n0 with the GL approach using the\n26\n\ndefault implementation with 500 boostrap replications. Results are reported in Table 5, where tests\nwith a p-value lower that 5% are highlighted by a symbol \u2713while inconclusive test outcomes (of\nthe GL tests) are highlighted by a question mark\u2019.\nFirst, we find that the test H\u03b1,\u03b4\n0\nis rejected on average one-third of the time for each country\nand the rejections do not align among countries. Second, we see that the rejection of the MVS hy-\npothesis comes from the potential of variance reduction in the domestic global minimum-variance\nportfolio. Rejections of H\u03b4\n0 drives the rejection of H\u03b1,\u03b4\n0\n. On the contrary, the maximum Sharpe\nportfolio spanning test H\u03b1\n0 is only rejected a few times.\nOn the other hand, we are unable to consistently reject the null hypothesis H\u03b1,\u03b4\n0\nusing GL,\nleading to inconclusive results when K is moderately large (see columns S&P 100 and Euronext\n100). In slightly more than 25% of the cases, GL yields inconclusive results when test assets are\nSMI constituents. However, we do not reject H\u03b1,\u03b4\n0\nusing GL in 2007 and 2017 when test assets\nare S&P 100 constituents. We reject H\u03b1,\u03b4\n0\naround 75% of the cases when SMI constituents are the\ntest assets.",
    "chunk_index": 30,
    "start_char": 62396,
    "end_char": 65253,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "the other hand, we are unable to consistently reject the null hypothesis H\u03b1,\u03b4\n0\nusing GL,\nleading to inconclusive results when K is moderately large (see columns S&P 100 and Euronext\n100). In slightly more than 25% of the cases, GL yields inconclusive results when test assets are\nSMI constituents. However, we do not reject H\u03b1,\u03b4\n0\nusing GL in 2007 and 2017 when test assets\nare S&P 100 constituents. We reject H\u03b1,\u03b4\n0\naround 75% of the cases when SMI constituents are the\ntest assets. This behavior of the GL test statistics is expected as it becomes less informative as K\nincreases (see Section 3.4.1). When focusing on H\u03b1\n0 , we are unable to reject the null hypothesis\nof Sharpe ratio portfolio spanning with any of the test assets universes under consideration using\nGL. Overall, we can conclude that the few rejections of H\u03b1,\u03b4\n0\nare driven by the rejection of H\u03b4\n0 .\nHowever, unlike BCS\u03bb\n2 , we obtain fewer rejections of H\u03b1,\u03b4\n0\n, which can be explained by the superior\nperformance of BCS\u03bb\n2 over GL in terms of size and power (see Sections 3.3 and 3.4).\n27\n\nTable 5: Results of the MVS Tests Over Time for the Various Investment Universes\nThis table reports the rejections at the 5% significance level (highlighted by a symbol \u2713) of the various MVS\ntests applied at the asset level. H\u03b1,\u03b4\n0\nis the MVS test, H\u03b1\n0 is the maximum Sharpe ratio spanning test, and\nH\u03b4\n0 is the global minimum-variance spanning test. GL is the test by Gungor and Luger (2016) implemented\nwith 500 bootstrap replications and BCS\u03bb\n2 is the our BCS test with L = 2. A question mark indicates an\ninconclusive test outcome.\nS&P 100\nEuronext 100\nSMI\nGL\nBCS\u03bb\n2\nGL\nBCS\u03bb\n2\nGL\nBCS\u03bb\n2\nYear\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b4\n0\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b4\n0\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b1,\u03b4\n0\nH\u03b1\n0\nH\u03b4\n0\n2007\n?\n\u2713\n\u2713\n\u2713\n2008\n?\n\u2713\n\u2713\n?\n\u2713\n?\n2009\n?\n\u2713\n\u2713\n\u2713\n?\n?\n2010\n?\n\u2713\n\u2713\n?\n\u2713\n\u2713\n\u2713\n2011\n?\n?\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n2012\n?\n?\n\u2713\n\u2713\n2013\n?\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2014\n?\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2015\n?\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2016\n?\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2017\n\u2713\n?\n?\n\u2713\n\u2713\n\u2713\n2018\n?\n\u2713\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2019\n?\n\u2713\n\u2713\n?\n\u2713\n\u2713\n\u2713\n2020\n?\n\u2713\n\u2713\n?\n?\n2021\n?\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2022\n?\n\u2713\n\u2713\n?\n?\n5. Conclusion\nThe paper proposes a new framework for mean-variance spanning (MVS) testing that is more\ngeneral and computationally efficient than existing methods. The proposed framework can be ap-\nplied to any test-asset dimension and only requires stationary asset returns. It uses new moment\nconditions for spanning and tests them in two steps.",
    "chunk_index": 31,
    "start_char": 64769,
    "end_char": 67142,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "\u2713\n\u2713\n\u2713\n2019\n?\n\u2713\n\u2713\n?\n\u2713\n\u2713\n\u2713\n2020\n?\n\u2713\n\u2713\n?\n?\n2021\n?\n\u2713\n?\n\u2713\n\u2713\n\u2713\n\u2713\n2022\n?\n\u2713\n\u2713\n?\n?\n5. Conclusion\nThe paper proposes a new framework for mean-variance spanning (MVS) testing that is more\ngeneral and computationally efficient than existing methods. The proposed framework can be ap-\nplied to any test-asset dimension and only requires stationary asset returns. It uses new moment\nconditions for spanning and tests them in two steps. First, it tests each component of the moment\nvector using robust Student-t tests based on the batch-mean method. Second, it combines the indi-\nvidual p-values using the Cauchy combination test (CCT) of Liu and Xie (2020), which accounts\nfor the cross-sectional dependence between the test statistics. Monte Carlo simulations show the\nproposed MVS tests have correct sizes and high power in most setups. Unlike state-of-the-art\nmethods, they also work well on skewed, heteroscedastic, and fat-tailed data. The methodology\nis applied to test if combining blue-chip stocks traded in the U.S., Europe, and Switzerland can\nimprove each country\u2019s domestic mean-variance efficient frontier. We find that the benefits of\ninternational diversification depend on economic conditions and vary across countries. We also\nhighlight that the rejection of the MVS hypothesis originates from the potential to reduce variance\n28\n\nwithin the domestic global minimum-variance portfolio.\nAcknowledgments\nDavid is grateful to IVADO and the Natural Sciences and Engineering Research Council of\nCanada (grant RGPIN-2022-03767). S\u00e9bastien acknowledges the research support of the French\nNational Research Agency Grants ANR-17-EURE-0020 and ANR-21-CE26-0007-01. Rosnel ac-\nknowledges the financial support of Fin-ML. We thank Marie-Claude Beaulieu and Olivier Scaillet\nfor their comments, and Richard Luger for providing us with the code for the GL test.\n29\n\nReferences\nBeaulieu, M.C., Dufour, J.M., Khalaf, L., 2007. Multivariate tests of mean\u2013variance efficiency with possibly non-\ngaussian errors: An exact simulation-based approach. Journal of Business & Economic Statistics 25, 398\u2013410.\nBeaulieu, M.C., Dufour, J.M., Khalaf, L., 2010. Asset-pricing anomalies and spanning: Multivariate and multifactor\ntests with heavy-tailed distributions. Journal of Empirical Finance 17, 763\u2013782.\nBekaert, G., Harvey, C.R., 1995. Time-varying world market integration. Journal of Finance 50, 403\u2013444.\nBenjamini, Y., Hochberg, Y., 1995. Controlling the false discovery rate: a practical and powerful approach to multiple\ntesting. Journal of the Royal Statistical Society: Series B (Methodological) 57, 289\u2013300.\nBenjamini, Y., Yekutieli, D., 2001. The control of the false discovery rate in multiple testing under dependency. Annals\nof Statistics , 1165\u20131188.\nBritten-Jones, M., 1999. The sampling error in estimates of mean-variance efficient portfolio weights. Journal of\nFinance 54, 655\u2013671.\nCarlstein, E., 1986. Asymptotic normality for a general statistic from a stationary sequence. Annals of Probability 14.\nCarlstein, E., et al., 1986. The use of subseries values for estimating the variance of a general statistic from a stationary\nsequence. Annals of statistics 14, 1171\u20131179.\nChen, Y., Yuen, K.C., 2009. Sums of pairwise quasi-asymptotically independent random variables with consistent\nvariation. Stochastic Models 25, 76\u201389.\nDarolles, S., Francq, C., Laurent, S., 2018.",
    "chunk_index": 32,
    "start_char": 66721,
    "end_char": 70088,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "portfolio weights. Journal of\nFinance 54, 655\u2013671.\nCarlstein, E., 1986. Asymptotic normality for a general statistic from a stationary sequence. Annals of Probability 14.\nCarlstein, E., et al., 1986. The use of subseries values for estimating the variance of a general statistic from a stationary\nsequence. Annals of statistics 14, 1171\u20131179.\nChen, Y., Yuen, K.C., 2009. Sums of pairwise quasi-asymptotically independent random variables with consistent\nvariation. Stochastic Models 25, 76\u201389.\nDarolles, S., Francq, C., Laurent, S., 2018. Asymptotics of cholesky garch models and time-varying conditional betas.\nJournal of Econometrics 204, 223\u2013247.\nDemarta, S., McNeil, A.J., 2005. The t copula and related copulas. International statistical review 73, 111\u2013129.\nDeRoon, F.A., Nijman, T.E., 2001. Testing for mean-variance spanning: A survey. Journal of Empirical Finance 8,\n111\u2013155.\nFama, E.F., French, K.R., 1993. Common risk factors in the returns on stocks and bonds. Journal of Financial\nEconomics 33, 3\u201356.\nFern\u00e1ndez, C., Steel, M.F.J., 1998. On bayesian modeling of fat tails and skewness. Journal of the American Statistical\nAssociation 93, 359\u2013371.\nFlegal, J.M., Jones, G.L., et al., 2010. Batch means and spectral variance estimators in Markov chain Monte Carlo.\nAnnals of Statistics 38, 1034\u20131070.\nFran\u00e7ois, P., H\u00fcbner, G., 2024. The Complete Guide to Portfolio Performance. Appraise, Analyze, Act. Wiley. Under\npress.\nGibbons, M.R., Ross, S.A., Shanken, J., 1989. A test of the efficiency of a given portfolio. Econometrica: Journal of\nthe Econometric Society , 1121\u20131152.\nGiot, P., Laurent, S., 2003. Value-at-risk for long and short trading positions. Journal of Applied Econometrics 18,\n641\u2013663.\nGungor, S., Luger, R., 2009. Exact distribution-free tests of mean-variance efficiency. Journal of Empirical Finance\n16, 816\u2013829.\nGungor, S., Luger, R., 2016. Multivariate tests of mean-variance efficiency and spanning with a large number of assets\nand time-varying covariances. Journal of Business & Economic Statistics 34, 161\u2013175.\nHochberg, Y., 1988. A sharper Bonferroni procedure for multiple tests of significance. Biometrika 75, 800\u2013802.\nHolm, S., 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics , 65\u201370.\nHommel, G., 1988. A stagewise rejective multiple test procedure based on a modified Bonferroni test. Biometrika 75,\n383\u2013386.\nHuberman, G., Kandel, S., 1987. Mean-variance spanning. Journal of Finance 42, 873\u2013888.\nIbragimov, R., M\u00fcller, U.K., 2010. t-statistic based correlation and heterogeneity robust inference. Journal of Business\n& Economic Statistics 28, 453\u2013468.\nKan, R., Zhou, G., 2012. Tests of mean-variance spanning. Annals of Economics and Finance 13, 139\u2013187.\nKempf, A., Memmel, C., 2006. Estimating the global minimum variance portfolio. Schmalenbach Business Review\n58, 332\u2013348.\nLazarus, E., Lewis, D.J., Stock, J.H., 2019. The size-power tradeoff in har inference. Available at SSRN 3436372 .\nLazarus, E., Lewis, D.J., Stock, J.H., Watson, M.W., 2018. Har inference: Recommendations for practice. Journal of\nBusiness & Economic Statistics 36, 541\u2013559.\nLee, S.S., Mykland, P.A., 2008. Jumps in financial markets: A new nonparametric test and jump dynamics. Review\nof Financial Studies 21, 2535\u20132563.\nLing, X., 2023. Additive P-Value Combination Test. Ph.D. thesis. Michigan Technological University.\nLintner, J., 1965. The valuation of risk assets and the selection of risky investments in stock portfolios and capital\nbudgets.",
    "chunk_index": 33,
    "start_char": 69550,
    "end_char": 73064,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "Available at SSRN 3436372 .\nLazarus, E., Lewis, D.J., Stock, J.H., Watson, M.W., 2018. Har inference: Recommendations for practice. Journal of\nBusiness & Economic Statistics 36, 541\u2013559.\nLee, S.S., Mykland, P.A., 2008. Jumps in financial markets: A new nonparametric test and jump dynamics. Review\nof Financial Studies 21, 2535\u20132563.\nLing, X., 2023. Additive P-Value Combination Test. Ph.D. thesis. Michigan Technological University.\nLintner, J., 1965. The valuation of risk assets and the selection of risky investments in stock portfolios and capital\nbudgets. Review of Economics and Statistics 47, 13\u201337.\nLiu, Y., Xie, J., 2020. Cauchy combination test: A powerful test with analytic p-value calculation under arbitrary\ndependency structures. Journal of the American Statistical Association 115, 393\u2013402.\n30\n\nLong, M., Li, Z., Zhang, W., Li, Q., 2023. The Cauchy combination test under arbitrary dependence structures. The\nAmerican Statistician 77, 134\u2013142.\nPedersen, R.S., 2020. Robust inference in conditionally heteroskedastic autoregressions. Econometric Reviews 39,\n244\u2013259.\nPeng, J., Wang, P., Zhou, N., Zhu, J., 2009. Partial correlation estimation by joint sparse regression models. Journal\nof the American Statistical Association 104, 735\u2013746.\nPesaran, M.H., Yamagata, T., 2012. Testing CAPM with a large number of assets. Working paper.\nRoll, R., 1977. A critique of the asset pricing theory\u2019s tests part i: On past and potential testability of the theory.\nJournal of Financial Economics 4, 129\u2013176.\nSharpe, W.F., 1964. Capital asset prices: A theory of market equilibrium under conditions of risk. Journal of Finance\n19, 425\u2013442.\nSherman, M., 1997. Subseries methods in regression. Journal of the American Statistical Association 92, 1041\u20131048.\nZhang, D., Wu, W.B., et al., 2017. Gaussian approximation for high dimensional time series. Annals of Statistics 45,\n1895\u20131919.\n31\n\nOnline Appendix\nHigh-Dimensional Mean-Variance Spanning\nTests\n\nI. Additional Lemma and Proofs\nLemma 1 derives the decomposition of a precision matrix using nodewize regressions. Similar\nresults can also be found in Peng et al. (2009).\nLemma 1. Consider the random vector xt \u2208Rd and the following sequence of d regressions\nxj,t = Pd\ni=1,i\u0338=j \u03b8i,jxi,t + vj,t with E[x\u2212j,tvj,t] = 0 and E[vj,t] = 0(d\u22121)\u00d71 for j = 1, . . . , d. Let\nV[vi,t] = g2\ni for i = 1, . . . , d, G \u2261Diag(g2\n1, . . . , g2\nd) and \u0398 \u2261(\u03b8i,j).\n1. Using the exogeneity conditions E[x\u2212j,tvj,t] = 0(d\u22121)\u00d71 for any j, and k \u0338= j, one can\ndeduce that E[vk, vj] = E[vk(xj,t \u2212Pd\ni=1,i\u0338=j \u03b8i,jxi,t)] = \u2212\u03b8j,kE[vk, xk] = \u2212\u03b8j,kE[vk, vk] =\n\u2212\u03b8j,kV(vk) = \u2212\u03b8j,kgk.\n2. Therefore, \u03a3\u22121 = E[xtx\u2032\nt]\u22121 = G\u22121(Id \u2212\u0398) if E[xtx\u2032\nt] is invertible.\n3. One also has that E[vtv\u2032\nt] = (Id \u2212\u0398)G.\nLemma 1 suggests that Proposition 1 is valid for any N and K < T.\nProof of Proposition 1. The result follow from a direct application of Lemma 1 to xt = (r1,t, 1, r2,j,t\u2212\nr1,t, r\u2032\n1,\u22121,t \u2212i\u2032\nK\u22121r1,t)\u2032. See Section 2.2 in the paper for an illustration.\nProof of Theorem 1. Since for b = 1, . . .",
    "chunk_index": 34,
    "start_char": 72503,
    "end_char": 75518,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "is invertible.\n3. One also has that E[vtv\u2032\nt] = (Id \u2212\u0398)G.\nLemma 1 suggests that Proposition 1 is valid for any N and K < T.\nProof of Proposition 1. The result follow from a direct application of Lemma 1 to xt = (r1,t, 1, r2,j,t\u2212\nr1,t, r\u2032\n1,\u22121,t \u2212i\u2032\nK\u22121r1,t)\u2032. See Section 2.2 in the paper for an illustration.\nProof of Theorem 1. Since for b = 1, . . . , B, \u02c6mj(\u02c6\u03b8j)b converges to \u02c6mj(\u03b8j)b at the rate\n\u221a\nT \u226b\n\u221aTb, standard results on batch-mean for mean estimate of a stationary sequence apply. That is,\n\u02c6vj(\u02c6\u03b8j)B \u2212V[mj(\u03b8j)] = op(1) while mj(\u02c6\u03b8j)b = mj(\u03b8j)b + op(1) and mj(\u02c6\u03b8j)b\u2032 = mj(\u03b8j)b\u2032 + op(1)\nhave zero covariance whenever b \u0338= b\u2032 and T \u2192\u221e. Furthermore, \u02c6mj(\u02c6\u03b8j)1, . . . , \u02c6mj(\u02c6\u03b8j)B\u22121 and\n\u02c6mj(\u02c6\u03b8)B behave asymptotically as i.i.d. normal random variables with mean mj(\u03b8) and variance\nV[mj(\u03b8j)] (Carlstein et al., 1986). Therefore, we can apply a Student-t test to test the mean of\nthe sample {mj(\u02c6\u03b8j)b}B\nb=1 as in (13). We refer to Carlstein et al. (1986); Pedersen (2020) and\nsubsequent references for more details about the theory of the batch-mean method for general\nstatistics under mixing conditions. See also Zhang et al. (2017, Theorem 5.1 and 5.2.) for the rate\nof convergence of (v1(\u02c6\u03b8j)B, . . . , vd(\u02c6\u03b8j)B)\u2032 under the physical dependence framework.\nI\n\nII. Additional Results\nTable A.1 reports the sample size T, the number benchmark assets K, and the number of test assets\nN used in the two empirical applications of Section 4 of the paper.\nTable A.1: Dimensions of the Universes in the Empirical Applications\nThe table reports the number of observations T, the number of benchmark assets K, and test assets N\nin the two empirical applications of Section 4. For the first application, the element under N for a given\nbenchmark universe equals the sum of those under the columns K of the other benchmark assets, except\nfor 2015. This is because of dual listed stocks in the SMI and Euronext 100. In the second application, the\nnumber of factors was the same every year: 5 for FF5, 6 for FF6, 25 for FF25, 100 for FF100, and 153 for\nKEW and KVW.\nApplication 1\nApplication 2\nS&P 100\nEuronext 100\nSMI\nS&P 500\nYear\nT\nK\nN\nK\nN\nK\nN\nT\nN\n2007\n244\n99\n118\n98\n119\n20\n197\n251\n496\n2008\n245\n99\n118\n98\n119\n20\n197\n253\n497\n2009\n245\n100\n115\n96\n119\n19\n196\n252\n498\n2010\n248\n100\n117\n98\n119\n19\n198\n252\n498\n2011\n248\n100\n115\n95\n120\n20\n195\n252\n496\n2012\n243\n99\n116\n96\n119\n20\n195\n250\n496\n2013\n243\n100\n119\n99\n120\n20\n199\n252\n497\n2014\n243\n100\n117\n97\n120\n20\n197\n252\n500\n2015\u2217\n246\n101\n118\n99\n120\n20\n199\n252\n495\n2016\n248\n102\n118\n98\n122\n20\n200\n252\n502\n2017\n245\n101\n119\n99\n121\n20\n200",
    "chunk_index": 35,
    "start_char": 75166,
    "end_char": 77726,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "252\n498\n2010\n248\n100\n117\n98\n119\n19\n198\n252\n498\n2011\n248\n100\n115\n95\n120\n20\n195\n252\n496\n2012\n243\n99\n116\n96\n119\n20\n195\n250\n496\n2013\n243\n100\n119\n99\n120\n20\n199\n252\n497\n2014\n243\n100\n117\n97\n120\n20\n197\n252\n500\n2015\u2217\n246\n101\n118\n99\n120\n20\n199\n252\n495\n2016\n248\n102\n118\n98\n122\n20\n200\n252\n502\n2017\n245\n101\n119\n99\n121\n20\n200\n251\n502\n2018\n242\n102\n118\n98\n122\n20\n200\n251\n505\n2019\n243\n100\n116\n97\n119\n19\n197\n252\n500\n2020\n246\n100\n110\n90\n120\n20\n190\n253\n501\n2021\n248\n101\n103\n83\n121\n20\n184\n252\n504\n2022\n247\n101\n101\n81\n121\n20\n182\n251\n501\nII",
    "chunk_index": 36,
    "start_char": 77415,
    "end_char": 77932,
    "paper_title": "High-Dimensional Mean-Variance Spanning Tests",
    "paper_category": "stat.ME",
    "paper_filename": "High-Dimensional_Mean-Variance_Spanning_Tests.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ME/High-Dimensional_Mean-Variance_Spanning_Tests.pdf"
  },
  {
    "text": "Robust Detection of Lead-Lag Relationships in\nLagged Multi-Factor Models\nYichi Zhang1,4, Mihai Cucuringu1,2,4,7, Alexander Y. Shestopaloff5,6, Stefan Zohren3,4,7\n1Department of Statistics, University of Oxford\n2Mathematical Institute, University of Oxford\n3Department of Engineering, University of Oxford\n4Oxford-Man Institute of Quantitative Finance, University of Oxford\n5School of Mathematical Sciences, Queen Mary University of London\n6Department of Mathematics and Statistics, Memorial University of Newfoundland\n7The Alan Turing Institute\nSeptember 20, 2023\nAbstract\nIn multivariate time series systems, key insights can be obtained by discovering lead-lag\nrelationships inherent in the data, which refer to the dependence between two time series\nshifted in time relative to one another, and which can be leveraged for the purposes of control,\nforecasting or clustering. We develop a clustering-driven methodology for robust detection of\nlead-lag relationships in lagged multi-factor models. Within our framework, the envisioned\npipeline takes as input a set of time series, and creates an enlarged universe of extracted\nsubsequence time series from each input time series, via a sliding window approach. This is\nthen followed by an application of various clustering techniques, (such as K-means++ and\nspectral clustering), employing a variety of pairwise similarity measures, including nonlinear\nones. Once the clusters have been extracted, lead-lag estimates across clusters are robustly\naggregated to enhance the identification of the consistent relationships in the original universe.\nWe establish connections to the multireference alignment problem for both the homogeneous\nand heterogeneous settings.\nSince multivariate time series are ubiquitous in a wide range\nof domains, we demonstrate that our method is not only able to robustly detect lead-lag\nrelationships in financial markets, but can also yield insightful results when applied to an\nenvironmental data set.\nKeywords: High-dimensional time series; Lead-lag relationships; Unsupervised learning;\nClustering; Financial markets\n1\narXiv:2305.06704v3 [stat.ML] 19 Sep 2023\n\nContents\n1\nIntroduction\n3\n2\nConnections to Multireference Alignment (MRA)\n5\n3\nModel setup\n6\n3.1\nDescription . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2\nNotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4\nMethodology\n8\n5\nSynthetic data experiments\n12\n5.1\nSetup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5.3\nSimulation . . . .",
    "chunk_index": 0,
    "start_char": 0,
    "end_char": 2700,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5.3\nSimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6\nFinancial data experiments\n18\n6.1\nData description\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n6.2\nData pre-processing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6.3\nBenchmark\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6.4\nTrading strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n6.5\nPerformance evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n6.6\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n7\nRobustness analysis\n28\n8\nCO2 emissions data\n33\n8.1\nData description\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n8.2\nPipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n8.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "chunk_index": 1,
    "start_char": 2520,
    "end_char": 3723,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n8.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n9\nConclusion\n35\nA Appendix\n41\nA.1 Synthetic data experiment: spectral clustering . . . . . . . . . . . . . . . . . . . . .\n41\nA.2 CO2 emissions plots\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nA.3 Futures data set details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n2\n\n1\nIntroduction\nWhen observed over time, natural physical systems frequently produce data recorded as high-\ndimensional, nonlinear time series, which are ubiquitous in a wide range of domains. Numerous con-\ntributions, covering different aspects of their analysis, have been made. For example, Cont (2001)\ndiscussed financial time series with a focus on various statistical properties, including distributional\nand tail properties, extreme fluctuations, etc. Cartea, Donnelly, and Jaimungal (2018) constructed\na metric for assessing volume imbalance in the limit order book sourced from the Nasdaq exchange,\nand demonstrated it is a good predictor of the sign of the next market order. Cao, Chen, and\nHull (2020) utilized neural networks to analyze volatility surface movements based on daily call\noptions data on the S&P 500 index. Drinkall, Zohren, and Pierrehumbert (2022) introduced a new\ntechnique that integrates time-series language models based on transformers into the field of infec-\ntious disease modelling. Sokolov et al. (2022) proposed a supervised machine learning framework\nfor analyzing the impact of environmental, social and governance (ESG) factors on fund flows in\nUS-domiciled equity mutual funds, and assessed whether sustainability has excess predictive power\non fund-level flows as compared to benchmark driven by non-ESG factors. To detect change-points\nin nonlinear time series regression, Cui, Yang, and Zhou (2021) utilized a density-weighted anti-\nsymmetric kernel function and identified the presence of change-points within the state domain,\nrather than the time domain. Furthermore, Vuleti\u00b4c, Prenzel, and Cucuringu (2023) examined the\nfeasibility of utilizing generative adversarial networks to forecast financial time series probabilis-\ntically, and to learn from asset co-movements while addressing problems associated with mode\ncollapse.\nKey insights regarding high-dimensional time series can be obtained by discovering latent struc-\ntures.",
    "chunk_index": 2,
    "start_char": 3548,
    "end_char": 6046,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "function and identified the presence of change-points within the state domain,\nrather than the time domain. Furthermore, Vuleti\u00b4c, Prenzel, and Cucuringu (2023) examined the\nfeasibility of utilizing generative adversarial networks to forecast financial time series probabilis-\ntically, and to learn from asset co-movements while addressing problems associated with mode\ncollapse.\nKey insights regarding high-dimensional time series can be obtained by discovering latent struc-\ntures. An example of a latent structure is lead-lag relationships, which are widely observed and\nfound in the realms of finance [Tolikas (2018), Buccheri, Corsi, and Peluso (2021), Miori and\nCucuringu (2022), Bennett, Cucuringu, and Reinert (2022), Albers et al. (2021), Ito and Sake-\nmoto (2020), Yao and H.-Y. Li (2020), Y. Li et al. (2021)], the environment [De Luca and Piz-\nzolante (2021), Wu et al. (2010)], and biology [Runge et al. (2019)].\nFor example, Miori and\nCucuringu (2022) explored lead-lag relationships within data-driven macroeconomic regimes by\nclustering the performance of diverse asset class indices in time relative to each other. Bennett,\nCucuringu, and Reinert (2022) constructed a directed network for encoding pairwise lead-lag rela-\ntionships between time series of equity prices in the US equity market, in order to detect pairs of\nlead-lag clusters that exhibited a high pairwise directed flow imbalance. Another finance-related\napplication study was derived from Albers et al. (2021), who analyzed the existence and strength\nof lead-lag relationships between pairs of Bitcoin markets. The importance and potentially high\nimpact of this problem are broadly recognized; however, to date, there has been limited progress\non the robust detection of lead-lag relationships in high-dimensional time series.\nClustering is widely used as part of the analysis of time series [Zolhavarieh, Aghabozorgi, and\nTeh (2014)]. For example, Lu, Reinert, and Cucuringu (2023) developed a similarity measure be-\ntween equities based on co-occurrence of trades from Lu, Reinert, and Cucuringu (2022) and used\nspectral clustering algorithms [Shi and Malik (2000), Ng, Jordan, and Weiss (2001), Cucuringu,\n3\n\nDavies, et al. (2019)] to detect dynamic communities within US equity markets. In particular,\nsubsequence time series clustering, as one of time series clustering, groups similar subsequences\ninto the same cluster. This type of clustering is used for detecting structures or patterns, and is\ntypically used as a subroutine in rule discovery [Uehara and Shimada (2002), Sarker et al. (2003)],\nindexing [C.-S. Li, Yu, and Castelli (1998), Radhakrishnan, Wilson, and Loizou (2000)], classifi-\ncation [Cotofrei and Stoffel (2002), Vijay and Nanda (2023)], prediction [Tino, Schittenkopf, and\nDorffner (2000), Schittenkopf, Ti\u02c7no, and Dorffner (2002)], and anomaly detection [Yairi, Kato, and\nHori (2001), Dong et al. (2023)]. However, despite its wide use, Lin, Keogh, and Truppel (2003) re-\nported a surprising result: clustering subsequences extracted from a single time series via a sliding\nwindow is meaningless. They illustrated that all previous results involving clustering subsequences\nof a single time series were inaccurate since the resultant cluster centers appeared to be a form of\nsine waves, regardless of the initial patterns in the input data [Keogh and Lin (2005)]. Afterwards,\nseveral lines of work [Madicar et al.",
    "chunk_index": 3,
    "start_char": 5563,
    "end_char": 8987,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Hori (2001), Dong et al. (2023)]. However, despite its wide use, Lin, Keogh, and Truppel (2003) re-\nported a surprising result: clustering subsequences extracted from a single time series via a sliding\nwindow is meaningless. They illustrated that all previous results involving clustering subsequences\nof a single time series were inaccurate since the resultant cluster centers appeared to be a form of\nsine waves, regardless of the initial patterns in the input data [Keogh and Lin (2005)]. Afterwards,\nseveral lines of work [Madicar et al. (2013), Rakthanmanon et al. (2011), Rodpongpun, Niennat-\ntrakul, and Ratanamahatana (2012)] proposed solutions to the aforementioned meaningless results\nof subsequence time series clustering, and achieved meaningful time series clusters. In contrast,\nwe take the approach of clustering every subsequence, extracted via a sliding window from a set of\ntime series.\nIn this paper, we develop a clustering-driven methodology to enable the robust detection of\nlead-lag relationships in high-dimensional time series. In the proposed pipeline, we are given as\ninput a set of n time series. With this in mind, we create an enlarged universe U of N = n \u00d7 h\ntime series by extracting h subsequence time series via a sliding window from each input time\nseries. To this enlarged universe U, we apply various clustering techniques (e.g, K-means++ and\nspectral clustering) by employing various pairwise similarity measures between subsequence time\nseries. The underlying clusters are then leveraged for the purpose of discovering the latent lead-\nlag relationships. In essence, the clustering step can be construed as an initial denoising step,\nin which we group together data relevant to estimating a subset of the lead-lag effects.\nOnce\nthe clusters have been extracted, the lead-lag estimates within each cluster are aggregated across\nclusters to enhance the identification of consistent relationships in the original universe. Our main\ncontributions are summarized as follows.\n4\n\nSummary of main contributions.\n1. We introduce a computationally scalable pipeline for the robust detection of lead-lag\nrelationships in high-dimensional time series.\n2. We demonstrate that our proposed methodology can reliably detect lead-lag relation-\nships in a range of factor model-based simulated high-dimensional time series.\n3. In a financial market setting, we leverage the detected lead-lag relationships to con-\nstruct a profitable trading strategy and show that our method outperforms the bench-\nmark in most of cases.\n4. We apply our method to a data set of CO2 emissions and demonstrate that it achieves\nresults consistent with the literature.\nPaper outline. This paper is organized as follows. We first discuss connections between lead-\nlag detection and the multireference alignment (MRA) problem in Section 2. Section 3 describes\nthe lagged multi-factor model and establishes the notation used in this paper. Section 4 describes\nour proposed method for detecting lead-lag relationships in high-dimensional time series. In Section\n5, we validate our method on synthetic data sets from the lagged multi-factor model, and show\nthat our method can reliably recover them. Since high-dimensional time series data are ubiquitous\nin finance, our application domain mainly targeted financial time series.",
    "chunk_index": 4,
    "start_char": 8446,
    "end_char": 11770,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "lag detection and the multireference alignment (MRA) problem in Section 2. Section 3 describes\nthe lagged multi-factor model and establishes the notation used in this paper. Section 4 describes\nour proposed method for detecting lead-lag relationships in high-dimensional time series. In Section\n5, we validate our method on synthetic data sets from the lagged multi-factor model, and show\nthat our method can reliably recover them. Since high-dimensional time series data are ubiquitous\nin finance, our application domain mainly targeted financial time series. Within this domain, each\ntime series corresponds to the excess return time series of a specific financial instrument.\nWe\nexplore lead-lag relationships in the US equity, ETF, and futures markets in Section 6. We then\nmove on to a robustness analysis in Section 7. To demonstrate the broader scope and applicability\nof our proposed algorithm, we also consider an application from the environmental sciences, looking\nat a data set of CO2 emissions in Section 8. Finally, we summarize our findings in Section 9, and\ndiscuss possible future research directions.\n2\nConnections to Multireference Alignment (MRA)\nOur proposed framework for lead-lag detection has strong connections to the multireference align-\nment (MRA) problem, which we briefly describe here. MRA aims to estimate one signal from n\ncyclically and noisy shifted copies of itself [Bandeira et al. (2014)]. In the homogeneous setting,\nlet x \u2208RL be the unknown signal and let Rr be the cyclic shift operator by r. We are given n\nmeasurements of the form\nyj = Rrjx + \u03b5j,\nj = 1, . . . , n,\n(1)\nwhere the \u03b5j \u223cN\n\u00000, \u03c32I\n\u0001\nare i.i.d. white Gaussian noise. The goal of MRA is to then estimate\nthe unknown signal x, up to a shift, in a regime with high-noise in which the shifts rj are also\nunknown.\nIn the heterogeneous setting that arises in areas such as cryo-electron microscopy, there are\n5\n\nk distinct signals x1, . . . , xk \u2208RL to be estimated. Each of the available n observations is derived\nfrom one of these k signals, yet the correspondence is unknown to the user. The model can be\nwritten as follows\nyj = Rrjxvj + \u03b5j,\nj = 1, . . . , n,\n(2)\nwhere the classes vj and the shifts rj are unknown, while the \u03b5j are i.i.d. Gaussian noise of variance\n\u03c32 as before. The goal is to estimate the signals x1, . . . , xk, up to shifts and ordering. Figure 1\nillustrates an example of nine observations xi, yi, zi (i = 1, 2, 3), each derived from one of the three\nsignals x, y, z \u2208R6 with a noisy cyclic shift.\nx\nx1\nx3\nx2\ny\ny1\ny2\ny3\nz\nz1\nz2\nz3\nFigure 1: Nine observations derived from one of three signals with noisy cyclic shifts.\nThe work of [Perry et al. (2019)] introduced the first known procedure to provably achieve\nsignal recovery in a low signal-to-noise ratio (SNR) regime for heterogeneous MRA. Moreover,\n[Boumal et al.",
    "chunk_index": 5,
    "start_char": 11210,
    "end_char": 14051,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "xi, yi, zi (i = 1, 2, 3), each derived from one of the three\nsignals x, y, z \u2208R6 with a noisy cyclic shift.\nx\nx1\nx3\nx2\ny\ny1\ny2\ny3\nz\nz1\nz2\nz3\nFigure 1: Nine observations derived from one of three signals with noisy cyclic shifts.\nThe work of [Perry et al. (2019)] introduced the first known procedure to provably achieve\nsignal recovery in a low signal-to-noise ratio (SNR) regime for heterogeneous MRA. Moreover,\n[Boumal et al. (2018)] considered the application of MRA to the 2D class averaging problem in\ncryo-EM, aiming to achieve classification, alignment and averaging concurrently in a single pass,\nwithout using any of these steps explicitly. The authors of [Boumal et al. (2018)] also proposed\nto use signal characteristics that are invariant under translations, and used them to recover the\noriginal signal. In the spirit of MRA, we will consider an analogous single-pass approach for robust\ndetection of lead-lag relationships.\n3\nModel setup\nIn this section, we will introduce the standard and lagged versions of the multi-factor model, which\nwe will assume as a model for our time series data. The lagged multi-factor model will be used to\nvalidate our method on a synthetic data scenario before proceeding to the real-world applications.\nThe gist of these models is to represent a time series as a (noisy) superposition of factors with\nvarying exposures to each factor. We will also summarize the notation used in this paper.\n6\n\n3.1\nDescription\nWe first recall the standard multi-factor model for a multivariate time series\nXt\ni =\nk\nX\nj=1\nBijf t\nj + \u03f5t\ni\ni = 1, . . . , n;\nt = 1, . . . , T,\n(3)\nwhere Xt\ni is the time series i (e.g., the excess return of a financial asset) at time t, k is the number\nof factors, Bij is the exposure of time series i to factor j, f t\nj is the factor j at time t, and \u03f5t\ni is the\nnoise at time t, with variance \u03c32. Furthermore, n is the number of time series, and T is the total\nnumber of time steps.\nIn this paper, we focus on the lagged version of the multi-factor model, which can be written as\nXt\ni =\nk\nX\nj=1\nBijf t\u2212Lij\nj\n+ \u03f5t\ni\ni = 1, . . . , n;\nt = 1, . . . , T,\n(4)\nwhere the only difference compared to the standard multi-factor model is the addition of Lij, the\nlag at which time series i is exposed to factor j. Thus, f t\u2212Lij\nj\nis the value of factor j at time t\u2212Lij.\nWe introduce two main settings in the lagged multi-factor model (4), as follows.\n\u2022 Single Membership: Each time series has a lagged exposure to a single factor. We consider\nthe following two main categories.\n\u2013 Homogeneous Setting: The model only has one factor, i.e. k = 1.",
    "chunk_index": 6,
    "start_char": 13624,
    "end_char": 16219,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "is the addition of Lij, the\nlag at which time series i is exposed to factor j. Thus, f t\u2212Lij\nj\nis the value of factor j at time t\u2212Lij.\nWe introduce two main settings in the lagged multi-factor model (4), as follows.\n\u2022 Single Membership: Each time series has a lagged exposure to a single factor. We consider\nthe following two main categories.\n\u2013 Homogeneous Setting: The model only has one factor, i.e. k = 1.\n\u2013 Heterogeneous Setting: The model has more than one factor, i.e. k \u22652. However,\neach time series is exposed only to a single factor.\n\u2022 Mixed Membership: Each time series is allowed to have a lagged exposure to more than\none factor, hence it is mixed. The model contains at least two factors, i.e. k \u22652.\nIn this paper, our goal will be the inference of Lij in the lagged multi-factor model, focusing on\nthe single membership setting. We do not focus on the inference of the unknown coefficient matrix\nB and factors f. As shown later, the inference of Lij alone is of practical importance in certain\napplications, e.g. finance. We leave the mixed membership setting for future research.\n3.2\nNotation\nWe first introduce the definition of a time series, subsequence time series and sliding window.\n\u2022 Time Series: A time series Xi = X1\ni , . . . , XT\ni is an ordered set of T real-valued variables.\n\u2022 Subsequence Time Series (STS): Given a time series Xi = X1\ni , . . . , XT\ni\nof length T, a\nSTS Y z\ni of time series Xi is a sample of length q < T of contiguous positions from Xi starting\nat z, that is, Yi = Xz\ni , . . . , Xz+q\u22121\ni\nwhere 1 \u2264z \u2264T \u2212q + 1.\n\u2022 Sliding Window: Given a set of time series Xt\ni (i = 1, . . . , n; t = 1, . . . , T), and a user-\ndefined STS length of q, an enlarged universe matrix U of N STS can be built by sliding a\nwindow shifted by s across Xt\ni. The size of the enlarged universe matrix U is N by q.\n7\n\nWe summarize the remaining notation conventions used in the paper in Table 1.\nTable 1: Variables and their description as used in our paper.\nVariables\nDescription\nk\nTotal number of factors\nK\nTotal number of clusters\nn\nTotal number of time series\nN\nTotal number of STS in the enlarged universe\nm\nNumber of lags\nM\nMaximum number of lags\nt\nTime\nT\nLength of time series\nq\nLength of STS\nh\nNumber of STS in each time series\nU\nUniverse set, it contains N STS with length of q\nl\nLength of sliding window\n\u03c3\nNoise\ns\nValue of sliding window shift\n\u03b4\nForward looking horizon windows\np\nPast looking horizon windows\nXn\u00d7T\nTime series matrix, where Xt\ni is the time series",
    "chunk_index": 7,
    "start_char": 15811,
    "end_char": 18306,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "number of STS in the enlarged universe\nm\nNumber of lags\nM\nMaximum number of lags\nt\nTime\nT\nLength of time series\nq\nLength of STS\nh\nNumber of STS in each time series\nU\nUniverse set, it contains N STS with length of q\nl\nLength of sliding window\n\u03c3\nNoise\ns\nValue of sliding window shift\n\u03b4\nForward looking horizon windows\np\nPast looking horizon windows\nXn\u00d7T\nTime series matrix, where Xt\ni is the time series i at time t\nXi\nTime series i\nY z\ni\nSTS from time series i starting at z\nBn\u00d7k\nLoading matrix, where Bij is the exposure of time series i to factor j\nfk\u00d71\nVector of returns of the k factors\nf t\nj\nValue of returns of factor j at time t\n\u03f5n\u00d71\nVector of noise\nLn\u00d7k\nLag matrix, where Lij is the lag of time series i to factor j\nUN\u00d7q\nUniverse matrix, where Uij is the STS i at time j\nCN\u00d7N\nCorrelation matrix, where Cij is the correlation of STS i and j\nPN\u00d7N\nSparse matrix, where Pij is assigned the weight of edge that connects STS i and j\nGN\u00d7N\nSimilarity matrix, where Gij is the similarity of STS i and j\n\u03d5d\nCluster d\n{Xi, Xj}\nPair of time series i and j\n\u2206d{Xi, Xj}\nSet of the relative lags between all pairs of time series i and j in cluster d\nVi\u00d7j\nVoting matrix, where vi\u00d7j is the number of lags between STS of i and j in the same cluster\n\u03b3{Xi, Xj}\nThe estimated value of the relative lags between all pairs of time series i and j in all clusters\n\u0393n\u00d7n\nLead-lag matrix, where \u0393ij is the lead or lag value between time series i and j\nEn\u00d7n\nError matrix, where Eij is the error of lead or lag value between time series i and j\n\u03a8n\u00d7n\nGround truth lead-lag matrix, where \u03a8ij is the ground truth of lead or lag value between\ntime series i and j\n4\nMethodology\nIn this section, we present our methodology to infer unknown lags Lij in detail, before applying it\nto synthetic, financial, and environmental data set.\nWe consider a set of time series Xn\u00d7T as our input. STS with length q are extracted from each\ntime series Xi by a sliding window shifted by s. Therefore, the number of STS for each time series\nin the ensemble is h = T \u2212q\ns\n+1, and the total number of STS from Xn\u00d7T is N = n\u00b7h. An enlarged\nuniverse matrix UN\u00d7q is constructed by collecting all STS, which is shown in Figure 2. Note that\nUN\u00d7q contains exactly the same information as Xn\u00d7T .\nAfter the STS extraction step, we employ various clustering techniques to group similar STS\n8\n\n X1\nSTS\nSTS\nSTS\n X2\n Xn\nU\nFigure 2: Left: STS are extracted from each time series via a sliding window approach. Right: An\nenlarged universe matrix UN\u00d7q is constructed by stacking all the extracted STS from the input set\nof time series Xn\u00d7T .",
    "chunk_index": 8,
    "start_char": 17905,
    "end_char": 20483,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "is N = n\u00b7h. An enlarged\nuniverse matrix UN\u00d7q is constructed by collecting all STS, which is shown in Figure 2. Note that\nUN\u00d7q contains exactly the same information as Xn\u00d7T .\nAfter the STS extraction step, we employ various clustering techniques to group similar STS\n8\n\n X1\nSTS\nSTS\nSTS\n X2\n Xn\nU\nFigure 2: Left: STS are extracted from each time series via a sliding window approach. Right: An\nenlarged universe matrix UN\u00d7q is constructed by stacking all the extracted STS from the input set\nof time series Xn\u00d7T .\nin the same cluster. One way is to apply K-means++ (KM) clustering [Arthur and Vassilvit-\nskii (2006)]. We follow this approach in Algorithm 1, which initializes the cluster centers before\nproceeding with the standard K-means algorithm. With the KM initialization, a solution that is\nwithin O(log(k)) of the optimal standard K-means solution is guaranteed.\nAlgorithm 1 : K-means++ Clustering\nInput: Universe matrix UN\u00d7q, the total number of clusters K.\nOutput: Clusters \u03d51, \u03d52, ..., \u03d5K.\n1: Randomly select an initial center \u03d51 from U.\n2: Repeat for i \u22081, 2, . . . , K \u22121, K. Select the next center \u03d5i = x \u2208U with the probability\nP(x) =\nD(x)2\nP\nx\u2032\u2208U D (x\u2032)2 ,\nwhere x\u2032 is the closest center that has already been chosen and D (x\u2032) is the distance to that\ncenter.\n3: Continue with the standard K-means algorithm.\nAlgorithm 2 : Spectral Clustering\nInput: Similarity matrix GN\u00d7N, the total number of clusters K.\nOutput: Clusters \u03d51, \u03d52, ..., \u03d5K.\n1: Compute normalized Laplacian L.\n2: Compute the eigenvectors v1, v2, ..., vK corresponding to K smallest eigenvalues of L.\n3: Construct matrix M \u2208RN\u00d7K with v1, v2, ..., vK as columns.\n4: Form matrix \u02dc\nM \u2208RN\u00d7K by normalizing row vectors of M to norm 1.\n5: Apply K-means clustering to assign rows of \u02dc\nM to clusters \u03d51, \u03d52, ..., \u03d5K.\nAn alternative method we considered is that of spectral (SP) clustering, described in Algo-\n9\n\nrithm 2. Note that if the size UN\u00d7q is prohibitively large due to a large number n of input time\nseries, in order to speed up the computation of eigenvalues and eigenvectors, one could apply the\nK-nearest neighbours algorithm (KNN) on UN\u00d7q, which leads to the sparse matrix PN\u00d7N. The\nentries of the similarity matrix GN\u00d7N are computed using a Gaussian kernel between neighbours\nfrom PN\u00d7N.\nGij = exp\n\u0010\n\u2212\n\r\rY a\ni \u2212Y b\nj\n\r\r2 /\n\u00002\u03c32\u0001\u0011\n,\n(5)\nwhere the parameter \u03c3 = 1/N by default. Finally, we apply SP clustering on GN\u00d7N.\nWe denote time series i and j by Xi and Xj respectively, and a pair consisting of them by\n{Xi, Xj}.\nFor each cluster \u03d5d (d = 1, . . . , K), we consider all possible pairwise relative lags\nof Xi and Xj.\nWe denote this set by \u2206d{Xi, Xj} and obtain it by computing the difference\nbetween starting indices of STS from Xi and Xj appearing in cluster d.",
    "chunk_index": 9,
    "start_char": 19972,
    "end_char": 22719,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "= 1/N by default. Finally, we apply SP clustering on GN\u00d7N.\nWe denote time series i and j by Xi and Xj respectively, and a pair consisting of them by\n{Xi, Xj}.\nFor each cluster \u03d5d (d = 1, . . . , K), we consider all possible pairwise relative lags\nof Xi and Xj.\nWe denote this set by \u2206d{Xi, Xj} and obtain it by computing the difference\nbetween starting indices of STS from Xi and Xj appearing in cluster d. Let Si = {s1\ni , . . . , sk\ni } and\nSj = {s1\nj, . . . , sl\nj} be the sets of STS starting indices of, respectively, Xi and Xj. To avoid double\ncounting, we consider only i < j. This amount to the following\n\u2206d{Xi, Xj} = \u2206d{(Y s1\ni\ni , Y\ns1\nj\nj ), . . . , (Y sk\ni\ni\n, Y\nsl\nj\nj )} = {s1\ni \u2212s1\nj, . . . , sk\ni \u2212sl\nj}.\n(6)\nWe then aggregate \u2206d{Xi, Xj} across all clusters and estimate the relative lag of {Xi, Xj} by\nconsidering the mode or median of the resulting set\n\u03b3{Xi, Xj} =\n\uf8f1\n\uf8f2\n\uf8f3\nMode(SK\nd=1 \u2206d{Xi, Xj})\nMode estimation\nMedian(SK\nd=1 \u2206d{Xi, Xj})\nMedian estimation\n(7)\nTable 2 shows the example of two time series X1 and X2, for which the ground truth lag value\nis 3. We extract eleven STS from each time series by a sliding window, and then perform KM\nclustering on these STS, setting the value of K to 11. In each cluster, we calculate the relative\nlags of STS \u2206d{X1, X2} (d = 1, . . . , 11). After that, we aggregate lags across all clusters together,\nas S11\nd=1 \u2206d{X1, X2} = {\u22127, 3, 3, 3, 3, 3, 3, 3, 3, \u221210, \u22129}. Figure 3 displays the histogram of the\nrelative lags of STS from X1 and X2.\n10\n\nTable 2: Example of calculating the relative lags\nof STS in each cluster from two time series.\nCluster\nSubsequence\nLag\n\u03d51\n(Y 9\n1 , Y 2\n2 )\n-7\n\u03d52\n(Y 7\n1 , Y 10\n2 )\n3\n\u03d53\n(Y 1\n1 , Y 4\n2 )\n3\n\u03d54\n(Y 2\n1 , Y 5\n2 )\n3\n\u03d55\n(Y 0\n1 , Y 3\n2 )\n3\n\u03d56\n(Y 6\n1 , Y 9\n2 )\n3\n\u03d57\n(Y 3\n1 , Y 6\n2 )\n3\n\u03d58\n(Y 4\n1 , Y 7\n2 )\n3\n\u03d59\n(Y 5\n1 , Y 8\n2 )\n3\n\u03d510\n(Y 1\n1 0, Y 0\n2 ), (Y 1\n1 0, Y 1\n2 )\n-10, -9\n\u03d511\nY 8\n1\nNaN\n8\n7\n6\n5\n4\n3\n2\n1 0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\nLag\n0\n1\n2\n3\n4\n5\n6\n7\n8\nCounts\nFigure 3: Histogram of the relative lags of STS\nfrom two time series.",
    "chunk_index": 10,
    "start_char": 22313,
    "end_char": 24341,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "(Y 4\n1 , Y 7\n2 )\n3\n\u03d59\n(Y 5\n1 , Y 8\n2 )\n3\n\u03d510\n(Y 1\n1 0, Y 0\n2 ), (Y 1\n1 0, Y 1\n2 )\n-10, -9\n\u03d511\nY 8\n1\nNaN\n8\n7\n6\n5\n4\n3\n2\n1 0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11\nLag\n0\n1\n2\n3\n4\n5\n6\n7\n8\nCounts\nFigure 3: Histogram of the relative lags of STS\nfrom two time series.\nBy considering the mode or median of S11\nd=1 \u2206d{X1, X2}, we arrive at \u03b3{X1, X2} = 3. From Ta-\nble 2 and Figure 3, we observe that even though there are outliers {\u22127, \u221210, \u22129} in S11\nd=1 \u2206d{X1, X2},\nwe are still able to correctly recover the ground truth value of 3.\nA refinement of the algorithm is to calculate a voting matrix Vn\u00d7n by counting the sum of\nthe number of lags between STS from Xi and Xj in the same clusters. We can then set a voting\nthreshold denoted by \u03b8 to filter out a small number of counts. This is motivated by the reasoning\nthat unless STS corresponding to a pair of time series are consistently clustered together across\ndifferent clusters, the resulting lead-lag estimate is unlikely to be accurate. This altogether amounts\nto\nVij =\n\uf8f1\n\uf8f2\n\uf8f3\n| SK\nd=1 \u2206d{Xi, Xj}|\nif\n| SK\nd=1 \u2206d{Xi, Xj}| \u2265\u03b8\n0\notherwise\n(8)\nwhere | \u00b7 | counts the number of elements in the SK\nd=1 \u2206d{Xi, Xj}.\nFinally, the lead-lag matrix \u0393n\u00d7n is built by\n\u0393ij =\n\uf8f1\n\uf8f2\n\uf8f3\n\u03b3{Xi, Xj}\nif\nVij \u0338= 0\n0\notherwise\n(9)\nWe summarize the above procedures in Algorithm 3, which is our main algorithm.\n11\n\nAlgorithm 3 : Lead-lag Relationship Detection Algorithm\nInput: Time series matrix Xn\u00d7T .\nOutput: Lead-lag matrix \u0393n\u00d7n.\n1: STS Y p\ni are extracted from each time series Xi by a sliding window.\n2: An enlarged universe matrix UN\u00d7q is created in Step 2.\n3: Apply KNN to create the sparse matrix PN\u00d7N, and the similarity matrix GN\u00d7N is then only\ncomputed using the Gaussian kernel between neighbours from PN\u00d7N.\n4: Clusters are extracted by performing KM clustering on UN\u00d7q or SP clustering on GN\u00d7N. For\neach cluster, record lags between every pair of time series {Xi, Xj}.\n5: For each pair of time series {Xi, Xj}, we calculate the voting matrix Vij by counting lags\nbetween STS from Xi and Xj across all clusters. We use a voting threshold \u03b8 to filter small\ncounts.\n6: Calculate the lead-lag matrix \u0393n\u00d7n by considering mode or median of lags based on the voting\nmatrix Vn\u00d7n.\n5\nSynthetic data experiments\nThe purpose of our synthetic data experiments is to assume that our data is generated by a multi-\nfactor model with known ground truth lead-lag matrix L, and then validate the performance of\nour proposed algorithms under different scenarios.",
    "chunk_index": 11,
    "start_char": 24097,
    "end_char": 26558,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "all clusters. We use a voting threshold \u03b8 to filter small\ncounts.\n6: Calculate the lead-lag matrix \u0393n\u00d7n by considering mode or median of lags based on the voting\nmatrix Vn\u00d7n.\n5\nSynthetic data experiments\nThe purpose of our synthetic data experiments is to assume that our data is generated by a multi-\nfactor model with known ground truth lead-lag matrix L, and then validate the performance of\nour proposed algorithms under different scenarios.\n5.1\nSetup\nAs noted earlier, our focus is the single membership setting. We generate synthetic data from the\nlagged multi-factor model (4) with k = {1, 2, 3} factors. We let M = 5, T = 100 and n = 6. The\nfactors f and errors \u03f5 are assumed to be i.i.d. N(0, 1). We define B and L as follows:\nTable 3: Top row: Loading matrix B. Bottom row: Lag matrix L.\nHomogeneous Setting\nHeterogeneous Setting\nB\nL\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n1\n2\n3\n4\n5\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n1\n0\n1\n0\n0\n1\n0\n1\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n2\n0\n4\n0\n0\n0\n0\n2\n0\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n3\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nk = 1\nk = 2\nk = 3\nWhen estimating the lead-lag matrix, we use a sliding window of length q = 90 and a shift\nof s = 1. After estimating the lead-lag matrix, we calculate the error matrix E to evaluate the\nperformance of the method, which we denote as\nEn\u00d7n = \u0393n\u00d7n \u2212\u03a8n\u00d7n,\n(10)\n12\n\nwhere \u0393n\u00d7n is the estimated lead-lag matrix, and \u03a8n\u00d7n is the ground truth lead-lag matrix, which\ncan be obtained from Ln\u00d7k.\n5.2\nResults\nWe first explore the data by computing the Pearson and distance correlations between STS in\nthe universe matrix UN\u00d7q, for scenarios with different k. Figure 4 shows the correlation between\nSTS in the two settings, homogeneous and heterogeneous. Each heatmap represents similarities\nbetween the STS, with a darker shade of red corresponding to a higher similarity.\nFor the homogeneous setting (k = 1), we note that all 6\u00d76 blocks, with each block corresponding\nto a different time series, have groups of red pixels stretching diagonally from the top left to the\nbottom right. This reflects a high similarity between pairs of STS starting at different initial time\npoints. Thus, for example, time series 1 and time series 2 have highest correlation between STS\nat a lag of 1, which reflects the ground truth. For the heterogeneous setting (k \u2208{2, 3}), we note\nthat only the diagonal blocks show high similarities between STS, which reflects the structure of\nB. Again, we note a high similarity between STS at the ground truth lag value.",
    "chunk_index": 12,
    "start_char": 26113,
    "end_char": 28669,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "to the\nbottom right. This reflects a high similarity between pairs of STS starting at different initial time\npoints. Thus, for example, time series 1 and time series 2 have highest correlation between STS\nat a lag of 1, which reflects the ground truth. For the heterogeneous setting (k \u2208{2, 3}), we note\nthat only the diagonal blocks show high similarities between STS, which reflects the structure of\nB. Again, we note a high similarity between STS at the ground truth lag value. Our proposed\nmethod clusters these similar STS together to estimate the lag between the corresponding time\nseries.\nHomogeneous Setting\nHeterogeneous Setting\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nY0\n1\nY9\n1\nY7\n2\nY5\n3\nY3\n4\nY1\n5\nY10\n5\nY8\n6\nY0\n1\nY10\n1\nY9\n2\nY8\n3\nY7\n4\nY6\n5\nY5\n6\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nk = 1\nk = 2\nk = 3\nFigure 4: Top panel: Heatmap of enlarged universe UN\u00d7q by Pearson correlation. Bottom panel:\nHeatmap of enlarged universe UN\u00d7q by distance correlation.\nWe next follow the methodology described in Section 4 to estimate the lead-lag matrix. In\nwhat follows, we use KM clustering to cluster STS. We set the number of clusters for a model with\nk factors to 11 \u00b7 k. A similar result, utilizing SP clustering, is shown in Appendix A.1 Figures [16,\n17]. We first consider not setting a voting threshold (\u03b8 = 1). Figure 5 contains the number of\nvotes associated with each time series pair. The top plots denote the voting matrix, the middle\nplots represent the error matrix for the mode estimation, and the bottom plots correspond to the\n13\n\nmedian estimation. For the homogeneous case, we can observe that the voting matrix consists of\nall 0s across the diagonal and is non-zero for all other entries.",
    "chunk_index": 13,
    "start_char": 28189,
    "end_char": 30471,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Figures [16,\n17]. We first consider not setting a voting threshold (\u03b8 = 1). Figure 5 contains the number of\nvotes associated with each time series pair. The top plots denote the voting matrix, the middle\nplots represent the error matrix for the mode estimation, and the bottom plots correspond to the\n13\n\nmedian estimation. For the homogeneous case, we can observe that the voting matrix consists of\nall 0s across the diagonal and is non-zero for all other entries. The error matrices based on mode\nand median estimation consist of all 0s, which indicates our algorithm fully recovers the lead-lag\nrelationships between these six time series. However, for the heterogeneous setting, we can observe\nthat the error matrices have some non-zero values.\nK-means++ clustering\nHomogeneous Setting\nHeterogeneous Setting\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n15 13 11 11 11\n15\n0\n13 11 11 12\n13 13\n0\n11 11 12\n11 11 11\n0\n11 11\n11 11 11 11\n0\n13\n11 12 12 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n7\n4\n4\n3\n9\n0\n9\n2\n2\n2\n7\n9\n0\n2\n1\n1\n4\n2\n2\n0\n11\n9\n4\n2\n1\n11\n0\n9\n3\n2\n1\n9\n9\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n7\n5\n3\n0\n0\n0\n9\n7\n5\n0\n0\n0 -10 -7\n-9\n-7\n-9 10\n0\n0\n0\n-5\n-7\n7\n0\n0\n0\n-3\n-5\n9\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n7\n5\n3\n0\n0\n0\n9\n7\n5\n0\n0\n0\n-7\n-7\n-9\n-7\n-9\n7\n0\n0\n0\n-5\n-7\n7\n0\n0\n0\n-3\n-5\n9\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS",
    "chunk_index": 14,
    "start_char": 30006,
    "end_char": 31689,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "0\n0\n0\n-3\n-5\n9\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n7\n5\n3\n0\n0\n0\n9\n7\n5\n0\n0\n0\n-7\n-7\n-9\n-7\n-9\n7\n0\n0\n0\n-5\n-7\n7\n0\n0\n0\n-3\n-5\n9\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n8\n1\n0\n0\n0\n8\n0\n3\n3\n2\n1\n1\n3\n0\n9\n1\n0\n0\n3\n9\n0\n3\n1\n0\n2\n1\n3\n0\n10\n0\n1\n0\n1\n10\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n-2\n0\n0\n0\n0\n0\n1\n-2\n-8\n-8\n2\n-1\n0\n0\n-6\n0\n0\n2\n0\n0\n-3\n-6\n0\n8\n6\n3\n0\n0\n0\n8\n0\n6\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n-2\n0\n0\n0\n0\n0\n1\n-2\n-6\n-8\n2\n-1\n0\n0\n-6\n0\n0\n2\n0\n0\n-3\n-6\n0\n6\n6\n3\n0\n0\n0\n8\n0\n6\n0\n0\nk = 1\nk = 2\nk = 3\nFigure 5: Top panel: Voting matrix without voting threshold (\u03b8 = 1). Middle panel: Error matrix\nbased on mode estimation without the voting threshold (\u03b8 = 1). Bottom panel: Error matrix\nbased on median estimation without the voting threshold (\u03b8 = 1).\nWe extend our experiment by setting the voting threshold \u03b8 = 6, for which results are shown in\nFigure 6. Thus, any value lower than 6 will be replaced by 0 in the voting matrix, and subsequently\nin the lead-lag matrix. We note that, for both the homogeneous and heterogeneous settings, the\nvoting matrix consists of non-zeros across the diagonals for each block, and the other blocks\nconsist only of 0s.\nWe conclude that our proposed method with a voting threshold has good\nperformance on synthetic data, and fully recovers the lead-lag relationship in both the homogeneous\nand heterogeneous settings.\n14\n\nK-means++ clustering\nHomogeneous Setting\nHeterogeneous Setting\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n15 13 11 11 11\n15\n0\n13 11 11 12\n13 13\n0\n11 11 12\n11 11 11\n0\n11 11\n11 11 11 11\n0\n13\n11 12 12 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0",
    "chunk_index": 15,
    "start_char": 31501,
    "end_char": 33253,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n15 13 11 11 11\n15\n0\n13 11 11 12\n13 13\n0\n11 11 12\n11 11 11\n0\n11 11\n11 11 11 11\n0\n13\n11 12 12 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n7\n0\n0\n0\n9\n0\n9\n0\n0\n0\n7\n9\n0\n0\n0\n0\n0\n0\n0\n0\n11\n9\n0\n0\n0\n11\n0\n9\n0\n0\n0\n9\n9\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n8\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n9\n0\n0\n0\n0\n9\n0\n0\n0\n0\n0\n0\n0\n0\n10\n0\n0\n0\n0\n10\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0",
    "chunk_index": 16,
    "start_char": 33047,
    "end_char": 34181,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nk = 1\nk = 2\nk = 3\nFigure 6: Top panel: Voting matrix with voting threshold (\u03b8 = 6). Middle panel: Error matrix\nbased on mode estimation with the voting threshold (\u03b8 = 6). Bottom panel: Error matrix based\non median estimation with the voting threshold (\u03b8 = 6).\n5.3\nSimulation\nIn the next set of synthetic experiments, we increase n to 60. The B and L matrices follow an\nanalogous pattern to the n = 6 case. The Adjusted Rand Index (ARI) calculates a similarity\nmetric between two clusterings by examining every sample pair and counting the number of pairs\nassigned to the same or different clusters in both the predicted and actual clusterings. The ARI is\ncalculated by adjusting the Rand Index for chance agreement, and it ranges from -1 to 1, where a\nscore of 1 indicates perfect agreement between the two clusterings, a score of 0 indicates random\nagreement, and a score less than 0 indicates disagreement between the two clusterings. As observed\nin Figure 7, it is evident that for both the homogeneous and heterogeneous settings, when the \u03c3\nranges from 0 to 1.5, the KM clustering achieves a stable ARI around 0.8, whereas the SP clustering\nfluctuates slightly in the earlier \u03c3 values. However, when the \u03c3 value exceeds 1.5, the SP clustering\ngains a slightly higher performance than the KM clustering, and the ARI for both methods drops\nsignificantly.\nFigure 8 shows the average and confidence interval for the MSE when estimating the true lag\nwith different \u03c3 levels based on 100 simulations without voting threshold (\u03b8 = 1, top panel) and\nwith voting threshold (\u03b8 = 6, bottom panel).\nIn the homogeneous setting (k = 1), both plots follow the same trend. Specifically, for \u03c3 ranging\nfrom 0 to 1.5, the MSE of the K-means++ mode (KM Mod) and spectral mode (SP Mod) is near\n0. In comparison, the MSE of K-means++ median (KM Med) and spectral median (SP Med) is\n15\n\nHomogeneous Setting\nHeterogeneous Setting\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\nk = 1\nk = 2\nk = 3\nFigure 7: Average and confidence interval for the ARI with different levels of \u03c3 based on 60 time\nseries and 100 simulations for every iteration.",
    "chunk_index": 17,
    "start_char": 34005,
    "end_char": 36476,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "is near\n0. In comparison, the MSE of K-means++ median (KM Med) and spectral median (SP Med) is\n15\n\nHomogeneous Setting\nHeterogeneous Setting\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.0\n0.2\n0.4\n0.6\n0.8\nARI\nKM\nSP\nk = 1\nk = 2\nk = 3\nFigure 7: Average and confidence interval for the ARI with different levels of \u03c3 based on 60 time\nseries and 100 simulations for every iteration.\nhigher. When \u03c3 ranges from 1.5 to 3.0, all four methods\u2019 MSE increases significantly due to the\nhigher noise level.\nIn the heterogeneous setting (k \u2208{2, 3}), we note that for methods not using the voting\nmechanism, the MSE is significantly higher, having a value of around 12. However, once the voting\nmechanism is added, the MSE becomes significantly lower. When the \u03c3 values range between 0 to\n1.5, the MSE of KM clustering goes down to near 0, whereas SP clustering have a slightly higher\nMSE. After the value of \u03c3 reaches 1.5 and beyond, the MSE increases dramatically for all four\nmethods.\nHomogeneous Setting\nHeterogeneous Setting\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n2\n4\n6\n8\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n2\n4\n6\n8\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n5\n10\n15\n20\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n2\n4\n6\n8\n10\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n5\n10\n15\n20\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0\n2\n4\n6\n8\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\nk = 1\nk = 2\nk = 3\nFigure 8: Top panel: Average and confidence interval for the MSE with different levels of \u03c3 based\non 100 simulations for every iteration without voting threshold (\u03b8 = 1). Bottom panel: Average\nand confidence interval for the MSE with different \u03c3 levels based on 100 simulations for every\niteration with voting threshold (\u03b8 = 6).\nWe next study the sensitivity of our proposed method to the value of the voting threshold \u03b8.\nIn Figure 9, we first observe that in the homogeneous setting (k = 1), the four methods maintain\na constant MSE as the voting threshold increases. This aligns with our expectations. Moreover,\nwe also notice that mode estimation performs better than median estimation.\nThis is evident\nthroughout the plots in Figure 7 and 8. Thus, the MSE for median estimation lies between 2\nand 3, whereas the MSE for the mode estimation drops to near 0. For the heterogeneous setting\n16\n\n(k \u2208{2, 3}), we note that as the voting threshold increases, the MSE decreases to near 0 for all\nmethods and remains low thereafter.",
    "chunk_index": 18,
    "start_char": 36006,
    "end_char": 38584,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "expectations. Moreover,\nwe also notice that mode estimation performs better than median estimation.\nThis is evident\nthroughout the plots in Figure 7 and 8. Thus, the MSE for median estimation lies between 2\nand 3, whereas the MSE for the mode estimation drops to near 0. For the heterogeneous setting\n16\n\n(k \u2208{2, 3}), we note that as the voting threshold increases, the MSE decreases to near 0 for all\nmethods and remains low thereafter. This altogether suggests that the proposed method is robust\nto the choice of the voting threshold \u03b8.\nHomogeneous Setting\nHeterogeneous Setting\n2\n4\n6\n8\n10\n0\n2\n4\n6\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n2\n4\n6\n8\n10\n0\n5\n10\n15\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\n2\n4\n6\n8\n10\n0\n5\n10\n15\nMSE\nKM_Mod\nKM_Med\nSP_Mod\nSP_Med\nk = 1\nk = 2\nk = 3\nFigure 9: Average and confidence interval for the MSE with different levels of \u03b8 based on 100\nsimulations when \u03c3 = 1.\nFigure 10 shows the average MSE as a function of \u03c3 and \u03b8. These are based on 100 simulations\nfor every setting. The plots for each panel represent the methods, which are KM Mod, KM Med,\nSP Mod, and SP Med, from top to bottom, respectively. For the homogeneous setting (k = 1), we\ncan observe that for all four methods, the MSE is heavily influenced by the increase in \u03c3, which\nis consistent with Figure 8. We also note that when the voting threshold increases, there are no\nsignificant changes in the MSE which is consistent with Figure 9. For the heterogeneous case, we\nnote that, in general, the optimal MSE is achieved when the voting threshold is approximately 6\nacross a wide range of values of \u03c3. This is consistent with Figures 8 and 9, and suggests that the\nchoice of the voting threshold is robust to model noise.\nHomogeneous Setting\nHeterogeneous Setting\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n0\n2\n4\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n1\n2\n3\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\n15\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0.0\n2.5\n5.0\n7.5\n10.0\n2.5\n5.0\n7.5\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\n15\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n2.5\n5.0\n7.5\nMSE\n17\n\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n2\n4\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10",
    "chunk_index": 19,
    "start_char": 38147,
    "end_char": 40316,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "6\n8\n10\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n2.5\n5.0\n7.5\nMSE\n17\n\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\n5\n2\n4\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n5\n10\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\n15\nMSE\n0\n1\n2\n3\n2\n4\n6\n8\n10\n0\n5\n10\n15\n5\n10\nMSE\nk = 1\nk = 2\nk = 3\nFigure 10: Average MSE as a function of \u03c3 and \u03b8 based on 100 simulations. From top panel to\nbottom panel, KM Mod, KM Med, SP Mod, and SP Med.\n6\nFinancial data experiments\n6.1\nData description\nIn this section, we apply our methods to a large-scale experiment using financial data. As men-\ntioned earlier, this is a context where lead-lag relationships naturally occur. For the financial data\nexperiments, we consider three data sets, which vary in terms of the number and type of assets\nand the number of days.\n\u2022 For the first data set, we look at US equities from Wharton\u2019s CRSP data set. There are a\ntotal of 679 equities within this data set, over a range of 5211 trading days from 2000/01/03\nto 2020/12/31.\n\u2022 The second data set is derived from the same Wharton\u2019s CRSP data set., but looks at Ex-\nchange Traded Funds (ETFs). It consists of 14 ETFs with 3324 trading days from 2006/04/12\nto 2019/07/01.\n\u2022 The third data set is the Pinnacle Data Corp CLC, which includes 52 futures contracts with\n5166 trading days from 2000/01/05 to 2020/10/16. All futures contracts are adjusted for\nrolling effects. This data set spans multiple asset classes, which include commodities, fixed\nincome, and currency futures.\nAll data sets are considered at daily frequency. We summarize all of the above in Table 4, with\nadditional details about Pinnacle Data Corp CLC available in Appendix A.3 Tables [17, 18, 19,\n20, 21, 22, 23, 24].\n18\n\nTable 4: Summary of the three financial data sets considered in the numerical experiments.\nData source\nType\nFreq\n# of assets\nStart date\nEnd date\n# of days\nWharton\u2019s CRSP\nEquity\nDaily\n679\n2000/01/03\n2020/12/31\n5211\nWharton\u2019s CRSP\nETF\nDaily\n14\n2006/04/12\n2019/07/01\n3324\nPinnacle Data Corp\nFutures Daily\n52\n2000/01/05\n2020/10/16\n5166\n6.2\nData pre-processing\nWith regard to the US equity and ETF data sets, we download the close-to-close adjusted daily\nreturns from Wharton\u2019s CRSP. Due to the large number of NaNs in the equity data set, we drop\nthe days for which more than 10% of the equities have zero returns as well as the equities for\nwhich more than 50% of days have zero returns.",
    "chunk_index": 20,
    "start_char": 40118,
    "end_char": 42532,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Corp\nFutures Daily\n52\n2000/01/05\n2020/10/16\n5166\n6.2\nData pre-processing\nWith regard to the US equity and ETF data sets, we download the close-to-close adjusted daily\nreturns from Wharton\u2019s CRSP. Due to the large number of NaNs in the equity data set, we drop\nthe days for which more than 10% of the equities have zero returns as well as the equities for\nwhich more than 50% of days have zero returns. Instead of working with raw returns, we consider\nthe market excess returns, a standard measure of how well each equity performed relative to the\nbroader market. For both of these data sets, the return of the S&P Composite Index is selected\nto compute the market excess returns by subtracting it from the return of each asset (i.e., for\nsimplicity, we assume each asset has \u03b2 = 1 exposure to the market).\nAlso, we winsorize the\nextreme value of excess returns for which any value is larger than 0.15 or smaller than -0.15.\nFor the futures data set, we download the close-to-close price series from the Pinnacle Data Corp\nCLC data set, and discard the days for which more than 10% of the futures have zero prices in the\nrespective dates, and drop the futures for which more than 160 days have zero prices. Afterwards,\nwe first use forward-fill, then backward-fill to fill out the zero prices.\nLastly, we compute the\nlog-return from the close-to-close price. The remainder of the data pre-processing is the same as\nabove.\n6.3\nBenchmark\nIn order to evaluate our proposed methodology, we also introduce a benchmark to detect lead-lag\nrelationships without the use of clustering. It is very common to compute a sample cross-correlation\nfunction (CCF) between two time series. A CCF between time series Xi and Xj evaluated at lag\nm is given by\nCCFij(m) = CORR({Xt\u2212m\ni\n}, {Xt\nj}),\n(11)\nwhere CORR() denotes a choice of the CCF. The corresponding lead-lag matrix \u0393n\u00d7n is estimated\nby computing the signed normalized area under the curve of CCF, given by\n\u0393ij = MAX(I(i, j), I(j, i)) \u00b7 SIGN(I(i, j) \u2212I(j, i))\nI(i, j) + I(j, i)\n,\n(12)\nwhere I(i, j) = PM\nm=1\n\f\fCCFij(m)\n\f\f for a user-specified maximum lag M.\nWe summarize the benchmark procedures in Algorithm 4.\n19\n\nAlgorithm 4 : CCF Algorithm\nInput: Time series matrix Xn\u00d7T .\nOutput: Lead-lag matrix \u0393n\u00d7n.\n1: Calculate CCF for every pair of time series {Xi, Xj}.\n2: Calculate the lead-lag matrix \u0393n\u00d7n by computing the signed normalized area under the curve\nof CCF.\n6.4\nTrading strategies\nGiven n time series each of length T, we first extract data by a sliding window with length l = 21.\nNext, we begin by extracting the STS with length q = 10 via a sliding window shifted by s = 1\nfrom the data, and cluster the STS to calculate the estimated lead-lag matrix \u0393n\u00d7n after applying\nthe voting threshold \u03b8 = 6, which is validated in the synthetic data experiment.",
    "chunk_index": 21,
    "start_char": 42131,
    "end_char": 44932,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "6.4\nTrading strategies\nGiven n time series each of length T, we first extract data by a sliding window with length l = 21.\nNext, we begin by extracting the STS with length q = 10 via a sliding window shifted by s = 1\nfrom the data, and cluster the STS to calculate the estimated lead-lag matrix \u0393n\u00d7n after applying\nthe voting threshold \u03b8 = 6, which is validated in the synthetic data experiment. We then utilize\nthe lead-lag matrix to rank the time series from the most leading to the most lagging using the\nRowSum ranking [Huber (1963), Gleich and L.-h. Lim (2011)], in order to then group the time\nseries into leaders and laggers, with the former used to predict the latter.\nLeaders D\u03b1\nHighest rank\n\u201cmost leading\u201d\n1\n2\n.\n.\n.\n.\n.\n.\nn\nLowest rank\n\u201cmost lagging\u201d\ntime\nLaggers G\u03b2\nt\nt+\u03b4\nLeaders D\u03b1\nHighest rank\n\u201cmost leading\u201d\n1\n2\n.\n.\n.\n.\n.\n.\nn\nLowest rank\n\u201cmost lagging\u201d\ntime\nLaggers G\u03b2\nt\nt+\u03b4\nG\u03b2\nD\u03b1\nFigure 11: G\u03b2 strategy: Use D\u03b1 predict G\u03b2 (left). D\u03b1 strategy: Use D\u03b1 predict D\u03b1 (right).\nMomentum is a widely studied phenomenon in the finance literature [Jegadeesh, Luo, et\nal. (2022), Jegadeesh and Titman (2001), Tan, Roberts, and Zohren (2023), Poh, Roberts, and\nZohren (2022), Wood, Giegerich, et al. (2021), Wood, Roberts, and Zohren (2021), B. Lim, Zohren,\nand Roberts (2019)], which refers to the tendency of assets that have performed well in the re-\ncent past to continue to perform well in the near future, and vice versa.\nWe denote the top\n\u03b1 = {0.7, 0.75, 0.8, 0.85} fraction of the time series as Leaders D\u03b1, and the bottom \u03b2 = 1 \u2212\u03b1\nas Laggers G\u03b2. We use the exponentially weighted moving average (EWMA) signal on the past\np = {1, 3, 5, 7} days of the average winsorized time series excess returns of the D\u03b1 to predict the\naverage future \u03b4 = {1, 3, 5, 7} days of the excess return of the G\u03b2 and D\u03b1. We assume the G\u03b2\ncan catch up with the D\u03b1, and the D\u03b1 provides the necessary momentum to maintain the trend\n20\n\nin \u03b4 days, respectively. This is depicted in Figure 11. Afterwards, we shift the sliding window by\ns = 1, and re-apply the method to calculate the lead-lag matrix \u0393n\u00d7n and rank the time series\nuntil the end of the time series. For clarity, Figure 12 reflects our trading pipeline at time t, and\nwe summarize the trading strategy in Algorithm 5.\nMarket Excess Return\nt-l t-l+q\nn\nt t+\u03b4\nt-p\nT\nFigure 12: Trading pipeline.\nAlgorithm 5 : Trading strategy\nInput: Time series matrix Xn\u00d7T .\n1: Build Xn\u00d7T by applying a sliding window of length l to obtain Xn\u00d7l.\n2: Use the Lead-lag Relationship Detection Algorithm on Xn\u00d7l to obtain the lead-lag matrix\n\u0393n\u00d7n.\n3: Based on \u0393n\u00d7n, rank the time series from the most leading to the most lagging using the\nRowSum ranking.",
    "chunk_index": 22,
    "start_char": 44537,
    "end_char": 47214,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "summarize the trading strategy in Algorithm 5.\nMarket Excess Return\nt-l t-l+q\nn\nt t+\u03b4\nt-p\nT\nFigure 12: Trading pipeline.\nAlgorithm 5 : Trading strategy\nInput: Time series matrix Xn\u00d7T .\n1: Build Xn\u00d7T by applying a sliding window of length l to obtain Xn\u00d7l.\n2: Use the Lead-lag Relationship Detection Algorithm on Xn\u00d7l to obtain the lead-lag matrix\n\u0393n\u00d7n.\n3: Based on \u0393n\u00d7n, rank the time series from the most leading to the most lagging using the\nRowSum ranking.\n4: Pick the top \u03b1 fraction of the time series as Leaders D\u03b1, and the bottom \u03b2 = 1 \u2212\u03b1 as Laggers\nG\u03b2.\n5: Apply the EWMA on the past p days of the average winsorized time series excess returns of\nthe D\u03b1 to predict the average future \u03b4 days of the excess return of the G\u03b2 and D\u03b1.\n6: Shift the sliding window by s, and re-apply Steps 1 - 5 until the end of the time series.\n6.5\nPerformance evaluation\nWhen assessing the effectiveness of various trading strategies, we rely on the following metrics to\nevaluate their performance:\nWe compute the Profit and Loss (PnL) of G\u03b2 on a given day t + \u03b4 as\nPnLt+\u03b4\nG\u03b2 = sign(EWMA(rett\u2212p\nD\u03b1 : rett\nD\u03b1)) \u00b7 rett+\u03b4\nG\u03b2 , t = l, . . . , T \u2212\u03b4,\n(13)\nsince the strategy earns money whenever the sign of the forecast agrees with the sign of the future\nreturn. Correspondingly, the PnL of D\u03b1 on a given day t + \u03b4 is given by:\nPnLt+\u03b4\nD\u03b1 = sign(EWMA(rett\u2212p\nD\u03b1 : rett\nD\u03b1)) \u00b7 rett+\u03b4\nD\u03b1 , t = l, . . . , T \u2212\u03b4,\n(14)\n21\n\nwhere rett\u2212p\nD\u03b1 and rett\nD\u03b1 are the excess return of D\u03b1 at t\u2212p and t, respectively, while EWMA(rett\u2212p\nD\u03b1 :\nrett\nD\u03b1) denotes the exponentially weighted moving average from the excess return of D\u03b1 from t\u2212p\nto t. Furthermore, rett+\u03b4\nG\u03b2 depicts the mean of the excess return of G\u03b2 at t + \u03b4, and rett+\u03b4\nD\u03b1 is the\nmean of the excess return of D\u03b1 at t + \u03b4. We rescale the PnL by their volatility to target equal\nrisk assignment, and set our annualized volatility target \u03c3tgt to be 0.15.\nPnLrescaled =\n\u03c3target\nSTD(PnL) \u00b7\n\u221a\n252 \u00b7 PnL.\n(15)\nThe cumulative PnL sums the daily PnL across all trading days\nCumulative PnL =\nX\nPnLrescaled.\n(16)\nThe annualized expected excess return (E[Returns]) is to measure the excess return earned by\nan investment over a benchmark index during a one-year period, and can be computed by\nE[Returns] = AVG(PnLrescaled) \u00b7 252.\n(17)\nThe annualized volatility is a measurement of the amount of risk associated with an investment\nover a one-year period\nVolatility = STD(PnLrescaled) \u00b7\n\u221a\n252.\n(18)\nFurthermore, we calculate the downside deviation and maximum drawdown to measure down-\nside risk, and then the Sortino ratio is often used by investors who are more concerned with\ndownside risk than with overall risk or volatility, which is derived by\nSortino ratio =\nE[Returns]\ndownside deviation.",
    "chunk_index": 23,
    "start_char": 46755,
    "end_char": 49462,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "period, and can be computed by\nE[Returns] = AVG(PnLrescaled) \u00b7 252.\n(17)\nThe annualized volatility is a measurement of the amount of risk associated with an investment\nover a one-year period\nVolatility = STD(PnLrescaled) \u00b7\n\u221a\n252.\n(18)\nFurthermore, we calculate the downside deviation and maximum drawdown to measure down-\nside risk, and then the Sortino ratio is often used by investors who are more concerned with\ndownside risk than with overall risk or volatility, which is derived by\nSortino ratio =\nE[Returns]\ndownside deviation.\n(19)\nThe Calmar ratio is often used by investors who are more concerned with long-term risk and\ndownside protection.\nA higher Calmar ratio indicates that the strategy has generated higher\nreturns relative to its maximum drawdown, while a lower Calmar ratio suggests that the strategy\nhas underperformed given the level of risk it has taken. It is calculated by\nCalmar ratio =\nE[Returns]\nmaximum drawdown.\n(20)\nThe hit rate measures the percentage of successful trades made by the strategy. It is also known\nas the win rate or success rate, and is defined as\nHit rate = |PnL+\nrescaled|\n|PnLrescaled|,\n(21)\nwhere |PnL+\nrescaled| is the number of profitable trades, and |PnLrescaled| is the total number of\ntrades.\nThe average profit / average loss (avg. profit / avg. loss) ratio measures the average size of\nprofits relative to the average size of losses generated by the strategy.\n22\n\nAvg. profit / avg. loss = AVG(PnL+\nrescaled)\nAVG(PnL\u2212\nrescaled),\n(22)\nwhere AVG(PnL+\nrescaled) is the average profit per trade, and AVG(PnL\u2212\nrescaled) is the average loss\nper trade.\nThe PnL per trade illustrates the amount earned by the strategy, in basis points for each basket\nof G\u03b2 or D\u03b1 traded in the markets (excluding transaction costs), and is given by\nPnL per trade = AVG(PnLrescaled) \u00b7 104,\n(23)\nwhere we assume that the strategy trades the same amount of notional every day (i.e., a constant\nunit bet size is used every trading day).\nWe also compute the annualized Sharpe ratio to quantify the profit gained per unit of risk\ntaken\nSharpe ratio = AVG(PnLrescaled)\nSTD(PnLrescaled) \u00b7\n\u221a\n252.\n(24)\nIt is important to assess the statistical significance of Sharpe ratio when back-testing a sample of\nhypothetical strategies [Bailey and De Prado (2014), Ledoit and Wolf (2008), Michael, Cucuringu,\nand Howison (2022)]. We use a test with the null hypothesis H0 : Sharpe ratio = 0, and implement\nthe method proposed by [Bailey and De Prado (2014)] to compute the test statistic\n(Sharpe ratio) \u00b7\n\u221a\nT \u22121\np\n1 \u2212\u03b31 \u00b7 (Sharpe ratio) + (\u03b32 \u22121) \u00b7 (Sharpe ratio)2/4\n,\n(25)\nwhere the Sharpe ratio is what we are testing, T is the length of the sample, and \u03b31 and \u03b32 are\nthe skewness and kurtosis of the returns distribution for the selected strategy, respectively. This\ntest statistic is assumed to be standard normal under the null hypothesis.\nTo evaluate the predictive performance of our method, we construct a simple trading strategy.",
    "chunk_index": 24,
    "start_char": 48929,
    "end_char": 51879,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "\u22121\np\n1 \u2212\u03b31 \u00b7 (Sharpe ratio) + (\u03b32 \u22121) \u00b7 (Sharpe ratio)2/4\n,\n(25)\nwhere the Sharpe ratio is what we are testing, T is the length of the sample, and \u03b31 and \u03b32 are\nthe skewness and kurtosis of the returns distribution for the selected strategy, respectively. This\ntest statistic is assumed to be standard normal under the null hypothesis.\nTo evaluate the predictive performance of our method, we construct a simple trading strategy.\nIf the strategy is profitable with a statistically significant Sharpe ratio, this indicates that we are\nable to leverage the discovered lead-lag relationships for the prediction task.\n6.6\nResults\nFor the equity data set, full results with different tuning settings are reported in supplemental\nmaterial [Zhang et al. (2023)] Table S1. We find that using the EWMA on the past seven days\nof the average winsorized time series excess returns of the D\u03b1 to predict the average future seven\ndays of the excess return of the G\u03b2 and D\u03b1 with \u03b1 = 0.75 performs consistently well across all\nmethods. In Tables [5, 6], we report the performance of the CCF and four methods based on the\nvarious metrics (rescaled to target volatility). Across the four methods we proposed, we note that\nthe Sharpe ratio values are relatively high. In addition to the Sharpe ratio, the P-value associated\nwith the Sharpe ratio also supports our hypothesis that the Sharpe ratio is statistically significant.\nIt is evident that most P-values are 0 while others are significantly lower than 0.05, which provides\nus with more confidence when analyzing the results of the experiment. In particular, according\nto the G\u03b2 strategy, it has been observed that KM Med outperforms other methods in terms of\n23\n\nmost metrics. Conversely, based on the D\u03b1 strategy, KM Mod performs better than other methods\nacross most metrics. It is noteworthy that KM clustering produces similar results for both the G\u03b2\nand D\u03b1 strategies, and they are also much faster than SP clustering in terms of running time. In\nFigure 13, we depict the cumulative PnL (rescaled to target volatility) across trading days for the\nscenarios in Tables [5, 6]. The left and right plots present G\u03b2, and D\u03b1 strategies, respectively. We\nalso present the benchmark performance for comparison.\nTable 5: Equity data set: performance metrics for G\u03b2 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.089\n0.126\n0.118\n0.127*\n0.104\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.105\n0.103\n0.101*\n0.103\n0.105\nMaximum drawdown\n-0.313\n-0.26\n-0.215\n-0.214*\n-0.287\nSortino ratio\n0.85\n1.222\n1.166\n1.237*\n0.988\nCalmar ratio\n0.285\n0.484\n0.548\n0.594*\n0.362\nHit rate\n0.499\n0.521*\n0.51\n0.516\n0.519\nAvg. profit / avg. loss\n1.117*\n1.068\n1.107\n1.091\n1.051\nPnL per trade\n3.542\n4.996\n4.672\n5.041*\n4.119\nSharpe ratio\n0.595\n0.839\n0.785\n0.847*\n0.692\nP-value\n0.009\n0*\n0*\n0*\n0.002\nTable 6: Equity data set: performance metrics for D\u03b1 strategy - rescaled to target volatility.",
    "chunk_index": 25,
    "start_char": 51450,
    "end_char": 54498,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "SP Med\nE[Returns]\n0.089\n0.126\n0.118\n0.127*\n0.104\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.105\n0.103\n0.101*\n0.103\n0.105\nMaximum drawdown\n-0.313\n-0.26\n-0.215\n-0.214*\n-0.287\nSortino ratio\n0.85\n1.222\n1.166\n1.237*\n0.988\nCalmar ratio\n0.285\n0.484\n0.548\n0.594*\n0.362\nHit rate\n0.499\n0.521*\n0.51\n0.516\n0.519\nAvg. profit / avg. loss\n1.117*\n1.068\n1.107\n1.091\n1.051\nPnL per trade\n3.542\n4.996\n4.672\n5.041*\n4.119\nSharpe ratio\n0.595\n0.839\n0.785\n0.847*\n0.692\nP-value\n0.009\n0*\n0*\n0*\n0.002\nTable 6: Equity data set: performance metrics for D\u03b1 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.101\n0.122\n0.095\n0.129\n0.134*\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.105*\n0.107\n0.108\n0.106\n0.105*\nMaximum drawdown\n-0.288\n-0.251\n-0.21*\n-0.283\n-0.227\nSortino ratio\n0.964\n1.148\n0.876\n1.222\n1.269*\nCalmar ratio\n0.352\n0.488\n0.452\n0.456\n0.59*\nHit rate\n0.518\n0.521\n0.513\n0.522\n0.525*\nAvg. profit / avg. loss\n1.05\n1.069\n1.066\n1.072*\n1.065\nPnL per trade\n4.018\n4.856\n3.765\n5.121\n5.312*\nSharpe ratio\n0.675\n0.816\n0.632\n0.86\n0.892*\nP-value\n0.003\n0*\n0.005\n0*\n0*\n24\n\nG\u03b2 strategy\nD\u03b1 strategy\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n0\n50\n100\n150\n200\n250\nKM_Mod (SR = 0.84)\nKM_Med (SR = 0.78)\nSP_Mod (SR = 0.85)\nSP_Med (SR = 0.69)\nCCF (SR = 0.6)\nTime (Year)\nCumulative Returns (%)\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n0\n50\n100\n150\n200\n250\nKM_Mod (SR = 0.82)\nKM_Med (SR = 0.63)\nSP_Mod (SR = 0.86)\nSP_Med (SR = 0.89)\nCCF (SR = 0.68)\nTime (Year)\nCumulative Returns (%)\nFigure 13: Equity data set: cumulative PnL for G\u03b2 strategy (left) and D\u03b1 strategy (right) - rescaled\nto target volatility. The experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nTables [7, 8] and Figure 14 present results for the ETF data set using the same settings as for\nthe equities data. Compared to equities data, we find no evidence of being able to consistently\ndetect lead-lag relationships leading to a profitable outcome. We note, though, the performance is\nstrong in some cases, namely the D\u03b1 strategy with the exception of KM Med. Full results across\nall tuning settings are reported in supplemental material [Zhang et al. (2023)] Table S2. Results\nfor futures data, again with the same settings as the equities data, are reported in Tables [9, 10]\nand Figure 15. For this data set, we do not see an ability to consistently detect profitable lead-\nlag relationships for any of the strategies. Full results across all tuning settings are reported in\nsupplemental material [Zhang et al. (2023)] Table S3.\nTable 7: ETF data set: performance metrics for G\u03b2 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.",
    "chunk_index": 26,
    "start_char": 53917,
    "end_char": 56733,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "equities data, are reported in Tables [9, 10]\nand Figure 15. For this data set, we do not see an ability to consistently detect profitable lead-\nlag relationships for any of the strategies. Full results across all tuning settings are reported in\nsupplemental material [Zhang et al. (2023)] Table S3.\nTable 7: ETF data set: performance metrics for G\u03b2 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n-0.019\n0.02\n0.022\n-0.005\n0.013\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.116\n0.115\n0.116\n0.113\n0.108\nMaximum drawdown\n-0.668\n-0.525\n-0.465\n-0.579\n-0.369\nSortino ratio\n-0.165\n0.176\n0.188\n-0.043\n0.122\nCalmar ratio\n-0.029\n0.038\n0.047\n-0.008\n0.036\nHit rate\n0.492\n0.512\n0.499\n0.503\n0.5\nAvg. profit / avg. loss\n1.006\n0.978\n1.034\n0.983\n1.016\nPnL per trade\n-0.76\n0.8\n0.87\n-0.191\n0.527\nSharpe ratio\n-0.128\n0.134\n0.146\n-0.032\n0.088\nP-value\n0.644\n0.628\n0.598\n0.908\n0.749\n25\n\nTable 8: ETF data set: performance metrics for D\u03b1 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.056\n0.065\n0.022\n0.073\n0.081\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.097\n0.097\n0.105\n0.1\n0.105\nMaximum drawdown\n-0.362\n-0.267\n-0.385\n-0.272\n-0.382\nSortino ratio\n0.581\n0.676\n0.213\n0.728\n0.771\nCalmar ratio\n0.156\n0.245\n0.058\n0.269\n0.211\nHit rate\n0.504\n0.502\n0.5\n0.51\n0.504\nAvg. profit / avg. loss\n1.056\n1.077\n1.027\n1.05\n1.087\nPnL per trade\n2.234\n2.591\n0.889\n2.901\n3.2\nSharpe ratio\n0.375\n0.435\n0.149\n0.487\n0.538\nP-value\n0.169\n0.105\n0.588\n0.074\n0.051\nG\u03b2 strategy\nD\u03b1 strategy\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n\u221260\n\u221240\n\u221220\n0\n20\n40\n60\nKM_Mod (SR = 0.13)\nKM_Med (SR = 0.15)\nSP_Mod (SR = -0.03)\nSP_Med (SR = 0.09)\nCCF (SR = -0.13)\nTime (Year)\nCumulative Returns (%)\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n0\n20\n40\n60\n80\n100\nKM_Mod (SR = 0.44)\nKM_Med (SR = 0.15)\nSP_Mod (SR = 0.49)\nSP_Med (SR = 0.54)\nCCF (SR = 0.38)\nTime (Year)\nCumulative Returns (%)\nFigure 14: ETF data set: cumulative PnL for G\u03b2 strategy (left) and D\u03b1 strategy (right) - rescaled\nto target volatility. The experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nTable 9: Futures data set: performance metrics for G\u03b2 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.013\n-0.001\n0.005\n-0.01\n0.022\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.104\n0.102\n0.103\n0.102\n0.105\nMaximum drawdown\n-0.535\n-0.587\n-0.534\n-0.508\n-0.457\nSortino ratio\n0.121\n-0.007\n0.045\n-0.097\n0.209\nCalmar ratio\n0.024\n-0.001\n0.009\n-0.019\n0.048\nHit rate\n0.502\n0.498\n0.495\n0.494\n0.508\nAvg.",
    "chunk_index": 27,
    "start_char": 56270,
    "end_char": 59116,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Futures data set: performance metrics for G\u03b2 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.013\n-0.001\n0.005\n-0.01\n0.022\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.104\n0.102\n0.103\n0.102\n0.105\nMaximum drawdown\n-0.535\n-0.587\n-0.534\n-0.508\n-0.457\nSortino ratio\n0.121\n-0.007\n0.045\n-0.097\n0.209\nCalmar ratio\n0.024\n-0.001\n0.009\n-0.019\n0.048\nHit rate\n0.502\n0.498\n0.495\n0.494\n0.508\nAvg. profit / avg. loss\n1.007\n1.009\n1.028\n1.013\n0.994\nPnL per trade\n0.499\n-0.028\n0.185\n-0.392\n0.869\nSharpe ratio\n0.084\n-0.005\n0.031\n-0.066\n0.146\nP-value\n0.705\n0.983\n0.888\n0.766\n0.51\n26\n\nTable 10: Futures data set: performance metrics for D\u03b1 strategy - rescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and \u03b1 = 0.25.\nG\u03b2 strategy\nBenchmark\nProposed\nCCF\nKM Mod\nKM Med\nSP Mod\nSP Med\nE[Returns]\n0.061\n0.036\n0.036\n0.027\n0.031\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.107\n0.108\n0.108\n0.108\n0.107\nMaximum drawdown\n-0.393\n-0.411\n-0.399\n-0.474\n-0.462\nSortino ratio\n0.574\n0.336\n0.335\n0.253\n0.29\nCalmar ratio\n0.156\n0.088\n0.091\n0.057\n0.067\nHit rate\n0.511\n0.502\n0.499\n0.498\n0.501\nAvg. profit / avg. loss\n1.032\n1.038\n1.05\n1.043\n1.037\nPnL per trade\n2.439\n1.441\n1.443\n1.079\n1.23\nSharpe ratio\n0.41\n0.242\n0.242\n0.181\n0.207\nP-value\n0.064\n0.274\n0.273\n0.412\n0.35\nG\u03b2 strategy\nD\u03b1 strategy\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n2020\n\u221280\n\u221260\n\u221240\n\u221220\n0\n20\n40\n60\n80\n100\nKM_Mod (SR = -0.0)\nKM_Med (SR = 0.03)\nSP_Mod (SR = -0.07)\nSP_Med (SR = 0.15)\nCCF (SR = 0.08)\nTime (Year)\nCumulative Returns (%)\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n2020\n\u221250\n0\n50\n100\nKM_Mod (SR = 0.24)\nKM_Med (SR = 0.24)\nSP_Mod (SR = 0.18)\nSP_Med (SR = 0.21)\nCCF (SR = 0.41)\nTime (Year)\nCumulative Returns (%)\nFigure 15: Futures data set: cumulative PnL for G\u03b2 strategy (left) and D\u03b1 strategy (right) -\nrescaled to target volatility.\nThe experiment has been set with the values p = 7, \u03b4 = 7, and\n\u03b1 = 0.25.\n27\n\n7\nRobustness analysis\nIn this section, we test the robustness of the benchmark and our proposed methods by conducting\nexperiments with different levels of \u03b1. For the equity data set, we consider \u03b1 values ranging from\n0.7 to 0.85 with an increment of 0.05. In Table 11, we note that the performance of all methods\ndoes not change significantly while maintaining a high Sharpe ratio. We notice that the P-values\nare almost all lower than 0.05, suggesting that all results are significant in our experiments. We\nalso note that for the SP Med, the G\u03b2 strategy has slightly lower performance.\nIn Table 12, we report on the performance of the four methods tested on the ETF data set.",
    "chunk_index": 28,
    "start_char": 58581,
    "end_char": 61321,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "In Table 11, we note that the performance of all methods\ndoes not change significantly while maintaining a high Sharpe ratio. We notice that the P-values\nare almost all lower than 0.05, suggesting that all results are significant in our experiments. We\nalso note that for the SP Med, the G\u03b2 strategy has slightly lower performance.\nIn Table 12, we report on the performance of the four methods tested on the ETF data set.\nHere, we only consider the \u03b1 values of 0.75 and 0.85 due to the smaller cross-section for this data\nset. The four methods achieve a fairly good performance for high alpha for the D\u03b1 strategy, except\nfor KM Med.\nFinally, in Table 13, we provide the performance of the four methods on the futures data set,\nwith \u03b1 ranging from 0.7 to 0.85 in increments of 0.05. We note that for this data set, the Sharpe\nratio for the G\u03b2 strategy tends to be more sensitive to change in \u03b1 and it is hard to achieve\nprofitability.\nTable 11: Equity data set: robustness analysis for \u03b1 - rescaled to target volatility. The experiment\nhas been set with the values p = 7, and \u03b4 = 7.\nCCF\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.082\n0.089\n0.089\n0.079\n0.088\n0.101\n0.108\n0.106\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.106\n0.105\n0.105\n0.105\n0.106\n0.105\n0.105\n0.105\nMaximum drawdown\n-0.313\n-0.313\n-0.29\n-0.259\n-0.267\n-0.288\n-0.254\n-0.254\nSortino ratio\n0.776\n0.85\n0.849\n0.757\n0.824\n0.964\n1.027\n1.009\nCalmar ratio\n0.263\n0.285\n0.306\n0.306\n0.328\n0.352\n0.425\n0.418\nHit rate\n0.504\n0.499\n0.505\n0.505\n0.517\n0.518\n0.521\n0.52\nAvg. profit / avg. loss\n1.086\n1.117\n1.091\n1.08\n1.037\n1.05\n1.047\n1.048\nPnL per trade\n3.262\n3.542\n3.526\n3.143\n3.473\n4.018\n4.288\n4.209\nSharpe ratio\n0.548\n0.595\n0.592\n0.528\n0.583\n0.675\n0.72\n0.707\nP-value\n0.015\n0.009\n0.009\n0.02\n0.01\n0.003\n0.001\n0.002\nKM Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.117\n0.126\n0.117\n0.143\n0.116\n0.122\n0.117\n0.123\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.104\n0.103\n0.103\n0.101\n0.105\n0.107\n0.106\n0.106\nMaximum drawdown\n-0.245\n-0.26\n-0.236\n-0.213\n-0.207\n-0.251\n-0.289\n-0.288\nSortino ratio\n1.127\n1.222\n1.137\n1.42\n1.099\n1.148\n1.102\n1.157\nCalmar ratio\n0.476\n0.484\n0.494\n0.672\n0.559\n0.488\n0.405\n0.427\nHit rate\n0.521\n0.521\n0.516\n0.522\n0.51\n0.521\n0.52\n0.522\nAvg. profit / avg. loss\n1.059\n1.068\n1.078\n1.085\n1.107\n1.069\n1.067\n1.065\nPnL per trade\n4.628\n4.996\n4.63\n5.676\n4.596\n4.856\n4.648\n4.881\nSharpe ratio\n0.778\n0.839\n0.778\n0.954\n0.772\n0.816\n0.781\n0.82\nP-value\n0.001\n0\n0.001\n0\n0.001\n0\n0.001\n0\n28\n\nKM Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.115\n0.118\n0.121\n0.138\n0.094\n0.095\n0.086\n0.1\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.101\n0.099\n0.097\n0.109\n0.108\n0.109\n0.108\nMaximum drawdown\n-0.235\n-0.215\n-0.22",
    "chunk_index": 29,
    "start_char": 60900,
    "end_char": 63733,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "1.065\nPnL per trade\n4.628\n4.996\n4.63\n5.676\n4.596\n4.856\n4.648\n4.881\nSharpe ratio\n0.778\n0.839\n0.778\n0.954\n0.772\n0.816\n0.781\n0.82\nP-value\n0.001\n0\n0.001\n0\n0.001\n0\n0.001\n0\n28\n\nKM Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.115\n0.118\n0.121\n0.138\n0.094\n0.095\n0.086\n0.1\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.101\n0.099\n0.097\n0.109\n0.108\n0.109\n0.108\nMaximum drawdown\n-0.235\n-0.215\n-0.22\n-0.177\n-0.198\n-0.21\n-0.286\n-0.259\nSortino ratio\n1.115\n1.166\n1.225\n1.428\n0.866\n0.876\n0.788\n0.925\nCalmar ratio\n0.487\n0.548\n0.549\n0.781\n0.476\n0.452\n0.299\n0.386\nHit rate\n0.517\n0.51\n0.509\n0.517\n0.515\n0.513\n0.512\n0.514\nAvg. profit / avg. loss\n1.074\n1.107\n1.113\n1.101\n1.058\n1.066\n1.058\n1.068\nPnL per trade\n4.545\n4.672\n4.796\n5.486\n3.741\n3.765\n3.397\n3.97\nSharpe ratio\n0.764\n0.785\n0.806\n0.922\n0.629\n0.632\n0.571\n0.667\nP-value\n0.001\n0\n0\n0\n0.005\n0.005\n0.012\n0.003\nSP Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.137\n0.127\n0.129\n0.129\n0.139\n0.129\n0.129\n0.123\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.102\n0.103\n0.103\n0.102\n0.105\n0.106\n0.105\n0.105\nMaximum drawdown\n-0.199\n-0.214\n-0.206\n-0.162\n-0.276\n-0.283\n-0.28\n-0.256\nSortino ratio\n1.346\n1.237\n1.258\n1.267\n1.333\n1.222\n1.226\n1.18\nCalmar ratio\n0.691\n0.594\n0.628\n0.798\n0.505\n0.456\n0.46\n0.482\nHit rate\n0.523\n0.516\n0.522\n0.522\n0.524\n0.522\n0.52\n0.521\nAvg. profit / avg. loss\n1.076\n1.091\n1.069\n1.07\n1.074\n1.072\n1.079\n1.068\nPnL per trade\n5.456\n5.041\n5.135\n5.131\n5.532\n5.121\n5.115\n4.895\nSharpe ratio\n0.917\n0.847\n0.863\n0.862\n0.929\n0.86\n0.859\n0.822\nP-value\n0\n0\n0\n0\n0\n0\n0\n0\nSP Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.115\n0.104\n0.098\n0.084\n0.142\n0.134\n0.134\n0.13\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.105\n0.105\n0.106\n0.105\n0.104\n0.105\n0.105\n0.104\nMaximum drawdown\n-0.279\n-0.287\n-0.271\n-0.256\n-0.221\n-0.227\n-0.217\n-0.215\nSortino ratio\n1.099\n0.988\n0.927\n0.794\n1.367\n1.269\n1.273\n1.253\nCalmar ratio\n0.414\n0.362\n0.363\n0.327\n0.644\n0.59\n0.617\n0.604\nHit rate\n0.52\n0.519\n0.518\n0.513\n0.527\n0.525\n0.526\n0.525\nAvg. profit / avg. loss\n1.059\n1.051\n1.046\n1.051\n1.067\n1.065\n1.06\n1.059\nPnL per trade\n4.579\n4.119\n3.904\n3.321\n5.648\n5.312\n5.316\n5.15\nSharpe ratio\n0.769\n0.692\n0.656\n0.558\n0.949\n0.892\n0.893\n0.865\nP-value\n0.001\n0.002\n0.004\n0.014\n0\n0\n0\n0\n29\n\nTable 12: ETF data set: robustness analysis for \u03b1 - rescaled to target volatility. The experiment\nhas been set with the values p = 7, and \u03b4 = 7.\nCCF\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n-0.019\n-0.038\n0.056\n0.041\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.116\n0.116\n0.097\n0.097\nMaximum drawdown\n-0.668\n-0.672\n-0.362\n-0.382\nSortino ratio\n-0.165\n-0.324\n0.581\n0.422\nCalmar ratio\n-0.029\n-0.056\n0.156\n0.107\nHit rate\n0.492\n0.501\n0.504\n0.494\nAvg. profit / avg.",
    "chunk_index": 30,
    "start_char": 63282,
    "end_char": 66111,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "- rescaled to target volatility. The experiment\nhas been set with the values p = 7, and \u03b4 = 7.\nCCF\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n-0.019\n-0.038\n0.056\n0.041\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.116\n0.116\n0.097\n0.097\nMaximum drawdown\n-0.668\n-0.672\n-0.362\n-0.382\nSortino ratio\n-0.165\n-0.324\n0.581\n0.422\nCalmar ratio\n-0.029\n-0.056\n0.156\n0.107\nHit rate\n0.492\n0.501\n0.504\n0.494\nAvg. profit / avg. loss\n1.006\n0.949\n1.056\n1.077\nPnL per trade\n-0.76\n-1.49\n2.234\n1.617\nSharpe ratio\n-0.128\n-0.25\n0.375\n0.272\nP-value\n0.644\n0.365\n0.169\n0.321\nKM Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n0.02\n-0.06\n0.065\n0.083\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.115\n0.122\n0.097\n0.096\nMaximum drawdown\n-0.525\n-0.707\n-0.267\n-0.292\nSortino ratio\n0.176\n-0.487\n0.676\n0.86\nCalmar ratio\n0.038\n-0.084\n0.245\n0.283\nHit rate\n0.512\n0.505\n0.502\n0.505\nAvg. profit / avg. loss\n0.978\n0.911\n1.077\n1.086\nPnL per trade\n0.8\n-2.363\n2.591\n3.28\nSharpe ratio\n0.134\n-0.397\n0.435\n0.551\nP-value\n0.628\n0.147\n0.105\n0.039\nKM Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n0.022\n0.004\n0.022\n0.052\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.116\n0.113\n0.105\n0.104\nMaximum drawdown\n-0.465\n-0.436\n-0.385\n-0.336\nSortino ratio\n0.188\n0.031\n0.213\n0.504\nCalmar ratio\n0.047\n0.008\n0.058\n0.155\nHit rate\n0.499\n0.505\n0.5\n0.502\nAvg. profit / avg. loss\n1.034\n0.984\n1.027\n1.06\nPnL per trade\n0.87\n0.14\n0.889\n2.071\nSharpe ratio\n0.146\n0.024\n0.149\n0.348\nP-value\n0.598\n0.932\n0.588\n0.205\nSP Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n-0.005\n-0.076\n0.073\n0.12\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.113\n0.118\n0.1\n0.095\nMaximum drawdown\n-0.579\n-0.726\n-0.272\n-0.258\nSortino ratio\n-0.043\n-0.648\n0.728\n1.259\nCalmar ratio\n-0.008\n-0.105\n0.269\n0.464\nHit rate\n0.503\n0.497\n0.51\n0.511\nAvg. profit / avg. loss\n0.983\n0.918\n1.05\n1.107\nPnL per trade\n-0.191\n-3.035\n2.901\n4.747\nSharpe ratio\n-0.032\n-0.51\n0.487\n0.797\nP-value\n0.908\n0.064\n0.074\n0.003\n30\n\nSP Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.75\n0.85\n0.75\n0.85\nE[Returns]\n0.013\n-0.025\n0.081\n0.08\nVolatility\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.108\n0.11\n0.105\n0.103\nMaximum drawdown\n-0.369\n-0.452\n-0.382\n-0.348\nSortino ratio\n0.122\n-0.232\n0.771\n0.779\nCalmar ratio\n0.036\n-0.056\n0.211\n0.23\nHit rate\n0.5\n0.495\n0.504\n0.51\nAvg. profit / avg. loss\n1.016\n0.987\n1.087\n1.061\nPnL per trade\n0.527\n-1.01\n3.2\n3.18\nSharpe ratio\n0.088\n-0.17\n0.538\n0.534\nP-value\n0.749\n0.54\n0.051\n0.051\nTable 13: Futures data set: robustness analysis for \u03b1 - rescaled to target volatility. The experiment\nhas been set with the values p = 7, and \u03b4 = 7.\nCCF\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.011\n0.013\n0.006\n-0.025\n0.053\n0.061\n0.064\n0.045\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.104\n0.104\n0.105\n0.107\n0.107\n0.107\n0.106\n0.107\nMaximum drawdown\n-0.594\n-0.535\n-0.545\n-0.667\n-0.432\n-0.393\n-0.38\n-0.426\nSortino ratio\n0.102\n0.121\n0.06\n-0.236\n0.496\n0.574\n0.603",
    "chunk_index": 31,
    "start_char": 65680,
    "end_char": 68644,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "rescaled to target volatility. The experiment\nhas been set with the values p = 7, and \u03b4 = 7.\nCCF\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.011\n0.013\n0.006\n-0.025\n0.053\n0.061\n0.064\n0.045\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.104\n0.104\n0.105\n0.107\n0.107\n0.107\n0.106\n0.107\nMaximum drawdown\n-0.594\n-0.535\n-0.545\n-0.667\n-0.432\n-0.393\n-0.38\n-0.426\nSortino ratio\n0.102\n0.121\n0.06\n-0.236\n0.496\n0.574\n0.603\n0.422\nCalmar ratio\n0.018\n0.024\n0.012\n-0.038\n0.123\n0.156\n0.169\n0.106\nHit rate\n0.501\n0.502\n0.504\n0.5\n0.509\n0.511\n0.503\n0.504\nAvg. profit / avg. loss\n1.008\n1.007\n0.993\n0.971\n1.031\n1.032\n1.072\n1.042\nPnL per trade\n0.421\n0.499\n0.249\n-1.001\n2.116\n2.439\n2.548\n1.793\nSharpe ratio\n0.071\n0.084\n0.042\n-0.168\n0.355\n0.41\n0.428\n0.301\nP-value\n0.749\n0.705\n0.85\n0.447\n0.108\n0.064\n0.053\n0.173\nKM Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.006\n-0.001\n0.006\n0.006\n0.048\n0.036\n0.038\n0.025\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.102\n0.1\n0.099\n0.108\n0.108\n0.108\n0.108\nMaximum drawdown\n-0.484\n-0.587\n-0.517\n-0.505\n-0.326\n-0.411\n-0.35\n-0.452\nSortino ratio\n0.06\n-0.007\n0.064\n0.059\n0.446\n0.336\n0.352\n0.234\nCalmar ratio\n0.013\n-0.001\n0.012\n0.011\n0.147\n0.088\n0.109\n0.056\nHit rate\n0.503\n0.498\n0.501\n0.496\n0.501\n0.502\n0.502\n0.496\nAvg. profit / avg. loss\n0.996\n1.009\n1.004\n1.025\n1.059\n1.038\n1.039\n1.05\nPnL per trade\n0.246\n-0.028\n0.254\n0.23\n1.901\n1.441\n1.514\n1.004\nSharpe ratio\n0.041\n-0.005\n0.043\n0.039\n0.319\n0.242\n0.254\n0.169\nP-value\n0.852\n0.983\n0.847\n0.862\n0.148\n0.274\n0.25\n0.446\n31\n\nKM Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.004\n0.005\n0.009\n0.017\n0.031\n0.036\n0.037\n0.03\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.103\n0.102\n0.102\n0.108\n0.108\n0.108\n0.108\nMaximum drawdown\n-0.551\n-0.534\n-0.479\n-0.454\n-0.414\n-0.399\n-0.434\n-0.459\nSortino ratio\n0.042\n0.045\n0.09\n0.163\n0.284\n0.335\n0.343\n0.28\nCalmar ratio\n0.008\n0.009\n0.019\n0.037\n0.074\n0.091\n0.085\n0.066\nHit rate\n0.496\n0.495\n0.495\n0.498\n0.501\n0.499\n0.5\n0.498\nAvg. profit / avg. loss\n1.023\n1.028\n1.03\n1.028\n1.036\n1.05\n1.047\n1.046\nPnL per trade\n0.172\n0.185\n0.363\n0.661\n1.217\n1.443\n1.472\n1.202\nSharpe ratio\n0.029\n0.031\n0.061\n0.111\n0.204\n0.242\n0.247\n0.202\nP-value\n0.896\n0.888\n0.783\n0.616\n0.356\n0.273\n0.264\n0.362\nSP Mod\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n-0.011\n-0.01\n0\n-0.007\n0.033\n0.027\n0.029\n0.031\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.102\n0.102\n0.102\n0.108\n0.108\n0.108\n0.108\nMaximum drawdown\n-0.528\n-0.508\n-0.43\n-0.504\n-0.465\n-0.474\n-0.467\n-0.502\nSortino ratio\n-0.106\n-0.097\n0.002\n-0.073\n0.309\n0.253\n0.271\n0.283\nCalmar ratio\n-0.021\n-0.019\n0\n-0.015\n0.072\n0.057\n0.062\n0.061\nHit rate\n0.496\n0.494\n0.495\n0.502\n0.5\n0.498\n0.5\n0.503\nAvg. profit / avg.",
    "chunk_index": 32,
    "start_char": 68180,
    "end_char": 71026,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "0.031\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.103\n0.102\n0.102\n0.102\n0.108\n0.108\n0.108\n0.108\nMaximum drawdown\n-0.528\n-0.508\n-0.43\n-0.504\n-0.465\n-0.474\n-0.467\n-0.502\nSortino ratio\n-0.106\n-0.097\n0.002\n-0.073\n0.309\n0.253\n0.271\n0.283\nCalmar ratio\n-0.021\n-0.019\n0\n-0.015\n0.072\n0.057\n0.062\n0.061\nHit rate\n0.496\n0.494\n0.495\n0.502\n0.5\n0.498\n0.5\n0.503\nAvg. profit / avg. loss\n1.004\n1.013\n1.021\n0.984\n1.044\n1.043\n1.037\n1.027\nPnL per trade\n-0.435\n-0.392\n0.007\n-0.295\n1.322\n1.079\n1.158\n1.213\nSharpe ratio\n-0.073\n-0.066\n0.001\n-0.05\n0.222\n0.181\n0.195\n0.204\nP-value\n0.742\n0.766\n0.996\n0.823\n0.315\n0.412\n0.379\n0.357\nSP Med\nG\u03b2 strategy\nD\u03b1 strategy\n\u03b1\n0.7\n0.75\n0.8\n0.85\n0.7\n0.75\n0.8\n0.85\nE[Returns]\n0.007\n0.022\n0.02\n0.004\n0.03\n0.031\n0.035\n0.036\nVolatility\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\n0.15\nDownside deviation\n0.105\n0.105\n0.103\n0.104\n0.107\n0.107\n0.107\n0.107\nMaximum drawdown\n-0.467\n-0.457\n-0.455\n-0.44\n-0.433\n-0.462\n-0.504\n-0.44\nSortino ratio\n0.071\n0.209\n0.191\n0.039\n0.282\n0.29\n0.323\n0.333\nCalmar ratio\n0.016\n0.048\n0.044\n0.009\n0.069\n0.067\n0.069\n0.081\nHit rate\n0.503\n0.508\n0.497\n0.492\n0.503\n0.501\n0.501\n0.501\nAvg. profit / avg. loss\n0.999\n0.994\n1.036\n1.037\n1.024\n1.037\n1.04\n1.04\nPnL per trade\n0.295\n0.869\n0.786\n0.162\n1.192\n1.23\n1.371\n1.417\nSharpe ratio\n0.05\n0.146\n0.132\n0.027\n0.2\n0.207\n0.23\n0.238\nP-value\n0.823\n0.51\n0.551\n0.902\n0.365\n0.35\n0.298\n0.282\n32\n\n8\nCO2 emissions data\n8.1\nData description\nWe acquire CO2 emissions data (metric tons per capita) for the period 1990\u20132019 from Climate\nWatch. This data is depicted in Figure 18 in the appendix, and Table 14 depicts the 31 countries\u2019\nnames and corresponding codes.\nTable 14: 31 Countries and their corresponding codes.\nCountry\nCode\nCountry\nCode\nCountry\nCode\nCountry\nCode\nAustria\nAUT\nBelgium\nBEL\nBulgaria\nBGR\nCroatia\nHRV\nCyprus\nCYP\nCzech Republic\nCZE\nDenmark\nDNK\nEstonia\nEST\nFinland\nFIN\nFrance\nFRA\nGermany\nDEU\nGreece\nGRC\nHungary\nHUN\nIceland\nISL\nIreland\nIRL\nItaly\nITA\nLatvia\nLVA\nLithuania\nLTU\nLuxemburg\nLUX\nMalta\nMLT\nNetherlands NLD\nNorway\nNOR\nPoland\nPOL\nPortugal\nPRT\nRomania\nROU\nSlovakia\nSVK\nSlovenia\nSVN\nSpain\nESP\nSweden\nSWE\nSwitzerland\nCHE\nUK\nGBR\n8.2\nPipeline\nGiven that n = 31 for the CO2 time series with a period of T = 30 years, we extract STS from\nthe data with length q = 16 via a sliding window that is shifted by s = 1. Next, we apply SP Med\nto cluster the STS, and calculate the lead-lag matrix \u0393n\u00d7n using a voting threshold (\u03b8 = 3). From\nthis, we rank the time series from the most leading to the most lagging by using RowSum ranking.\n8.3\nResults\nWe find that the first among the leading countries is Poland, while there are five countries that\nare tied for second place. These countries are Sweden, Belgium, Slovakia, Denmark and Germany.\nMoreover, we find that the last two lagging countries are Bulgaria and Estonia. We summarize\nthe top ten leading countries, and the last ten lagging countries in Tables [15, 16].",
    "chunk_index": 33,
    "start_char": 70634,
    "end_char": 73532,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "rank the time series from the most leading to the most lagging by using RowSum ranking.\n8.3\nResults\nWe find that the first among the leading countries is Poland, while there are five countries that\nare tied for second place. These countries are Sweden, Belgium, Slovakia, Denmark and Germany.\nMoreover, we find that the last two lagging countries are Bulgaria and Estonia. We summarize\nthe top ten leading countries, and the last ten lagging countries in Tables [15, 16].\nBy investigating the top-ranked countries Poland and Sweden, we can attempt to explain the\ntrend in these countries\u2019 CO2 emission rates. We note that these countries belong to the same\npolitical group, the European Union. Thus, one possibility is that they follow similar protocols in\nterms of regulations and restrictions. These restrictions and regulations will help alleviate CO2\nemissions.\nThe combustion of fossil fuels such as gasoline, natural gas and coal represents one of the main\ncontributors to carbon dioxide pollution. Piotr \u02d9Zuk, Pawe l \u02d9Zuk, and Pluci\u00b4nski (2021) suggested\nthat indigenous coal is the primary source of mixed energy in Poland, and since the country joined\nthe European Union in 2004, it has become clear that Poland will do much to protect its domestic\ncoal sector, thus, will reject demands for aggressive harmonized decarbonization efforts, despite\nbeing identified as a leader with rapid emissions reduction taking place in the 1990s and early\n2000s. At the same time, Kolasa-Wiecek (2015) proposed that the Polish energy sector is actively\n33\n\nparticipating in efforts to reduce emissions of greenhouse gases (GHG) into the environment by\nlowering the quantity of coal in the fuel mix and increasing the usage of renewable energy sources.\nAlso, Rybak et al. (2022) posited that both Poland and Sweden are the leading countries that\neffectively reduce GHG and apply environmental taxes.\nWe note that since joining the EU in 1995, Sweden experienced a rapid drop in CO2 emissions.\nIn Sweden, De Luca and Pizzolante (2021) proffered that many incentives have been designed to\nencourage consumers to purchase cleaner automobiles that produce fewer CO2 emissions. Also,\nDe Luca and Pizzolante (2021) further argued that the Swedish government introduced a new\nbonus-penalty system for the purpose of administering incentives and taxes on light vehicles. De\nLuca and Pizzolante (2021) suggested this is based on the strong leadership of Germany, because\nit is one of the first countries in the world to formulate an approach that proved favourable to the\nenvironment, particularly in the field of transportation. For example, the German government first\nimplemented an Eco-tax in 1999, which led to increases in the price of fuel and parking fees, and\na reduction in the amount of available parking space for vehicles. Indeed, Germany is a leading\ncountry in reducing CO2 emissions itself.\nIn contrast, we also perform an analysis of the two most lagging countries. In Estonia, the\nwork of Moora et al. (2017) demonstrated that shale oil is a low-grade carbonaceous fossil fuel,\nwhich is utilized locally as their main source of energy.",
    "chunk_index": 34,
    "start_char": 73061,
    "end_char": 76200,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "first\nimplemented an Eco-tax in 1999, which led to increases in the price of fuel and parking fees, and\na reduction in the amount of available parking space for vehicles. Indeed, Germany is a leading\ncountry in reducing CO2 emissions itself.\nIn contrast, we also perform an analysis of the two most lagging countries. In Estonia, the\nwork of Moora et al. (2017) demonstrated that shale oil is a low-grade carbonaceous fossil fuel,\nwhich is utilized locally as their main source of energy.\nBurning shale oil produces high CO2\nemissions, which sets it apart from other forms of fossil fuel combustion. Moreover, the power\nsector appears to be the most profound emitter of CO2 and is responsible for significant amounts\nof waste. Bulgaria, which has a fleet of old cars ranging from 15 to 20 years, has the extremely\ndifficult challenge of reducing automobile pollution in practice. Moreover, a study by Hristov and\nStefanov (2019) demonstrated that upgrading the transportation infrastructure does not always\nhave a positive impact on the decrease in transportation emissions. Furthermore, the only native\nenergy source is lignite, which has a low caloric content. Although the future of fossil fuels is\nuncertain after 2030, it should be noted that EU Regulation 2018/842 will not impose stringent\nlimitations on GHG emissions in Bulgaria until 2030, as posited by Vitkov (2020).\nTable 15: Top ten ranking of leading countries.\nCountry\nRank\nPoland\n1\nSweden\n2\nBelgium\n2\nSlovakia\n2\nDenmark\n2\nGermany\n2\nNetherlands\n7\nHungary\n8\nFrance\n9\nUK\n10\nTable 16: Last ten ranking of lagging countries.\nCountry\nRank\nAustria\n22\nSlovenia\n23\nCroatia\n23\nLuxembourg\n25\nRomania\n26\nMalta\n27\nLatvia\n28\nLithuania\n28\nBulgaria\n30\nEstonia\n31\n34\n\n9\nConclusion\nWe develop a clustering-driven methodology for robust detection of lead-lag relationships in high-\ndimensional multivariate time series, in the setting of lagged multi-factor models. We consider a\ncollection of time series as input, and generate an expanded universe by extracting STS from each\ntime series via a sliding window. Next, we employ several clustering approaches which include KM\nand SP clustering to cluster the resulting STS. After extracting the clusters, lead-lag estimates\nfrom the clusters are merged to help identify the consistent relationships that existed in the original\nuniverse.\nWhen applied to financial data sets, our proposed methods attain promising Sharpe ratios,\nand are statistically significant when compared to the benchmark. In addition to the financial\ndomain, our methods can also be applied to other fields, such as environmental sciences, which\nis consistent with the results of others\u2019 previous work. Thus, our method is generally applicable\nto a variety of multivariate time series data sets. One possible future work is to fine-tune the\nweights of the portfolios of laggers. In our current work related to financial data, we buy or sell\nbaskets of lagging assets based on equal weight according to the trading signal, while adjusting\nthe weight of the portfolios of laggers based on the ranking of the laggers.",
    "chunk_index": 35,
    "start_char": 75712,
    "end_char": 78795,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "the results of others\u2019 previous work. Thus, our method is generally applicable\nto a variety of multivariate time series data sets. One possible future work is to fine-tune the\nweights of the portfolios of laggers. In our current work related to financial data, we buy or sell\nbaskets of lagging assets based on equal weight according to the trading signal, while adjusting\nthe weight of the portfolios of laggers based on the ranking of the laggers. In another direction\nof future work, we intend to experiment with other clustering methods applicable in our setting\n[Cucuringu, H. Li, et al. (2020), Cucuringu, Davies, et al. (2019)], since our current work has only\nexperimented with KM and SP clustering. Moreover, we also intend to test various alternative\nranking algorithms [Bradley and Terry (1952), Fogel, d\u2019Aspremont, and Vojnovic (2014), Page\net al. (1999), Cucuringu (2016), He et al. (2022), d\u2019Aspremont, Cucuringu, and Tyagi (2021)],\nand consider momentum reversal strategies within the trading pipeline. Lastly, one could further\nexplore the more challenging mixed membership model defined in Section 3.\n35\n\nReferences\n[1]\nJakob Albers et al. \u201cFragmentation, price formation and cross-impact in bitcoin markets\u201d.\nIn: Applied Mathematical Finance 28.5 (2021), pp. 395\u2013448.\n[2]\nDavid Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Tech.\nrep. Stanford, 2006.\n[3]\nDavid H Bailey and Marcos Lopez De Prado. \u201cThe deflated Sharpe ratio: correcting for selec-\ntion bias, backtest overfitting, and non-normality\u201d. In: The Journal of Portfolio Management\n40.5 (2014), pp. 94\u2013107.\n[4]\nAfonso S Bandeira et al. \u201cMultireference alignment using semidefinite programming\u201d. In: Pro-\nceedings of the 5th conference on Innovations in theoretical computer science. 2014, pp. 459\u2013\n470.\n[5]\nStefanos Bennett, Mihai Cucuringu, and Gesine Reinert. \u201cLead\u2013lag detection and network\nclustering for multivariate time series with an application to the US equity market\u201d. In:\nMachine Learning (2022), pp. 1\u201342.\n[6]\nNicolas Boumal et al. \u201cHeterogeneous multireference alignment: A single pass approach\u201d. In:\n2018 52nd Annual Conference on Information Sciences and Systems (CISS). IEEE. 2018,\npp. 1\u20136.\n[7]\nRalph Allan Bradley and Milton E Terry. \u201cRank analysis of incomplete block designs: I. The\nmethod of paired comparisons\u201d. In: Biometrika 39.3/4 (1952), pp. 324\u2013345.\n[8]\nGiuseppe Buccheri, Fulvio Corsi, and Stefano Peluso. \u201cHigh-frequency lead-lag effects and\ncross-asset linkages: a multi-asset lagged adjustment model\u201d. In: Journal of Business & Eco-\nnomic Statistics 39.3 (2021), pp. 605\u2013621.\n[9]\nJay Cao, Jacky Chen, and John Hull. \u201cA neural network approach to understanding implied\nvolatility movements\u201d. In: Quantitative Finance 20.9 (2020), pp. 1405\u20131413.\n[10]\nAlvaro Cartea, Ryan Donnelly, and Sebastian Jaimungal. \u201cEnhancing trading strategies with\norder book signals\u201d. In: Applied Mathematical Finance 25.1 (2018), pp. 1\u201335.\n[11]\nRama Cont. \u201cEmpirical properties of asset returns: stylized facts and statistical issues\u201d. In:\nQuantitative finance 1.2 (2001), p. 223.\n[12]\nPaul Cotofrei and Kilian Stoffel. \u201cClassification rules+ time= temporal rules\u201d. In: Interna-\ntional Conference on Computational Science. Springer. 2002, pp. 572\u2013581.\n[13]\nMihai Cucuringu. \u201cSync-rank: Robust ranking, constrained ranking and rank aggregation\nvia eigenvector and SDP synchronization\u201d. In: IEEE Transactions on Network Science and\nEngineering 3.1 (2016), pp.",
    "chunk_index": 36,
    "start_char": 78346,
    "end_char": 81819,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "Jaimungal. \u201cEnhancing trading strategies with\norder book signals\u201d. In: Applied Mathematical Finance 25.1 (2018), pp. 1\u201335.\n[11]\nRama Cont. \u201cEmpirical properties of asset returns: stylized facts and statistical issues\u201d. In:\nQuantitative finance 1.2 (2001), p. 223.\n[12]\nPaul Cotofrei and Kilian Stoffel. \u201cClassification rules+ time= temporal rules\u201d. In: Interna-\ntional Conference on Computational Science. Springer. 2002, pp. 572\u2013581.\n[13]\nMihai Cucuringu. \u201cSync-rank: Robust ranking, constrained ranking and rank aggregation\nvia eigenvector and SDP synchronization\u201d. In: IEEE Transactions on Network Science and\nEngineering 3.1 (2016), pp. 58\u201379.\n[14]\nMihai Cucuringu, Peter Davies, et al. \u201cSPONGE: A generalized eigenproblem for cluster-\ning signed networks\u201d. In: The 22nd International Conference on Artificial Intelligence and\nStatistics. PMLR. 2019, pp. 1088\u20131098.\n36\n\n[15]\nMihai Cucuringu, Huan Li, et al. \u201cHermitian matrices for clustering directed graphs: in-\nsights and applications\u201d. In: International Conference on Artificial Intelligence and Statistics.\nPMLR. 2020, pp. 983\u2013992.\n[16]\nYan Cui, Jun Yang, and Zhou Zhou. \u201cState-domain change point detection for nonlinear\ntime series regression\u201d. In: Journal of Econometrics (2021).\n[17]\nAlexandre d\u2019Aspremont, Mihai Cucuringu, and Hemant Tyagi. \u201cRanking and synchronization\nfrom pairwise measurements via SVD\u201d. In: The Journal of Machine Learning Research 22.1\n(2021), pp. 866\u2013928.\n[18]\nGiovanni De Luca and Federica Pizzolante. \u201cDetecting Leaders Country from Road Transport\nEmission Time-Series\u201d. In: Environments 8.3 (2021), p. 18.\n[19]\nChang Dong et al. \u201cSubsequence Time Series Clustering-Based Unsupervised Approach for\nAnomaly Detection of Axial Piston Pumps\u201d. In: IEEE Transactions on Instrumentation and\nMeasurement 72 (2023), pp. 1\u201312. doi: 10.1109/TIM.2023.3264045.\n[20]\nFelix Drinkall, Stefan Zohren, and Janet B Pierrehumbert. \u201cForecasting COVID-19 Caseloads\nUsing Unsupervised Embedding Clusters of Social Media Posts\u201d. In: arXiv preprint arXiv:2205.10408\n(2022).\n[21]\nFajwel Fogel, Alexandre d\u2019Aspremont, and Milan Vojnovic. \u201cSerialrank: Spectral ranking\nusing seriation\u201d. In: Advances in neural information processing systems 27 (2014).\n[22]\nDavid F Gleich and Lek-heng Lim. \u201cRank aggregation via nuclear norm minimization\u201d. In:\nProceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and\ndata mining. 2011, pp. 60\u201368.\n[23]\nYixuan He et al. \u201cGNNRank: Learning global rankings from pairwise comparisons via di-\nrected graph neural networks\u201d. In: International Conference on Machine Learning. PMLR.\n2022, pp. 8581\u20138612.\n[24]\nR Hristov and S Stefanov. \u201cStudy of the potential for reducing CO2 emissions from road\ntransport in Bulgaria\u201d. In: IOP Conference Series: Materials Science and Engineering. Vol. 614.\n1. IOP Publishing. 2019, p. 012009.\n[25]\nPeter J Huber. \u201cPairwise comparison and ranking: optimum properties of the row sum pro-\ncedure\u201d. In: The annals of mathematical statistics (1963), pp. 511\u2013520.\n[26]\nKatsuya Ito and Ryuta Sakemoto. \u201cDirect estimation of lead\u2013lag relationships using multi-\nnomial dynamic time warping\u201d. In: Asia-Pacific Financial Markets 27.3 (2020), pp. 325\u2013\n342.\n[27]\nNarasimhan Jegadeesh, Jiang Luo, et al. \u201cMomentum and short-term reversals: theory and\nevidence\u201d. In: Nanyang Business School Research Paper 22-13 (2022).\n[28]\nNarasimhan Jegadeesh and Sheridan Titman. \u201cProfitability of momentum strategies: An\nevaluation of alternative explanations\u201d. In: The Journal of finance 56.2 (2001), pp. 699\u2013720.\n37",
    "chunk_index": 37,
    "start_char": 81179,
    "end_char": 84721,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "estimation of lead\u2013lag relationships using multi-\nnomial dynamic time warping\u201d. In: Asia-Pacific Financial Markets 27.3 (2020), pp. 325\u2013\n342.\n[27]\nNarasimhan Jegadeesh, Jiang Luo, et al. \u201cMomentum and short-term reversals: theory and\nevidence\u201d. In: Nanyang Business School Research Paper 22-13 (2022).\n[28]\nNarasimhan Jegadeesh and Sheridan Titman. \u201cProfitability of momentum strategies: An\nevaluation of alternative explanations\u201d. In: The Journal of finance 56.2 (2001), pp. 699\u2013720.\n37\n\n[29]\nEamonn Keogh and Jessica Lin. \u201cClustering of time-series subsequences is meaningless: im-\nplications for previous and future research\u201d. In: Knowledge and information systems 8.2\n(2005), pp. 154\u2013177.\n[30]\nAlicja Kolasa-Wiecek. \u201cStepwise multiple regression method of greenhouse gas emission mod-\neling in the energy sector in Poland\u201d. In: Journal of Environmental Sciences 30 (2015),\npp. 47\u201354.\n[31]\nOliver Ledoit and Michael Wolf. \u201cRobust performance hypothesis testing with the Sharpe\nratio\u201d. In: Journal of Empirical Finance 15.5 (2008), pp. 850\u2013859.\n[32]\nChung-Sheng Li, Philip S Yu, and Vittorio Castelli. \u201cMALM: A framework for mining se-\nquence database at multiple abstraction levels\u201d. In: Proceedings of the seventh international\nconference on Information and knowledge management. 1998, pp. 267\u2013272.\n[33]\nYongli Li et al. \u201cDynamic patterns of daily lead-lag networks in stock markets\u201d. In: Quanti-\ntative Finance 21.12 (2021), pp. 2055\u20132068.\n[34]\nBryan Lim, Stefan Zohren, and Stephen Roberts. \u201cEnhancing time-series momentum strate-\ngies using deep neural networks\u201d. In: The Journal of Financial Data Science 1.4 (2019),\npp. 19\u201338.\n[35]\nJessica Lin, Eamonn Keogh, and Wagner Truppel. \u201cClustering of streaming time series is\nmeaningless\u201d. In: Proceedings of the 8th ACM SIGMOD workshop on Research issues in data\nmining and knowledge discovery. 2003, pp. 56\u201365.\n[36]\nYutong Lu, Gesine Reinert, and Mihai Cucuringu. \u201cCo-trading networks for modeling dy-\nnamic interdependency structures and estimating high-dimensional covariances in US equity\nmarkets\u201d. In: arXiv preprint arXiv:2302.09382 (2023).\n[37]\nYutong Lu, Gesine Reinert, and Mihai Cucuringu. \u201cTrade co-occurrence, trade flow decompo-\nsition, and conditional order imbalance in equity markets\u201d. In: arXiv preprint arXiv:2209.10334\n(2022).\n[38]\nNavin Madicar et al. \u201cParameter-free subsequences time series clustering with various-width\nclusters\u201d. In: 2013 5th International Conference on Knowledge and Smart Technology (KST).\nIEEE. 2013, pp. 150\u2013155.\n[39]\nNikolas Michael, Mihai Cucuringu, and Sam Howison. \u201cOption Volume Imbalance as a pre-\ndictor for equity market returns\u201d. In: arXiv preprint arXiv:2201.09319 (2022).\n[40]\nDeborah Miori and Mihai Cucuringu. \u201cReturns-Driven Macro Regimes and Characteristic\nLead-Lag Behaviour between Asset Classes\u201d. In: arXiv preprint arXiv:2209.00268 (2022).\n[41]\nHarri Moora et al. \u201cDetermination of biomass content in combusted municipal waste and\nassociated CO2 emissions in Estonia\u201d. In: Energy Procedia 128 (2017), pp. 222\u2013229.\n[42]\nAndrew Ng, Michael Jordan, and Yair Weiss. \u201cOn spectral clustering: Analysis and an algo-\nrithm\u201d. In: Advances in neural information processing systems 14 (2001).\n[43]\nLawrence Page et al. The PageRank citation ranking: Bringing order to the web. Tech. rep.\nStanford InfoLab, 1999.\n38\n\n[44]\nAmelia Perry et al. \u201cThe sample complexity of multireference alignment\u201d. In: SIAM Journal\non Mathematics of Data Science 1.3 (2019), pp. 497\u2013517.\n[45]\nDaniel Poh, Stephen Roberts, and Stefan Zohren.",
    "chunk_index": 38,
    "start_char": 84234,
    "end_char": 87759,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "[42]\nAndrew Ng, Michael Jordan, and Yair Weiss. \u201cOn spectral clustering: Analysis and an algo-\nrithm\u201d. In: Advances in neural information processing systems 14 (2001).\n[43]\nLawrence Page et al. The PageRank citation ranking: Bringing order to the web. Tech. rep.\nStanford InfoLab, 1999.\n38\n\n[44]\nAmelia Perry et al. \u201cThe sample complexity of multireference alignment\u201d. In: SIAM Journal\non Mathematics of Data Science 1.3 (2019), pp. 497\u2013517.\n[45]\nDaniel Poh, Stephen Roberts, and Stefan Zohren. \u201cTransfer ranking in finance: applications to\ncross-sectional momentum with data scarcity\u201d. In: arXiv preprint arXiv:2208.09968 (2022).\n[46]\nN Radhakrishnan, James D Wilson, and Philipos C Loizou. \u201cAn alternate partitioning tech-\nnique to quantify the regularity of complex time series\u201d. In: International Journal of Bifur-\ncation and Chaos 10.07 (2000), pp. 1773\u20131779.\n[47]\nThanawin Rakthanmanon et al. \u201cTime series epenthesis: Clustering time series streams re-\nquires ignoring some data\u201d. In: 2011 IEEE 11th International Conference on Data Mining.\nIEEE. 2011, pp. 547\u2013556.\n[48]\nSura Rodpongpun, Vit Niennattrakul, and Chotirat Ann Ratanamahatana. \u201cSelective sub-\nsequence time series clustering\u201d. In: Knowledge-Based Systems 35 (2012), pp. 361\u2013368.\n[49]\nJakob Runge et al. \u201cDetecting and quantifying causal associations in large nonlinear time\nseries datasets\u201d. In: Science advances 5.11 (2019), eaau4996.\n[50]\nAurelia Rybak et al. \u201cThe Impact of Environmental Taxes on the Level of Greenhouse Gas\nEmissions in Poland and Sweden\u201d. In: Energies 15.12 (2022), p. 4465.\n[51]\nBiplab Kumer Sarker et al. \u201cParallel algorithms for mining association rules in time series\ndata\u201d. In: International Symposium on Parallel and Distributed Processing and Applications.\nSpringer. 2003, pp. 273\u2013284.\n[52]\nChristian Schittenkopf, Peter Ti\u02c7no, and Georg Dorffner. \u201cThe benefit of information reduc-\ntion for trading strategies\u201d. In: Applied Economics 34.7 (2002), pp. 917\u2013930.\n[53]\nJianbo Shi and Jitendra Malik. \u201cNormalized cuts and image segmentation\u201d. In: IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence 22.8 (2000), pp. 888\u2013905.\n[54]\nAlik Sokolov et al. \u201cAssessing the Impact of Sustainability on Fund Flows: An Excess Infor-\nmation Approach and US-Based Case Study\u201d. In: The Journal of Impact and ESG Investing\n(2022).\n[55]\nWee Ling Tan, Stephen Roberts, and Stefan Zohren. \u201cSpatio-Temporal Momentum: Jointly\nLearning Time-Series and Cross-Sectional Strategies\u201d. In: arXiv preprint arXiv:2302.10175\n(2023).\n[56]\nPeter Tino, Christian Schittenkopf, and Georg Dorffner. \u201cTemporal pattern recognition in\nnoisy non-stationary time series based on quantization into symbolic streams. Lessons learned\nfrom financial volatility trading.\u201d In: (2000).\n[57]\nKonstantinos Tolikas. \u201cThe lead-lag relation between the stock and the bond markets\u201d. In:\nThe European Journal of Finance 24.10 (2018), pp. 849\u2013866.\n[58]\nKuniaki Uehara and Mitsuomi Shimada. \u201cExtraction of primitive motion and discovery of\nassociation rules from human motion data\u201d. In: Progress in Discovery Science. Springer,\n2002, pp. 338\u2013348.\n39\n\n[59]\nRahul Kumar Vijay and Satyasai Jagannath Nanda. \u201cEarthquake pattern analysis using sub-\nsequence time series clustering\u201d. In: Pattern Analysis and Applications 26.1 (2023), pp. 19\u2013\n37.\n[60]\nNikolai Vitkov. \u201cGreenhouse Emissions And Prospects For Local Fuel TPPs In Bulgaria\u201d.\nIn: 2020 12th Electrical Engineering Faculty Conference (BulEF). IEEE. 2020, pp. 1\u20135.\n[61]\nMilena Vuleti\u00b4c, Felix Prenzel, and Mihai Cucuringu.",
    "chunk_index": 39,
    "start_char": 87265,
    "end_char": 90792,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "of primitive motion and discovery of\nassociation rules from human motion data\u201d. In: Progress in Discovery Science. Springer,\n2002, pp. 338\u2013348.\n39\n\n[59]\nRahul Kumar Vijay and Satyasai Jagannath Nanda. \u201cEarthquake pattern analysis using sub-\nsequence time series clustering\u201d. In: Pattern Analysis and Applications 26.1 (2023), pp. 19\u2013\n37.\n[60]\nNikolai Vitkov. \u201cGreenhouse Emissions And Prospects For Local Fuel TPPs In Bulgaria\u201d.\nIn: 2020 12th Electrical Engineering Faculty Conference (BulEF). IEEE. 2020, pp. 1\u20135.\n[61]\nMilena Vuleti\u00b4c, Felix Prenzel, and Mihai Cucuringu. \u201cFin-GAN: Forecasting and Classifying\nFinancial Time Series via Generative Adversarial Networks\u201d. In: Available at SSRN 4328302\n(2023).\n[62]\nKieran Wood, Sven Giegerich, et al. \u201cTrading with the Momentum Transformer: An Intelli-\ngent and Interpretable Architecture\u201d. In: arXiv preprint arXiv:2112.08534 (2021).\n[63]\nKieran Wood, Stephen Roberts, and Stefan Zohren. \u201cSlow momentum with fast rever-\nsion: A trading strategy using deep learning and changepoint detection\u201d. In: arXiv preprint\narXiv:2105.13727 (2021).\n[64]\nDi Wu et al. \u201cDetecting leaders from correlated time series\u201d. In: International Conference\non Database Systems for Advanced Applications. Springer. 2010, pp. 352\u2013367.\n[65]\nTakehisa Yairi, Yoshikiyo Kato, and Koichi Hori. \u201cFault detection by mining association rules\nfrom house-keeping data\u201d. In: proceedings of the 6th International Symposium on Artificial\nIntelligence, Robotics and Automation in Space. Vol. 18. Citeseer. 2001, p. 21.\n[66]\nCan-Zhong Yao and Hong-Yu Li. \u201cTime-varying lead\u2013lag structure between investor senti-\nment and stock market\u201d. In: The North American Journal of Economics and Finance 52\n(2020), p. 101148.\n[67]\nYichi Zhang et al. Supplemental Material: Robust Detection of Lead-Lag Relationships in\nLagged Multi-Factor Models. Mendeley Data. Version V4. 2023. doi: 10.17632/2djzdjvn96.\n2.\n[68]\nSeyedjamal Zolhavarieh, Saeed Aghabozorgi, and Ying Wah Teh. \u201cA review of subsequence\ntime series clustering\u201d. In: The Scientific World Journal 2014 (2014).\n[69]\nPiotr \u02d9Zuk, Pawe l \u02d9Zuk, and Przemys law Pluci\u00b4nski. \u201cCoal basin in Upper Silesia and energy\ntransition in Poland in the context of pandemic: The socio-political diversity of preferences\nin energy and environmental policy\u201d. In: Resources Policy 71 (2021), p. 101987.\n40\n\nA\nAppendix\nA.1\nSynthetic data experiment: spectral clustering\nFigures [16, 17] display the result of applying SP clustering, the voting matrix and the error\nmatrix based on mode and median estimation without and with voting threshold with different\nk = {1, 2, 3}.\nSpectral clustering\nHomogeneous Setting\nHeterogeneous Setting\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n12 11 11 11 11\n12\n0\n11 11 10 10\n11 11\n0\n11 11 11\n11 11 11\n0\n11 11\n11 10 11 11\n0\n13\n11 10 11 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0",
    "chunk_index": 40,
    "start_char": 90220,
    "end_char": 93183,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "11 11\n11 11 11\n0\n11 11\n11 10 11 11\n0\n13\n11 10 11 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n9\n2\n2\n2\n9\n0\n11\n0\n0\n1\n9\n11\n0\n0\n0\n1\n2\n0\n0\n0\n11 10\n2\n0\n0\n11\n0\n10\n2\n1\n1\n10 10\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n1\n-1\n3\n0\n0\n0\n0\n0\n5\n0\n0\n0\n0\n0\n7\n-1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n-3\n-5\n-7\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n4\n2\n3\n0\n0\n0\n0\n0\n5\n0\n0\n0\n0\n0\n7\n-4\n0\n0\n0\n0\n0\n-2\n0\n0\n0\n0\n0\n-3\n-5\n-7\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n3\n0\n0\n2\n9\n0\n1\n1\n1\n0\n3\n1\n0\n8\n0\n2\n0\n1\n8\n0\n0\n0\n0\n1\n0\n0\n0\n8\n2\n0\n2\n0\n8\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n8\n0\n0\n-6\n-2\n-7\n0\n0\n6\n0\n0\n0\n8\n0\n2\n0\n0\n0\n0\n0\n7\n0\n0\n0\n0\n-8\n0\n-8\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n8\n0\n0\n-6\n-2\n-7\n0\n0\n6\n0\n0\n0\n8\n0\n2\n0\n0\n0\n0\n0\n7\n0\n0\n0\n0\n-8\n0\n-8\n0\n0\n0\nk = 1\nk = 2\nk = 3\nFigure 16: Top panel: Voting matrix without voting threshold (\u03b8 = 1).",
    "chunk_index": 41,
    "start_char": 92995,
    "end_char": 94223,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "0\n8\n0\n2\n0\n0\n0\n0\n0\n7\n0\n0\n0\n0\n-8\n0\n-8\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n8\n0\n0\n-6\n-2\n-7\n0\n0\n6\n0\n0\n0\n8\n0\n2\n0\n0\n0\n0\n0\n7\n0\n0\n0\n0\n-8\n0\n-8\n0\n0\n0\nk = 1\nk = 2\nk = 3\nFigure 16: Top panel: Voting matrix without voting threshold (\u03b8 = 1). Middle panel: Error matrix\nbased on mode estimation without the voting threshold (\u03b8 = 1). Bottom panel: Error matrix based\non median estimation without the voting threshold (\u03b8 = 1).\n41\n\nSpectral clustering\nHomogeneous Setting\nHeterogeneous Setting\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n12 11 11 11 11\n12\n0\n11 11 10 10\n11 11\n0\n11 11 11\n11 11 11\n0\n11 11\n11 10 11 11\n0\n13\n11 10 11 11 13\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n9\n0\n0\n0\n9\n0\n11\n0\n0\n0\n9\n11\n0\n0\n0\n0\n0\n0\n0\n0\n11 10\n0\n0\n0\n11\n0\n10\n0\n0\n0\n10 10\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1",
    "chunk_index": 42,
    "start_char": 93957,
    "end_char": 95306,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n9\n0\n0\n0\n0\n9\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n0\n0\n8\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nTS 1 TS 2 TS 3 TS 4 TS 5 TS 6\nTS 1\nTS 2\nTS 3\nTS 4\nTS 5\nTS 6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nk = 1\nk = 2\nk = 3\nFigure 17: Top panel: Voting matrix with voting threshold (\u03b8 = 6). Middle panel: Error matrix\nbased on mode estimation with the voting threshold (\u03b8 = 6). Bottom panel: Error matrix based\non median estimation with the voting threshold (\u03b8 = 6).\nA.2\nCO2 emissions plots\nFigure 18 shows 31 countries\u2019 CO2 emissions data (metric tons per capita) for the period 1990\u20132019\nfrom Climate Watch.\n42\n\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n9\nEmissions\nAUT\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\nEmissions\nBEL\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n7\n8\nEmissions\nBGR\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n5\nEmissions\nHRV\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n7\nEmissions\nCYP\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n10\n12\n14\nEmissions\nCZE\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5.0\n7.5\n10.0\n12.5\nEmissions\nDNK\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n10\n15\n20\nEmissions\nEST\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\n12\n14\nEmissions\nFIN\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5\n6\nEmissions\nFRA\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\n12\nEmissions\nDEU\n1990\n1994\n1998\n2002\n2006",
    "chunk_index": 43,
    "start_char": 95134,
    "end_char": 96938,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5.0\n7.5\n10.0\n12.5\nEmissions\nDNK\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n10\n15\n20\nEmissions\nEST\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\n12\n14\nEmissions\nFIN\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5\n6\nEmissions\nFRA\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\n12\nEmissions\nDEU\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n8\nEmissions\nGRC\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n5\n6\nEmissions\nHUN\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n8\nEmissions\nISL\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n8\n10\nEmissions\nIRL\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n7\n8\nEmissions\nITA\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n6\nEmissions\nLVA\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n6\n8\nEmissions\nLTU\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n15\n20\n25\n30\nEmissions\nLUX\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n6\nEmissions\nMLT\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n9\n10\n11\nEmissions\nNLD\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n7\n8\n9\nEmissions\nNOR\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n7.5\n8.0\n8.5\n9.0\nEmissions\nPOL\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5\n6\nEmissions\nPRT\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n6\nEmissions\nROU\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n8\n10\nEmissions\nSVK\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n7\n8\nEmissions\nSVN\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5\n6\n7\n8\nEmissions\nESP\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n4\n6\nEmissions\nSWE\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n5\n6\nEmissions\nCHE\n1990\n1994\n1998\n2002\n2006\n2010\n2014\n2018\nTime (Year)\n6\n8\n10\nEmissions\nGBR\nFigure 18: 31 countries\u2019 CO2 emissions (metric tons per capita) during the period 1990\u20132019.\n43\n\nA.3\nFutures data set details\nTables [17, 18, 19, 20, 21, 22, 23, 24] show the futures contracts we used and its description from the\nPinnacle Data Corp CLC Database. The data is based on ratio-adjusted methods, which removes\nthe contract-to-contract gap, yet it will not go negative as it reduces the size of the price bars if\nthey go lower and increases them if they go higher.\nTable 17: Grains\nIdentifier\nDescription\nKW\nWHEAT, KC\nMW\nWHEAT, MINN\nNR\nROUGH RICE\nW\nWHEAT, CBOT\nZC\nCORN, Electronic\nZL\nSOYBEAN OIL, Electronic\nZM\nSOYBEAN MEAL, Electronic\nZO\nOATS, Electronic\nZR\nROUGH RICE, Electronic\nZS\nSOYBEANS, Electronic\nZW\nWHEAT, Electronic\nTable 18: Meats\nIdentifier\nDescription\nDA\nMILK III, Comp.",
    "chunk_index": 44,
    "start_char": 96539,
    "end_char": 99152,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  },
  {
    "text": "which removes\nthe contract-to-contract gap, yet it will not go negative as it reduces the size of the price bars if\nthey go lower and increases them if they go higher.\nTable 17: Grains\nIdentifier\nDescription\nKW\nWHEAT, KC\nMW\nWHEAT, MINN\nNR\nROUGH RICE\nW\nWHEAT, CBOT\nZC\nCORN, Electronic\nZL\nSOYBEAN OIL, Electronic\nZM\nSOYBEAN MEAL, Electronic\nZO\nOATS, Electronic\nZR\nROUGH RICE, Electronic\nZS\nSOYBEANS, Electronic\nZW\nWHEAT, Electronic\nTable 18: Meats\nIdentifier\nDescription\nDA\nMILK III, Comp.\nZF\nFEEDER CATTLE, Electronic\nZT\nLIVE CATTLE, Electronic\nZZ\nLEAN HOGS, Electronic\nTable 19: Wood fibre\nIdentifier\nDescription\nLB\nLUMBER\nTable 20: Metals\nIdentifier\nDescription\nZG\nGOLD, Electronic\nZI\nSILVER, Electronic\nZP\nPLATINUM, electronic\nZA\nPALLADIUM, electronic\nZK\nCOPPER, electronic\n44\n\nTable 21: Indexes\nIdentifier\nDescription\nAX\nGERMAN DAX INDEX\nCA\nCAC40 INDEX\nDX\nUS DOLLAR INDEX\nEN\nNASDAQ, MINI\nES\nS & P 500, MINI\nGI\nGOLDMAN SAKS C. I.\nLX\nFTSE 100 INDEX\nMD\nS & P 400 (Mini electronic)\nNK\nNIKKEI INDEX\nSC\nS & P 500, composite\nTable 22: Bonds\nIdentifier\nDescription\nDT\nEURO BOND (BUND)\nFB\nT-NOTE, 5yr composite\nGS\nGILT, LONG BOND\nSS\nSTERLING, SHORT\nTY\nT-NOTE, 10yr composite\nTU\nT-NOTES, 2yr composite\nUS\nT-BONDS, composite\nUB\nEURO BOBL\nUZ\nEURO SCHATZ\nTable 23: Currency\nIdentifier\nDescription\nAN\nAUSTRALIAN $$ composite\nBN\nBRITISH POUND, composite\nCN\nCANADIAN $$ composite\nEC\nEURODOLLAR, composite\nFN\nEURO, composite\nJN\nJAPANESE YEN, composite\nMP\nMEXICAN PESO\nSN\nSWISS FRANC, composite\nTable 24: Oils\nIdentifier\nDescription\nZB\nRBOB, Electronic\nZH\nHEATING OIL, electronic\nZN\nNATURAL GAS, electronic\nZU\nCRUDE OIL, Electronic\n45",
    "chunk_index": 45,
    "start_char": 98665,
    "end_char": 100284,
    "paper_title": "Robust Detection of Lead-Lag Relationships in Lagg",
    "paper_category": "stat.ML",
    "paper_filename": "Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf",
    "paper_filepath": "/Users/prithirajbhuyan/Documents/Spring 2026/AI Model Dev/Alpha Research Portal/app/../quant_library/stat.ML/Robust_Detection_of_Lead-Lag_Relationships_in_Lagg.pdf"
  }
]